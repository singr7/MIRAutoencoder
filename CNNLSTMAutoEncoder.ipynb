{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNLSTMAutoEncoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN9f0LFMAgY3yLdN2d95yel",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singr7/MIRAutoencoder/blob/master/CNNLSTMAutoEncoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9JqXiXhqdkN"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORCKRmYPHk-z"
      },
      "source": [
        "#Mount the google drive\n",
        "#Create list of numpy files for western and indian dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPnD6kkyHfx8",
        "outputId": "08841fbe-8d29-4df6-8655-ecfc464c8e68"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "western_files = []\n",
        "western_file_dir = \"/content/drive/My Drive/MusicResearchColabNB/Western_numpy\"\n",
        "for r,d, fileList in os.walk(western_file_dir):\n",
        "  for file in fileList:\n",
        "    western_files.append(os.path.join(r,file))\n",
        "\n",
        "indian_files = []\n",
        "indian_file_dir = \"/content/drive/My Drive/MusicResearchColabNB/IndianDataset/Indian_numpy\"\n",
        "for r,d, fileList in os.walk(indian_file_dir):\n",
        "  for file in fileList:\n",
        "    indian_files.append(os.path.join(r,file))\n",
        "\n",
        "print(len(western_files))\n",
        "print(len(indian_files))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "7592\n",
            "2008\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMQbF-ylLmdK"
      },
      "source": [
        "# Balance the western dataset by taking files equal to Indian dataset files = 2008"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXHKJAOLL-iX",
        "outputId": "6cbb1b5d-a52f-4fd3-b88e-8ba3ae707353"
      },
      "source": [
        "import random \n",
        "#randomize the selection. To avoid getting a different random sample with every run, use seed\n",
        "random.seed(23)\n",
        "bal_western_files = random.sample(western_files,2008)\n",
        "len(bal_western_files)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2008"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeBwpwaTX3Jo"
      },
      "source": [
        "#Define configuration class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP4323imT53f"
      },
      "source": [
        "class Configuration:\n",
        "  seq_len = 200  # taking half of the original timesteps extracted \n",
        "  input_dim = 26  #num of mels\n",
        "  embedding_dim = 64\n",
        "  batch_size = 2\n",
        "  base_dir = \"/content/drive/My Drive/MusicResearchColabNB/vajra/westernAE\"   # need to be edited..\n",
        "  loss_function = torch.nn.MSELoss(reduction='sum')\n",
        "  lr=1e-3  # I edited it from 1e-3 to 1e-5\n",
        "  n_epochs = 5\n",
        "  model_file = \"/content/drive/My Drive/MusicResearchColabNB/vajra/westernAE/models/mel.pkl\"  #need need edits\n",
        "  results_dir = os.path.join(base_dir, \"./results\")  # may need edits\n",
        "  checkpoint_model_file = \"/content/drive/My Drive/MusicResearchColabNB/vajra/westernAE/models/mel_checkpoint.pkl\" #may need edits\n",
        "  kernel_size = 3  #why?\n",
        "  k_folds = 10 "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RALRBXgZZBA"
      },
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "  def __init__(self, seq_len, n_features, embedding_dim=64, kernel_size=3, stride=1):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.seq_len, self.n_features = seq_len, n_features\n",
        "    self.embedding_dim, self.hidden_dim = embedding_dim, 2 * embedding_dim\n",
        "\n",
        "\n",
        "    self.conv = nn.Conv1d(in_channels=seq_len,out_channels=seq_len,kernel_size=kernel_size,stride=stride, groups=seq_len)\n",
        "    conv_op_dim = int(((n_features - kernel_size)/ stride) + 1)\n",
        "\n",
        "    self.rnn1 = nn.LSTM(\n",
        "      input_size=conv_op_dim,\n",
        "      hidden_size=self.hidden_dim,\n",
        "      num_layers=1,\n",
        "      batch_first=True\n",
        "    )\n",
        "    self.rnn2 = nn.LSTM(\n",
        "      input_size=self.hidden_dim,\n",
        "      hidden_size=embedding_dim,\n",
        "      num_layers=1,\n",
        "      batch_first=True\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    #x = x.reshape((10, self.seq_len, self.n_features))\n",
        "   # print('In Encoder')\n",
        "   # print(x.shape)\n",
        "    x = self.conv(x)\n",
        "    x, (_, _) = self.rnn1(x)\n",
        "    x, (hidden_n, _) = self.rnn2(x)\n",
        "    return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkW-A8TzZdGT"
      },
      "source": [
        "class Decoder(torch.nn.Module):\n",
        "  def __init__(self, seq_len, embedding_dim=64, n_features=26):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.seq_len, self.embedding_dim = seq_len, embedding_dim\n",
        "    self.hidden_dim, self.n_features = 2 * embedding_dim, n_features\n",
        "    self.rnn1 = nn.LSTM(\n",
        "      input_size=embedding_dim,\n",
        "      hidden_size=embedding_dim,\n",
        "      num_layers=1,\n",
        "      batch_first=True\n",
        "    )\n",
        "    self.rnn2 = nn.LSTM(\n",
        "      input_size=embedding_dim,\n",
        "      hidden_size=self.hidden_dim,\n",
        "      num_layers=1,\n",
        "      batch_first=True\n",
        "    )\n",
        "    self.output_layer = nn.Linear(self.hidden_dim * self.seq_len, n_features * self.seq_len)\n",
        "  def forward(self, x):\n",
        "    x, (hidden_n, cell_n) = self.rnn1(x)\n",
        "    x, (hidden_n, cell_n) = self.rnn2(x)\n",
        "    #print(\"in decoder\", x.shape)\n",
        "    x = x.contiguous()\n",
        "    x = x.view(x.shape[0], -1)\n",
        "    x = self.output_layer(x)\n",
        "    return x.reshape(x.shape[0],self.seq_len, self.n_features)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mqrvU5MZfEA"
      },
      "source": [
        "class RecurrentAutoencoder(torch.nn.Module):\n",
        "  def __init__(self, seq_len, n_features, embedding_dim=64, device='cpu'):\n",
        "    super(RecurrentAutoencoder, self).__init__()\n",
        "    self.encoder = Encoder(seq_len, n_features, embedding_dim).to(device)\n",
        "    self.decoder = Decoder(seq_len, embedding_dim, n_features).to(device)\n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)\n",
        "    x = self.decoder(x)\n",
        "    return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRQ-t9aNZiUD",
        "outputId": "e522ad94-8a34-463c-e2b1-2826f947cd38"
      },
      "source": [
        "x = torch.randn(10, 26, 400)\n",
        "print(x.shape)\n",
        "x = x.permute(0, 2, 1)\n",
        "print(x.shape)\n",
        "\n",
        "encoder = Encoder(400, 26, embedding_dim=64, kernel_size=3, stride=1)\n",
        "encoded = encoder(x)\n",
        "print(encoded.shape)\n",
        "\n",
        "decoder = Decoder(400, 64, 26)\n",
        "decoded = decoder(encoded)\n",
        "print(decoded.shape)\n",
        "\n",
        "rae = RecurrentAutoencoder(400, 26, 64)\n",
        "output = rae(x)\n",
        "\n",
        "print(output.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 26, 400])\n",
            "torch.Size([10, 400, 26])\n",
            "torch.Size([10, 400, 64])\n",
            "torch.Size([10, 400, 26])\n",
            "torch.Size([10, 400, 26])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDpec6ICZnKN"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data.sampler import SequentialSampler\n",
        "\n",
        "class CustomDatasetMel(Dataset):\n",
        "\n",
        "    def __init__(self, dataList):\n",
        "        self.data = dataList\n",
        "        #self.labels = labelList\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        import numpy as np\n",
        "        fileName = self.data[index]\n",
        "        \n",
        "        mel_spect = np.load(fileName)\n",
        "        data = torch.tensor(mel_spect[:,:200], dtype=torch.float)\n",
        "        data = data.permute(1, 0)\n",
        "        #data = torch.unsqueeze(data, dim =0)\n",
        "\n",
        "        #label = torch.tensor(self.labels[index])\n",
        "        return data"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAv1gLc6Zwwr",
        "outputId": "7483453e-5ce8-460b-f864-be18dc97eaa4"
      },
      "source": [
        "import copy\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "class TrainingWrapper:\n",
        "\n",
        "  def __init__(self, config, training_loader, test_loader, device, val_loader=None, cross=10):\n",
        "    self.config = config\n",
        "    self.training_loader = training_loader\n",
        "    self.test_loader = test_loader\n",
        "    self.val_loader = val_loader\n",
        "    self.device = device\n",
        "    self.model = RecurrentAutoencoder(self.config.seq_len, self.config.input_dim, self.config.embedding_dim, device=self.device)\n",
        "    self.model = self.model.to(self.device)\n",
        "    self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.lr)\n",
        "    self.criterion = self.config.loss_function.to(self.device)\n",
        "    self.history = dict(train=[], val=[], cross_val=[])\n",
        "    self.best_model_wts = copy.deepcopy(self.model.state_dict())\n",
        "    self.best_loss = 10000.0\n",
        "    #print(self.config.base_dir + self.config.model_file)\n",
        "    torch.save(self.model.state_dict(),  self.config.model_file)\n",
        "    self.cross = cross\n",
        "    \n",
        "\n",
        "  def combine_images(self, generated_images):\n",
        "    num = generated_images.shape[0]\n",
        "    width = int(math.sqrt(num))\n",
        "    height = int(math.ceil(float(num)/width))\n",
        "    shape = generated_images.shape[1:3]\n",
        "    image = np.zeros((height*shape[0], width*shape[1]),\n",
        "                     dtype=generated_images.dtype)\n",
        "    for index, img in enumerate(generated_images):\n",
        "        i = int(index/width)\n",
        "        j = index % width\n",
        "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
        "            img[:, :, 0]\n",
        "    return image\n",
        "\n",
        "\n",
        "  def show_reconstruction(self, test_loader, n_images):\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    from PIL import Image\n",
        "    import numpy as np\n",
        "\n",
        "    self.model.eval()\n",
        "    for x, _ in self.test_loader:\n",
        "        x = x[:min(n_images, x.size(0))].to(self,device)\n",
        "        _, x_recon = self.model(x)\n",
        "        data = np.concatenate([x.data.cpu(), x_recon.data.cpu()])\n",
        "        img = self.combine_images(np.transpose(data, [0, 2, 3, 1]))\n",
        "        image = img * 255\n",
        "        Image.fromarray(image.astype(np.uint8)).save(self.config.base_dir + \"/real_and_recon.png\")\n",
        "        print()\n",
        "        print('Reconstructed images are saved to %s/real_and_recon.png' % self.config.base_dir)\n",
        "        print('-' * 70)\n",
        "        plt.imshow(plt.imread(self.config.base_dir + \"/real_and_recon.png\", ))\n",
        "        plt.show()\n",
        "        break\n",
        "\n",
        "  def visualizeTraining(self, epoch, trn_losses, tst_losses, val_losses, save_dir,cross):\n",
        "    # visualize the loss as the network trained\n",
        "    import matplotlib.pyplot as plt\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    plt.plot(range(0, len(trn_losses)), trn_losses, label='Training Loss')\n",
        "    if tst_losses:\n",
        "      plt.plot(range(0, len(tst_losses)), tst_losses, label='Validation Loss')\n",
        "    if val_losses:\n",
        "      plt.plot(range(0, len(val_losses)), val_losses, label='Cross Validation Loss')\n",
        "\n",
        "    minposs = tst_losses.index(min(tst_losses))\n",
        "    plt.axvline(minposs, linestyle='--', color='r', label='Early Stopping Checkpoint')\n",
        "\n",
        "\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('loss')\n",
        "    # plt.ylim(0, 0.5)  # consistent scale\n",
        "    # plt.xlim(0, len(trn_losses))  # consistent scale\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    fig.savefig(os.path.join(save_dir , 'loss_plot_{}.png'.format(cross)), bbox_inches='tight')\n",
        "\n",
        "  def train(self):\n",
        "    for epoch in range(1, self.config.n_epochs + 1):\n",
        "      self.model = self.model.train()\n",
        "      train_losses = []\n",
        "      for i, data in enumerate(self.training_loader,0):\n",
        "        x = data\n",
        "        self.optimizer.zero_grad()\n",
        "        x = x.to(self.device)\n",
        "\n",
        "        output = self.model(x)\n",
        "        loss = self.criterion(output, x)\n",
        "\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        train_losses.append(loss.item())\n",
        "        print(\"in training loop, epoch {}, step {}, the loss is {}\".format(epoch, i, loss.item()))\n",
        "\n",
        "      val_losses = []\n",
        "      self.model = self.model.eval()\n",
        "      with torch.no_grad():\n",
        "        for i, data in enumerate(self.test_loader):\n",
        "          x = data\n",
        "          x = x.to(device)\n",
        "          output = self.model(x)\n",
        "          loss = self.criterion(output, x)\n",
        "          val_losses.append(loss.item())\n",
        "\n",
        "\n",
        "      cross_val_losses = []\n",
        "      self.model = self.model.eval()\n",
        "      with torch.no_grad():\n",
        "        for i, data in enumerate(self.val_loader):\n",
        "          x = data\n",
        "          x = x.to(device)\n",
        "          output = self.model(x)\n",
        "          loss = self.criterion(output, x)\n",
        "          cross_val_losses.append(loss.item())\n",
        "\n",
        "      train_loss = np.mean(train_losses)\n",
        "      val_loss = np.mean(val_losses)\n",
        "      cross_val_loss = np.mean(cross_val_losses)\n",
        "\n",
        "\n",
        "      self.history['train'].append(train_loss)\n",
        "      self.history['val'].append(val_loss)\n",
        "      self.history['cross_val'].append(cross_val_loss)\n",
        "\n",
        "      if val_loss < self.best_loss:\n",
        "        self.best_loss = val_loss\n",
        "        self.best_model_wts = copy.deepcopy(self.model.state_dict())\n",
        "      if epoch % 5 == 0:\n",
        "        self.visualizeTraining(epoch, trn_losses= self.history['train'], tst_losses=self.history['val'], val_losses =self.history['cross_val'], save_dir=self.config.base_dir + \"/results\",cross=fold)\n",
        "        torch.save(self.model.state_dict(),  self.config.checkpoint_model_file + \"_\" + str(self.cross))\n",
        "      print(f'k-fold {fold}:: Epoch {epoch}: train loss {train_loss} val loss {val_loss}')\n",
        "    self.model.load_state_dict(self.best_model_wts)\n",
        "    torch.save(self.model.state_dict(), self.config.model_file)\n",
        "    return self.model.eval(), self.history\n",
        "\n",
        "  \n",
        "class TestingWrapper:\n",
        "  def __init__(self, config, device):\n",
        "    self.config = config\n",
        "    self.device = device\n",
        "    self.model = RecurrentAutoencoder(self.config.seq_len, self.config.input_dim, self.config.embedding_dim, device=self.device)\n",
        "    PATH =  self.config.checkpoint_model_file\n",
        "    print(PATH)\n",
        "    self.model.load_state_dict(torch.load(PATH, map_location=self.device))\n",
        "    self.model = self.model.to(self.device)\n",
        "\n",
        "  def combine_images(self, generated_images):\n",
        "    num = generated_images.shape[0]\n",
        "    width = int(math.sqrt(num))\n",
        "    height = int(math.ceil(float(num)/width))\n",
        "    shape = generated_images.shape[1:3]\n",
        "    image = np.zeros((height*shape[0], width*shape[1]),\n",
        "                     dtype=generated_images.dtype)\n",
        "    for index, img in enumerate(generated_images):\n",
        "        i = int(index/width)\n",
        "        j = index % width\n",
        "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
        "            img[:, :, 0]\n",
        "    return image\n",
        "\n",
        "\n",
        "  def show_reconstruction(self, test_loader, n_images):\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    from PIL import Image\n",
        "    import numpy as np\n",
        "\n",
        "    self.model.eval()\n",
        "    for x, _ in test_loader:\n",
        "      x = x[:min(n_images, x.size(0))].to(self.device)\n",
        "      x_recon = self.model(x)\n",
        "      data = np.concatenate([x.data.cpu(), x_recon.data.cpu()])\n",
        "      img = self.combine_images(np.transpose(data, [0, 2, 3, 1]))\n",
        "      image = img * 255\n",
        "      Image.fromarray(image.astype(np.uint8)).save(self.config.base_dir + \"/real_and_recon.png\")\n",
        "      print()\n",
        "      print('Reconstructed images are saved to %s/real_and_recon.png' % self.config.base_dir)\n",
        "      print('-' * 70)\n",
        "      plt.imshow(plt.imread(self.config.base_dir + \"/real_and_recon.png\", ))\n",
        "      plt.show()\n",
        "      break\n",
        "\n",
        "  def save_reconstruction(self, test_loader):\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    from PIL import Image\n",
        "    import numpy as np\n",
        "    import uuid\n",
        "\n",
        "\n",
        "    self.model.eval()\n",
        "    with torch.no_grad(): \n",
        "      fileCount = 0\n",
        "      for x, _ in test_loader:\n",
        "        x = x.to(self.device)\n",
        "        x_recon = self.model(x)\n",
        "        x_recon = x_recon.data.cpu().detach().numpy()\n",
        "        for mel in x_recon:\n",
        "          #print(mel.shape)\n",
        "          unique_filename = str(uuid.uuid4())\n",
        "          filename = self.config.base_dir + \"/reconstruction/\" + unique_filename + \".npy\"\n",
        "          np.save(filename, mel)\n",
        "          fileCount = fileCount + 1\n",
        "          print(\"saving file {} at index {}\".format(filename, fileCount))\n",
        "\n",
        "mode = 'train'\n",
        "data = \"mel\"\n",
        "#data = \"mnist\"\n",
        "config = Configuration()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import random\n",
        "import os\n",
        "\n",
        "def seed_everything(seed=1234):\n",
        "  \n",
        "  random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  np.random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def visualizeTraining(epoch, trn_losses, tst_losses, val_losses, save_dir):\n",
        "    # visualize the loss as the network trained\n",
        "    import matplotlib.pyplot as plt\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    plt.plot(range(0, len(trn_losses)), trn_losses, label='Training Loss')\n",
        "    #if tst_losses:\n",
        "    plt.plot(range(0, len(tst_losses)), tst_losses, label='Validation Loss')\n",
        "    #if val_losses:\n",
        "    plt.plot(range(0, len(val_losses)), val_losses, label='Cross Validation Loss')\n",
        "\n",
        "    #minposs = tst_losses.index(min(tst_losses))\n",
        "    #plt.axvline(minposs, linestyle='--', color='r', label='Early Stopping Checkpoint')\n",
        "\n",
        "\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('loss')\n",
        "    # plt.ylim(0, 0.5)  # consistent scale\n",
        "    # plt.xlim(0, len(trn_losses))  # consistent scale\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    fig.savefig(os.path.join(save_dir , 'loss_plot_{}.png'.format(\"MEAN\")), bbox_inches='tight')\n",
        "\n",
        "seed_everything()\n",
        "\n",
        "train_data = bal_western_files\n",
        "#labels = [1] * len(bal_western_files)\n",
        "\n",
        "val_data = indian_files\n",
        "#val_labels = [1] * len(indian_files)\n",
        "\n",
        "train_loss_mean_list = []\n",
        "test_loss_mean_list = []\n",
        "val_loss_mean_list = []\n",
        "\n",
        "# Cross validation runs\n",
        "# use sklearn KFolds\n",
        "from sklearn.model_selection import KFold\n",
        "kfold = KFold(n_splits=config.k_folds , shuffle=True)\n",
        "\n",
        "train_dataset = CustomDatasetMel(train_data)\n",
        "val_dataset = CustomDatasetMel(val_data)\n",
        "#Load the cross val dataset which is Full Indian dataset\n",
        "#It is identical for all K-folds\n",
        "crossval_loader = torch.utils.data.DataLoader(\n",
        "                      val_dataset,\n",
        "                      batch_size=config.batch_size, \n",
        "                      sampler=SequentialSampler(val_dataset), \n",
        "                      drop_last=False)  \n",
        "\n",
        "for fold, (train_ids, test_ids) in enumerate(kfold.split(train_dataset)):\n",
        "    # Print\n",
        "  print(f'FOLD {fold}')\n",
        "  print('--------------------------------')\n",
        "    \n",
        "    # Sample elements randomly from a given list of ids, no replacement.\n",
        "  train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "  test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
        "  \n",
        "    # Define data loaders for training and testing data in this fold\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "                      train_dataset, \n",
        "                      batch_size=config.batch_size,\n",
        "                      sampler=train_subsampler,\n",
        "                      drop_last=False)\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "                      train_dataset,\n",
        "                      batch_size=config.batch_size,\n",
        "                      sampler=test_subsampler,\n",
        "                      drop_last=False)\n",
        "    \n",
        "  print(\"length of of train_loader is {} & length of traindataset is {}\".format(len(train_loader),len(train_dataset)))\n",
        "  print(\"length of of test_loader is {}\".format(len(test_loader)))\n",
        "  print(\"length of of val_loader is {}\".format(len(crossval_loader)))\n",
        "    \n",
        "  if mode==\"train\":\n",
        "    trainingWrapper = TrainingWrapper(config=config, training_loader=train_loader, test_loader=test_loader, device=device, val_loader=crossval_loader, cross=fold)\n",
        "    model, history = trainingWrapper.train()\n",
        "    train_loss_mean_list.append(history['train'])\n",
        "    test_loss_mean_list.append(history['val'])\n",
        "    val_loss_mean_list.append(history['cross_val'])\n",
        "    \n",
        "\n",
        "    if data==\"mnist\":\n",
        "      #trainingWrapper.show_reconstruction(test_loader=test_loader, n_images=50)\n",
        "      pass\n",
        "\n",
        "  elif mode==\"test\":\n",
        "    testWrapper = TestingWrapper(config=config, device=device)\n",
        "    testWrapper.save_reconstruction(test_loader)\n",
        "  \n",
        "  train_loss_mean_list = np.mean(train_loss_mean_list, axis=0)\n",
        "  test_loss_mean_list = np.mean(test_loss_mean_list, axis=0)\n",
        "  val_loss_mean_list = np.mean(val_loss_mean_list, axis=0)\n",
        "  print(\"train_loss_mean_list\",train_loss_mean_list)\n",
        "  print(\"test_loss_mean_list\",test_loss_mean_list)\n",
        "  print(\"val_loss_mean_list\",val_loss_mean_list)\n",
        "  visualizeTraining(0, train_loss_mean_list, test_loss_mean_list, val_loss_mean_list, save_dir = config.base_dir + \"/results\")\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOLD 0\n",
            "--------------------------------\n",
            "length of of train_loader is 225 & length of traindataset is 500\n",
            "length of of test_loader is 25\n",
            "length of of val_loader is 250\n",
            "in training loop, epoch 1, step 0, the loss is 11342971.0\n",
            "in training loop, epoch 1, step 1, the loss is 14443143.0\n",
            "in training loop, epoch 1, step 2, the loss is 10070807.0\n",
            "in training loop, epoch 1, step 3, the loss is 13510089.0\n",
            "in training loop, epoch 1, step 4, the loss is 7301430.5\n",
            "in training loop, epoch 1, step 5, the loss is 9700632.0\n",
            "in training loop, epoch 1, step 6, the loss is 7095275.5\n",
            "in training loop, epoch 1, step 7, the loss is 1198279.25\n",
            "in training loop, epoch 1, step 8, the loss is 2542923.0\n",
            "in training loop, epoch 1, step 9, the loss is 5698427.0\n",
            "in training loop, epoch 1, step 10, the loss is 3012565.75\n",
            "in training loop, epoch 1, step 11, the loss is 688139.375\n",
            "in training loop, epoch 1, step 12, the loss is 972426.875\n",
            "in training loop, epoch 1, step 13, the loss is 2011847.375\n",
            "in training loop, epoch 1, step 14, the loss is 4475319.5\n",
            "in training loop, epoch 1, step 15, the loss is 3976283.25\n",
            "in training loop, epoch 1, step 16, the loss is 13240490.0\n",
            "in training loop, epoch 1, step 17, the loss is 594131.875\n",
            "in training loop, epoch 1, step 18, the loss is 3491036.0\n",
            "in training loop, epoch 1, step 19, the loss is 2900247.5\n",
            "in training loop, epoch 1, step 20, the loss is 1350443.75\n",
            "in training loop, epoch 1, step 21, the loss is 2094980.75\n",
            "in training loop, epoch 1, step 22, the loss is 3435587.75\n",
            "in training loop, epoch 1, step 23, the loss is 2846273.5\n",
            "in training loop, epoch 1, step 24, the loss is 4263426.5\n",
            "in training loop, epoch 1, step 25, the loss is 1562378.375\n",
            "in training loop, epoch 1, step 26, the loss is 3048827.75\n",
            "in training loop, epoch 1, step 27, the loss is 806390.0\n",
            "in training loop, epoch 1, step 28, the loss is 1032023.875\n",
            "in training loop, epoch 1, step 29, the loss is 456888.375\n",
            "in training loop, epoch 1, step 30, the loss is 1847786.0\n",
            "in training loop, epoch 1, step 31, the loss is 1148575.25\n",
            "in training loop, epoch 1, step 32, the loss is 4320390.5\n",
            "in training loop, epoch 1, step 33, the loss is 6025390.5\n",
            "in training loop, epoch 1, step 34, the loss is 1712834.25\n",
            "in training loop, epoch 1, step 35, the loss is 905322.0\n",
            "in training loop, epoch 1, step 36, the loss is 3640470.5\n",
            "in training loop, epoch 1, step 37, the loss is 775686.25\n",
            "in training loop, epoch 1, step 38, the loss is 1968879.875\n",
            "in training loop, epoch 1, step 39, the loss is 1048311.8125\n",
            "in training loop, epoch 1, step 40, the loss is 1630028.5\n",
            "in training loop, epoch 1, step 41, the loss is 1702195.25\n",
            "in training loop, epoch 1, step 42, the loss is 819943.4375\n",
            "in training loop, epoch 1, step 43, the loss is 1678116.875\n",
            "in training loop, epoch 1, step 44, the loss is 1744731.125\n",
            "in training loop, epoch 1, step 45, the loss is 1803324.625\n",
            "in training loop, epoch 1, step 46, the loss is 4557418.0\n",
            "in training loop, epoch 1, step 47, the loss is 1035709.0\n",
            "in training loop, epoch 1, step 48, the loss is 1948671.5\n",
            "in training loop, epoch 1, step 49, the loss is 1918787.875\n",
            "in training loop, epoch 1, step 50, the loss is 1090215.875\n",
            "in training loop, epoch 1, step 51, the loss is 593569.4375\n",
            "in training loop, epoch 1, step 52, the loss is 372179.53125\n",
            "in training loop, epoch 1, step 53, the loss is 1332689.5\n",
            "in training loop, epoch 1, step 54, the loss is 688774.75\n",
            "in training loop, epoch 1, step 55, the loss is 2770274.5\n",
            "in training loop, epoch 1, step 56, the loss is 723089.9375\n",
            "in training loop, epoch 1, step 57, the loss is 3368230.0\n",
            "in training loop, epoch 1, step 58, the loss is 1239052.375\n",
            "in training loop, epoch 1, step 59, the loss is 1956197.5\n",
            "in training loop, epoch 1, step 60, the loss is 702746.875\n",
            "in training loop, epoch 1, step 61, the loss is 1547363.0\n",
            "in training loop, epoch 1, step 62, the loss is 2156450.0\n",
            "in training loop, epoch 1, step 63, the loss is 1564617.375\n",
            "in training loop, epoch 1, step 64, the loss is 1408060.0\n",
            "in training loop, epoch 1, step 65, the loss is 918521.6875\n",
            "in training loop, epoch 1, step 66, the loss is 976976.0\n",
            "in training loop, epoch 1, step 67, the loss is 1836032.5\n",
            "in training loop, epoch 1, step 68, the loss is 759221.375\n",
            "in training loop, epoch 1, step 69, the loss is 6135411.5\n",
            "in training loop, epoch 1, step 70, the loss is 1430403.0\n",
            "in training loop, epoch 1, step 71, the loss is 646641.625\n",
            "in training loop, epoch 1, step 72, the loss is 1409032.375\n",
            "in training loop, epoch 1, step 73, the loss is 2241175.0\n",
            "in training loop, epoch 1, step 74, the loss is 1846400.375\n",
            "in training loop, epoch 1, step 75, the loss is 1062319.625\n",
            "in training loop, epoch 1, step 76, the loss is 1209386.625\n",
            "in training loop, epoch 1, step 77, the loss is 3050839.75\n",
            "in training loop, epoch 1, step 78, the loss is 3131325.75\n",
            "in training loop, epoch 1, step 79, the loss is 803539.5\n",
            "in training loop, epoch 1, step 80, the loss is 2223191.75\n",
            "in training loop, epoch 1, step 81, the loss is 838929.375\n",
            "in training loop, epoch 1, step 82, the loss is 1102465.5\n",
            "in training loop, epoch 1, step 83, the loss is 8912506.0\n",
            "in training loop, epoch 1, step 84, the loss is 1318692.0\n",
            "in training loop, epoch 1, step 85, the loss is 1287399.375\n",
            "in training loop, epoch 1, step 86, the loss is 938749.9375\n",
            "in training loop, epoch 1, step 87, the loss is 1632862.125\n",
            "in training loop, epoch 1, step 88, the loss is 2214188.75\n",
            "in training loop, epoch 1, step 89, the loss is 1351279.5\n",
            "in training loop, epoch 1, step 90, the loss is 3350710.75\n",
            "in training loop, epoch 1, step 91, the loss is 1331757.875\n",
            "in training loop, epoch 1, step 92, the loss is 807818.875\n",
            "in training loop, epoch 1, step 93, the loss is 718378.375\n",
            "in training loop, epoch 1, step 94, the loss is 893827.125\n",
            "in training loop, epoch 1, step 95, the loss is 789859.3125\n",
            "in training loop, epoch 1, step 96, the loss is 818170.125\n",
            "in training loop, epoch 1, step 97, the loss is 842972.0\n",
            "in training loop, epoch 1, step 98, the loss is 1675587.875\n",
            "in training loop, epoch 1, step 99, the loss is 1575063.625\n",
            "in training loop, epoch 1, step 100, the loss is 1432310.875\n",
            "in training loop, epoch 1, step 101, the loss is 3342492.75\n",
            "in training loop, epoch 1, step 102, the loss is 2877960.5\n",
            "in training loop, epoch 1, step 103, the loss is 1936079.5\n",
            "in training loop, epoch 1, step 104, the loss is 1874489.25\n",
            "in training loop, epoch 1, step 105, the loss is 3507752.0\n",
            "in training loop, epoch 1, step 106, the loss is 2231669.75\n",
            "in training loop, epoch 1, step 107, the loss is 2089812.375\n",
            "in training loop, epoch 1, step 108, the loss is 1267703.5\n",
            "in training loop, epoch 1, step 109, the loss is 494621.75\n",
            "in training loop, epoch 1, step 110, the loss is 1916462.375\n",
            "in training loop, epoch 1, step 111, the loss is 445680.375\n",
            "in training loop, epoch 1, step 112, the loss is 910284.5625\n",
            "in training loop, epoch 1, step 113, the loss is 1163591.25\n",
            "in training loop, epoch 1, step 114, the loss is 1107598.75\n",
            "in training loop, epoch 1, step 115, the loss is 1278332.0\n",
            "in training loop, epoch 1, step 116, the loss is 2567720.5\n",
            "in training loop, epoch 1, step 117, the loss is 761693.25\n",
            "in training loop, epoch 1, step 118, the loss is 872643.25\n",
            "in training loop, epoch 1, step 119, the loss is 1143011.875\n",
            "in training loop, epoch 1, step 120, the loss is 954546.125\n",
            "in training loop, epoch 1, step 121, the loss is 1191282.25\n",
            "in training loop, epoch 1, step 122, the loss is 816848.625\n",
            "in training loop, epoch 1, step 123, the loss is 1753758.0\n",
            "in training loop, epoch 1, step 124, the loss is 1777489.5\n",
            "in training loop, epoch 1, step 125, the loss is 695425.8125\n",
            "in training loop, epoch 1, step 126, the loss is 1237699.625\n",
            "in training loop, epoch 1, step 127, the loss is 1414551.0\n",
            "in training loop, epoch 1, step 128, the loss is 883606.625\n",
            "in training loop, epoch 1, step 129, the loss is 2608588.25\n",
            "in training loop, epoch 1, step 130, the loss is 557431.5\n",
            "in training loop, epoch 1, step 131, the loss is 439902.375\n",
            "in training loop, epoch 1, step 132, the loss is 1281711.75\n",
            "in training loop, epoch 1, step 133, the loss is 615759.625\n",
            "in training loop, epoch 1, step 134, the loss is 2912212.0\n",
            "in training loop, epoch 1, step 135, the loss is 1167843.375\n",
            "in training loop, epoch 1, step 136, the loss is 620510.4375\n",
            "in training loop, epoch 1, step 137, the loss is 1197137.5\n",
            "in training loop, epoch 1, step 138, the loss is 992682.4375\n",
            "in training loop, epoch 1, step 139, the loss is 1033140.125\n",
            "in training loop, epoch 1, step 140, the loss is 1214560.625\n",
            "in training loop, epoch 1, step 141, the loss is 1037180.9375\n",
            "in training loop, epoch 1, step 142, the loss is 653702.5\n",
            "in training loop, epoch 1, step 143, the loss is 925768.0\n",
            "in training loop, epoch 1, step 144, the loss is 1465809.0\n",
            "in training loop, epoch 1, step 145, the loss is 298564.15625\n",
            "in training loop, epoch 1, step 146, the loss is 1315043.25\n",
            "in training loop, epoch 1, step 147, the loss is 764188.5625\n",
            "in training loop, epoch 1, step 148, the loss is 761416.25\n",
            "in training loop, epoch 1, step 149, the loss is 836832.625\n",
            "in training loop, epoch 1, step 150, the loss is 1096772.625\n",
            "in training loop, epoch 1, step 151, the loss is 1162914.25\n",
            "in training loop, epoch 1, step 152, the loss is 518491.53125\n",
            "in training loop, epoch 1, step 153, the loss is 636774.0\n",
            "in training loop, epoch 1, step 154, the loss is 1108718.25\n",
            "in training loop, epoch 1, step 155, the loss is 1046542.25\n",
            "in training loop, epoch 1, step 156, the loss is 1010471.5625\n",
            "in training loop, epoch 1, step 157, the loss is 2083870.25\n",
            "in training loop, epoch 1, step 158, the loss is 1110892.75\n",
            "in training loop, epoch 1, step 159, the loss is 1529459.125\n",
            "in training loop, epoch 1, step 160, the loss is 1528327.5\n",
            "in training loop, epoch 1, step 161, the loss is 1108896.75\n",
            "in training loop, epoch 1, step 162, the loss is 364472.3125\n",
            "in training loop, epoch 1, step 163, the loss is 923652.4375\n",
            "in training loop, epoch 1, step 164, the loss is 964461.0\n",
            "in training loop, epoch 1, step 165, the loss is 1321039.5\n",
            "in training loop, epoch 1, step 166, the loss is 766072.8125\n",
            "in training loop, epoch 1, step 167, the loss is 599659.5625\n",
            "in training loop, epoch 1, step 168, the loss is 655671.1875\n",
            "in training loop, epoch 1, step 169, the loss is 754043.9375\n",
            "in training loop, epoch 1, step 170, the loss is 1078623.0\n",
            "in training loop, epoch 1, step 171, the loss is 718275.5\n",
            "in training loop, epoch 1, step 172, the loss is 460138.5\n",
            "in training loop, epoch 1, step 173, the loss is 1074286.625\n",
            "in training loop, epoch 1, step 174, the loss is 645903.25\n",
            "in training loop, epoch 1, step 175, the loss is 325822.0625\n",
            "in training loop, epoch 1, step 176, the loss is 623296.375\n",
            "in training loop, epoch 1, step 177, the loss is 605691.0625\n",
            "in training loop, epoch 1, step 178, the loss is 879250.25\n",
            "in training loop, epoch 1, step 179, the loss is 704555.75\n",
            "in training loop, epoch 1, step 180, the loss is 214613.75\n",
            "in training loop, epoch 1, step 181, the loss is 1562011.75\n",
            "in training loop, epoch 1, step 182, the loss is 584315.125\n",
            "in training loop, epoch 1, step 183, the loss is 636566.75\n",
            "in training loop, epoch 1, step 184, the loss is 733061.125\n",
            "in training loop, epoch 1, step 185, the loss is 809530.625\n",
            "in training loop, epoch 1, step 186, the loss is 2163088.0\n",
            "in training loop, epoch 1, step 187, the loss is 534949.0\n",
            "in training loop, epoch 1, step 188, the loss is 666839.5\n",
            "in training loop, epoch 1, step 189, the loss is 2592760.0\n",
            "in training loop, epoch 1, step 190, the loss is 730494.75\n",
            "in training loop, epoch 1, step 191, the loss is 362556.625\n",
            "in training loop, epoch 1, step 192, the loss is 709517.0625\n",
            "in training loop, epoch 1, step 193, the loss is 504971.71875\n",
            "in training loop, epoch 1, step 194, the loss is 889165.375\n",
            "in training loop, epoch 1, step 195, the loss is 1597684.5\n",
            "in training loop, epoch 1, step 196, the loss is 1157391.75\n",
            "in training loop, epoch 1, step 197, the loss is 497881.65625\n",
            "in training loop, epoch 1, step 198, the loss is 1537329.0\n",
            "in training loop, epoch 1, step 199, the loss is 617837.125\n",
            "in training loop, epoch 1, step 200, the loss is 542713.125\n",
            "in training loop, epoch 1, step 201, the loss is 1346513.875\n",
            "in training loop, epoch 1, step 202, the loss is 1428928.5\n",
            "in training loop, epoch 1, step 203, the loss is 883608.375\n",
            "in training loop, epoch 1, step 204, the loss is 467105.34375\n",
            "in training loop, epoch 1, step 205, the loss is 757934.375\n",
            "in training loop, epoch 1, step 206, the loss is 929776.125\n",
            "in training loop, epoch 1, step 207, the loss is 565240.75\n",
            "in training loop, epoch 1, step 208, the loss is 1106423.25\n",
            "in training loop, epoch 1, step 209, the loss is 343505.75\n",
            "in training loop, epoch 1, step 210, the loss is 352637.4375\n",
            "in training loop, epoch 1, step 211, the loss is 639757.0625\n",
            "in training loop, epoch 1, step 212, the loss is 409171.65625\n",
            "in training loop, epoch 1, step 213, the loss is 918266.875\n",
            "in training loop, epoch 1, step 214, the loss is 717005.75\n",
            "in training loop, epoch 1, step 215, the loss is 1369601.125\n",
            "in training loop, epoch 1, step 216, the loss is 407602.03125\n",
            "in training loop, epoch 1, step 217, the loss is 874893.875\n",
            "in training loop, epoch 1, step 218, the loss is 623155.6875\n",
            "in training loop, epoch 1, step 219, the loss is 582772.125\n",
            "in training loop, epoch 1, step 220, the loss is 1220279.625\n",
            "in training loop, epoch 1, step 221, the loss is 419925.1875\n",
            "in training loop, epoch 1, step 222, the loss is 688565.8125\n",
            "in training loop, epoch 1, step 223, the loss is 815642.5\n",
            "in training loop, epoch 1, step 224, the loss is 914318.625\n",
            "Epoch 1: train loss 1766548.735 val loss 788471.57875\n",
            "in training loop, epoch 2, step 0, the loss is 932511.625\n",
            "in training loop, epoch 2, step 1, the loss is 1914514.875\n",
            "in training loop, epoch 2, step 2, the loss is 1564668.25\n",
            "in training loop, epoch 2, step 3, the loss is 1242591.875\n",
            "in training loop, epoch 2, step 4, the loss is 622995.6875\n",
            "in training loop, epoch 2, step 5, the loss is 1292160.0\n",
            "in training loop, epoch 2, step 6, the loss is 669066.125\n",
            "in training loop, epoch 2, step 7, the loss is 481503.34375\n",
            "in training loop, epoch 2, step 8, the loss is 855025.5\n",
            "in training loop, epoch 2, step 9, the loss is 435083.71875\n",
            "in training loop, epoch 2, step 10, the loss is 746235.1875\n",
            "in training loop, epoch 2, step 11, the loss is 806454.875\n",
            "in training loop, epoch 2, step 12, the loss is 594430.0625\n",
            "in training loop, epoch 2, step 13, the loss is 1815628.875\n",
            "in training loop, epoch 2, step 14, the loss is 857043.0\n",
            "in training loop, epoch 2, step 15, the loss is 979204.1875\n",
            "in training loop, epoch 2, step 16, the loss is 492899.1875\n",
            "in training loop, epoch 2, step 17, the loss is 665299.875\n",
            "in training loop, epoch 2, step 18, the loss is 730669.1875\n",
            "in training loop, epoch 2, step 19, the loss is 633207.5\n",
            "in training loop, epoch 2, step 20, the loss is 1388748.75\n",
            "in training loop, epoch 2, step 21, the loss is 574878.25\n",
            "in training loop, epoch 2, step 22, the loss is 854827.75\n",
            "in training loop, epoch 2, step 23, the loss is 395212.0625\n",
            "in training loop, epoch 2, step 24, the loss is 633122.8125\n",
            "in training loop, epoch 2, step 25, the loss is 801127.375\n",
            "in training loop, epoch 2, step 26, the loss is 1146730.375\n",
            "in training loop, epoch 2, step 27, the loss is 1089996.375\n",
            "in training loop, epoch 2, step 28, the loss is 685289.5625\n",
            "in training loop, epoch 2, step 29, the loss is 617967.375\n",
            "in training loop, epoch 2, step 30, the loss is 573959.5\n",
            "in training loop, epoch 2, step 31, the loss is 230951.203125\n",
            "in training loop, epoch 2, step 32, the loss is 336892.40625\n",
            "in training loop, epoch 2, step 33, the loss is 1127800.625\n",
            "in training loop, epoch 2, step 34, the loss is 940124.4375\n",
            "in training loop, epoch 2, step 35, the loss is 841171.875\n",
            "in training loop, epoch 2, step 36, the loss is 815718.0\n",
            "in training loop, epoch 2, step 37, the loss is 660177.25\n",
            "in training loop, epoch 2, step 38, the loss is 1716783.875\n",
            "in training loop, epoch 2, step 39, the loss is 762755.0\n",
            "in training loop, epoch 2, step 40, the loss is 342405.5\n",
            "in training loop, epoch 2, step 41, the loss is 968269.9375\n",
            "in training loop, epoch 2, step 42, the loss is 269999.75\n",
            "in training loop, epoch 2, step 43, the loss is 872460.1875\n",
            "in training loop, epoch 2, step 44, the loss is 455837.25\n",
            "in training loop, epoch 2, step 45, the loss is 445955.15625\n",
            "in training loop, epoch 2, step 46, the loss is 879416.875\n",
            "in training loop, epoch 2, step 47, the loss is 240456.828125\n",
            "in training loop, epoch 2, step 48, the loss is 745360.5625\n",
            "in training loop, epoch 2, step 49, the loss is 936478.9375\n",
            "in training loop, epoch 2, step 50, the loss is 357171.5625\n",
            "in training loop, epoch 2, step 51, the loss is 650516.3125\n",
            "in training loop, epoch 2, step 52, the loss is 439890.125\n",
            "in training loop, epoch 2, step 53, the loss is 527738.9375\n",
            "in training loop, epoch 2, step 54, the loss is 1205168.625\n",
            "in training loop, epoch 2, step 55, the loss is 783479.0625\n",
            "in training loop, epoch 2, step 56, the loss is 931920.1875\n",
            "in training loop, epoch 2, step 57, the loss is 1128188.25\n",
            "in training loop, epoch 2, step 58, the loss is 821781.0625\n",
            "in training loop, epoch 2, step 59, the loss is 761688.75\n",
            "in training loop, epoch 2, step 60, the loss is 466351.1875\n",
            "in training loop, epoch 2, step 61, the loss is 633211.625\n",
            "in training loop, epoch 2, step 62, the loss is 336196.90625\n",
            "in training loop, epoch 2, step 63, the loss is 938780.5\n",
            "in training loop, epoch 2, step 64, the loss is 1496291.625\n",
            "in training loop, epoch 2, step 65, the loss is 515327.96875\n",
            "in training loop, epoch 2, step 66, the loss is 591729.5625\n",
            "in training loop, epoch 2, step 67, the loss is 488206.71875\n",
            "in training loop, epoch 2, step 68, the loss is 290376.28125\n",
            "in training loop, epoch 2, step 69, the loss is 857521.3125\n",
            "in training loop, epoch 2, step 70, the loss is 387514.96875\n",
            "in training loop, epoch 2, step 71, the loss is 980918.6875\n",
            "in training loop, epoch 2, step 72, the loss is 768771.875\n",
            "in training loop, epoch 2, step 73, the loss is 676146.75\n",
            "in training loop, epoch 2, step 74, the loss is 348518.0625\n",
            "in training loop, epoch 2, step 75, the loss is 525855.0\n",
            "in training loop, epoch 2, step 76, the loss is 349437.28125\n",
            "in training loop, epoch 2, step 77, the loss is 1018372.3125\n",
            "in training loop, epoch 2, step 78, the loss is 449054.71875\n",
            "in training loop, epoch 2, step 79, the loss is 1027891.0\n",
            "in training loop, epoch 2, step 80, the loss is 559130.25\n",
            "in training loop, epoch 2, step 81, the loss is 1768888.625\n",
            "in training loop, epoch 2, step 82, the loss is 638484.1875\n",
            "in training loop, epoch 2, step 83, the loss is 1160758.75\n",
            "in training loop, epoch 2, step 84, the loss is 721070.1875\n",
            "in training loop, epoch 2, step 85, the loss is 1195699.625\n",
            "in training loop, epoch 2, step 86, the loss is 1880279.0\n",
            "in training loop, epoch 2, step 87, the loss is 1383784.125\n",
            "in training loop, epoch 2, step 88, the loss is 423665.96875\n",
            "in training loop, epoch 2, step 89, the loss is 534853.0625\n",
            "in training loop, epoch 2, step 90, the loss is 452415.25\n",
            "in training loop, epoch 2, step 91, the loss is 476370.53125\n",
            "in training loop, epoch 2, step 92, the loss is 672615.0\n",
            "in training loop, epoch 2, step 93, the loss is 159961.671875\n",
            "in training loop, epoch 2, step 94, the loss is 364323.25\n",
            "in training loop, epoch 2, step 95, the loss is 366455.40625\n",
            "in training loop, epoch 2, step 96, the loss is 605750.6875\n",
            "in training loop, epoch 2, step 97, the loss is 603648.125\n",
            "in training loop, epoch 2, step 98, the loss is 361846.46875\n",
            "in training loop, epoch 2, step 99, the loss is 450168.25\n",
            "in training loop, epoch 2, step 100, the loss is 1316219.875\n",
            "in training loop, epoch 2, step 101, the loss is 595464.8125\n",
            "in training loop, epoch 2, step 102, the loss is 904670.1875\n",
            "in training loop, epoch 2, step 103, the loss is 1329834.25\n",
            "in training loop, epoch 2, step 104, the loss is 873308.625\n",
            "in training loop, epoch 2, step 105, the loss is 744197.625\n",
            "in training loop, epoch 2, step 106, the loss is 870721.5\n",
            "in training loop, epoch 2, step 107, the loss is 283798.375\n",
            "in training loop, epoch 2, step 108, the loss is 371761.09375\n",
            "in training loop, epoch 2, step 109, the loss is 402112.03125\n",
            "in training loop, epoch 2, step 110, the loss is 450844.1875\n",
            "in training loop, epoch 2, step 111, the loss is 655228.8125\n",
            "in training loop, epoch 2, step 112, the loss is 810526.4375\n",
            "in training loop, epoch 2, step 113, the loss is 271263.0625\n",
            "in training loop, epoch 2, step 114, the loss is 1312497.375\n",
            "in training loop, epoch 2, step 115, the loss is 715223.3125\n",
            "in training loop, epoch 2, step 116, the loss is 1145971.625\n",
            "in training loop, epoch 2, step 117, the loss is 1318506.75\n",
            "in training loop, epoch 2, step 118, the loss is 834548.875\n",
            "in training loop, epoch 2, step 119, the loss is 798934.1875\n",
            "in training loop, epoch 2, step 120, the loss is 453518.625\n",
            "in training loop, epoch 2, step 121, the loss is 574225.75\n",
            "in training loop, epoch 2, step 122, the loss is 895745.75\n",
            "in training loop, epoch 2, step 123, the loss is 570966.9375\n",
            "in training loop, epoch 2, step 124, the loss is 427583.84375\n",
            "in training loop, epoch 2, step 125, the loss is 313952.0\n",
            "in training loop, epoch 2, step 126, the loss is 564125.375\n",
            "in training loop, epoch 2, step 127, the loss is 1877002.25\n",
            "in training loop, epoch 2, step 128, the loss is 1878392.75\n",
            "in training loop, epoch 2, step 129, the loss is 681777.3125\n",
            "in training loop, epoch 2, step 130, the loss is 457835.8125\n",
            "in training loop, epoch 2, step 131, the loss is 776994.0\n",
            "in training loop, epoch 2, step 132, the loss is 916400.0\n",
            "in training loop, epoch 2, step 133, the loss is 723418.6875\n",
            "in training loop, epoch 2, step 134, the loss is 1362580.875\n",
            "in training loop, epoch 2, step 135, the loss is 425715.3125\n",
            "in training loop, epoch 2, step 136, the loss is 1463627.25\n",
            "in training loop, epoch 2, step 137, the loss is 1425890.375\n",
            "in training loop, epoch 2, step 138, the loss is 651620.3125\n",
            "in training loop, epoch 2, step 139, the loss is 620922.5\n",
            "in training loop, epoch 2, step 140, the loss is 874396.0\n",
            "in training loop, epoch 2, step 141, the loss is 1044554.0\n",
            "in training loop, epoch 2, step 142, the loss is 1163691.25\n",
            "in training loop, epoch 2, step 143, the loss is 355880.875\n",
            "in training loop, epoch 2, step 144, the loss is 1421045.0\n",
            "in training loop, epoch 2, step 145, the loss is 1027757.3125\n",
            "in training loop, epoch 2, step 146, the loss is 898314.25\n",
            "in training loop, epoch 2, step 147, the loss is 659128.4375\n",
            "in training loop, epoch 2, step 148, the loss is 1511035.875\n",
            "in training loop, epoch 2, step 149, the loss is 1384466.375\n",
            "in training loop, epoch 2, step 150, the loss is 510348.15625\n",
            "in training loop, epoch 2, step 151, the loss is 537077.875\n",
            "in training loop, epoch 2, step 152, the loss is 4137529.25\n",
            "in training loop, epoch 2, step 153, the loss is 687175.0\n",
            "in training loop, epoch 2, step 154, the loss is 556469.0\n",
            "in training loop, epoch 2, step 155, the loss is 1434864.625\n",
            "in training loop, epoch 2, step 156, the loss is 865790.0625\n",
            "in training loop, epoch 2, step 157, the loss is 1058847.75\n",
            "in training loop, epoch 2, step 158, the loss is 1182810.25\n",
            "in training loop, epoch 2, step 159, the loss is 1334080.5\n",
            "in training loop, epoch 2, step 160, the loss is 1115997.25\n",
            "in training loop, epoch 2, step 161, the loss is 683883.625\n",
            "in training loop, epoch 2, step 162, the loss is 564904.4375\n",
            "in training loop, epoch 2, step 163, the loss is 622722.625\n",
            "in training loop, epoch 2, step 164, the loss is 807135.3125\n",
            "in training loop, epoch 2, step 165, the loss is 799590.0625\n",
            "in training loop, epoch 2, step 166, the loss is 1010557.0625\n",
            "in training loop, epoch 2, step 167, the loss is 632873.5\n",
            "in training loop, epoch 2, step 168, the loss is 575115.25\n",
            "in training loop, epoch 2, step 169, the loss is 628943.0625\n",
            "in training loop, epoch 2, step 170, the loss is 1112289.75\n",
            "in training loop, epoch 2, step 171, the loss is 789281.125\n",
            "in training loop, epoch 2, step 172, the loss is 552591.0\n",
            "in training loop, epoch 2, step 173, the loss is 1632408.0\n",
            "in training loop, epoch 2, step 174, the loss is 692461.375\n",
            "in training loop, epoch 2, step 175, the loss is 760967.0\n",
            "in training loop, epoch 2, step 176, the loss is 1560980.25\n",
            "in training loop, epoch 2, step 177, the loss is 314080.125\n",
            "in training loop, epoch 2, step 178, the loss is 708708.625\n",
            "in training loop, epoch 2, step 179, the loss is 947504.3125\n",
            "in training loop, epoch 2, step 180, the loss is 1416087.125\n",
            "in training loop, epoch 2, step 181, the loss is 619491.4375\n",
            "in training loop, epoch 2, step 182, the loss is 1041369.8125\n",
            "in training loop, epoch 2, step 183, the loss is 901531.25\n",
            "in training loop, epoch 2, step 184, the loss is 271886.9375\n",
            "in training loop, epoch 2, step 185, the loss is 537164.0625\n",
            "in training loop, epoch 2, step 186, the loss is 513078.5\n",
            "in training loop, epoch 2, step 187, the loss is 1252362.25\n",
            "in training loop, epoch 2, step 188, the loss is 384609.5625\n",
            "in training loop, epoch 2, step 189, the loss is 761650.9375\n",
            "in training loop, epoch 2, step 190, the loss is 1030847.8125\n",
            "in training loop, epoch 2, step 191, the loss is 522061.25\n",
            "in training loop, epoch 2, step 192, the loss is 1149467.0\n",
            "in training loop, epoch 2, step 193, the loss is 1664124.75\n",
            "in training loop, epoch 2, step 194, the loss is 772064.75\n",
            "in training loop, epoch 2, step 195, the loss is 493649.9375\n",
            "in training loop, epoch 2, step 196, the loss is 1094860.25\n",
            "in training loop, epoch 2, step 197, the loss is 711473.0\n",
            "in training loop, epoch 2, step 198, the loss is 517591.5\n",
            "in training loop, epoch 2, step 199, the loss is 255512.046875\n",
            "in training loop, epoch 2, step 200, the loss is 1141926.125\n",
            "in training loop, epoch 2, step 201, the loss is 404498.25\n",
            "in training loop, epoch 2, step 202, the loss is 682543.3125\n",
            "in training loop, epoch 2, step 203, the loss is 217542.59375\n",
            "in training loop, epoch 2, step 204, the loss is 882527.6875\n",
            "in training loop, epoch 2, step 205, the loss is 597515.1875\n",
            "in training loop, epoch 2, step 206, the loss is 570425.625\n",
            "in training loop, epoch 2, step 207, the loss is 596486.5\n",
            "in training loop, epoch 2, step 208, the loss is 317357.09375\n",
            "in training loop, epoch 2, step 209, the loss is 550720.875\n",
            "in training loop, epoch 2, step 210, the loss is 552871.25\n",
            "in training loop, epoch 2, step 211, the loss is 827391.75\n",
            "in training loop, epoch 2, step 212, the loss is 1037767.0625\n",
            "in training loop, epoch 2, step 213, the loss is 1225871.5\n",
            "in training loop, epoch 2, step 214, the loss is 697707.625\n",
            "in training loop, epoch 2, step 215, the loss is 1170006.125\n",
            "in training loop, epoch 2, step 216, the loss is 324506.8125\n",
            "in training loop, epoch 2, step 217, the loss is 752712.1875\n",
            "in training loop, epoch 2, step 218, the loss is 882149.375\n",
            "in training loop, epoch 2, step 219, the loss is 907118.1875\n",
            "in training loop, epoch 2, step 220, the loss is 495898.53125\n",
            "in training loop, epoch 2, step 221, the loss is 564619.0625\n",
            "in training loop, epoch 2, step 222, the loss is 573730.25\n",
            "in training loop, epoch 2, step 223, the loss is 545137.25\n",
            "in training loop, epoch 2, step 224, the loss is 404497.53125\n",
            "Epoch 2: train loss 805674.8918055556 val loss 723329.32125\n",
            "in training loop, epoch 3, step 0, the loss is 585708.0\n",
            "in training loop, epoch 3, step 1, the loss is 460945.03125\n",
            "in training loop, epoch 3, step 2, the loss is 561500.1875\n",
            "in training loop, epoch 3, step 3, the loss is 544001.125\n",
            "in training loop, epoch 3, step 4, the loss is 711095.8125\n",
            "in training loop, epoch 3, step 5, the loss is 762128.875\n",
            "in training loop, epoch 3, step 6, the loss is 247959.09375\n",
            "in training loop, epoch 3, step 7, the loss is 500453.875\n",
            "in training loop, epoch 3, step 8, the loss is 367955.96875\n",
            "in training loop, epoch 3, step 9, the loss is 4951740.5\n",
            "in training loop, epoch 3, step 10, the loss is 529078.875\n",
            "in training loop, epoch 3, step 11, the loss is 719158.875\n",
            "in training loop, epoch 3, step 12, the loss is 1270415.625\n",
            "in training loop, epoch 3, step 13, the loss is 636555.375\n",
            "in training loop, epoch 3, step 14, the loss is 805284.75\n",
            "in training loop, epoch 3, step 15, the loss is 1585191.25\n",
            "in training loop, epoch 3, step 16, the loss is 896591.0625\n",
            "in training loop, epoch 3, step 17, the loss is 668820.875\n",
            "in training loop, epoch 3, step 18, the loss is 802502.3125\n",
            "in training loop, epoch 3, step 19, the loss is 1613361.625\n",
            "in training loop, epoch 3, step 20, the loss is 631308.875\n",
            "in training loop, epoch 3, step 21, the loss is 483832.78125\n",
            "in training loop, epoch 3, step 22, the loss is 965449.8125\n",
            "in training loop, epoch 3, step 23, the loss is 824056.5625\n",
            "in training loop, epoch 3, step 24, the loss is 798056.9375\n",
            "in training loop, epoch 3, step 25, the loss is 898502.5\n",
            "in training loop, epoch 3, step 26, the loss is 575910.875\n",
            "in training loop, epoch 3, step 27, the loss is 822298.6875\n",
            "in training loop, epoch 3, step 28, the loss is 659234.1875\n",
            "in training loop, epoch 3, step 29, the loss is 711837.75\n",
            "in training loop, epoch 3, step 30, the loss is 780650.0625\n",
            "in training loop, epoch 3, step 31, the loss is 739008.5\n",
            "in training loop, epoch 3, step 32, the loss is 582397.5625\n",
            "in training loop, epoch 3, step 33, the loss is 287392.6875\n",
            "in training loop, epoch 3, step 34, the loss is 1846360.375\n",
            "in training loop, epoch 3, step 35, the loss is 818824.5\n",
            "in training loop, epoch 3, step 36, the loss is 446340.1875\n",
            "in training loop, epoch 3, step 37, the loss is 1467003.0\n",
            "in training loop, epoch 3, step 38, the loss is 562546.375\n",
            "in training loop, epoch 3, step 39, the loss is 678180.0\n",
            "in training loop, epoch 3, step 40, the loss is 434848.34375\n",
            "in training loop, epoch 3, step 41, the loss is 513460.875\n",
            "in training loop, epoch 3, step 42, the loss is 650638.375\n",
            "in training loop, epoch 3, step 43, the loss is 332924.125\n",
            "in training loop, epoch 3, step 44, the loss is 254943.109375\n",
            "in training loop, epoch 3, step 45, the loss is 817896.75\n",
            "in training loop, epoch 3, step 46, the loss is 1163336.5\n",
            "in training loop, epoch 3, step 47, the loss is 473886.21875\n",
            "in training loop, epoch 3, step 48, the loss is 1193686.625\n",
            "in training loop, epoch 3, step 49, the loss is 656976.6875\n",
            "in training loop, epoch 3, step 50, the loss is 1605091.875\n",
            "in training loop, epoch 3, step 51, the loss is 698788.6875\n",
            "in training loop, epoch 3, step 52, the loss is 454550.53125\n",
            "in training loop, epoch 3, step 53, the loss is 866781.75\n",
            "in training loop, epoch 3, step 54, the loss is 695473.25\n",
            "in training loop, epoch 3, step 55, the loss is 1210692.75\n",
            "in training loop, epoch 3, step 56, the loss is 472490.75\n",
            "in training loop, epoch 3, step 57, the loss is 1064841.375\n",
            "in training loop, epoch 3, step 58, the loss is 508585.5625\n",
            "in training loop, epoch 3, step 59, the loss is 477085.0625\n",
            "in training loop, epoch 3, step 60, the loss is 418874.15625\n",
            "in training loop, epoch 3, step 61, the loss is 504125.15625\n",
            "in training loop, epoch 3, step 62, the loss is 1026793.0\n",
            "in training loop, epoch 3, step 63, the loss is 1773257.625\n",
            "in training loop, epoch 3, step 64, the loss is 1731179.75\n",
            "in training loop, epoch 3, step 65, the loss is 356760.0\n",
            "in training loop, epoch 3, step 66, the loss is 973630.6875\n",
            "in training loop, epoch 3, step 67, the loss is 1337798.75\n",
            "in training loop, epoch 3, step 68, the loss is 745492.6875\n",
            "in training loop, epoch 3, step 69, the loss is 905830.625\n",
            "in training loop, epoch 3, step 70, the loss is 1069901.875\n",
            "in training loop, epoch 3, step 71, the loss is 906218.9375\n",
            "in training loop, epoch 3, step 72, the loss is 1797139.25\n",
            "in training loop, epoch 3, step 73, the loss is 472442.6875\n",
            "in training loop, epoch 3, step 74, the loss is 666682.5\n",
            "in training loop, epoch 3, step 75, the loss is 609251.5625\n",
            "in training loop, epoch 3, step 76, the loss is 356696.25\n",
            "in training loop, epoch 3, step 77, the loss is 587787.25\n",
            "in training loop, epoch 3, step 78, the loss is 1610273.125\n",
            "in training loop, epoch 3, step 79, the loss is 858141.25\n",
            "in training loop, epoch 3, step 80, the loss is 646655.5\n",
            "in training loop, epoch 3, step 81, the loss is 588465.1875\n",
            "in training loop, epoch 3, step 82, the loss is 1249589.875\n",
            "in training loop, epoch 3, step 83, the loss is 880770.3125\n",
            "in training loop, epoch 3, step 84, the loss is 425566.0\n",
            "in training loop, epoch 3, step 85, the loss is 330728.0625\n",
            "in training loop, epoch 3, step 86, the loss is 251627.59375\n",
            "in training loop, epoch 3, step 87, the loss is 746795.75\n",
            "in training loop, epoch 3, step 88, the loss is 436377.71875\n",
            "in training loop, epoch 3, step 89, the loss is 381259.28125\n",
            "in training loop, epoch 3, step 90, the loss is 894152.625\n",
            "in training loop, epoch 3, step 91, the loss is 1648782.5\n",
            "in training loop, epoch 3, step 92, the loss is 268780.4375\n",
            "in training loop, epoch 3, step 93, the loss is 835182.625\n",
            "in training loop, epoch 3, step 94, the loss is 726715.0\n",
            "in training loop, epoch 3, step 95, the loss is 465252.09375\n",
            "in training loop, epoch 3, step 96, the loss is 830733.8125\n",
            "in training loop, epoch 3, step 97, the loss is 944817.4375\n",
            "in training loop, epoch 3, step 98, the loss is 703321.6875\n",
            "in training loop, epoch 3, step 99, the loss is 820687.8125\n",
            "in training loop, epoch 3, step 100, the loss is 598786.0625\n",
            "in training loop, epoch 3, step 101, the loss is 911431.5\n",
            "in training loop, epoch 3, step 102, the loss is 781610.625\n",
            "in training loop, epoch 3, step 103, the loss is 828390.8125\n",
            "in training loop, epoch 3, step 104, the loss is 698512.5625\n",
            "in training loop, epoch 3, step 105, the loss is 585095.375\n",
            "in training loop, epoch 3, step 106, the loss is 650998.75\n",
            "in training loop, epoch 3, step 107, the loss is 1067644.75\n",
            "in training loop, epoch 3, step 108, the loss is 1311930.125\n",
            "in training loop, epoch 3, step 109, the loss is 709334.3125\n",
            "in training loop, epoch 3, step 110, the loss is 394013.9375\n",
            "in training loop, epoch 3, step 111, the loss is 291773.5625\n",
            "in training loop, epoch 3, step 112, the loss is 204963.953125\n",
            "in training loop, epoch 3, step 113, the loss is 801034.375\n",
            "in training loop, epoch 3, step 114, the loss is 516760.625\n",
            "in training loop, epoch 3, step 115, the loss is 576505.0625\n",
            "in training loop, epoch 3, step 116, the loss is 794227.375\n",
            "in training loop, epoch 3, step 117, the loss is 823553.0\n",
            "in training loop, epoch 3, step 118, the loss is 638320.0625\n",
            "in training loop, epoch 3, step 119, the loss is 926582.75\n",
            "in training loop, epoch 3, step 120, the loss is 333089.25\n",
            "in training loop, epoch 3, step 121, the loss is 1051014.375\n",
            "in training loop, epoch 3, step 122, the loss is 788196.3125\n",
            "in training loop, epoch 3, step 123, the loss is 678321.4375\n",
            "in training loop, epoch 3, step 124, the loss is 439016.9375\n",
            "in training loop, epoch 3, step 125, the loss is 1062555.75\n",
            "in training loop, epoch 3, step 126, the loss is 1395264.375\n",
            "in training loop, epoch 3, step 127, the loss is 414254.84375\n",
            "in training loop, epoch 3, step 128, the loss is 328845.96875\n",
            "in training loop, epoch 3, step 129, the loss is 366795.75\n",
            "in training loop, epoch 3, step 130, the loss is 553913.3125\n",
            "in training loop, epoch 3, step 131, the loss is 863377.0\n",
            "in training loop, epoch 3, step 132, the loss is 196933.5\n",
            "in training loop, epoch 3, step 133, the loss is 580266.3125\n",
            "in training loop, epoch 3, step 134, the loss is 691309.375\n",
            "in training loop, epoch 3, step 135, the loss is 1706303.5\n",
            "in training loop, epoch 3, step 136, the loss is 788328.6875\n",
            "in training loop, epoch 3, step 137, the loss is 599019.0\n",
            "in training loop, epoch 3, step 138, the loss is 977245.5625\n",
            "in training loop, epoch 3, step 139, the loss is 653113.9375\n",
            "in training loop, epoch 3, step 140, the loss is 806328.8125\n",
            "in training loop, epoch 3, step 141, the loss is 443349.9375\n",
            "in training loop, epoch 3, step 142, the loss is 702965.4375\n",
            "in training loop, epoch 3, step 143, the loss is 315117.65625\n",
            "in training loop, epoch 3, step 144, the loss is 1036265.8125\n",
            "in training loop, epoch 3, step 145, the loss is 1118509.25\n",
            "in training loop, epoch 3, step 146, the loss is 878998.1875\n",
            "in training loop, epoch 3, step 147, the loss is 268079.21875\n",
            "in training loop, epoch 3, step 148, the loss is 1367013.875\n",
            "in training loop, epoch 3, step 149, the loss is 945714.75\n",
            "in training loop, epoch 3, step 150, the loss is 1356998.75\n",
            "in training loop, epoch 3, step 151, the loss is 460521.875\n",
            "in training loop, epoch 3, step 152, the loss is 821784.6875\n",
            "in training loop, epoch 3, step 153, the loss is 649187.4375\n",
            "in training loop, epoch 3, step 154, the loss is 728252.25\n",
            "in training loop, epoch 3, step 155, the loss is 417751.1875\n",
            "in training loop, epoch 3, step 156, the loss is 677597.25\n",
            "in training loop, epoch 3, step 157, the loss is 617291.1875\n",
            "in training loop, epoch 3, step 158, the loss is 535925.375\n",
            "in training loop, epoch 3, step 159, the loss is 351065.8125\n",
            "in training loop, epoch 3, step 160, the loss is 585834.0625\n",
            "in training loop, epoch 3, step 161, the loss is 780593.0\n",
            "in training loop, epoch 3, step 162, the loss is 426411.625\n",
            "in training loop, epoch 3, step 163, the loss is 649673.9375\n",
            "in training loop, epoch 3, step 164, the loss is 864936.25\n",
            "in training loop, epoch 3, step 165, the loss is 901610.8125\n",
            "in training loop, epoch 3, step 166, the loss is 978827.4375\n",
            "in training loop, epoch 3, step 167, the loss is 991252.5\n",
            "in training loop, epoch 3, step 168, the loss is 683869.875\n",
            "in training loop, epoch 3, step 169, the loss is 773555.75\n",
            "in training loop, epoch 3, step 170, the loss is 816083.4375\n",
            "in training loop, epoch 3, step 171, the loss is 278878.71875\n",
            "in training loop, epoch 3, step 172, the loss is 1510990.5\n",
            "in training loop, epoch 3, step 173, the loss is 904356.6875\n",
            "in training loop, epoch 3, step 174, the loss is 482040.9375\n",
            "in training loop, epoch 3, step 175, the loss is 644068.1875\n",
            "in training loop, epoch 3, step 176, the loss is 731696.1875\n",
            "in training loop, epoch 3, step 177, the loss is 435260.1875\n",
            "in training loop, epoch 3, step 178, the loss is 391886.15625\n",
            "in training loop, epoch 3, step 179, the loss is 525099.375\n",
            "in training loop, epoch 3, step 180, the loss is 398231.1875\n",
            "in training loop, epoch 3, step 181, the loss is 954435.25\n",
            "in training loop, epoch 3, step 182, the loss is 312453.15625\n",
            "in training loop, epoch 3, step 183, the loss is 1495529.125\n",
            "in training loop, epoch 3, step 184, the loss is 159138.78125\n",
            "in training loop, epoch 3, step 185, the loss is 941669.625\n",
            "in training loop, epoch 3, step 186, the loss is 649407.4375\n",
            "in training loop, epoch 3, step 187, the loss is 482711.4375\n",
            "in training loop, epoch 3, step 188, the loss is 1005057.3125\n",
            "in training loop, epoch 3, step 189, the loss is 1152121.125\n",
            "in training loop, epoch 3, step 190, the loss is 1521308.625\n",
            "in training loop, epoch 3, step 191, the loss is 1089964.75\n",
            "in training loop, epoch 3, step 192, the loss is 636461.125\n",
            "in training loop, epoch 3, step 193, the loss is 608555.875\n",
            "in training loop, epoch 3, step 194, the loss is 591065.4375\n",
            "in training loop, epoch 3, step 195, the loss is 542745.1875\n",
            "in training loop, epoch 3, step 196, the loss is 542283.625\n",
            "in training loop, epoch 3, step 197, the loss is 267734.1875\n",
            "in training loop, epoch 3, step 198, the loss is 1071516.25\n",
            "in training loop, epoch 3, step 199, the loss is 947195.6875\n",
            "in training loop, epoch 3, step 200, the loss is 617596.9375\n",
            "in training loop, epoch 3, step 201, the loss is 1030725.4375\n",
            "in training loop, epoch 3, step 202, the loss is 732153.5\n",
            "in training loop, epoch 3, step 203, the loss is 802799.875\n",
            "in training loop, epoch 3, step 204, the loss is 514058.25\n",
            "in training loop, epoch 3, step 205, the loss is 760193.125\n",
            "in training loop, epoch 3, step 206, the loss is 656880.625\n",
            "in training loop, epoch 3, step 207, the loss is 1024128.8125\n",
            "in training loop, epoch 3, step 208, the loss is 1082919.5\n",
            "in training loop, epoch 3, step 209, the loss is 431008.625\n",
            "in training loop, epoch 3, step 210, the loss is 303107.6875\n",
            "in training loop, epoch 3, step 211, the loss is 724557.1875\n",
            "in training loop, epoch 3, step 212, the loss is 617650.1875\n",
            "in training loop, epoch 3, step 213, the loss is 502269.5625\n",
            "in training loop, epoch 3, step 214, the loss is 544635.5\n",
            "in training loop, epoch 3, step 215, the loss is 1289839.375\n",
            "in training loop, epoch 3, step 216, the loss is 1201109.0\n",
            "in training loop, epoch 3, step 217, the loss is 647090.5\n",
            "in training loop, epoch 3, step 218, the loss is 678285.625\n",
            "in training loop, epoch 3, step 219, the loss is 1803063.625\n",
            "in training loop, epoch 3, step 220, the loss is 958179.75\n",
            "in training loop, epoch 3, step 221, the loss is 589962.6875\n",
            "in training loop, epoch 3, step 222, the loss is 472835.3125\n",
            "in training loop, epoch 3, step 223, the loss is 1070149.375\n",
            "in training loop, epoch 3, step 224, the loss is 718213.75\n",
            "Epoch 3: train loss 776481.9426388888 val loss 710508.6775\n",
            "in training loop, epoch 4, step 0, the loss is 578798.0\n",
            "in training loop, epoch 4, step 1, the loss is 1649955.75\n",
            "in training loop, epoch 4, step 2, the loss is 967267.1875\n",
            "in training loop, epoch 4, step 3, the loss is 280444.71875\n",
            "in training loop, epoch 4, step 4, the loss is 548467.9375\n",
            "in training loop, epoch 4, step 5, the loss is 786481.75\n",
            "in training loop, epoch 4, step 6, the loss is 867107.5\n",
            "in training loop, epoch 4, step 7, the loss is 809778.8125\n",
            "in training loop, epoch 4, step 8, the loss is 510293.875\n",
            "in training loop, epoch 4, step 9, the loss is 515384.8125\n",
            "in training loop, epoch 4, step 10, the loss is 1085238.5\n",
            "in training loop, epoch 4, step 11, the loss is 771161.5625\n",
            "in training loop, epoch 4, step 12, the loss is 719755.125\n",
            "in training loop, epoch 4, step 13, the loss is 453394.5\n",
            "in training loop, epoch 4, step 14, the loss is 284672.375\n",
            "in training loop, epoch 4, step 15, the loss is 900175.5\n",
            "in training loop, epoch 4, step 16, the loss is 804281.5\n",
            "in training loop, epoch 4, step 17, the loss is 738564.0\n",
            "in training loop, epoch 4, step 18, the loss is 959985.75\n",
            "in training loop, epoch 4, step 19, the loss is 919873.0\n",
            "in training loop, epoch 4, step 20, the loss is 736439.0\n",
            "in training loop, epoch 4, step 21, the loss is 868388.625\n",
            "in training loop, epoch 4, step 22, the loss is 182768.5625\n",
            "in training loop, epoch 4, step 23, the loss is 534101.9375\n",
            "in training loop, epoch 4, step 24, the loss is 526198.5\n",
            "in training loop, epoch 4, step 25, the loss is 645945.875\n",
            "in training loop, epoch 4, step 26, the loss is 540789.3125\n",
            "in training loop, epoch 4, step 27, the loss is 849845.875\n",
            "in training loop, epoch 4, step 28, the loss is 753149.8125\n",
            "in training loop, epoch 4, step 29, the loss is 1709889.125\n",
            "in training loop, epoch 4, step 30, the loss is 346822.6875\n",
            "in training loop, epoch 4, step 31, the loss is 1217682.25\n",
            "in training loop, epoch 4, step 32, the loss is 557348.875\n",
            "in training loop, epoch 4, step 33, the loss is 560774.375\n",
            "in training loop, epoch 4, step 34, the loss is 553920.9375\n",
            "in training loop, epoch 4, step 35, the loss is 402962.28125\n",
            "in training loop, epoch 4, step 36, the loss is 1042556.0625\n",
            "in training loop, epoch 4, step 37, the loss is 365624.28125\n",
            "in training loop, epoch 4, step 38, the loss is 389692.625\n",
            "in training loop, epoch 4, step 39, the loss is 439231.78125\n",
            "in training loop, epoch 4, step 40, the loss is 472746.96875\n",
            "in training loop, epoch 4, step 41, the loss is 1160436.375\n",
            "in training loop, epoch 4, step 42, the loss is 546259.6875\n",
            "in training loop, epoch 4, step 43, the loss is 497661.9375\n",
            "in training loop, epoch 4, step 44, the loss is 1126893.625\n",
            "in training loop, epoch 4, step 45, the loss is 573793.125\n",
            "in training loop, epoch 4, step 46, the loss is 817761.75\n",
            "in training loop, epoch 4, step 47, the loss is 306724.625\n",
            "in training loop, epoch 4, step 48, the loss is 1115870.25\n",
            "in training loop, epoch 4, step 49, the loss is 646532.1875\n",
            "in training loop, epoch 4, step 50, the loss is 575113.75\n",
            "in training loop, epoch 4, step 51, the loss is 699009.125\n",
            "in training loop, epoch 4, step 52, the loss is 1012972.5\n",
            "in training loop, epoch 4, step 53, the loss is 988180.4375\n",
            "in training loop, epoch 4, step 54, the loss is 578723.75\n",
            "in training loop, epoch 4, step 55, the loss is 223412.828125\n",
            "in training loop, epoch 4, step 56, the loss is 302562.6875\n",
            "in training loop, epoch 4, step 57, the loss is 901775.0625\n",
            "in training loop, epoch 4, step 58, the loss is 772623.5625\n",
            "in training loop, epoch 4, step 59, the loss is 741769.5625\n",
            "in training loop, epoch 4, step 60, the loss is 446732.9375\n",
            "in training loop, epoch 4, step 61, the loss is 745109.9375\n",
            "in training loop, epoch 4, step 62, the loss is 1032864.75\n",
            "in training loop, epoch 4, step 63, the loss is 286469.0625\n",
            "in training loop, epoch 4, step 64, the loss is 563160.5\n",
            "in training loop, epoch 4, step 65, the loss is 760476.125\n",
            "in training loop, epoch 4, step 66, the loss is 1615298.75\n",
            "in training loop, epoch 4, step 67, the loss is 304252.5\n",
            "in training loop, epoch 4, step 68, the loss is 690213.375\n",
            "in training loop, epoch 4, step 69, the loss is 542478.875\n",
            "in training loop, epoch 4, step 70, the loss is 1190751.25\n",
            "in training loop, epoch 4, step 71, the loss is 471626.1875\n",
            "in training loop, epoch 4, step 72, the loss is 865982.3125\n",
            "in training loop, epoch 4, step 73, the loss is 1114366.875\n",
            "in training loop, epoch 4, step 74, the loss is 382436.0625\n",
            "in training loop, epoch 4, step 75, the loss is 688905.25\n",
            "in training loop, epoch 4, step 76, the loss is 567370.5\n",
            "in training loop, epoch 4, step 77, the loss is 830342.5\n",
            "in training loop, epoch 4, step 78, the loss is 569605.3125\n",
            "in training loop, epoch 4, step 79, the loss is 1096884.625\n",
            "in training loop, epoch 4, step 80, the loss is 484441.15625\n",
            "in training loop, epoch 4, step 81, the loss is 313855.1875\n",
            "in training loop, epoch 4, step 82, the loss is 713593.125\n",
            "in training loop, epoch 4, step 83, the loss is 1708666.0\n",
            "in training loop, epoch 4, step 84, the loss is 572966.625\n",
            "in training loop, epoch 4, step 85, the loss is 512398.34375\n",
            "in training loop, epoch 4, step 86, the loss is 549794.125\n",
            "in training loop, epoch 4, step 87, the loss is 1084888.0\n",
            "in training loop, epoch 4, step 88, the loss is 421390.96875\n",
            "in training loop, epoch 4, step 89, the loss is 494362.1875\n",
            "in training loop, epoch 4, step 90, the loss is 521648.28125\n",
            "in training loop, epoch 4, step 91, the loss is 718998.75\n",
            "in training loop, epoch 4, step 92, the loss is 953087.1875\n",
            "in training loop, epoch 4, step 93, the loss is 397645.25\n",
            "in training loop, epoch 4, step 94, the loss is 420272.5\n",
            "in training loop, epoch 4, step 95, the loss is 674244.0\n",
            "in training loop, epoch 4, step 96, the loss is 550685.375\n",
            "in training loop, epoch 4, step 97, the loss is 778133.5625\n",
            "in training loop, epoch 4, step 98, the loss is 554637.8125\n",
            "in training loop, epoch 4, step 99, the loss is 692829.1875\n",
            "in training loop, epoch 4, step 100, the loss is 579416.0\n",
            "in training loop, epoch 4, step 101, the loss is 735160.125\n",
            "in training loop, epoch 4, step 102, the loss is 766762.625\n",
            "in training loop, epoch 4, step 103, the loss is 471297.25\n",
            "in training loop, epoch 4, step 104, the loss is 582601.1875\n",
            "in training loop, epoch 4, step 105, the loss is 662927.125\n",
            "in training loop, epoch 4, step 106, the loss is 255241.125\n",
            "in training loop, epoch 4, step 107, the loss is 664466.375\n",
            "in training loop, epoch 4, step 108, the loss is 1270075.125\n",
            "in training loop, epoch 4, step 109, the loss is 576763.375\n",
            "in training loop, epoch 4, step 110, the loss is 324654.34375\n",
            "in training loop, epoch 4, step 111, the loss is 624842.1875\n",
            "in training loop, epoch 4, step 112, the loss is 373835.8125\n",
            "in training loop, epoch 4, step 113, the loss is 138425.0625\n",
            "in training loop, epoch 4, step 114, the loss is 518995.3125\n",
            "in training loop, epoch 4, step 115, the loss is 425441.25\n",
            "in training loop, epoch 4, step 116, the loss is 582587.5\n",
            "in training loop, epoch 4, step 117, the loss is 1253245.75\n",
            "in training loop, epoch 4, step 118, the loss is 722122.875\n",
            "in training loop, epoch 4, step 119, the loss is 485200.5625\n",
            "in training loop, epoch 4, step 120, the loss is 1114372.25\n",
            "in training loop, epoch 4, step 121, the loss is 532666.125\n",
            "in training loop, epoch 4, step 122, the loss is 417425.65625\n",
            "in training loop, epoch 4, step 123, the loss is 541063.0\n",
            "in training loop, epoch 4, step 124, the loss is 659285.3125\n",
            "in training loop, epoch 4, step 125, the loss is 283920.8125\n",
            "in training loop, epoch 4, step 126, the loss is 493242.25\n",
            "in training loop, epoch 4, step 127, the loss is 721458.0625\n",
            "in training loop, epoch 4, step 128, the loss is 633945.375\n",
            "in training loop, epoch 4, step 129, the loss is 953145.0\n",
            "in training loop, epoch 4, step 130, the loss is 492107.1875\n",
            "in training loop, epoch 4, step 131, the loss is 506196.3125\n",
            "in training loop, epoch 4, step 132, the loss is 440839.71875\n",
            "in training loop, epoch 4, step 133, the loss is 441052.09375\n",
            "in training loop, epoch 4, step 134, the loss is 751335.5\n",
            "in training loop, epoch 4, step 135, the loss is 949991.9375\n",
            "in training loop, epoch 4, step 136, the loss is 1146307.75\n",
            "in training loop, epoch 4, step 137, the loss is 313533.90625\n",
            "in training loop, epoch 4, step 138, the loss is 691496.5625\n",
            "in training loop, epoch 4, step 139, the loss is 577247.0\n",
            "in training loop, epoch 4, step 140, the loss is 905591.875\n",
            "in training loop, epoch 4, step 141, the loss is 364974.75\n",
            "in training loop, epoch 4, step 142, the loss is 352012.875\n",
            "in training loop, epoch 4, step 143, the loss is 964674.25\n",
            "in training loop, epoch 4, step 144, the loss is 465787.125\n",
            "in training loop, epoch 4, step 145, the loss is 461387.21875\n",
            "in training loop, epoch 4, step 146, the loss is 376718.03125\n",
            "in training loop, epoch 4, step 147, the loss is 395634.59375\n",
            "in training loop, epoch 4, step 148, the loss is 464693.0\n",
            "in training loop, epoch 4, step 149, the loss is 280964.28125\n",
            "in training loop, epoch 4, step 150, the loss is 599510.9375\n",
            "in training loop, epoch 4, step 151, the loss is 414895.3125\n",
            "in training loop, epoch 4, step 152, the loss is 1179583.125\n",
            "in training loop, epoch 4, step 153, the loss is 1369445.0\n",
            "in training loop, epoch 4, step 154, the loss is 502483.21875\n",
            "in training loop, epoch 4, step 155, the loss is 409531.375\n",
            "in training loop, epoch 4, step 156, the loss is 272743.0625\n",
            "in training loop, epoch 4, step 157, the loss is 257302.109375\n",
            "in training loop, epoch 4, step 158, the loss is 1010383.1875\n",
            "in training loop, epoch 4, step 159, the loss is 486530.78125\n",
            "in training loop, epoch 4, step 160, the loss is 398022.78125\n",
            "in training loop, epoch 4, step 161, the loss is 734179.3125\n",
            "in training loop, epoch 4, step 162, the loss is 375543.59375\n",
            "in training loop, epoch 4, step 163, the loss is 905328.375\n",
            "in training loop, epoch 4, step 164, the loss is 653614.25\n",
            "in training loop, epoch 4, step 165, the loss is 815316.375\n",
            "in training loop, epoch 4, step 166, the loss is 432004.875\n",
            "in training loop, epoch 4, step 167, the loss is 663222.0\n",
            "in training loop, epoch 4, step 168, the loss is 219736.359375\n",
            "in training loop, epoch 4, step 169, the loss is 434749.03125\n",
            "in training loop, epoch 4, step 170, the loss is 736152.75\n",
            "in training loop, epoch 4, step 171, the loss is 469658.28125\n",
            "in training loop, epoch 4, step 172, the loss is 448496.90625\n",
            "in training loop, epoch 4, step 173, the loss is 2389701.25\n",
            "in training loop, epoch 4, step 174, the loss is 663318.875\n",
            "in training loop, epoch 4, step 175, the loss is 995848.8125\n",
            "in training loop, epoch 4, step 176, the loss is 778937.9375\n",
            "in training loop, epoch 4, step 177, the loss is 475692.5\n",
            "in training loop, epoch 4, step 178, the loss is 514412.96875\n",
            "in training loop, epoch 4, step 179, the loss is 520224.3125\n",
            "in training loop, epoch 4, step 180, the loss is 1261155.5\n",
            "in training loop, epoch 4, step 181, the loss is 851418.375\n",
            "in training loop, epoch 4, step 182, the loss is 805193.875\n",
            "in training loop, epoch 4, step 183, the loss is 442430.375\n",
            "in training loop, epoch 4, step 184, the loss is 509582.84375\n",
            "in training loop, epoch 4, step 185, the loss is 745805.0625\n",
            "in training loop, epoch 4, step 186, the loss is 726519.75\n",
            "in training loop, epoch 4, step 187, the loss is 616524.1875\n",
            "in training loop, epoch 4, step 188, the loss is 1802864.125\n",
            "in training loop, epoch 4, step 189, the loss is 1003457.375\n",
            "in training loop, epoch 4, step 190, the loss is 508609.75\n",
            "in training loop, epoch 4, step 191, the loss is 1115287.5\n",
            "in training loop, epoch 4, step 192, the loss is 847190.125\n",
            "in training loop, epoch 4, step 193, the loss is 765287.5625\n",
            "in training loop, epoch 4, step 194, the loss is 699114.125\n",
            "in training loop, epoch 4, step 195, the loss is 731152.5625\n",
            "in training loop, epoch 4, step 196, the loss is 541429.3125\n",
            "in training loop, epoch 4, step 197, the loss is 687074.375\n",
            "in training loop, epoch 4, step 198, the loss is 1072545.625\n",
            "in training loop, epoch 4, step 199, the loss is 463192.0625\n",
            "in training loop, epoch 4, step 200, the loss is 482026.15625\n",
            "in training loop, epoch 4, step 201, the loss is 771185.25\n",
            "in training loop, epoch 4, step 202, the loss is 613555.125\n",
            "in training loop, epoch 4, step 203, the loss is 477859.25\n",
            "in training loop, epoch 4, step 204, the loss is 733274.75\n",
            "in training loop, epoch 4, step 205, the loss is 697412.625\n",
            "in training loop, epoch 4, step 206, the loss is 727189.0625\n",
            "in training loop, epoch 4, step 207, the loss is 983730.125\n",
            "in training loop, epoch 4, step 208, the loss is 556272.75\n",
            "in training loop, epoch 4, step 209, the loss is 288768.625\n",
            "in training loop, epoch 4, step 210, the loss is 630829.25\n",
            "in training loop, epoch 4, step 211, the loss is 460417.9375\n",
            "in training loop, epoch 4, step 212, the loss is 975262.625\n",
            "in training loop, epoch 4, step 213, the loss is 741045.0\n",
            "in training loop, epoch 4, step 214, the loss is 672073.5\n",
            "in training loop, epoch 4, step 215, the loss is 387960.6875\n",
            "in training loop, epoch 4, step 216, the loss is 474772.25\n",
            "in training loop, epoch 4, step 217, the loss is 211518.21875\n",
            "in training loop, epoch 4, step 218, the loss is 539591.1875\n",
            "in training loop, epoch 4, step 219, the loss is 1012488.4375\n",
            "in training loop, epoch 4, step 220, the loss is 780923.125\n",
            "in training loop, epoch 4, step 221, the loss is 292711.5\n",
            "in training loop, epoch 4, step 222, the loss is 694310.1875\n",
            "in training loop, epoch 4, step 223, the loss is 396381.5\n",
            "in training loop, epoch 4, step 224, the loss is 943829.4375\n",
            "Epoch 4: train loss 675088.1222916667 val loss 600602.40875\n",
            "in training loop, epoch 5, step 0, the loss is 508315.1875\n",
            "in training loop, epoch 5, step 1, the loss is 484280.34375\n",
            "in training loop, epoch 5, step 2, the loss is 420326.6875\n",
            "in training loop, epoch 5, step 3, the loss is 463269.9375\n",
            "in training loop, epoch 5, step 4, the loss is 772582.0625\n",
            "in training loop, epoch 5, step 5, the loss is 423488.75\n",
            "in training loop, epoch 5, step 6, the loss is 494863.09375\n",
            "in training loop, epoch 5, step 7, the loss is 364775.875\n",
            "in training loop, epoch 5, step 8, the loss is 602905.0\n",
            "in training loop, epoch 5, step 9, the loss is 238930.0\n",
            "in training loop, epoch 5, step 10, the loss is 544505.4375\n",
            "in training loop, epoch 5, step 11, the loss is 553736.3125\n",
            "in training loop, epoch 5, step 12, the loss is 610319.875\n",
            "in training loop, epoch 5, step 13, the loss is 344632.0\n",
            "in training loop, epoch 5, step 14, the loss is 427448.75\n",
            "in training loop, epoch 5, step 15, the loss is 1317885.125\n",
            "in training loop, epoch 5, step 16, the loss is 382440.25\n",
            "in training loop, epoch 5, step 17, the loss is 313553.9375\n",
            "in training loop, epoch 5, step 18, the loss is 640709.0\n",
            "in training loop, epoch 5, step 19, the loss is 331168.84375\n",
            "in training loop, epoch 5, step 20, the loss is 486432.9375\n",
            "in training loop, epoch 5, step 21, the loss is 507290.375\n",
            "in training loop, epoch 5, step 22, the loss is 314287.59375\n",
            "in training loop, epoch 5, step 23, the loss is 558791.5\n",
            "in training loop, epoch 5, step 24, the loss is 511174.625\n",
            "in training loop, epoch 5, step 25, the loss is 619057.6875\n",
            "in training loop, epoch 5, step 26, the loss is 317517.1875\n",
            "in training loop, epoch 5, step 27, the loss is 447786.03125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3I63RgNmk73"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}