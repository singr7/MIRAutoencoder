{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNLSTMAutoEncoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM43BwU/hfV8ja7BfpyrkxW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singr7/MIRAutoencoder/blob/master/CNNLSTMAutoEncoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9JqXiXhqdkN"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORCKRmYPHk-z"
      },
      "source": [
        "#Mount the google drive\n",
        "#Create list of numpy files for western and indian dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPnD6kkyHfx8",
        "outputId": "ef1716e9-7f4b-426f-a11a-a8c53aad9986"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "western_files = []\n",
        "western_file_dir = \"/content/drive/My Drive/MusicResearchColabNB/Western_numpy\"\n",
        "for r,d, fileList in os.walk(western_file_dir):\n",
        "  for file in fileList:\n",
        "    western_files.append(os.path.join(r,file))\n",
        "\n",
        "indian_files = []\n",
        "indian_file_dir = \"/content/drive/My Drive/MusicResearchColabNB/IndianDataset/Indian_numpy\"\n",
        "for r,d, fileList in os.walk(indian_file_dir):\n",
        "  for file in fileList:\n",
        "    indian_files.append(os.path.join(r,file))\n",
        "\n",
        "print(len(western_files))\n",
        "print(len(indian_files))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "7592\n",
            "2008\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMQbF-ylLmdK"
      },
      "source": [
        "# Balance the western dataset by taking files equal to Indian dataset files = 2008"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXHKJAOLL-iX",
        "outputId": "7ec65fbd-3f39-4af0-faf1-b5c0f979854e"
      },
      "source": [
        "import random \n",
        "#randomize the selection. To avoid getting a different random sample with every run, use seed\n",
        "random.seed(234)\n",
        "bal_western_files = random.sample(western_files,2008)\n",
        "len(bal_western_files)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2008"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeBwpwaTX3Jo"
      },
      "source": [
        "#Define configuration class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP4323imT53f"
      },
      "source": [
        "class Configuration:\n",
        "  seq_len = 200  # taking half of the original timesteps extracted \n",
        "  input_dim = 26  #num of mels\n",
        "  embedding_dim = 64\n",
        "  batch_size = 2\n",
        "  base_dir = \"/content/drive/My Drive/MusicResearchColabNB/vajra/westernAE\"   # need to be edited..\n",
        "  loss_function = torch.nn.MSELoss(reduction='sum')\n",
        "  lr=1e-3  # I edited it from 1e-3 to 1e-5\n",
        "  n_epochs = 4\n",
        "  model_file = \"/content/drive/My Drive/MusicResearchColabNB/vajra/westernAE/models/mel.pkl\"  #need need edits\n",
        "  results_dir = os.path.join(base_dir, \"./results\")  # may need edits\n",
        "  checkpoint_model_file = \"/content/drive/My Drive/MusicResearchColabNB/vajra/westernAE/models/mel_checkpoint.pkl\" #may need edits\n",
        "  kernel_size = 3  #why?\n",
        "  k_folds = 10 "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RALRBXgZZBA"
      },
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "  def __init__(self, seq_len, n_features, embedding_dim=64, kernel_size=3, stride=1):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.seq_len, self.n_features = seq_len, n_features\n",
        "    self.embedding_dim, self.hidden_dim = embedding_dim, 2 * embedding_dim\n",
        "\n",
        "\n",
        "    self.conv = nn.Conv1d(in_channels=seq_len,out_channels=seq_len,kernel_size=kernel_size,stride=stride, groups=seq_len)\n",
        "    conv_op_dim = int(((n_features - kernel_size)/ stride) + 1)\n",
        "\n",
        "    self.rnn1 = nn.LSTM(\n",
        "      input_size=conv_op_dim,\n",
        "      hidden_size=self.hidden_dim,\n",
        "      num_layers=1,\n",
        "      batch_first=True\n",
        "    )\n",
        "    self.rnn2 = nn.LSTM(\n",
        "      input_size=self.hidden_dim,\n",
        "      hidden_size=embedding_dim,\n",
        "      num_layers=1,\n",
        "      batch_first=True\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    #x = x.reshape((10, self.seq_len, self.n_features))\n",
        "   # print('In Encoder')\n",
        "   # print(x.shape)\n",
        "    x = self.conv(x)\n",
        "    x, (_, _) = self.rnn1(x)\n",
        "    x, (hidden_n, _) = self.rnn2(x)\n",
        "    return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkW-A8TzZdGT"
      },
      "source": [
        "class Decoder(torch.nn.Module):\n",
        "  def __init__(self, seq_len, embedding_dim=64, n_features=26):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.seq_len, self.embedding_dim = seq_len, embedding_dim\n",
        "    self.hidden_dim, self.n_features = 2 * embedding_dim, n_features\n",
        "    self.rnn1 = nn.LSTM(\n",
        "      input_size=embedding_dim,\n",
        "      hidden_size=embedding_dim,\n",
        "      num_layers=1,\n",
        "      batch_first=True\n",
        "    )\n",
        "    self.rnn2 = nn.LSTM(\n",
        "      input_size=embedding_dim,\n",
        "      hidden_size=self.hidden_dim,\n",
        "      num_layers=1,\n",
        "      batch_first=True\n",
        "    )\n",
        "    self.output_layer = nn.Linear(self.hidden_dim * self.seq_len, n_features * self.seq_len)\n",
        "  def forward(self, x):\n",
        "    x, (hidden_n, cell_n) = self.rnn1(x)\n",
        "    x, (hidden_n, cell_n) = self.rnn2(x)\n",
        "    #print(\"in decoder\", x.shape)\n",
        "    x = x.contiguous()\n",
        "    x = x.view(x.shape[0], -1)\n",
        "    x = self.output_layer(x)\n",
        "    return x.reshape(x.shape[0],self.seq_len, self.n_features)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mqrvU5MZfEA"
      },
      "source": [
        "class RecurrentAutoencoder(torch.nn.Module):\n",
        "  def __init__(self, seq_len, n_features, embedding_dim=64, device='cpu'):\n",
        "    super(RecurrentAutoencoder, self).__init__()\n",
        "    self.encoder = Encoder(seq_len, n_features, embedding_dim).to(device)\n",
        "    self.decoder = Decoder(seq_len, embedding_dim, n_features).to(device)\n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)\n",
        "    x = self.decoder(x)\n",
        "    return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRQ-t9aNZiUD"
      },
      "source": [
        "x = torch.randn(10, 26, 400)\n",
        "print(x.shape)\n",
        "x = x.permute(0, 2, 1)\n",
        "print(x.shape)\n",
        "\n",
        "encoder = Encoder(400, 26, embedding_dim=64, kernel_size=3, stride=1)\n",
        "encoded = encoder(x)\n",
        "print(encoded.shape)\n",
        "\n",
        "decoder = Decoder(400, 64, 26)\n",
        "decoded = decoder(encoded)\n",
        "print(decoded.shape)\n",
        "\n",
        "rae = RecurrentAutoencoder(400, 26, 64)\n",
        "output = rae(x)\n",
        "\n",
        "print(output.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDpec6ICZnKN"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data.sampler import SequentialSampler\n",
        "\n",
        "class CustomDatasetMel(Dataset):\n",
        "\n",
        "    def __init__(self, dataList):\n",
        "        self.data = dataList\n",
        "        #self.labels = labelList\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        import numpy as np\n",
        "        fileName = self.data[index]\n",
        "        \n",
        "        mel_spect = np.load(fileName)\n",
        "        data = torch.tensor(mel_spect[:,:200], dtype=torch.float)\n",
        "        data = data.permute(1, 0)\n",
        "        #data = torch.unsqueeze(data, dim =0)\n",
        "\n",
        "        #label = torch.tensor(self.labels[index])\n",
        "        return data"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uAv1gLc6Zwwr",
        "outputId": "95cb31dd-ba4e-48bb-937c-1fda758a6142"
      },
      "source": [
        "import copy\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "class TrainingWrapper:\n",
        "\n",
        "  def __init__(self, config, training_loader, test_loader, device, val_loader=None, cross=10):\n",
        "    self.config = config\n",
        "    self.training_loader = training_loader\n",
        "    self.test_loader = test_loader\n",
        "    self.val_loader = val_loader\n",
        "    self.device = device\n",
        "    self.model = RecurrentAutoencoder(self.config.seq_len, self.config.input_dim, self.config.embedding_dim, device=self.device)\n",
        "    self.model = self.model.to(self.device)\n",
        "    self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.lr)\n",
        "    self.criterion = self.config.loss_function.to(self.device)\n",
        "    self.history = dict(train=[], val=[], cross_val=[])\n",
        "    self.best_model_wts = copy.deepcopy(self.model.state_dict())\n",
        "    self.best_loss = 10000.0\n",
        "    #print(self.config.base_dir + self.config.model_file)\n",
        "    torch.save(self.model.state_dict(),  self.config.model_file)\n",
        "    self.cross = cross\n",
        "    \n",
        "\n",
        "  def combine_images(self, generated_images):\n",
        "    num = generated_images.shape[0]\n",
        "    width = int(math.sqrt(num))\n",
        "    height = int(math.ceil(float(num)/width))\n",
        "    shape = generated_images.shape[1:3]\n",
        "    image = np.zeros((height*shape[0], width*shape[1]),\n",
        "                     dtype=generated_images.dtype)\n",
        "    for index, img in enumerate(generated_images):\n",
        "        i = int(index/width)\n",
        "        j = index % width\n",
        "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
        "            img[:, :, 0]\n",
        "    return image\n",
        "\n",
        "\n",
        "  def show_reconstruction(self, test_loader, n_images):\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    from PIL import Image\n",
        "    import numpy as np\n",
        "\n",
        "    self.model.eval()\n",
        "    for x, _ in self.test_loader:\n",
        "        x = x[:min(n_images, x.size(0))].to(self,device)\n",
        "        _, x_recon = self.model(x)\n",
        "        data = np.concatenate([x.data.cpu(), x_recon.data.cpu()])\n",
        "        img = self.combine_images(np.transpose(data, [0, 2, 3, 1]))\n",
        "        image = img * 255\n",
        "        Image.fromarray(image.astype(np.uint8)).save(self.config.base_dir + \"/real_and_recon.png\")\n",
        "        print()\n",
        "        print('Reconstructed images are saved to %s/real_and_recon.png' % self.config.base_dir)\n",
        "        print('-' * 70)\n",
        "        plt.imshow(plt.imread(self.config.base_dir + \"/real_and_recon.png\", ))\n",
        "        plt.show()\n",
        "        break\n",
        "\n",
        "  def visualizeTraining(self, epoch, trn_losses, tst_losses, val_losses, save_dir,cross):\n",
        "    # visualize the loss as the network trained\n",
        "    import matplotlib.pyplot as plt\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    plt.plot(range(0, len(trn_losses)), trn_losses, label='Training Loss')\n",
        "    if tst_losses:\n",
        "      plt.plot(range(0, len(tst_losses)), tst_losses, label='Validation Loss')\n",
        "    if val_losses:\n",
        "      plt.plot(range(0, len(val_losses)), val_losses, label='Cross Validation Loss')\n",
        "\n",
        "    minposs = tst_losses.index(min(tst_losses))\n",
        "    plt.axvline(minposs, linestyle='--', color='r', label='Early Stopping Checkpoint')\n",
        "\n",
        "\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('loss')\n",
        "    # plt.ylim(0, 0.5)  # consistent scale\n",
        "    # plt.xlim(0, len(trn_losses))  # consistent scale\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    fig.savefig(os.path.join(save_dir , 'loss_plot_{}.png'.format(cross)), bbox_inches='tight')\n",
        "\n",
        "  def train(self):\n",
        "    self.model.load_state_dict(torch.load(config.checkpoint_model_file))\n",
        "    for epoch in range(1, self.config.n_epochs + 1):\n",
        "      self.model = self.model.train()\n",
        "      train_losses = []\n",
        "      for i, data in enumerate(self.training_loader,0):\n",
        "        x = data\n",
        "        self.optimizer.zero_grad()\n",
        "        x = x.to(self.device)\n",
        "\n",
        "        output = self.model(x)\n",
        "        loss = self.criterion(output, x)\n",
        "\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        train_losses.append(loss.item())\n",
        "        print(\"in training loop, epoch {}, step {}, the loss is {}\".format(epoch, i, loss.item()))\n",
        "\n",
        "      val_losses = []\n",
        "      self.model = self.model.eval()\n",
        "      with torch.no_grad():\n",
        "        for i, data in enumerate(self.test_loader):\n",
        "          x = data\n",
        "          x = x.to(device)\n",
        "          output = self.model(x)\n",
        "          loss = self.criterion(output, x)\n",
        "          val_losses.append(loss.item())\n",
        "\n",
        "\n",
        "      cross_val_losses = []\n",
        "      self.model = self.model.eval()\n",
        "      with torch.no_grad():\n",
        "        for i, data in enumerate(self.val_loader):\n",
        "          x = data\n",
        "          x = x.to(device)\n",
        "          output = self.model(x)\n",
        "          loss = self.criterion(output, x)\n",
        "          cross_val_losses.append(loss.item())\n",
        "\n",
        "      train_loss = np.mean(train_losses)\n",
        "      val_loss = np.mean(val_losses)\n",
        "      cross_val_loss = np.mean(cross_val_losses)\n",
        "\n",
        "\n",
        "      self.history['train'].append(train_loss)\n",
        "      self.history['val'].append(val_loss)\n",
        "      self.history['cross_val'].append(cross_val_loss)\n",
        "\n",
        "      if val_loss < self.best_loss:\n",
        "        self.best_loss = val_loss\n",
        "        self.best_model_wts = copy.deepcopy(self.model.state_dict())\n",
        "      torch.save(self.model.state_dict(),  self.config.checkpoint_model_file)\n",
        "      if epoch % 2 == 0:\n",
        "        self.visualizeTraining(epoch, trn_losses= self.history['train'], tst_losses=self.history['val'], val_losses =self.history['cross_val'], save_dir=self.config.base_dir + \"/results\",cross=fold)\n",
        "      print(f'k-fold {fold}:: Epoch {epoch}: train loss {train_loss} val loss {val_loss}')\n",
        "    self.model.load_state_dict(self.best_model_wts)\n",
        "    torch.save(self.model.state_dict(), self.config.model_file)\n",
        "    return self.model.eval(), self.history\n",
        "\n",
        "  \n",
        "class TestingWrapper:\n",
        "  def __init__(self, config, device):\n",
        "    self.config = config\n",
        "    self.device = device\n",
        "    self.model = RecurrentAutoencoder(self.config.seq_len, self.config.input_dim, self.config.embedding_dim, device=self.device)\n",
        "    PATH =  self.config.checkpoint_model_file\n",
        "    print(PATH)\n",
        "    self.model.load_state_dict(torch.load(PATH, map_location=self.device))\n",
        "    self.model = self.model.to(self.device)\n",
        "\n",
        "  def combine_images(self, generated_images):\n",
        "    num = generated_images.shape[0]\n",
        "    width = int(math.sqrt(num))\n",
        "    height = int(math.ceil(float(num)/width))\n",
        "    shape = generated_images.shape[1:3]\n",
        "    image = np.zeros((height*shape[0], width*shape[1]),\n",
        "                     dtype=generated_images.dtype)\n",
        "    for index, img in enumerate(generated_images):\n",
        "        i = int(index/width)\n",
        "        j = index % width\n",
        "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
        "            img[:, :, 0]\n",
        "    return image\n",
        "\n",
        "\n",
        "  def show_reconstruction(self, test_loader, n_images):\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    from PIL import Image\n",
        "    import numpy as np\n",
        "\n",
        "    self.model.eval()\n",
        "    for x, _ in test_loader:\n",
        "      x = x[:min(n_images, x.size(0))].to(self.device)\n",
        "      x_recon = self.model(x)\n",
        "      data = np.concatenate([x.data.cpu(), x_recon.data.cpu()])\n",
        "      img = self.combine_images(np.transpose(data, [0, 2, 3, 1]))\n",
        "      image = img * 255\n",
        "      Image.fromarray(image.astype(np.uint8)).save(self.config.base_dir + \"/real_and_recon.png\")\n",
        "      print()\n",
        "      print('Reconstructed images are saved to %s/real_and_recon.png' % self.config.base_dir)\n",
        "      print('-' * 70)\n",
        "      plt.imshow(plt.imread(self.config.base_dir + \"/real_and_recon.png\", ))\n",
        "      plt.show()\n",
        "      break\n",
        "\n",
        "  def save_reconstruction(self, test_loader):\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    from PIL import Image\n",
        "    import numpy as np\n",
        "    import uuid\n",
        "\n",
        "\n",
        "    self.model.eval()\n",
        "    with torch.no_grad(): \n",
        "      fileCount = 0\n",
        "      for x, _ in test_loader:\n",
        "        x = x.to(self.device)\n",
        "        x_recon = self.model(x)\n",
        "        x_recon = x_recon.data.cpu().detach().numpy()\n",
        "        for mel in x_recon:\n",
        "          #print(mel.shape)\n",
        "          unique_filename = str(uuid.uuid4())\n",
        "          filename = self.config.base_dir + \"/reconstruction/\" + unique_filename + \".npy\"\n",
        "          np.save(filename, mel)\n",
        "          fileCount = fileCount + 1\n",
        "          print(\"saving file {} at index {}\".format(filename, fileCount))\n",
        "\n",
        "mode = 'train'\n",
        "data = \"mel\"\n",
        "#data = \"mnist\"\n",
        "config = Configuration()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import random\n",
        "import os\n",
        "\n",
        "def seed_everything(seed=1234):\n",
        "  \n",
        "  random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  np.random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def visualizeTraining(epoch, trn_losses, tst_losses, val_losses, save_dir):\n",
        "    # visualize the loss as the network trained\n",
        "    import matplotlib.pyplot as plt\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    plt.plot(range(0, len(trn_losses)), trn_losses, label='Training Loss')\n",
        "    #if tst_losses:\n",
        "    plt.plot(range(0, len(tst_losses)), tst_losses, label='Validation Loss')\n",
        "    #if val_losses:\n",
        "    plt.plot(range(0, len(val_losses)), val_losses, label='Cross Validation Loss')\n",
        "\n",
        "    #minposs = tst_losses.index(min(tst_losses))\n",
        "    #plt.axvline(minposs, linestyle='--', color='r', label='Early Stopping Checkpoint')\n",
        "\n",
        "\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('loss')\n",
        "    # plt.ylim(0, 0.5)  # consistent scale\n",
        "    # plt.xlim(0, len(trn_losses))  # consistent scale\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    fig.savefig(os.path.join(save_dir , 'loss_plot_{}.png'.format(\"MEAN\")), bbox_inches='tight')\n",
        "\n",
        "seed_everything()\n",
        "\n",
        "train_data = bal_western_files\n",
        "#labels = [1] * len(bal_western_files)\n",
        "\n",
        "val_data = indian_files\n",
        "#val_labels = [1] * len(indian_files)\n",
        "\n",
        "train_loss_mean_list = []\n",
        "test_loss_mean_list = []\n",
        "val_loss_mean_list = []\n",
        "\n",
        "# Cross validation runs\n",
        "# use sklearn KFolds\n",
        "from sklearn.model_selection import KFold\n",
        "kfold = KFold(n_splits=config.k_folds , shuffle=True)\n",
        "\n",
        "train_dataset = CustomDatasetMel(train_data)\n",
        "val_dataset = CustomDatasetMel(val_data)\n",
        "#Load the cross val dataset which is Full Indian dataset\n",
        "#It is identical for all K-folds\n",
        "crossval_loader = torch.utils.data.DataLoader(\n",
        "                      val_dataset,\n",
        "                      batch_size=config.batch_size, \n",
        "                      sampler=SequentialSampler(val_dataset), \n",
        "                      drop_last=False)  \n",
        "\n",
        "for fold, (train_ids, test_ids) in enumerate(kfold.split(train_dataset)):\n",
        "    # Print\n",
        "  print(f'FOLD {fold}')\n",
        "  print('--------------------------------')\n",
        "    \n",
        "    # Sample elements randomly from a given list of ids, no replacement.\n",
        "  train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "  test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
        "  \n",
        "    # Define data loaders for training and testing data in this fold\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "                      train_dataset, \n",
        "                      batch_size=config.batch_size,\n",
        "                      sampler=train_subsampler,\n",
        "                      drop_last=False)\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "                      train_dataset,\n",
        "                      batch_size=config.batch_size,\n",
        "                      sampler=test_subsampler,\n",
        "                      drop_last=False)\n",
        "    \n",
        "  print(\"length of of train_loader is {} & length of traindataset is {}\".format(len(train_loader),len(train_dataset)))\n",
        "  print(\"length of of test_loader is {}\".format(len(test_loader)))\n",
        "  print(\"length of of val_loader is {}\".format(len(crossval_loader)))\n",
        "    \n",
        "  if mode==\"train\":\n",
        "    trainingWrapper = TrainingWrapper(config=config, training_loader=train_loader, test_loader=test_loader, device=device, val_loader=crossval_loader, cross=fold)\n",
        "    model, history = trainingWrapper.train()\n",
        "    np.append(train_loss_mean_list,history['train'])\n",
        "    np.append(test_loss_mean_list,history['val'])\n",
        "    np.append(val_loss_mean_list,history['cross_val'])\n",
        "    \n",
        "\n",
        "    if data==\"mnist\":\n",
        "      #trainingWrapper.show_reconstruction(test_loader=test_loader, n_images=50)\n",
        "      pass\n",
        "\n",
        "  elif mode==\"test\":\n",
        "    testWrapper = TestingWrapper(config=config, device=device)\n",
        "    testWrapper.save_reconstruction(test_loader)\n",
        "  \n",
        "  train_loss_mean_list = np.mean(train_loss_mean_list, axis=0)\n",
        "  test_loss_mean_list = np.mean(test_loss_mean_list, axis=0)\n",
        "  val_loss_mean_list = np.mean(val_loss_mean_list, axis=0)\n",
        "  print(\"train_loss_mean_list\",train_loss_mean_list)\n",
        "  print(\"test_loss_mean_list\",test_loss_mean_list)\n",
        "  print(\"val_loss_mean_list\",val_loss_mean_list)\n",
        "  visualizeTraining(0, train_loss_mean_list, test_loss_mean_list, val_loss_mean_list, save_dir = config.base_dir + \"/results\")\n",
        "\n",
        "  "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOLD 0\n",
            "--------------------------------\n",
            "length of of train_loader is 904 & length of traindataset is 2008\n",
            "length of of test_loader is 101\n",
            "length of of val_loader is 1004\n",
            "in training loop, epoch 1, step 0, the loss is 228926.84375\n",
            "in training loop, epoch 1, step 1, the loss is 394696.6875\n",
            "in training loop, epoch 1, step 2, the loss is 137517.65625\n",
            "in training loop, epoch 1, step 3, the loss is 240893.65625\n",
            "in training loop, epoch 1, step 4, the loss is 347350.125\n",
            "in training loop, epoch 1, step 5, the loss is 243719.71875\n",
            "in training loop, epoch 1, step 6, the loss is 211065.6875\n",
            "in training loop, epoch 1, step 7, the loss is 300800.875\n",
            "in training loop, epoch 1, step 8, the loss is 396990.40625\n",
            "in training loop, epoch 1, step 9, the loss is 283740.65625\n",
            "in training loop, epoch 1, step 10, the loss is 170806.65625\n",
            "in training loop, epoch 1, step 11, the loss is 363535.75\n",
            "in training loop, epoch 1, step 12, the loss is 398992.96875\n",
            "in training loop, epoch 1, step 13, the loss is 304943.9375\n",
            "in training loop, epoch 1, step 14, the loss is 172212.0\n",
            "in training loop, epoch 1, step 15, the loss is 293068.3125\n",
            "in training loop, epoch 1, step 16, the loss is 334026.1875\n",
            "in training loop, epoch 1, step 17, the loss is 268071.375\n",
            "in training loop, epoch 1, step 18, the loss is 245960.390625\n",
            "in training loop, epoch 1, step 19, the loss is 504234.25\n",
            "in training loop, epoch 1, step 20, the loss is 313656.8125\n",
            "in training loop, epoch 1, step 21, the loss is 178567.046875\n",
            "in training loop, epoch 1, step 22, the loss is 315007.875\n",
            "in training loop, epoch 1, step 23, the loss is 360262.5625\n",
            "in training loop, epoch 1, step 24, the loss is 341490.84375\n",
            "in training loop, epoch 1, step 25, the loss is 277145.46875\n",
            "in training loop, epoch 1, step 26, the loss is 322137.875\n",
            "in training loop, epoch 1, step 27, the loss is 259372.015625\n",
            "in training loop, epoch 1, step 28, the loss is 277390.03125\n",
            "in training loop, epoch 1, step 29, the loss is 382483.125\n",
            "in training loop, epoch 1, step 30, the loss is 201593.6875\n",
            "in training loop, epoch 1, step 31, the loss is 312861.96875\n",
            "in training loop, epoch 1, step 32, the loss is 246493.828125\n",
            "in training loop, epoch 1, step 33, the loss is 458125.375\n",
            "in training loop, epoch 1, step 34, the loss is 220337.453125\n",
            "in training loop, epoch 1, step 35, the loss is 194227.546875\n",
            "in training loop, epoch 1, step 36, the loss is 304799.0\n",
            "in training loop, epoch 1, step 37, the loss is 300257.375\n",
            "in training loop, epoch 1, step 38, the loss is 374112.78125\n",
            "in training loop, epoch 1, step 39, the loss is 192798.140625\n",
            "in training loop, epoch 1, step 40, the loss is 191130.4375\n",
            "in training loop, epoch 1, step 41, the loss is 318264.34375\n",
            "in training loop, epoch 1, step 42, the loss is 388269.71875\n",
            "in training loop, epoch 1, step 43, the loss is 294862.40625\n",
            "in training loop, epoch 1, step 44, the loss is 249748.765625\n",
            "in training loop, epoch 1, step 45, the loss is 280346.90625\n",
            "in training loop, epoch 1, step 46, the loss is 251631.671875\n",
            "in training loop, epoch 1, step 47, the loss is 257162.921875\n",
            "in training loop, epoch 1, step 48, the loss is 180550.921875\n",
            "in training loop, epoch 1, step 49, the loss is 257221.953125\n",
            "in training loop, epoch 1, step 50, the loss is 288450.90625\n",
            "in training loop, epoch 1, step 51, the loss is 306747.46875\n",
            "in training loop, epoch 1, step 52, the loss is 408286.96875\n",
            "in training loop, epoch 1, step 53, the loss is 277541.125\n",
            "in training loop, epoch 1, step 54, the loss is 249342.390625\n",
            "in training loop, epoch 1, step 55, the loss is 312830.03125\n",
            "in training loop, epoch 1, step 56, the loss is 506976.5625\n",
            "in training loop, epoch 1, step 57, the loss is 300236.8125\n",
            "in training loop, epoch 1, step 58, the loss is 164143.9375\n",
            "in training loop, epoch 1, step 59, the loss is 403200.59375\n",
            "in training loop, epoch 1, step 60, the loss is 434348.78125\n",
            "in training loop, epoch 1, step 61, the loss is 206389.328125\n",
            "in training loop, epoch 1, step 62, the loss is 409647.21875\n",
            "in training loop, epoch 1, step 63, the loss is 251950.84375\n",
            "in training loop, epoch 1, step 64, the loss is 292968.9375\n",
            "in training loop, epoch 1, step 65, the loss is 218416.40625\n",
            "in training loop, epoch 1, step 66, the loss is 431640.0625\n",
            "in training loop, epoch 1, step 67, the loss is 307503.0625\n",
            "in training loop, epoch 1, step 68, the loss is 338253.125\n",
            "in training loop, epoch 1, step 69, the loss is 151607.359375\n",
            "in training loop, epoch 1, step 70, the loss is 333509.125\n",
            "in training loop, epoch 1, step 71, the loss is 259848.078125\n",
            "in training loop, epoch 1, step 72, the loss is 459401.8125\n",
            "in training loop, epoch 1, step 73, the loss is 425875.8125\n",
            "in training loop, epoch 1, step 74, the loss is 163868.953125\n",
            "in training loop, epoch 1, step 75, the loss is 175593.671875\n",
            "in training loop, epoch 1, step 76, the loss is 421046.6875\n",
            "in training loop, epoch 1, step 77, the loss is 237013.515625\n",
            "in training loop, epoch 1, step 78, the loss is 235703.46875\n",
            "in training loop, epoch 1, step 79, the loss is 271436.3125\n",
            "in training loop, epoch 1, step 80, the loss is 417827.84375\n",
            "in training loop, epoch 1, step 81, the loss is 358019.625\n",
            "in training loop, epoch 1, step 82, the loss is 375121.0\n",
            "in training loop, epoch 1, step 83, the loss is 340552.90625\n",
            "in training loop, epoch 1, step 84, the loss is 288683.21875\n",
            "in training loop, epoch 1, step 85, the loss is 292191.625\n",
            "in training loop, epoch 1, step 86, the loss is 309765.1875\n",
            "in training loop, epoch 1, step 87, the loss is 287753.5625\n",
            "in training loop, epoch 1, step 88, the loss is 342348.375\n",
            "in training loop, epoch 1, step 89, the loss is 755130.625\n",
            "in training loop, epoch 1, step 90, the loss is 243413.71875\n",
            "in training loop, epoch 1, step 91, the loss is 206297.71875\n",
            "in training loop, epoch 1, step 92, the loss is 385546.28125\n",
            "in training loop, epoch 1, step 93, the loss is 196291.0\n",
            "in training loop, epoch 1, step 94, the loss is 333149.8125\n",
            "in training loop, epoch 1, step 95, the loss is 256291.078125\n",
            "in training loop, epoch 1, step 96, the loss is 443248.25\n",
            "in training loop, epoch 1, step 97, the loss is 225847.78125\n",
            "in training loop, epoch 1, step 98, the loss is 488930.4375\n",
            "in training loop, epoch 1, step 99, the loss is 356902.5\n",
            "in training loop, epoch 1, step 100, the loss is 146018.453125\n",
            "in training loop, epoch 1, step 101, the loss is 388649.125\n",
            "in training loop, epoch 1, step 102, the loss is 190816.359375\n",
            "in training loop, epoch 1, step 103, the loss is 224079.328125\n",
            "in training loop, epoch 1, step 104, the loss is 207612.359375\n",
            "in training loop, epoch 1, step 105, the loss is 361980.4375\n",
            "in training loop, epoch 1, step 106, the loss is 163073.734375\n",
            "in training loop, epoch 1, step 107, the loss is 478815.65625\n",
            "in training loop, epoch 1, step 108, the loss is 383953.96875\n",
            "in training loop, epoch 1, step 109, the loss is 360121.1875\n",
            "in training loop, epoch 1, step 110, the loss is 233806.890625\n",
            "in training loop, epoch 1, step 111, the loss is 302607.8125\n",
            "in training loop, epoch 1, step 112, the loss is 418538.5\n",
            "in training loop, epoch 1, step 113, the loss is 194086.59375\n",
            "in training loop, epoch 1, step 114, the loss is 246400.75\n",
            "in training loop, epoch 1, step 115, the loss is 342018.96875\n",
            "in training loop, epoch 1, step 116, the loss is 293829.96875\n",
            "in training loop, epoch 1, step 117, the loss is 215421.53125\n",
            "in training loop, epoch 1, step 118, the loss is 308387.78125\n",
            "in training loop, epoch 1, step 119, the loss is 182444.96875\n",
            "in training loop, epoch 1, step 120, the loss is 173391.953125\n",
            "in training loop, epoch 1, step 121, the loss is 252173.171875\n",
            "in training loop, epoch 1, step 122, the loss is 213619.65625\n",
            "in training loop, epoch 1, step 123, the loss is 464271.0\n",
            "in training loop, epoch 1, step 124, the loss is 249123.03125\n",
            "in training loop, epoch 1, step 125, the loss is 262286.03125\n",
            "in training loop, epoch 1, step 126, the loss is 320174.3125\n",
            "in training loop, epoch 1, step 127, the loss is 231825.421875\n",
            "in training loop, epoch 1, step 128, the loss is 263885.59375\n",
            "in training loop, epoch 1, step 129, the loss is 401420.15625\n",
            "in training loop, epoch 1, step 130, the loss is 143063.34375\n",
            "in training loop, epoch 1, step 131, the loss is 246911.953125\n",
            "in training loop, epoch 1, step 132, the loss is 267589.8125\n",
            "in training loop, epoch 1, step 133, the loss is 190618.8125\n",
            "in training loop, epoch 1, step 134, the loss is 218634.84375\n",
            "in training loop, epoch 1, step 135, the loss is 229177.34375\n",
            "in training loop, epoch 1, step 136, the loss is 381797.21875\n",
            "in training loop, epoch 1, step 137, the loss is 210158.1875\n",
            "in training loop, epoch 1, step 138, the loss is 388049.125\n",
            "in training loop, epoch 1, step 139, the loss is 321041.125\n",
            "in training loop, epoch 1, step 140, the loss is 415886.375\n",
            "in training loop, epoch 1, step 141, the loss is 377921.65625\n",
            "in training loop, epoch 1, step 142, the loss is 224680.328125\n",
            "in training loop, epoch 1, step 143, the loss is 186613.375\n",
            "in training loop, epoch 1, step 144, the loss is 429626.6875\n",
            "in training loop, epoch 1, step 145, the loss is 303169.5\n",
            "in training loop, epoch 1, step 146, the loss is 238385.453125\n",
            "in training loop, epoch 1, step 147, the loss is 239498.171875\n",
            "in training loop, epoch 1, step 148, the loss is 264329.5\n",
            "in training loop, epoch 1, step 149, the loss is 344002.65625\n",
            "in training loop, epoch 1, step 150, the loss is 289622.3125\n",
            "in training loop, epoch 1, step 151, the loss is 264924.90625\n",
            "in training loop, epoch 1, step 152, the loss is 222148.140625\n",
            "in training loop, epoch 1, step 153, the loss is 291341.84375\n",
            "in training loop, epoch 1, step 154, the loss is 157217.171875\n",
            "in training loop, epoch 1, step 155, the loss is 107030.15625\n",
            "in training loop, epoch 1, step 156, the loss is 250785.46875\n",
            "in training loop, epoch 1, step 157, the loss is 362315.5625\n",
            "in training loop, epoch 1, step 158, the loss is 273456.15625\n",
            "in training loop, epoch 1, step 159, the loss is 279347.5\n",
            "in training loop, epoch 1, step 160, the loss is 281323.34375\n",
            "in training loop, epoch 1, step 161, the loss is 660592.125\n",
            "in training loop, epoch 1, step 162, the loss is 301697.5625\n",
            "in training loop, epoch 1, step 163, the loss is 268702.5\n",
            "in training loop, epoch 1, step 164, the loss is 223840.015625\n",
            "in training loop, epoch 1, step 165, the loss is 291399.875\n",
            "in training loop, epoch 1, step 166, the loss is 205982.09375\n",
            "in training loop, epoch 1, step 167, the loss is 406294.65625\n",
            "in training loop, epoch 1, step 168, the loss is 235098.65625\n",
            "in training loop, epoch 1, step 169, the loss is 256258.109375\n",
            "in training loop, epoch 1, step 170, the loss is 382606.375\n",
            "in training loop, epoch 1, step 171, the loss is 215888.46875\n",
            "in training loop, epoch 1, step 172, the loss is 232493.75\n",
            "in training loop, epoch 1, step 173, the loss is 317174.6875\n",
            "in training loop, epoch 1, step 174, the loss is 221020.734375\n",
            "in training loop, epoch 1, step 175, the loss is 422556.125\n",
            "in training loop, epoch 1, step 176, the loss is 224280.265625\n",
            "in training loop, epoch 1, step 177, the loss is 241746.921875\n",
            "in training loop, epoch 1, step 178, the loss is 406084.40625\n",
            "in training loop, epoch 1, step 179, the loss is 174613.1875\n",
            "in training loop, epoch 1, step 180, the loss is 189398.078125\n",
            "in training loop, epoch 1, step 181, the loss is 556132.625\n",
            "in training loop, epoch 1, step 182, the loss is 398879.4375\n",
            "in training loop, epoch 1, step 183, the loss is 193825.765625\n",
            "in training loop, epoch 1, step 184, the loss is 290670.90625\n",
            "in training loop, epoch 1, step 185, the loss is 331998.5625\n",
            "in training loop, epoch 1, step 186, the loss is 192139.71875\n",
            "in training loop, epoch 1, step 187, the loss is 379807.4375\n",
            "in training loop, epoch 1, step 188, the loss is 246887.796875\n",
            "in training loop, epoch 1, step 189, the loss is 513958.09375\n",
            "in training loop, epoch 1, step 190, the loss is 210220.140625\n",
            "in training loop, epoch 1, step 191, the loss is 290517.53125\n",
            "in training loop, epoch 1, step 192, the loss is 206673.796875\n",
            "in training loop, epoch 1, step 193, the loss is 158700.875\n",
            "in training loop, epoch 1, step 194, the loss is 422353.71875\n",
            "in training loop, epoch 1, step 195, the loss is 338857.71875\n",
            "in training loop, epoch 1, step 196, the loss is 391609.59375\n",
            "in training loop, epoch 1, step 197, the loss is 118509.21875\n",
            "in training loop, epoch 1, step 198, the loss is 293676.78125\n",
            "in training loop, epoch 1, step 199, the loss is 293869.84375\n",
            "in training loop, epoch 1, step 200, the loss is 257991.265625\n",
            "in training loop, epoch 1, step 201, the loss is 301831.625\n",
            "in training loop, epoch 1, step 202, the loss is 237429.578125\n",
            "in training loop, epoch 1, step 203, the loss is 420155.96875\n",
            "in training loop, epoch 1, step 204, the loss is 285356.75\n",
            "in training loop, epoch 1, step 205, the loss is 269302.84375\n",
            "in training loop, epoch 1, step 206, the loss is 408841.5\n",
            "in training loop, epoch 1, step 207, the loss is 322147.75\n",
            "in training loop, epoch 1, step 208, the loss is 218335.21875\n",
            "in training loop, epoch 1, step 209, the loss is 345642.8125\n",
            "in training loop, epoch 1, step 210, the loss is 212619.140625\n",
            "in training loop, epoch 1, step 211, the loss is 265819.5625\n",
            "in training loop, epoch 1, step 212, the loss is 374209.3125\n",
            "in training loop, epoch 1, step 213, the loss is 223195.90625\n",
            "in training loop, epoch 1, step 214, the loss is 227360.234375\n",
            "in training loop, epoch 1, step 215, the loss is 352243.625\n",
            "in training loop, epoch 1, step 216, the loss is 211779.109375\n",
            "in training loop, epoch 1, step 217, the loss is 326578.03125\n",
            "in training loop, epoch 1, step 218, the loss is 259820.5\n",
            "in training loop, epoch 1, step 219, the loss is 476501.8125\n",
            "in training loop, epoch 1, step 220, the loss is 448911.84375\n",
            "in training loop, epoch 1, step 221, the loss is 285234.3125\n",
            "in training loop, epoch 1, step 222, the loss is 187407.640625\n",
            "in training loop, epoch 1, step 223, the loss is 287616.875\n",
            "in training loop, epoch 1, step 224, the loss is 383152.9375\n",
            "in training loop, epoch 1, step 225, the loss is 210896.328125\n",
            "in training loop, epoch 1, step 226, the loss is 380288.53125\n",
            "in training loop, epoch 1, step 227, the loss is 257716.328125\n",
            "in training loop, epoch 1, step 228, the loss is 345570.03125\n",
            "in training loop, epoch 1, step 229, the loss is 334101.1875\n",
            "in training loop, epoch 1, step 230, the loss is 109982.8828125\n",
            "in training loop, epoch 1, step 231, the loss is 450855.96875\n",
            "in training loop, epoch 1, step 232, the loss is 216564.0\n",
            "in training loop, epoch 1, step 233, the loss is 285083.84375\n",
            "in training loop, epoch 1, step 234, the loss is 295647.71875\n",
            "in training loop, epoch 1, step 235, the loss is 335530.5625\n",
            "in training loop, epoch 1, step 236, the loss is 172725.828125\n",
            "in training loop, epoch 1, step 237, the loss is 298904.96875\n",
            "in training loop, epoch 1, step 238, the loss is 228327.484375\n",
            "in training loop, epoch 1, step 239, the loss is 327884.375\n",
            "in training loop, epoch 1, step 240, the loss is 190816.625\n",
            "in training loop, epoch 1, step 241, the loss is 223399.359375\n",
            "in training loop, epoch 1, step 242, the loss is 310118.9375\n",
            "in training loop, epoch 1, step 243, the loss is 423851.5625\n",
            "in training loop, epoch 1, step 244, the loss is 271782.53125\n",
            "in training loop, epoch 1, step 245, the loss is 227096.65625\n",
            "in training loop, epoch 1, step 246, the loss is 279806.09375\n",
            "in training loop, epoch 1, step 247, the loss is 219004.5\n",
            "in training loop, epoch 1, step 248, the loss is 175158.046875\n",
            "in training loop, epoch 1, step 249, the loss is 272444.9375\n",
            "in training loop, epoch 1, step 250, the loss is 287161.84375\n",
            "in training loop, epoch 1, step 251, the loss is 136426.53125\n",
            "in training loop, epoch 1, step 252, the loss is 287376.09375\n",
            "in training loop, epoch 1, step 253, the loss is 308226.9375\n",
            "in training loop, epoch 1, step 254, the loss is 254446.78125\n",
            "in training loop, epoch 1, step 255, the loss is 209107.671875\n",
            "in training loop, epoch 1, step 256, the loss is 388403.0\n",
            "in training loop, epoch 1, step 257, the loss is 291965.25\n",
            "in training loop, epoch 1, step 258, the loss is 290266.875\n",
            "in training loop, epoch 1, step 259, the loss is 418751.03125\n",
            "in training loop, epoch 1, step 260, the loss is 299805.5625\n",
            "in training loop, epoch 1, step 261, the loss is 1031129.0\n",
            "in training loop, epoch 1, step 262, the loss is 293345.40625\n",
            "in training loop, epoch 1, step 263, the loss is 394829.375\n",
            "in training loop, epoch 1, step 264, the loss is 225216.65625\n",
            "in training loop, epoch 1, step 265, the loss is 165678.28125\n",
            "in training loop, epoch 1, step 266, the loss is 300331.25\n",
            "in training loop, epoch 1, step 267, the loss is 223191.84375\n",
            "in training loop, epoch 1, step 268, the loss is 315596.59375\n",
            "in training loop, epoch 1, step 269, the loss is 403428.21875\n",
            "in training loop, epoch 1, step 270, the loss is 264849.25\n",
            "in training loop, epoch 1, step 271, the loss is 248822.078125\n",
            "in training loop, epoch 1, step 272, the loss is 307289.34375\n",
            "in training loop, epoch 1, step 273, the loss is 281232.84375\n",
            "in training loop, epoch 1, step 274, the loss is 1025793.1875\n",
            "in training loop, epoch 1, step 275, the loss is 612095.875\n",
            "in training loop, epoch 1, step 276, the loss is 379360.34375\n",
            "in training loop, epoch 1, step 277, the loss is 498169.8125\n",
            "in training loop, epoch 1, step 278, the loss is 665263.9375\n",
            "in training loop, epoch 1, step 279, the loss is 320673.9375\n",
            "in training loop, epoch 1, step 280, the loss is 483448.84375\n",
            "in training loop, epoch 1, step 281, the loss is 479027.40625\n",
            "in training loop, epoch 1, step 282, the loss is 354627.15625\n",
            "in training loop, epoch 1, step 283, the loss is 504324.75\n",
            "in training loop, epoch 1, step 284, the loss is 337570.21875\n",
            "in training loop, epoch 1, step 285, the loss is 220281.125\n",
            "in training loop, epoch 1, step 286, the loss is 239479.109375\n",
            "in training loop, epoch 1, step 287, the loss is 511634.25\n",
            "in training loop, epoch 1, step 288, the loss is 508759.40625\n",
            "in training loop, epoch 1, step 289, the loss is 269767.875\n",
            "in training loop, epoch 1, step 290, the loss is 381326.1875\n",
            "in training loop, epoch 1, step 291, the loss is 251139.40625\n",
            "in training loop, epoch 1, step 292, the loss is 283103.0\n",
            "in training loop, epoch 1, step 293, the loss is 220184.765625\n",
            "in training loop, epoch 1, step 294, the loss is 380501.90625\n",
            "in training loop, epoch 1, step 295, the loss is 338685.5625\n",
            "in training loop, epoch 1, step 296, the loss is 253974.984375\n",
            "in training loop, epoch 1, step 297, the loss is 238256.90625\n",
            "in training loop, epoch 1, step 298, the loss is 287010.0\n",
            "in training loop, epoch 1, step 299, the loss is 314991.03125\n",
            "in training loop, epoch 1, step 300, the loss is 436347.1875\n",
            "in training loop, epoch 1, step 301, the loss is 349536.71875\n",
            "in training loop, epoch 1, step 302, the loss is 358541.40625\n",
            "in training loop, epoch 1, step 303, the loss is 401323.3125\n",
            "in training loop, epoch 1, step 304, the loss is 391599.09375\n",
            "in training loop, epoch 1, step 305, the loss is 284909.3125\n",
            "in training loop, epoch 1, step 306, the loss is 395745.15625\n",
            "in training loop, epoch 1, step 307, the loss is 576486.0\n",
            "in training loop, epoch 1, step 308, the loss is 369634.0625\n",
            "in training loop, epoch 1, step 309, the loss is 291479.1875\n",
            "in training loop, epoch 1, step 310, the loss is 183368.15625\n",
            "in training loop, epoch 1, step 311, the loss is 233452.359375\n",
            "in training loop, epoch 1, step 312, the loss is 277611.65625\n",
            "in training loop, epoch 1, step 313, the loss is 499668.5625\n",
            "in training loop, epoch 1, step 314, the loss is 435190.125\n",
            "in training loop, epoch 1, step 315, the loss is 242758.03125\n",
            "in training loop, epoch 1, step 316, the loss is 158029.59375\n",
            "in training loop, epoch 1, step 317, the loss is 352123.4375\n",
            "in training loop, epoch 1, step 318, the loss is 632760.625\n",
            "in training loop, epoch 1, step 319, the loss is 300372.53125\n",
            "in training loop, epoch 1, step 320, the loss is 397330.5625\n",
            "in training loop, epoch 1, step 321, the loss is 201362.875\n",
            "in training loop, epoch 1, step 322, the loss is 376111.21875\n",
            "in training loop, epoch 1, step 323, the loss is 365277.0\n",
            "in training loop, epoch 1, step 324, the loss is 183828.515625\n",
            "in training loop, epoch 1, step 325, the loss is 443949.375\n",
            "in training loop, epoch 1, step 326, the loss is 601551.6875\n",
            "in training loop, epoch 1, step 327, the loss is 376397.75\n",
            "in training loop, epoch 1, step 328, the loss is 292648.1875\n",
            "in training loop, epoch 1, step 329, the loss is 242194.84375\n",
            "in training loop, epoch 1, step 330, the loss is 210524.203125\n",
            "in training loop, epoch 1, step 331, the loss is 471079.21875\n",
            "in training loop, epoch 1, step 332, the loss is 297098.5625\n",
            "in training loop, epoch 1, step 333, the loss is 524974.5625\n",
            "in training loop, epoch 1, step 334, the loss is 355103.6875\n",
            "in training loop, epoch 1, step 335, the loss is 150621.5625\n",
            "in training loop, epoch 1, step 336, the loss is 152805.640625\n",
            "in training loop, epoch 1, step 337, the loss is 669038.125\n",
            "in training loop, epoch 1, step 338, the loss is 356793.34375\n",
            "in training loop, epoch 1, step 339, the loss is 608628.1875\n",
            "in training loop, epoch 1, step 340, the loss is 197087.46875\n",
            "in training loop, epoch 1, step 341, the loss is 342238.6875\n",
            "in training loop, epoch 1, step 342, the loss is 416150.03125\n",
            "in training loop, epoch 1, step 343, the loss is 463344.59375\n",
            "in training loop, epoch 1, step 344, the loss is 257936.0625\n",
            "in training loop, epoch 1, step 345, the loss is 478907.8125\n",
            "in training loop, epoch 1, step 346, the loss is 292225.0625\n",
            "in training loop, epoch 1, step 347, the loss is 296495.25\n",
            "in training loop, epoch 1, step 348, the loss is 300423.4375\n",
            "in training loop, epoch 1, step 349, the loss is 508575.28125\n",
            "in training loop, epoch 1, step 350, the loss is 442331.5625\n",
            "in training loop, epoch 1, step 351, the loss is 224964.25\n",
            "in training loop, epoch 1, step 352, the loss is 769807.0625\n",
            "in training loop, epoch 1, step 353, the loss is 185808.0625\n",
            "in training loop, epoch 1, step 354, the loss is 373212.625\n",
            "in training loop, epoch 1, step 355, the loss is 399371.28125\n",
            "in training loop, epoch 1, step 356, the loss is 304573.6875\n",
            "in training loop, epoch 1, step 357, the loss is 224896.328125\n",
            "in training loop, epoch 1, step 358, the loss is 240029.171875\n",
            "in training loop, epoch 1, step 359, the loss is 318108.0625\n",
            "in training loop, epoch 1, step 360, the loss is 226255.796875\n",
            "in training loop, epoch 1, step 361, the loss is 346882.125\n",
            "in training loop, epoch 1, step 362, the loss is 341433.0625\n",
            "in training loop, epoch 1, step 363, the loss is 252293.78125\n",
            "in training loop, epoch 1, step 364, the loss is 302116.4375\n",
            "in training loop, epoch 1, step 365, the loss is 268172.21875\n",
            "in training loop, epoch 1, step 366, the loss is 264184.96875\n",
            "in training loop, epoch 1, step 367, the loss is 165563.546875\n",
            "in training loop, epoch 1, step 368, the loss is 191260.734375\n",
            "in training loop, epoch 1, step 369, the loss is 184854.078125\n",
            "in training loop, epoch 1, step 370, the loss is 396940.59375\n",
            "in training loop, epoch 1, step 371, the loss is 303313.53125\n",
            "in training loop, epoch 1, step 372, the loss is 530809.125\n",
            "in training loop, epoch 1, step 373, the loss is 354677.5625\n",
            "in training loop, epoch 1, step 374, the loss is 387250.8125\n",
            "in training loop, epoch 1, step 375, the loss is 262130.609375\n",
            "in training loop, epoch 1, step 376, the loss is 100109.890625\n",
            "in training loop, epoch 1, step 377, the loss is 292229.09375\n",
            "in training loop, epoch 1, step 378, the loss is 178844.6875\n",
            "in training loop, epoch 1, step 379, the loss is 295123.9375\n",
            "in training loop, epoch 1, step 380, the loss is 170905.0625\n",
            "in training loop, epoch 1, step 381, the loss is 263511.65625\n",
            "in training loop, epoch 1, step 382, the loss is 238573.625\n",
            "in training loop, epoch 1, step 383, the loss is 389327.5\n",
            "in training loop, epoch 1, step 384, the loss is 277869.25\n",
            "in training loop, epoch 1, step 385, the loss is 191300.96875\n",
            "in training loop, epoch 1, step 386, the loss is 223827.421875\n",
            "in training loop, epoch 1, step 387, the loss is 231879.0625\n",
            "in training loop, epoch 1, step 388, the loss is 328259.375\n",
            "in training loop, epoch 1, step 389, the loss is 284861.125\n",
            "in training loop, epoch 1, step 390, the loss is 384766.625\n",
            "in training loop, epoch 1, step 391, the loss is 387073.25\n",
            "in training loop, epoch 1, step 392, the loss is 158287.40625\n",
            "in training loop, epoch 1, step 393, the loss is 245339.09375\n",
            "in training loop, epoch 1, step 394, the loss is 343755.53125\n",
            "in training loop, epoch 1, step 395, the loss is 261147.90625\n",
            "in training loop, epoch 1, step 396, the loss is 234842.5\n",
            "in training loop, epoch 1, step 397, the loss is 301520.0625\n",
            "in training loop, epoch 1, step 398, the loss is 477860.96875\n",
            "in training loop, epoch 1, step 399, the loss is 348937.6875\n",
            "in training loop, epoch 1, step 400, the loss is 205310.03125\n",
            "in training loop, epoch 1, step 401, the loss is 315066.09375\n",
            "in training loop, epoch 1, step 402, the loss is 577964.375\n",
            "in training loop, epoch 1, step 403, the loss is 347470.09375\n",
            "in training loop, epoch 1, step 404, the loss is 185415.71875\n",
            "in training loop, epoch 1, step 405, the loss is 338211.03125\n",
            "in training loop, epoch 1, step 406, the loss is 276389.375\n",
            "in training loop, epoch 1, step 407, the loss is 336928.375\n",
            "in training loop, epoch 1, step 408, the loss is 331976.59375\n",
            "in training loop, epoch 1, step 409, the loss is 272073.3125\n",
            "in training loop, epoch 1, step 410, the loss is 352426.4375\n",
            "in training loop, epoch 1, step 411, the loss is 259649.703125\n",
            "in training loop, epoch 1, step 412, the loss is 567799.8125\n",
            "in training loop, epoch 1, step 413, the loss is 575309.125\n",
            "in training loop, epoch 1, step 414, the loss is 221575.40625\n",
            "in training loop, epoch 1, step 415, the loss is 280528.4375\n",
            "in training loop, epoch 1, step 416, the loss is 542023.9375\n",
            "in training loop, epoch 1, step 417, the loss is 440429.125\n",
            "in training loop, epoch 1, step 418, the loss is 599350.875\n",
            "in training loop, epoch 1, step 419, the loss is 372460.3125\n",
            "in training loop, epoch 1, step 420, the loss is 404518.21875\n",
            "in training loop, epoch 1, step 421, the loss is 364983.90625\n",
            "in training loop, epoch 1, step 422, the loss is 395270.25\n",
            "in training loop, epoch 1, step 423, the loss is 226516.140625\n",
            "in training loop, epoch 1, step 424, the loss is 166090.8125\n",
            "in training loop, epoch 1, step 425, the loss is 261364.421875\n",
            "in training loop, epoch 1, step 426, the loss is 540626.875\n",
            "in training loop, epoch 1, step 427, the loss is 318267.4375\n",
            "in training loop, epoch 1, step 428, the loss is 334242.09375\n",
            "in training loop, epoch 1, step 429, the loss is 325172.125\n",
            "in training loop, epoch 1, step 430, the loss is 273103.53125\n",
            "in training loop, epoch 1, step 431, the loss is 189397.0\n",
            "in training loop, epoch 1, step 432, the loss is 418172.6875\n",
            "in training loop, epoch 1, step 433, the loss is 312605.8125\n",
            "in training loop, epoch 1, step 434, the loss is 168831.671875\n",
            "in training loop, epoch 1, step 435, the loss is 247301.40625\n",
            "in training loop, epoch 1, step 436, the loss is 175716.96875\n",
            "in training loop, epoch 1, step 437, the loss is 281796.0625\n",
            "in training loop, epoch 1, step 438, the loss is 168639.46875\n",
            "in training loop, epoch 1, step 439, the loss is 565251.125\n",
            "in training loop, epoch 1, step 440, the loss is 197762.5625\n",
            "in training loop, epoch 1, step 441, the loss is 242217.375\n",
            "in training loop, epoch 1, step 442, the loss is 300782.9375\n",
            "in training loop, epoch 1, step 443, the loss is 500121.5625\n",
            "in training loop, epoch 1, step 444, the loss is 321381.625\n",
            "in training loop, epoch 1, step 445, the loss is 252323.0625\n",
            "in training loop, epoch 1, step 446, the loss is 274758.59375\n",
            "in training loop, epoch 1, step 447, the loss is 482618.78125\n",
            "in training loop, epoch 1, step 448, the loss is 229964.84375\n",
            "in training loop, epoch 1, step 449, the loss is 240413.75\n",
            "in training loop, epoch 1, step 450, the loss is 585314.25\n",
            "in training loop, epoch 1, step 451, the loss is 293540.84375\n",
            "in training loop, epoch 1, step 452, the loss is 338763.4375\n",
            "in training loop, epoch 1, step 453, the loss is 281839.15625\n",
            "in training loop, epoch 1, step 454, the loss is 335834.84375\n",
            "in training loop, epoch 1, step 455, the loss is 333748.625\n",
            "in training loop, epoch 1, step 456, the loss is 284299.0\n",
            "in training loop, epoch 1, step 457, the loss is 250158.515625\n",
            "in training loop, epoch 1, step 458, the loss is 200470.484375\n",
            "in training loop, epoch 1, step 459, the loss is 76012.2890625\n",
            "in training loop, epoch 1, step 460, the loss is 383707.5625\n",
            "in training loop, epoch 1, step 461, the loss is 348924.125\n",
            "in training loop, epoch 1, step 462, the loss is 499314.1875\n",
            "in training loop, epoch 1, step 463, the loss is 240280.359375\n",
            "in training loop, epoch 1, step 464, the loss is 327011.53125\n",
            "in training loop, epoch 1, step 465, the loss is 251699.421875\n",
            "in training loop, epoch 1, step 466, the loss is 294057.03125\n",
            "in training loop, epoch 1, step 467, the loss is 201668.6875\n",
            "in training loop, epoch 1, step 468, the loss is 273837.125\n",
            "in training loop, epoch 1, step 469, the loss is 137010.921875\n",
            "in training loop, epoch 1, step 470, the loss is 155278.984375\n",
            "in training loop, epoch 1, step 471, the loss is 177781.140625\n",
            "in training loop, epoch 1, step 472, the loss is 229104.140625\n",
            "in training loop, epoch 1, step 473, the loss is 383024.25\n",
            "in training loop, epoch 1, step 474, the loss is 139429.046875\n",
            "in training loop, epoch 1, step 475, the loss is 484722.8125\n",
            "in training loop, epoch 1, step 476, the loss is 383268.8125\n",
            "in training loop, epoch 1, step 477, the loss is 227699.875\n",
            "in training loop, epoch 1, step 478, the loss is 196363.640625\n",
            "in training loop, epoch 1, step 479, the loss is 192112.15625\n",
            "in training loop, epoch 1, step 480, the loss is 188715.484375\n",
            "in training loop, epoch 1, step 481, the loss is 332116.9375\n",
            "in training loop, epoch 1, step 482, the loss is 173120.421875\n",
            "in training loop, epoch 1, step 483, the loss is 210249.875\n",
            "in training loop, epoch 1, step 484, the loss is 503404.875\n",
            "in training loop, epoch 1, step 485, the loss is 221346.421875\n",
            "in training loop, epoch 1, step 486, the loss is 527022.625\n",
            "in training loop, epoch 1, step 487, the loss is 428624.9375\n",
            "in training loop, epoch 1, step 488, the loss is 354275.625\n",
            "in training loop, epoch 1, step 489, the loss is 300268.5\n",
            "in training loop, epoch 1, step 490, the loss is 277957.625\n",
            "in training loop, epoch 1, step 491, the loss is 220083.5\n",
            "in training loop, epoch 1, step 492, the loss is 155170.703125\n",
            "in training loop, epoch 1, step 493, the loss is 304830.09375\n",
            "in training loop, epoch 1, step 494, the loss is 200275.78125\n",
            "in training loop, epoch 1, step 495, the loss is 246488.5\n",
            "in training loop, epoch 1, step 496, the loss is 171135.46875\n",
            "in training loop, epoch 1, step 497, the loss is 273513.875\n",
            "in training loop, epoch 1, step 498, the loss is 335906.71875\n",
            "in training loop, epoch 1, step 499, the loss is 303245.40625\n",
            "in training loop, epoch 1, step 500, the loss is 401040.90625\n",
            "in training loop, epoch 1, step 501, the loss is 140146.515625\n",
            "in training loop, epoch 1, step 502, the loss is 287583.5625\n",
            "in training loop, epoch 1, step 503, the loss is 307535.25\n",
            "in training loop, epoch 1, step 504, the loss is 175017.546875\n",
            "in training loop, epoch 1, step 505, the loss is 218194.796875\n",
            "in training loop, epoch 1, step 506, the loss is 140317.140625\n",
            "in training loop, epoch 1, step 507, the loss is 240018.578125\n",
            "in training loop, epoch 1, step 508, the loss is 247798.3125\n",
            "in training loop, epoch 1, step 509, the loss is 254253.59375\n",
            "in training loop, epoch 1, step 510, the loss is 344130.28125\n",
            "in training loop, epoch 1, step 511, the loss is 434385.75\n",
            "in training loop, epoch 1, step 512, the loss is 413517.34375\n",
            "in training loop, epoch 1, step 513, the loss is 263686.71875\n",
            "in training loop, epoch 1, step 514, the loss is 299856.625\n",
            "in training loop, epoch 1, step 515, the loss is 225624.28125\n",
            "in training loop, epoch 1, step 516, the loss is 200209.234375\n",
            "in training loop, epoch 1, step 517, the loss is 237782.5\n",
            "in training loop, epoch 1, step 518, the loss is 281106.15625\n",
            "in training loop, epoch 1, step 519, the loss is 258143.8125\n",
            "in training loop, epoch 1, step 520, the loss is 102852.859375\n",
            "in training loop, epoch 1, step 521, the loss is 367574.75\n",
            "in training loop, epoch 1, step 522, the loss is 290044.4375\n",
            "in training loop, epoch 1, step 523, the loss is 275989.75\n",
            "in training loop, epoch 1, step 524, the loss is 607282.875\n",
            "in training loop, epoch 1, step 525, the loss is 449134.5\n",
            "in training loop, epoch 1, step 526, the loss is 261839.84375\n",
            "in training loop, epoch 1, step 527, the loss is 376557.90625\n",
            "in training loop, epoch 1, step 528, the loss is 650068.75\n",
            "in training loop, epoch 1, step 529, the loss is 175402.8125\n",
            "in training loop, epoch 1, step 530, the loss is 233078.03125\n",
            "in training loop, epoch 1, step 531, the loss is 218444.703125\n",
            "in training loop, epoch 1, step 532, the loss is 237389.6875\n",
            "in training loop, epoch 1, step 533, the loss is 171164.390625\n",
            "in training loop, epoch 1, step 534, the loss is 865427.75\n",
            "in training loop, epoch 1, step 535, the loss is 233476.125\n",
            "in training loop, epoch 1, step 536, the loss is 379693.0625\n",
            "in training loop, epoch 1, step 537, the loss is 243283.3125\n",
            "in training loop, epoch 1, step 538, the loss is 358064.15625\n",
            "in training loop, epoch 1, step 539, the loss is 581891.1875\n",
            "in training loop, epoch 1, step 540, the loss is 287487.28125\n",
            "in training loop, epoch 1, step 541, the loss is 419879.125\n",
            "in training loop, epoch 1, step 542, the loss is 255196.5625\n",
            "in training loop, epoch 1, step 543, the loss is 272867.4375\n",
            "in training loop, epoch 1, step 544, the loss is 278328.1875\n",
            "in training loop, epoch 1, step 545, the loss is 401058.0\n",
            "in training loop, epoch 1, step 546, the loss is 228695.796875\n",
            "in training loop, epoch 1, step 547, the loss is 223767.03125\n",
            "in training loop, epoch 1, step 548, the loss is 577711.1875\n",
            "in training loop, epoch 1, step 549, the loss is 289965.46875\n",
            "in training loop, epoch 1, step 550, the loss is 284979.9375\n",
            "in training loop, epoch 1, step 551, the loss is 262743.4375\n",
            "in training loop, epoch 1, step 552, the loss is 265048.90625\n",
            "in training loop, epoch 1, step 553, the loss is 409503.71875\n",
            "in training loop, epoch 1, step 554, the loss is 166116.15625\n",
            "in training loop, epoch 1, step 555, the loss is 331745.21875\n",
            "in training loop, epoch 1, step 556, the loss is 313893.0\n",
            "in training loop, epoch 1, step 557, the loss is 202257.78125\n",
            "in training loop, epoch 1, step 558, the loss is 324073.84375\n",
            "in training loop, epoch 1, step 559, the loss is 241868.90625\n",
            "in training loop, epoch 1, step 560, the loss is 280748.90625\n",
            "in training loop, epoch 1, step 561, the loss is 155993.125\n",
            "in training loop, epoch 1, step 562, the loss is 323518.0625\n",
            "in training loop, epoch 1, step 563, the loss is 362619.84375\n",
            "in training loop, epoch 1, step 564, the loss is 132694.828125\n",
            "in training loop, epoch 1, step 565, the loss is 280180.0\n",
            "in training loop, epoch 1, step 566, the loss is 338061.28125\n",
            "in training loop, epoch 1, step 567, the loss is 364025.375\n",
            "in training loop, epoch 1, step 568, the loss is 306250.6875\n",
            "in training loop, epoch 1, step 569, the loss is 210179.984375\n",
            "in training loop, epoch 1, step 570, the loss is 239434.890625\n",
            "in training loop, epoch 1, step 571, the loss is 133752.03125\n",
            "in training loop, epoch 1, step 572, the loss is 388700.6875\n",
            "in training loop, epoch 1, step 573, the loss is 216140.1875\n",
            "in training loop, epoch 1, step 574, the loss is 248077.71875\n",
            "in training loop, epoch 1, step 575, the loss is 373400.6875\n",
            "in training loop, epoch 1, step 576, the loss is 286280.4375\n",
            "in training loop, epoch 1, step 577, the loss is 724897.9375\n",
            "in training loop, epoch 1, step 578, the loss is 369085.5\n",
            "in training loop, epoch 1, step 579, the loss is 363308.875\n",
            "in training loop, epoch 1, step 580, the loss is 135249.1875\n",
            "in training loop, epoch 1, step 581, the loss is 336119.125\n",
            "in training loop, epoch 1, step 582, the loss is 349925.125\n",
            "in training loop, epoch 1, step 583, the loss is 108747.25\n",
            "in training loop, epoch 1, step 584, the loss is 185929.875\n",
            "in training loop, epoch 1, step 585, the loss is 260182.8125\n",
            "in training loop, epoch 1, step 586, the loss is 247051.765625\n",
            "in training loop, epoch 1, step 587, the loss is 459659.375\n",
            "in training loop, epoch 1, step 588, the loss is 227362.5\n",
            "in training loop, epoch 1, step 589, the loss is 390578.46875\n",
            "in training loop, epoch 1, step 590, the loss is 214415.921875\n",
            "in training loop, epoch 1, step 591, the loss is 289247.03125\n",
            "in training loop, epoch 1, step 592, the loss is 181486.421875\n",
            "in training loop, epoch 1, step 593, the loss is 252415.640625\n",
            "in training loop, epoch 1, step 594, the loss is 226412.765625\n",
            "in training loop, epoch 1, step 595, the loss is 362626.96875\n",
            "in training loop, epoch 1, step 596, the loss is 540850.25\n",
            "in training loop, epoch 1, step 597, the loss is 184808.3125\n",
            "in training loop, epoch 1, step 598, the loss is 308700.125\n",
            "in training loop, epoch 1, step 599, the loss is 317960.3125\n",
            "in training loop, epoch 1, step 600, the loss is 212003.796875\n",
            "in training loop, epoch 1, step 601, the loss is 171522.484375\n",
            "in training loop, epoch 1, step 602, the loss is 275317.3125\n",
            "in training loop, epoch 1, step 603, the loss is 175318.4375\n",
            "in training loop, epoch 1, step 604, the loss is 338468.21875\n",
            "in training loop, epoch 1, step 605, the loss is 363487.96875\n",
            "in training loop, epoch 1, step 606, the loss is 179060.03125\n",
            "in training loop, epoch 1, step 607, the loss is 190057.75\n",
            "in training loop, epoch 1, step 608, the loss is 214365.515625\n",
            "in training loop, epoch 1, step 609, the loss is 223264.28125\n",
            "in training loop, epoch 1, step 610, the loss is 195432.140625\n",
            "in training loop, epoch 1, step 611, the loss is 511422.53125\n",
            "in training loop, epoch 1, step 612, the loss is 160417.359375\n",
            "in training loop, epoch 1, step 613, the loss is 190290.765625\n",
            "in training loop, epoch 1, step 614, the loss is 255825.9375\n",
            "in training loop, epoch 1, step 615, the loss is 170355.59375\n",
            "in training loop, epoch 1, step 616, the loss is 211736.21875\n",
            "in training loop, epoch 1, step 617, the loss is 602662.375\n",
            "in training loop, epoch 1, step 618, the loss is 227289.953125\n",
            "in training loop, epoch 1, step 619, the loss is 329615.90625\n",
            "in training loop, epoch 1, step 620, the loss is 191587.546875\n",
            "in training loop, epoch 1, step 621, the loss is 239408.1875\n",
            "in training loop, epoch 1, step 622, the loss is 251155.125\n",
            "in training loop, epoch 1, step 623, the loss is 167943.578125\n",
            "in training loop, epoch 1, step 624, the loss is 167879.40625\n",
            "in training loop, epoch 1, step 625, the loss is 240980.15625\n",
            "in training loop, epoch 1, step 626, the loss is 187729.25\n",
            "in training loop, epoch 1, step 627, the loss is 220302.46875\n",
            "in training loop, epoch 1, step 628, the loss is 341688.40625\n",
            "in training loop, epoch 1, step 629, the loss is 327358.75\n",
            "in training loop, epoch 1, step 630, the loss is 160544.21875\n",
            "in training loop, epoch 1, step 631, the loss is 297727.75\n",
            "in training loop, epoch 1, step 632, the loss is 349622.28125\n",
            "in training loop, epoch 1, step 633, the loss is 321639.90625\n",
            "in training loop, epoch 1, step 634, the loss is 303831.84375\n",
            "in training loop, epoch 1, step 635, the loss is 378236.40625\n",
            "in training loop, epoch 1, step 636, the loss is 201077.421875\n",
            "in training loop, epoch 1, step 637, the loss is 345645.78125\n",
            "in training loop, epoch 1, step 638, the loss is 322829.59375\n",
            "in training loop, epoch 1, step 639, the loss is 345448.5625\n",
            "in training loop, epoch 1, step 640, the loss is 430761.5\n",
            "in training loop, epoch 1, step 641, the loss is 187706.234375\n",
            "in training loop, epoch 1, step 642, the loss is 223231.03125\n",
            "in training loop, epoch 1, step 643, the loss is 286922.875\n",
            "in training loop, epoch 1, step 644, the loss is 197598.828125\n",
            "in training loop, epoch 1, step 645, the loss is 388730.84375\n",
            "in training loop, epoch 1, step 646, the loss is 315658.40625\n",
            "in training loop, epoch 1, step 647, the loss is 210854.3125\n",
            "in training loop, epoch 1, step 648, the loss is 339126.34375\n",
            "in training loop, epoch 1, step 649, the loss is 227098.484375\n",
            "in training loop, epoch 1, step 650, the loss is 320573.5\n",
            "in training loop, epoch 1, step 651, the loss is 212749.890625\n",
            "in training loop, epoch 1, step 652, the loss is 254142.0625\n",
            "in training loop, epoch 1, step 653, the loss is 172477.03125\n",
            "in training loop, epoch 1, step 654, the loss is 331245.6875\n",
            "in training loop, epoch 1, step 655, the loss is 409100.78125\n",
            "in training loop, epoch 1, step 656, the loss is 312065.5\n",
            "in training loop, epoch 1, step 657, the loss is 290524.5\n",
            "in training loop, epoch 1, step 658, the loss is 233318.71875\n",
            "in training loop, epoch 1, step 659, the loss is 196583.421875\n",
            "in training loop, epoch 1, step 660, the loss is 200151.75\n",
            "in training loop, epoch 1, step 661, the loss is 222688.125\n",
            "in training loop, epoch 1, step 662, the loss is 216492.890625\n",
            "in training loop, epoch 1, step 663, the loss is 365186.96875\n",
            "in training loop, epoch 1, step 664, the loss is 343669.5\n",
            "in training loop, epoch 1, step 665, the loss is 189637.28125\n",
            "in training loop, epoch 1, step 666, the loss is 298404.25\n",
            "in training loop, epoch 1, step 667, the loss is 258687.921875\n",
            "in training loop, epoch 1, step 668, the loss is 326086.46875\n",
            "in training loop, epoch 1, step 669, the loss is 271092.28125\n",
            "in training loop, epoch 1, step 670, the loss is 229408.421875\n",
            "in training loop, epoch 1, step 671, the loss is 336548.625\n",
            "in training loop, epoch 1, step 672, the loss is 295033.46875\n",
            "in training loop, epoch 1, step 673, the loss is 384333.5\n",
            "in training loop, epoch 1, step 674, the loss is 234219.9375\n",
            "in training loop, epoch 1, step 675, the loss is 338433.6875\n",
            "in training loop, epoch 1, step 676, the loss is 198409.03125\n",
            "in training loop, epoch 1, step 677, the loss is 194820.75\n",
            "in training loop, epoch 1, step 678, the loss is 253609.125\n",
            "in training loop, epoch 1, step 679, the loss is 668487.9375\n",
            "in training loop, epoch 1, step 680, the loss is 267648.96875\n",
            "in training loop, epoch 1, step 681, the loss is 358674.0\n",
            "in training loop, epoch 1, step 682, the loss is 98862.0859375\n",
            "in training loop, epoch 1, step 683, the loss is 332070.5\n",
            "in training loop, epoch 1, step 684, the loss is 259916.875\n",
            "in training loop, epoch 1, step 685, the loss is 376126.125\n",
            "in training loop, epoch 1, step 686, the loss is 224406.765625\n",
            "in training loop, epoch 1, step 687, the loss is 581513.4375\n",
            "in training loop, epoch 1, step 688, the loss is 230196.140625\n",
            "in training loop, epoch 1, step 689, the loss is 347127.03125\n",
            "in training loop, epoch 1, step 690, the loss is 601089.25\n",
            "in training loop, epoch 1, step 691, the loss is 331780.125\n",
            "in training loop, epoch 1, step 692, the loss is 312344.34375\n",
            "in training loop, epoch 1, step 693, the loss is 232230.046875\n",
            "in training loop, epoch 1, step 694, the loss is 581875.875\n",
            "in training loop, epoch 1, step 695, the loss is 165018.1875\n",
            "in training loop, epoch 1, step 696, the loss is 431915.5625\n",
            "in training loop, epoch 1, step 697, the loss is 256390.40625\n",
            "in training loop, epoch 1, step 698, the loss is 207130.4375\n",
            "in training loop, epoch 1, step 699, the loss is 290326.46875\n",
            "in training loop, epoch 1, step 700, the loss is 353182.125\n",
            "in training loop, epoch 1, step 701, the loss is 170832.34375\n",
            "in training loop, epoch 1, step 702, the loss is 624442.125\n",
            "in training loop, epoch 1, step 703, the loss is 541296.25\n",
            "in training loop, epoch 1, step 704, the loss is 326216.78125\n",
            "in training loop, epoch 1, step 705, the loss is 220829.125\n",
            "in training loop, epoch 1, step 706, the loss is 347587.5\n",
            "in training loop, epoch 1, step 707, the loss is 219302.65625\n",
            "in training loop, epoch 1, step 708, the loss is 312950.875\n",
            "in training loop, epoch 1, step 709, the loss is 288610.6875\n",
            "in training loop, epoch 1, step 710, the loss is 276135.1875\n",
            "in training loop, epoch 1, step 711, the loss is 293791.21875\n",
            "in training loop, epoch 1, step 712, the loss is 143067.109375\n",
            "in training loop, epoch 1, step 713, the loss is 224314.65625\n",
            "in training loop, epoch 1, step 714, the loss is 268528.09375\n",
            "in training loop, epoch 1, step 715, the loss is 344363.25\n",
            "in training loop, epoch 1, step 716, the loss is 322684.4375\n",
            "in training loop, epoch 1, step 717, the loss is 203645.078125\n",
            "in training loop, epoch 1, step 718, the loss is 359542.875\n",
            "in training loop, epoch 1, step 719, the loss is 183388.34375\n",
            "in training loop, epoch 1, step 720, the loss is 271440.0625\n",
            "in training loop, epoch 1, step 721, the loss is 178955.765625\n",
            "in training loop, epoch 1, step 722, the loss is 228507.703125\n",
            "in training loop, epoch 1, step 723, the loss is 323211.28125\n",
            "in training loop, epoch 1, step 724, the loss is 285413.4375\n",
            "in training loop, epoch 1, step 725, the loss is 360435.78125\n",
            "in training loop, epoch 1, step 726, the loss is 266114.46875\n",
            "in training loop, epoch 1, step 727, the loss is 218632.59375\n",
            "in training loop, epoch 1, step 728, the loss is 306792.34375\n",
            "in training loop, epoch 1, step 729, the loss is 303563.53125\n",
            "in training loop, epoch 1, step 730, the loss is 269997.25\n",
            "in training loop, epoch 1, step 731, the loss is 381935.53125\n",
            "in training loop, epoch 1, step 732, the loss is 283973.4375\n",
            "in training loop, epoch 1, step 733, the loss is 263202.625\n",
            "in training loop, epoch 1, step 734, the loss is 328457.90625\n",
            "in training loop, epoch 1, step 735, the loss is 557085.6875\n",
            "in training loop, epoch 1, step 736, the loss is 124347.203125\n",
            "in training loop, epoch 1, step 737, the loss is 341300.53125\n",
            "in training loop, epoch 1, step 738, the loss is 229239.09375\n",
            "in training loop, epoch 1, step 739, the loss is 302519.46875\n",
            "in training loop, epoch 1, step 740, the loss is 258745.03125\n",
            "in training loop, epoch 1, step 741, the loss is 324868.0625\n",
            "in training loop, epoch 1, step 742, the loss is 137471.21875\n",
            "in training loop, epoch 1, step 743, the loss is 210124.78125\n",
            "in training loop, epoch 1, step 744, the loss is 180376.796875\n",
            "in training loop, epoch 1, step 745, the loss is 283737.90625\n",
            "in training loop, epoch 1, step 746, the loss is 188323.3125\n",
            "in training loop, epoch 1, step 747, the loss is 463615.125\n",
            "in training loop, epoch 1, step 748, the loss is 181355.3125\n",
            "in training loop, epoch 1, step 749, the loss is 747930.375\n",
            "in training loop, epoch 1, step 750, the loss is 372834.71875\n",
            "in training loop, epoch 1, step 751, the loss is 197330.046875\n",
            "in training loop, epoch 1, step 752, the loss is 218979.421875\n",
            "in training loop, epoch 1, step 753, the loss is 252436.546875\n",
            "in training loop, epoch 1, step 754, the loss is 284108.71875\n",
            "in training loop, epoch 1, step 755, the loss is 214689.96875\n",
            "in training loop, epoch 1, step 756, the loss is 1133063.0\n",
            "in training loop, epoch 1, step 757, the loss is 364548.5\n",
            "in training loop, epoch 1, step 758, the loss is 278303.21875\n",
            "in training loop, epoch 1, step 759, the loss is 360595.3125\n",
            "in training loop, epoch 1, step 760, the loss is 692860.0625\n",
            "in training loop, epoch 1, step 761, the loss is 262842.53125\n",
            "in training loop, epoch 1, step 762, the loss is 155051.78125\n",
            "in training loop, epoch 1, step 763, the loss is 213346.234375\n",
            "in training loop, epoch 1, step 764, the loss is 333524.40625\n",
            "in training loop, epoch 1, step 765, the loss is 235296.359375\n",
            "in training loop, epoch 1, step 766, the loss is 341193.6875\n",
            "in training loop, epoch 1, step 767, the loss is 229972.5\n",
            "in training loop, epoch 1, step 768, the loss is 359660.90625\n",
            "in training loop, epoch 1, step 769, the loss is 213211.203125\n",
            "in training loop, epoch 1, step 770, the loss is 181112.34375\n",
            "in training loop, epoch 1, step 771, the loss is 493253.25\n",
            "in training loop, epoch 1, step 772, the loss is 213323.921875\n",
            "in training loop, epoch 1, step 773, the loss is 138864.25\n",
            "in training loop, epoch 1, step 774, the loss is 250736.28125\n",
            "in training loop, epoch 1, step 775, the loss is 221761.640625\n",
            "in training loop, epoch 1, step 776, the loss is 362823.03125\n",
            "in training loop, epoch 1, step 777, the loss is 416738.75\n",
            "in training loop, epoch 1, step 778, the loss is 582028.375\n",
            "in training loop, epoch 1, step 779, the loss is 314768.46875\n",
            "in training loop, epoch 1, step 780, the loss is 263559.0\n",
            "in training loop, epoch 1, step 781, the loss is 252897.90625\n",
            "in training loop, epoch 1, step 782, the loss is 322417.90625\n",
            "in training loop, epoch 1, step 783, the loss is 273368.25\n",
            "in training loop, epoch 1, step 784, the loss is 352673.28125\n",
            "in training loop, epoch 1, step 785, the loss is 311468.75\n",
            "in training loop, epoch 1, step 786, the loss is 318631.9375\n",
            "in training loop, epoch 1, step 787, the loss is 150983.09375\n",
            "in training loop, epoch 1, step 788, the loss is 262574.0\n",
            "in training loop, epoch 1, step 789, the loss is 337076.28125\n",
            "in training loop, epoch 1, step 790, the loss is 260217.953125\n",
            "in training loop, epoch 1, step 791, the loss is 158254.09375\n",
            "in training loop, epoch 1, step 792, the loss is 355421.8125\n",
            "in training loop, epoch 1, step 793, the loss is 252631.28125\n",
            "in training loop, epoch 1, step 794, the loss is 212915.921875\n",
            "in training loop, epoch 1, step 795, the loss is 381360.5\n",
            "in training loop, epoch 1, step 796, the loss is 274068.90625\n",
            "in training loop, epoch 1, step 797, the loss is 525538.8125\n",
            "in training loop, epoch 1, step 798, the loss is 327215.78125\n",
            "in training loop, epoch 1, step 799, the loss is 209205.703125\n",
            "in training loop, epoch 1, step 800, the loss is 291301.78125\n",
            "in training loop, epoch 1, step 801, the loss is 374262.09375\n",
            "in training loop, epoch 1, step 802, the loss is 219661.390625\n",
            "in training loop, epoch 1, step 803, the loss is 178614.375\n",
            "in training loop, epoch 1, step 804, the loss is 202557.34375\n",
            "in training loop, epoch 1, step 805, the loss is 347091.1875\n",
            "in training loop, epoch 1, step 806, the loss is 226781.625\n",
            "in training loop, epoch 1, step 807, the loss is 227500.796875\n",
            "in training loop, epoch 1, step 808, the loss is 199470.375\n",
            "in training loop, epoch 1, step 809, the loss is 200752.078125\n",
            "in training loop, epoch 1, step 810, the loss is 570676.6875\n",
            "in training loop, epoch 1, step 811, the loss is 183534.34375\n",
            "in training loop, epoch 1, step 812, the loss is 379177.46875\n",
            "in training loop, epoch 1, step 813, the loss is 189368.59375\n",
            "in training loop, epoch 1, step 814, the loss is 189668.625\n",
            "in training loop, epoch 1, step 815, the loss is 245138.71875\n",
            "in training loop, epoch 1, step 816, the loss is 414009.8125\n",
            "in training loop, epoch 1, step 817, the loss is 277803.5625\n",
            "in training loop, epoch 1, step 818, the loss is 312450.375\n",
            "in training loop, epoch 1, step 819, the loss is 108869.8125\n",
            "in training loop, epoch 1, step 820, the loss is 345061.625\n",
            "in training loop, epoch 1, step 821, the loss is 162791.40625\n",
            "in training loop, epoch 1, step 822, the loss is 346064.46875\n",
            "in training loop, epoch 1, step 823, the loss is 309044.84375\n",
            "in training loop, epoch 1, step 824, the loss is 321197.1875\n",
            "in training loop, epoch 1, step 825, the loss is 232588.84375\n",
            "in training loop, epoch 1, step 826, the loss is 202345.890625\n",
            "in training loop, epoch 1, step 827, the loss is 220955.0625\n",
            "in training loop, epoch 1, step 828, the loss is 432077.46875\n",
            "in training loop, epoch 1, step 829, the loss is 378320.0\n",
            "in training loop, epoch 1, step 830, the loss is 406909.0\n",
            "in training loop, epoch 1, step 831, the loss is 384779.96875\n",
            "in training loop, epoch 1, step 832, the loss is 314378.84375\n",
            "in training loop, epoch 1, step 833, the loss is 362990.59375\n",
            "in training loop, epoch 1, step 834, the loss is 228231.1875\n",
            "in training loop, epoch 1, step 835, the loss is 208207.875\n",
            "in training loop, epoch 1, step 836, the loss is 366028.75\n",
            "in training loop, epoch 1, step 837, the loss is 219365.96875\n",
            "in training loop, epoch 1, step 838, the loss is 335823.4375\n",
            "in training loop, epoch 1, step 839, the loss is 325947.8125\n",
            "in training loop, epoch 1, step 840, the loss is 440792.59375\n",
            "in training loop, epoch 1, step 841, the loss is 309715.8125\n",
            "in training loop, epoch 1, step 842, the loss is 407566.1875\n",
            "in training loop, epoch 1, step 843, the loss is 267816.375\n",
            "in training loop, epoch 1, step 844, the loss is 282052.5\n",
            "in training loop, epoch 1, step 845, the loss is 244785.3125\n",
            "in training loop, epoch 1, step 846, the loss is 200489.84375\n",
            "in training loop, epoch 1, step 847, the loss is 198228.171875\n",
            "in training loop, epoch 1, step 848, the loss is 235028.5\n",
            "in training loop, epoch 1, step 849, the loss is 213757.28125\n",
            "in training loop, epoch 1, step 850, the loss is 309961.28125\n",
            "in training loop, epoch 1, step 851, the loss is 245642.390625\n",
            "in training loop, epoch 1, step 852, the loss is 243627.703125\n",
            "in training loop, epoch 1, step 853, the loss is 268079.28125\n",
            "in training loop, epoch 1, step 854, the loss is 330781.4375\n",
            "in training loop, epoch 1, step 855, the loss is 216407.375\n",
            "in training loop, epoch 1, step 856, the loss is 259192.1875\n",
            "in training loop, epoch 1, step 857, the loss is 210864.140625\n",
            "in training loop, epoch 1, step 858, the loss is 390427.9375\n",
            "in training loop, epoch 1, step 859, the loss is 225570.453125\n",
            "in training loop, epoch 1, step 860, the loss is 431151.96875\n",
            "in training loop, epoch 1, step 861, the loss is 273703.9375\n",
            "in training loop, epoch 1, step 862, the loss is 291692.90625\n",
            "in training loop, epoch 1, step 863, the loss is 191338.046875\n",
            "in training loop, epoch 1, step 864, the loss is 181469.296875\n",
            "in training loop, epoch 1, step 865, the loss is 205128.21875\n",
            "in training loop, epoch 1, step 866, the loss is 409558.3125\n",
            "in training loop, epoch 1, step 867, the loss is 381299.3125\n",
            "in training loop, epoch 1, step 868, the loss is 234734.015625\n",
            "in training loop, epoch 1, step 869, the loss is 195003.65625\n",
            "in training loop, epoch 1, step 870, the loss is 299645.15625\n",
            "in training loop, epoch 1, step 871, the loss is 232458.21875\n",
            "in training loop, epoch 1, step 872, the loss is 551268.625\n",
            "in training loop, epoch 1, step 873, the loss is 195528.953125\n",
            "in training loop, epoch 1, step 874, the loss is 337229.09375\n",
            "in training loop, epoch 1, step 875, the loss is 333926.09375\n",
            "in training loop, epoch 1, step 876, the loss is 237609.265625\n",
            "in training loop, epoch 1, step 877, the loss is 289739.3125\n",
            "in training loop, epoch 1, step 878, the loss is 260934.015625\n",
            "in training loop, epoch 1, step 879, the loss is 280970.15625\n",
            "in training loop, epoch 1, step 880, the loss is 415572.4375\n",
            "in training loop, epoch 1, step 881, the loss is 171933.390625\n",
            "in training loop, epoch 1, step 882, the loss is 280147.8125\n",
            "in training loop, epoch 1, step 883, the loss is 281238.15625\n",
            "in training loop, epoch 1, step 884, the loss is 248266.15625\n",
            "in training loop, epoch 1, step 885, the loss is 295382.03125\n",
            "in training loop, epoch 1, step 886, the loss is 403214.1875\n",
            "in training loop, epoch 1, step 887, the loss is 581908.1875\n",
            "in training loop, epoch 1, step 888, the loss is 446720.5625\n",
            "in training loop, epoch 1, step 889, the loss is 309329.15625\n",
            "in training loop, epoch 1, step 890, the loss is 167427.4375\n",
            "in training loop, epoch 1, step 891, the loss is 272902.4375\n",
            "in training loop, epoch 1, step 892, the loss is 244945.109375\n",
            "in training loop, epoch 1, step 893, the loss is 183831.9375\n",
            "in training loop, epoch 1, step 894, the loss is 307528.84375\n",
            "in training loop, epoch 1, step 895, the loss is 107522.2421875\n",
            "in training loop, epoch 1, step 896, the loss is 336616.875\n",
            "in training loop, epoch 1, step 897, the loss is 159683.34375\n",
            "in training loop, epoch 1, step 898, the loss is 338635.625\n",
            "in training loop, epoch 1, step 899, the loss is 130981.25\n",
            "in training loop, epoch 1, step 900, the loss is 261039.390625\n",
            "in training loop, epoch 1, step 901, the loss is 220835.96875\n",
            "in training loop, epoch 1, step 902, the loss is 221952.796875\n",
            "in training loop, epoch 1, step 903, the loss is 127452.390625\n",
            "k-fold 0:: Epoch 1: train loss 302594.0265659569 val loss 320749.6085241337\n",
            "in training loop, epoch 2, step 0, the loss is 210120.8125\n",
            "in training loop, epoch 2, step 1, the loss is 382203.6875\n",
            "in training loop, epoch 2, step 2, the loss is 353605.6875\n",
            "in training loop, epoch 2, step 3, the loss is 316565.3125\n",
            "in training loop, epoch 2, step 4, the loss is 218625.0\n",
            "in training loop, epoch 2, step 5, the loss is 267655.625\n",
            "in training loop, epoch 2, step 6, the loss is 196667.96875\n",
            "in training loop, epoch 2, step 7, the loss is 331922.71875\n",
            "in training loop, epoch 2, step 8, the loss is 254395.125\n",
            "in training loop, epoch 2, step 9, the loss is 197340.65625\n",
            "in training loop, epoch 2, step 10, the loss is 342752.375\n",
            "in training loop, epoch 2, step 11, the loss is 341629.875\n",
            "in training loop, epoch 2, step 12, the loss is 207212.34375\n",
            "in training loop, epoch 2, step 13, the loss is 301507.78125\n",
            "in training loop, epoch 2, step 14, the loss is 226579.015625\n",
            "in training loop, epoch 2, step 15, the loss is 251164.875\n",
            "in training loop, epoch 2, step 16, the loss is 257031.453125\n",
            "in training loop, epoch 2, step 17, the loss is 241895.96875\n",
            "in training loop, epoch 2, step 18, the loss is 328894.59375\n",
            "in training loop, epoch 2, step 19, the loss is 263602.46875\n",
            "in training loop, epoch 2, step 20, the loss is 126317.6484375\n",
            "in training loop, epoch 2, step 21, the loss is 191400.3125\n",
            "in training loop, epoch 2, step 22, the loss is 237274.5\n",
            "in training loop, epoch 2, step 23, the loss is 234229.671875\n",
            "in training loop, epoch 2, step 24, the loss is 216979.546875\n",
            "in training loop, epoch 2, step 25, the loss is 285397.375\n",
            "in training loop, epoch 2, step 26, the loss is 306111.0\n",
            "in training loop, epoch 2, step 27, the loss is 238837.625\n",
            "in training loop, epoch 2, step 28, the loss is 233777.0625\n",
            "in training loop, epoch 2, step 29, the loss is 169713.90625\n",
            "in training loop, epoch 2, step 30, the loss is 321325.4375\n",
            "in training loop, epoch 2, step 31, the loss is 241548.078125\n",
            "in training loop, epoch 2, step 32, the loss is 294220.5\n",
            "in training loop, epoch 2, step 33, the loss is 264501.28125\n",
            "in training loop, epoch 2, step 34, the loss is 209919.0\n",
            "in training loop, epoch 2, step 35, the loss is 220746.09375\n",
            "in training loop, epoch 2, step 36, the loss is 297509.90625\n",
            "in training loop, epoch 2, step 37, the loss is 270056.28125\n",
            "in training loop, epoch 2, step 38, the loss is 355840.3125\n",
            "in training loop, epoch 2, step 39, the loss is 259684.6875\n",
            "in training loop, epoch 2, step 40, the loss is 251889.953125\n",
            "in training loop, epoch 2, step 41, the loss is 230189.125\n",
            "in training loop, epoch 2, step 42, the loss is 216103.171875\n",
            "in training loop, epoch 2, step 43, the loss is 280492.40625\n",
            "in training loop, epoch 2, step 44, the loss is 227986.828125\n",
            "in training loop, epoch 2, step 45, the loss is 210817.625\n",
            "in training loop, epoch 2, step 46, the loss is 266576.75\n",
            "in training loop, epoch 2, step 47, the loss is 213672.609375\n",
            "in training loop, epoch 2, step 48, the loss is 111433.6015625\n",
            "in training loop, epoch 2, step 49, the loss is 160768.703125\n",
            "in training loop, epoch 2, step 50, the loss is 304813.65625\n",
            "in training loop, epoch 2, step 51, the loss is 229586.703125\n",
            "in training loop, epoch 2, step 52, the loss is 397796.6875\n",
            "in training loop, epoch 2, step 53, the loss is 201506.703125\n",
            "in training loop, epoch 2, step 54, the loss is 271567.28125\n",
            "in training loop, epoch 2, step 55, the loss is 253622.921875\n",
            "in training loop, epoch 2, step 56, the loss is 171308.546875\n",
            "in training loop, epoch 2, step 57, the loss is 296553.90625\n",
            "in training loop, epoch 2, step 58, the loss is 296694.40625\n",
            "in training loop, epoch 2, step 59, the loss is 250683.5\n",
            "in training loop, epoch 2, step 60, the loss is 372550.78125\n",
            "in training loop, epoch 2, step 61, the loss is 190884.75\n",
            "in training loop, epoch 2, step 62, the loss is 329235.84375\n",
            "in training loop, epoch 2, step 63, the loss is 327410.53125\n",
            "in training loop, epoch 2, step 64, the loss is 240345.578125\n",
            "in training loop, epoch 2, step 65, the loss is 153319.9375\n",
            "in training loop, epoch 2, step 66, the loss is 167425.703125\n",
            "in training loop, epoch 2, step 67, the loss is 242445.71875\n",
            "in training loop, epoch 2, step 68, the loss is 92854.0390625\n",
            "in training loop, epoch 2, step 69, the loss is 190498.59375\n",
            "in training loop, epoch 2, step 70, the loss is 328340.3125\n",
            "in training loop, epoch 2, step 71, the loss is 322469.28125\n",
            "in training loop, epoch 2, step 72, the loss is 431370.53125\n",
            "in training loop, epoch 2, step 73, the loss is 132810.25\n",
            "in training loop, epoch 2, step 74, the loss is 250049.0625\n",
            "in training loop, epoch 2, step 75, the loss is 473426.15625\n",
            "in training loop, epoch 2, step 76, the loss is 184234.453125\n",
            "in training loop, epoch 2, step 77, the loss is 374979.0625\n",
            "in training loop, epoch 2, step 78, the loss is 407327.375\n",
            "in training loop, epoch 2, step 79, the loss is 322302.9375\n",
            "in training loop, epoch 2, step 80, the loss is 219633.25\n",
            "in training loop, epoch 2, step 81, the loss is 360428.0\n",
            "in training loop, epoch 2, step 82, the loss is 288808.53125\n",
            "in training loop, epoch 2, step 83, the loss is 579512.0\n",
            "in training loop, epoch 2, step 84, the loss is 194577.734375\n",
            "in training loop, epoch 2, step 85, the loss is 290059.46875\n",
            "in training loop, epoch 2, step 86, the loss is 274781.4375\n",
            "in training loop, epoch 2, step 87, the loss is 348842.375\n",
            "in training loop, epoch 2, step 88, the loss is 215485.765625\n",
            "in training loop, epoch 2, step 89, the loss is 321309.875\n",
            "in training loop, epoch 2, step 90, the loss is 303558.9375\n",
            "in training loop, epoch 2, step 91, the loss is 213262.046875\n",
            "in training loop, epoch 2, step 92, the loss is 176242.09375\n",
            "in training loop, epoch 2, step 93, the loss is 276715.6875\n",
            "in training loop, epoch 2, step 94, the loss is 162889.09375\n",
            "in training loop, epoch 2, step 95, the loss is 240439.796875\n",
            "in training loop, epoch 2, step 96, the loss is 311309.03125\n",
            "in training loop, epoch 2, step 97, the loss is 246978.25\n",
            "in training loop, epoch 2, step 98, the loss is 335193.6875\n",
            "in training loop, epoch 2, step 99, the loss is 229447.359375\n",
            "in training loop, epoch 2, step 100, the loss is 269188.96875\n",
            "in training loop, epoch 2, step 101, the loss is 210455.109375\n",
            "in training loop, epoch 2, step 102, the loss is 258076.25\n",
            "in training loop, epoch 2, step 103, the loss is 258263.21875\n",
            "in training loop, epoch 2, step 104, the loss is 288465.8125\n",
            "in training loop, epoch 2, step 105, the loss is 375884.71875\n",
            "in training loop, epoch 2, step 106, the loss is 96022.046875\n",
            "in training loop, epoch 2, step 107, the loss is 178381.46875\n",
            "in training loop, epoch 2, step 108, the loss is 139784.28125\n",
            "in training loop, epoch 2, step 109, the loss is 160931.984375\n",
            "in training loop, epoch 2, step 110, the loss is 155879.03125\n",
            "in training loop, epoch 2, step 111, the loss is 298082.1875\n",
            "in training loop, epoch 2, step 112, the loss is 231814.96875\n",
            "in training loop, epoch 2, step 113, the loss is 197089.53125\n",
            "in training loop, epoch 2, step 114, the loss is 380890.375\n",
            "in training loop, epoch 2, step 115, the loss is 207662.0\n",
            "in training loop, epoch 2, step 116, the loss is 279349.625\n",
            "in training loop, epoch 2, step 117, the loss is 174586.4375\n",
            "in training loop, epoch 2, step 118, the loss is 188223.875\n",
            "in training loop, epoch 2, step 119, the loss is 261644.421875\n",
            "in training loop, epoch 2, step 120, the loss is 357526.5\n",
            "in training loop, epoch 2, step 121, the loss is 177141.03125\n",
            "in training loop, epoch 2, step 122, the loss is 181471.21875\n",
            "in training loop, epoch 2, step 123, the loss is 262999.3125\n",
            "in training loop, epoch 2, step 124, the loss is 311527.15625\n",
            "in training loop, epoch 2, step 125, the loss is 223686.625\n",
            "in training loop, epoch 2, step 126, the loss is 246827.1875\n",
            "in training loop, epoch 2, step 127, the loss is 231544.625\n",
            "in training loop, epoch 2, step 128, the loss is 335015.8125\n",
            "in training loop, epoch 2, step 129, the loss is 266781.9375\n",
            "in training loop, epoch 2, step 130, the loss is 216350.53125\n",
            "in training loop, epoch 2, step 131, the loss is 294136.15625\n",
            "in training loop, epoch 2, step 132, the loss is 465014.5\n",
            "in training loop, epoch 2, step 133, the loss is 273825.4375\n",
            "in training loop, epoch 2, step 134, the loss is 237652.671875\n",
            "in training loop, epoch 2, step 135, the loss is 418735.46875\n",
            "in training loop, epoch 2, step 136, the loss is 252260.875\n",
            "in training loop, epoch 2, step 137, the loss is 151394.390625\n",
            "in training loop, epoch 2, step 138, the loss is 224101.40625\n",
            "in training loop, epoch 2, step 139, the loss is 356208.8125\n",
            "in training loop, epoch 2, step 140, the loss is 195297.671875\n",
            "in training loop, epoch 2, step 141, the loss is 278458.84375\n",
            "in training loop, epoch 2, step 142, the loss is 310073.8125\n",
            "in training loop, epoch 2, step 143, the loss is 328381.6875\n",
            "in training loop, epoch 2, step 144, the loss is 228244.453125\n",
            "in training loop, epoch 2, step 145, the loss is 178705.46875\n",
            "in training loop, epoch 2, step 146, the loss is 336275.1875\n",
            "in training loop, epoch 2, step 147, the loss is 256708.25\n",
            "in training loop, epoch 2, step 148, the loss is 313321.3125\n",
            "in training loop, epoch 2, step 149, the loss is 216900.953125\n",
            "in training loop, epoch 2, step 150, the loss is 299358.8125\n",
            "in training loop, epoch 2, step 151, the loss is 270437.71875\n",
            "in training loop, epoch 2, step 152, the loss is 245823.953125\n",
            "in training loop, epoch 2, step 153, the loss is 191915.96875\n",
            "in training loop, epoch 2, step 154, the loss is 219417.40625\n",
            "in training loop, epoch 2, step 155, the loss is 269085.59375\n",
            "in training loop, epoch 2, step 156, the loss is 248194.203125\n",
            "in training loop, epoch 2, step 157, the loss is 159756.296875\n",
            "in training loop, epoch 2, step 158, the loss is 231786.8125\n",
            "in training loop, epoch 2, step 159, the loss is 245975.8125\n",
            "in training loop, epoch 2, step 160, the loss is 196271.390625\n",
            "in training loop, epoch 2, step 161, the loss is 254650.265625\n",
            "in training loop, epoch 2, step 162, the loss is 364146.21875\n",
            "in training loop, epoch 2, step 163, the loss is 115308.6328125\n",
            "in training loop, epoch 2, step 164, the loss is 188471.203125\n",
            "in training loop, epoch 2, step 165, the loss is 269463.65625\n",
            "in training loop, epoch 2, step 166, the loss is 202548.34375\n",
            "in training loop, epoch 2, step 167, the loss is 311198.96875\n",
            "in training loop, epoch 2, step 168, the loss is 228664.359375\n",
            "in training loop, epoch 2, step 169, the loss is 222688.34375\n",
            "in training loop, epoch 2, step 170, the loss is 140046.1875\n",
            "in training loop, epoch 2, step 171, the loss is 319954.5\n",
            "in training loop, epoch 2, step 172, the loss is 352700.5\n",
            "in training loop, epoch 2, step 173, the loss is 230969.71875\n",
            "in training loop, epoch 2, step 174, the loss is 187477.171875\n",
            "in training loop, epoch 2, step 175, the loss is 304616.53125\n",
            "in training loop, epoch 2, step 176, the loss is 338102.96875\n",
            "in training loop, epoch 2, step 177, the loss is 184269.40625\n",
            "in training loop, epoch 2, step 178, the loss is 229994.265625\n",
            "in training loop, epoch 2, step 179, the loss is 350071.1875\n",
            "in training loop, epoch 2, step 180, the loss is 178988.984375\n",
            "in training loop, epoch 2, step 181, the loss is 328955.25\n",
            "in training loop, epoch 2, step 182, the loss is 180473.78125\n",
            "in training loop, epoch 2, step 183, the loss is 284550.9375\n",
            "in training loop, epoch 2, step 184, the loss is 205326.15625\n",
            "in training loop, epoch 2, step 185, the loss is 222738.125\n",
            "in training loop, epoch 2, step 186, the loss is 178878.515625\n",
            "in training loop, epoch 2, step 187, the loss is 199365.75\n",
            "in training loop, epoch 2, step 188, the loss is 162008.34375\n",
            "in training loop, epoch 2, step 189, the loss is 246623.015625\n",
            "in training loop, epoch 2, step 190, the loss is 279379.28125\n",
            "in training loop, epoch 2, step 191, the loss is 377345.28125\n",
            "in training loop, epoch 2, step 192, the loss is 144092.8125\n",
            "in training loop, epoch 2, step 193, the loss is 147701.703125\n",
            "in training loop, epoch 2, step 194, the loss is 158083.4375\n",
            "in training loop, epoch 2, step 195, the loss is 198370.453125\n",
            "in training loop, epoch 2, step 196, the loss is 297538.28125\n",
            "in training loop, epoch 2, step 197, the loss is 310341.125\n",
            "in training loop, epoch 2, step 198, the loss is 229234.671875\n",
            "in training loop, epoch 2, step 199, the loss is 370974.34375\n",
            "in training loop, epoch 2, step 200, the loss is 224697.359375\n",
            "in training loop, epoch 2, step 201, the loss is 207812.1875\n",
            "in training loop, epoch 2, step 202, the loss is 296596.09375\n",
            "in training loop, epoch 2, step 203, the loss is 242676.375\n",
            "in training loop, epoch 2, step 204, the loss is 200976.140625\n",
            "in training loop, epoch 2, step 205, the loss is 275731.46875\n",
            "in training loop, epoch 2, step 206, the loss is 432604.15625\n",
            "in training loop, epoch 2, step 207, the loss is 625741.4375\n",
            "in training loop, epoch 2, step 208, the loss is 287497.5\n",
            "in training loop, epoch 2, step 209, the loss is 233498.640625\n",
            "in training loop, epoch 2, step 210, the loss is 171939.421875\n",
            "in training loop, epoch 2, step 211, the loss is 332785.59375\n",
            "in training loop, epoch 2, step 212, the loss is 206723.453125\n",
            "in training loop, epoch 2, step 213, the loss is 187664.25\n",
            "in training loop, epoch 2, step 214, the loss is 213490.453125\n",
            "in training loop, epoch 2, step 215, the loss is 255010.109375\n",
            "in training loop, epoch 2, step 216, the loss is 339116.40625\n",
            "in training loop, epoch 2, step 217, the loss is 472315.71875\n",
            "in training loop, epoch 2, step 218, the loss is 267393.96875\n",
            "in training loop, epoch 2, step 219, the loss is 163317.484375\n",
            "in training loop, epoch 2, step 220, the loss is 400908.03125\n",
            "in training loop, epoch 2, step 221, the loss is 170476.234375\n",
            "in training loop, epoch 2, step 222, the loss is 229089.546875\n",
            "in training loop, epoch 2, step 223, the loss is 160665.734375\n",
            "in training loop, epoch 2, step 224, the loss is 333745.6875\n",
            "in training loop, epoch 2, step 225, the loss is 230247.21875\n",
            "in training loop, epoch 2, step 226, the loss is 196387.609375\n",
            "in training loop, epoch 2, step 227, the loss is 343777.90625\n",
            "in training loop, epoch 2, step 228, the loss is 326451.625\n",
            "in training loop, epoch 2, step 229, the loss is 145591.65625\n",
            "in training loop, epoch 2, step 230, the loss is 200561.125\n",
            "in training loop, epoch 2, step 231, the loss is 147965.765625\n",
            "in training loop, epoch 2, step 232, the loss is 232451.671875\n",
            "in training loop, epoch 2, step 233, the loss is 254939.5\n",
            "in training loop, epoch 2, step 234, the loss is 232396.984375\n",
            "in training loop, epoch 2, step 235, the loss is 234956.625\n",
            "in training loop, epoch 2, step 236, the loss is 235941.9375\n",
            "in training loop, epoch 2, step 237, the loss is 246211.234375\n",
            "in training loop, epoch 2, step 238, the loss is 138253.40625\n",
            "in training loop, epoch 2, step 239, the loss is 140463.890625\n",
            "in training loop, epoch 2, step 240, the loss is 179080.28125\n",
            "in training loop, epoch 2, step 241, the loss is 202064.4375\n",
            "in training loop, epoch 2, step 242, the loss is 221263.0\n",
            "in training loop, epoch 2, step 243, the loss is 221325.078125\n",
            "in training loop, epoch 2, step 244, the loss is 192490.484375\n",
            "in training loop, epoch 2, step 245, the loss is 156532.5625\n",
            "in training loop, epoch 2, step 246, the loss is 165667.78125\n",
            "in training loop, epoch 2, step 247, the loss is 180292.984375\n",
            "in training loop, epoch 2, step 248, the loss is 124784.09375\n",
            "in training loop, epoch 2, step 249, the loss is 124330.703125\n",
            "in training loop, epoch 2, step 250, the loss is 350962.21875\n",
            "in training loop, epoch 2, step 251, the loss is 217120.640625\n",
            "in training loop, epoch 2, step 252, the loss is 320097.75\n",
            "in training loop, epoch 2, step 253, the loss is 181424.375\n",
            "in training loop, epoch 2, step 254, the loss is 140518.734375\n",
            "in training loop, epoch 2, step 255, the loss is 272794.46875\n",
            "in training loop, epoch 2, step 256, the loss is 174446.859375\n",
            "in training loop, epoch 2, step 257, the loss is 131164.0\n",
            "in training loop, epoch 2, step 258, the loss is 204564.0625\n",
            "in training loop, epoch 2, step 259, the loss is 209966.984375\n",
            "in training loop, epoch 2, step 260, the loss is 275026.53125\n",
            "in training loop, epoch 2, step 261, the loss is 169409.1875\n",
            "in training loop, epoch 2, step 262, the loss is 298733.65625\n",
            "in training loop, epoch 2, step 263, the loss is 391282.75\n",
            "in training loop, epoch 2, step 264, the loss is 163713.71875\n",
            "in training loop, epoch 2, step 265, the loss is 147258.75\n",
            "in training loop, epoch 2, step 266, the loss is 284330.46875\n",
            "in training loop, epoch 2, step 267, the loss is 149910.734375\n",
            "in training loop, epoch 2, step 268, the loss is 166914.59375\n",
            "in training loop, epoch 2, step 269, the loss is 212291.78125\n",
            "in training loop, epoch 2, step 270, the loss is 291308.75\n",
            "in training loop, epoch 2, step 271, the loss is 213955.625\n",
            "in training loop, epoch 2, step 272, the loss is 236950.125\n",
            "in training loop, epoch 2, step 273, the loss is 119038.3984375\n",
            "in training loop, epoch 2, step 274, the loss is 217850.46875\n",
            "in training loop, epoch 2, step 275, the loss is 298556.125\n",
            "in training loop, epoch 2, step 276, the loss is 376121.25\n",
            "in training loop, epoch 2, step 277, the loss is 322602.125\n",
            "in training loop, epoch 2, step 278, the loss is 184509.5\n",
            "in training loop, epoch 2, step 279, the loss is 279016.6875\n",
            "in training loop, epoch 2, step 280, the loss is 272668.75\n",
            "in training loop, epoch 2, step 281, the loss is 158116.28125\n",
            "in training loop, epoch 2, step 282, the loss is 272876.875\n",
            "in training loop, epoch 2, step 283, the loss is 317489.125\n",
            "in training loop, epoch 2, step 284, the loss is 360088.8125\n",
            "in training loop, epoch 2, step 285, the loss is 196476.234375\n",
            "in training loop, epoch 2, step 286, the loss is 240739.53125\n",
            "in training loop, epoch 2, step 287, the loss is 283668.15625\n",
            "in training loop, epoch 2, step 288, the loss is 269582.15625\n",
            "in training loop, epoch 2, step 289, the loss is 297300.65625\n",
            "in training loop, epoch 2, step 290, the loss is 410622.125\n",
            "in training loop, epoch 2, step 291, the loss is 158925.828125\n",
            "in training loop, epoch 2, step 292, the loss is 229688.375\n",
            "in training loop, epoch 2, step 293, the loss is 195694.78125\n",
            "in training loop, epoch 2, step 294, the loss is 144945.65625\n",
            "in training loop, epoch 2, step 295, the loss is 345761.375\n",
            "in training loop, epoch 2, step 296, the loss is 306702.59375\n",
            "in training loop, epoch 2, step 297, the loss is 332322.59375\n",
            "in training loop, epoch 2, step 298, the loss is 248789.125\n",
            "in training loop, epoch 2, step 299, the loss is 250095.046875\n",
            "in training loop, epoch 2, step 300, the loss is 86460.328125\n",
            "in training loop, epoch 2, step 301, the loss is 271212.96875\n",
            "in training loop, epoch 2, step 302, the loss is 116265.2421875\n",
            "in training loop, epoch 2, step 303, the loss is 320906.0\n",
            "in training loop, epoch 2, step 304, the loss is 244967.171875\n",
            "in training loop, epoch 2, step 305, the loss is 253433.625\n",
            "in training loop, epoch 2, step 306, the loss is 313371.1875\n",
            "in training loop, epoch 2, step 307, the loss is 295578.125\n",
            "in training loop, epoch 2, step 308, the loss is 291023.15625\n",
            "in training loop, epoch 2, step 309, the loss is 367839.34375\n",
            "in training loop, epoch 2, step 310, the loss is 164272.109375\n",
            "in training loop, epoch 2, step 311, the loss is 283097.375\n",
            "in training loop, epoch 2, step 312, the loss is 317662.5\n",
            "in training loop, epoch 2, step 313, the loss is 234472.34375\n",
            "in training loop, epoch 2, step 314, the loss is 310490.40625\n",
            "in training loop, epoch 2, step 315, the loss is 147833.859375\n",
            "in training loop, epoch 2, step 316, the loss is 247356.734375\n",
            "in training loop, epoch 2, step 317, the loss is 251822.09375\n",
            "in training loop, epoch 2, step 318, the loss is 175505.703125\n",
            "in training loop, epoch 2, step 319, the loss is 208050.15625\n",
            "in training loop, epoch 2, step 320, the loss is 148253.28125\n",
            "in training loop, epoch 2, step 321, the loss is 176959.484375\n",
            "in training loop, epoch 2, step 322, the loss is 228678.578125\n",
            "in training loop, epoch 2, step 323, the loss is 238450.5\n",
            "in training loop, epoch 2, step 324, the loss is 261981.25\n",
            "in training loop, epoch 2, step 325, the loss is 431610.53125\n",
            "in training loop, epoch 2, step 326, the loss is 315103.875\n",
            "in training loop, epoch 2, step 327, the loss is 299003.21875\n",
            "in training loop, epoch 2, step 328, the loss is 203505.5\n",
            "in training loop, epoch 2, step 329, the loss is 166264.734375\n",
            "in training loop, epoch 2, step 330, the loss is 139710.359375\n",
            "in training loop, epoch 2, step 331, the loss is 178369.5\n",
            "in training loop, epoch 2, step 332, the loss is 153077.1875\n",
            "in training loop, epoch 2, step 333, the loss is 120879.4921875\n",
            "in training loop, epoch 2, step 334, the loss is 99599.7421875\n",
            "in training loop, epoch 2, step 335, the loss is 141988.296875\n",
            "in training loop, epoch 2, step 336, the loss is 231016.0\n",
            "in training loop, epoch 2, step 337, the loss is 179636.640625\n",
            "in training loop, epoch 2, step 338, the loss is 243805.515625\n",
            "in training loop, epoch 2, step 339, the loss is 416448.5\n",
            "in training loop, epoch 2, step 340, the loss is 124316.7578125\n",
            "in training loop, epoch 2, step 341, the loss is 208802.421875\n",
            "in training loop, epoch 2, step 342, the loss is 292486.65625\n",
            "in training loop, epoch 2, step 343, the loss is 181254.09375\n",
            "in training loop, epoch 2, step 344, the loss is 228600.140625\n",
            "in training loop, epoch 2, step 345, the loss is 237933.15625\n",
            "in training loop, epoch 2, step 346, the loss is 264608.3125\n",
            "in training loop, epoch 2, step 347, the loss is 248241.953125\n",
            "in training loop, epoch 2, step 348, the loss is 311454.46875\n",
            "in training loop, epoch 2, step 349, the loss is 311349.53125\n",
            "in training loop, epoch 2, step 350, the loss is 343889.65625\n",
            "in training loop, epoch 2, step 351, the loss is 160808.28125\n",
            "in training loop, epoch 2, step 352, the loss is 165148.96875\n",
            "in training loop, epoch 2, step 353, the loss is 178298.15625\n",
            "in training loop, epoch 2, step 354, the loss is 144330.265625\n",
            "in training loop, epoch 2, step 355, the loss is 280751.625\n",
            "in training loop, epoch 2, step 356, the loss is 378582.4375\n",
            "in training loop, epoch 2, step 357, the loss is 188993.671875\n",
            "in training loop, epoch 2, step 358, the loss is 412828.15625\n",
            "in training loop, epoch 2, step 359, the loss is 406192.1875\n",
            "in training loop, epoch 2, step 360, the loss is 340317.5\n",
            "in training loop, epoch 2, step 361, the loss is 163508.46875\n",
            "in training loop, epoch 2, step 362, the loss is 233116.296875\n",
            "in training loop, epoch 2, step 363, the loss is 212400.34375\n",
            "in training loop, epoch 2, step 364, the loss is 164687.75\n",
            "in training loop, epoch 2, step 365, the loss is 261149.453125\n",
            "in training loop, epoch 2, step 366, the loss is 363562.5625\n",
            "in training loop, epoch 2, step 367, the loss is 530772.875\n",
            "in training loop, epoch 2, step 368, the loss is 370093.4375\n",
            "in training loop, epoch 2, step 369, the loss is 267777.65625\n",
            "in training loop, epoch 2, step 370, the loss is 256984.953125\n",
            "in training loop, epoch 2, step 371, the loss is 163596.84375\n",
            "in training loop, epoch 2, step 372, the loss is 761778.5\n",
            "in training loop, epoch 2, step 373, the loss is 438026.4375\n",
            "in training loop, epoch 2, step 374, the loss is 431101.21875\n",
            "in training loop, epoch 2, step 375, the loss is 453952.5\n",
            "in training loop, epoch 2, step 376, the loss is 415729.5625\n",
            "in training loop, epoch 2, step 377, the loss is 451193.28125\n",
            "in training loop, epoch 2, step 378, the loss is 289971.65625\n",
            "in training loop, epoch 2, step 379, the loss is 271588.6875\n",
            "in training loop, epoch 2, step 380, the loss is 245533.234375\n",
            "in training loop, epoch 2, step 381, the loss is 276938.8125\n",
            "in training loop, epoch 2, step 382, the loss is 925058.625\n",
            "in training loop, epoch 2, step 383, the loss is 232785.1875\n",
            "in training loop, epoch 2, step 384, the loss is 806462.5\n",
            "in training loop, epoch 2, step 385, the loss is 624002.375\n",
            "in training loop, epoch 2, step 386, the loss is 352978.09375\n",
            "in training loop, epoch 2, step 387, the loss is 641197.4375\n",
            "in training loop, epoch 2, step 388, the loss is 272597.71875\n",
            "in training loop, epoch 2, step 389, the loss is 317762.75\n",
            "in training loop, epoch 2, step 390, the loss is 773415.625\n",
            "in training loop, epoch 2, step 391, the loss is 282767.78125\n",
            "in training loop, epoch 2, step 392, the loss is 602588.5\n",
            "in training loop, epoch 2, step 393, the loss is 221099.40625\n",
            "in training loop, epoch 2, step 394, the loss is 326141.03125\n",
            "in training loop, epoch 2, step 395, the loss is 334574.96875\n",
            "in training loop, epoch 2, step 396, the loss is 530300.25\n",
            "in training loop, epoch 2, step 397, the loss is 477178.625\n",
            "in training loop, epoch 2, step 398, the loss is 419099.15625\n",
            "in training loop, epoch 2, step 399, the loss is 488533.25\n",
            "in training loop, epoch 2, step 400, the loss is 374756.4375\n",
            "in training loop, epoch 2, step 401, the loss is 456130.1875\n",
            "in training loop, epoch 2, step 402, the loss is 203321.4375\n",
            "in training loop, epoch 2, step 403, the loss is 361719.375\n",
            "in training loop, epoch 2, step 404, the loss is 319378.03125\n",
            "in training loop, epoch 2, step 405, the loss is 286221.21875\n",
            "in training loop, epoch 2, step 406, the loss is 284404.0\n",
            "in training loop, epoch 2, step 407, the loss is 263553.53125\n",
            "in training loop, epoch 2, step 408, the loss is 214473.625\n",
            "in training loop, epoch 2, step 409, the loss is 478808.9375\n",
            "in training loop, epoch 2, step 410, the loss is 331914.25\n",
            "in training loop, epoch 2, step 411, the loss is 341769.1875\n",
            "in training loop, epoch 2, step 412, the loss is 260004.25\n",
            "in training loop, epoch 2, step 413, the loss is 451543.21875\n",
            "in training loop, epoch 2, step 414, the loss is 282042.0\n",
            "in training loop, epoch 2, step 415, the loss is 310837.5625\n",
            "in training loop, epoch 2, step 416, the loss is 464820.53125\n",
            "in training loop, epoch 2, step 417, the loss is 226474.75\n",
            "in training loop, epoch 2, step 418, the loss is 153966.0625\n",
            "in training loop, epoch 2, step 419, the loss is 504624.1875\n",
            "in training loop, epoch 2, step 420, the loss is 332055.125\n",
            "in training loop, epoch 2, step 421, the loss is 292068.3125\n",
            "in training loop, epoch 2, step 422, the loss is 346923.0625\n",
            "in training loop, epoch 2, step 423, the loss is 360360.1875\n",
            "in training loop, epoch 2, step 424, the loss is 166737.9375\n",
            "in training loop, epoch 2, step 425, the loss is 220650.140625\n",
            "in training loop, epoch 2, step 426, the loss is 213524.9375\n",
            "in training loop, epoch 2, step 427, the loss is 460126.34375\n",
            "in training loop, epoch 2, step 428, the loss is 240546.59375\n",
            "in training loop, epoch 2, step 429, the loss is 258492.546875\n",
            "in training loop, epoch 2, step 430, the loss is 153604.703125\n",
            "in training loop, epoch 2, step 431, the loss is 233707.546875\n",
            "in training loop, epoch 2, step 432, the loss is 275240.875\n",
            "in training loop, epoch 2, step 433, the loss is 191942.96875\n",
            "in training loop, epoch 2, step 434, the loss is 323558.4375\n",
            "in training loop, epoch 2, step 435, the loss is 217393.234375\n",
            "in training loop, epoch 2, step 436, the loss is 263986.84375\n",
            "in training loop, epoch 2, step 437, the loss is 260746.90625\n",
            "in training loop, epoch 2, step 438, the loss is 344785.125\n",
            "in training loop, epoch 2, step 439, the loss is 218725.390625\n",
            "in training loop, epoch 2, step 440, the loss is 300430.6875\n",
            "in training loop, epoch 2, step 441, the loss is 203191.515625\n",
            "in training loop, epoch 2, step 442, the loss is 353096.0625\n",
            "in training loop, epoch 2, step 443, the loss is 163074.578125\n",
            "in training loop, epoch 2, step 444, the loss is 253747.6875\n",
            "in training loop, epoch 2, step 445, the loss is 278742.5625\n",
            "in training loop, epoch 2, step 446, the loss is 348071.0625\n",
            "in training loop, epoch 2, step 447, the loss is 115232.046875\n",
            "in training loop, epoch 2, step 448, the loss is 283487.875\n",
            "in training loop, epoch 2, step 449, the loss is 142457.15625\n",
            "in training loop, epoch 2, step 450, the loss is 325586.3125\n",
            "in training loop, epoch 2, step 451, the loss is 209979.03125\n",
            "in training loop, epoch 2, step 452, the loss is 176467.125\n",
            "in training loop, epoch 2, step 453, the loss is 214624.3125\n",
            "in training loop, epoch 2, step 454, the loss is 245277.90625\n",
            "in training loop, epoch 2, step 455, the loss is 223441.015625\n",
            "in training loop, epoch 2, step 456, the loss is 222779.21875\n",
            "in training loop, epoch 2, step 457, the loss is 129883.765625\n",
            "in training loop, epoch 2, step 458, the loss is 353801.15625\n",
            "in training loop, epoch 2, step 459, the loss is 269998.5\n",
            "in training loop, epoch 2, step 460, the loss is 239258.28125\n",
            "in training loop, epoch 2, step 461, the loss is 225368.859375\n",
            "in training loop, epoch 2, step 462, the loss is 468229.25\n",
            "in training loop, epoch 2, step 463, the loss is 426261.96875\n",
            "in training loop, epoch 2, step 464, the loss is 279347.6875\n",
            "in training loop, epoch 2, step 465, the loss is 246850.171875\n",
            "in training loop, epoch 2, step 466, the loss is 344960.9375\n",
            "in training loop, epoch 2, step 467, the loss is 399777.25\n",
            "in training loop, epoch 2, step 468, the loss is 313274.0\n",
            "in training loop, epoch 2, step 469, the loss is 182420.640625\n",
            "in training loop, epoch 2, step 470, the loss is 276982.71875\n",
            "in training loop, epoch 2, step 471, the loss is 680345.125\n",
            "in training loop, epoch 2, step 472, the loss is 160622.40625\n",
            "in training loop, epoch 2, step 473, the loss is 521471.15625\n",
            "in training loop, epoch 2, step 474, the loss is 360064.5\n",
            "in training loop, epoch 2, step 475, the loss is 818869.125\n",
            "in training loop, epoch 2, step 476, the loss is 240032.078125\n",
            "in training loop, epoch 2, step 477, the loss is 366169.03125\n",
            "in training loop, epoch 2, step 478, the loss is 680483.0\n",
            "in training loop, epoch 2, step 479, the loss is 571054.8125\n",
            "in training loop, epoch 2, step 480, the loss is 272677.875\n",
            "in training loop, epoch 2, step 481, the loss is 219684.609375\n",
            "in training loop, epoch 2, step 482, the loss is 257453.046875\n",
            "in training loop, epoch 2, step 483, the loss is 239951.46875\n",
            "in training loop, epoch 2, step 484, the loss is 490666.75\n",
            "in training loop, epoch 2, step 485, the loss is 100300.421875\n",
            "in training loop, epoch 2, step 486, the loss is 154325.828125\n",
            "in training loop, epoch 2, step 487, the loss is 413219.40625\n",
            "in training loop, epoch 2, step 488, the loss is 130396.203125\n",
            "in training loop, epoch 2, step 489, the loss is 380614.5625\n",
            "in training loop, epoch 2, step 490, the loss is 168588.59375\n",
            "in training loop, epoch 2, step 491, the loss is 285495.8125\n",
            "in training loop, epoch 2, step 492, the loss is 370839.5625\n",
            "in training loop, epoch 2, step 493, the loss is 298869.875\n",
            "in training loop, epoch 2, step 494, the loss is 247100.625\n",
            "in training loop, epoch 2, step 495, the loss is 461863.65625\n",
            "in training loop, epoch 2, step 496, the loss is 163987.375\n",
            "in training loop, epoch 2, step 497, the loss is 350151.625\n",
            "in training loop, epoch 2, step 498, the loss is 294645.40625\n",
            "in training loop, epoch 2, step 499, the loss is 191932.8125\n",
            "in training loop, epoch 2, step 500, the loss is 208223.0\n",
            "in training loop, epoch 2, step 501, the loss is 227359.21875\n",
            "in training loop, epoch 2, step 502, the loss is 333426.75\n",
            "in training loop, epoch 2, step 503, the loss is 93635.21875\n",
            "in training loop, epoch 2, step 504, the loss is 130163.421875\n",
            "in training loop, epoch 2, step 505, the loss is 289748.34375\n",
            "in training loop, epoch 2, step 506, the loss is 229836.125\n",
            "in training loop, epoch 2, step 507, the loss is 161603.71875\n",
            "in training loop, epoch 2, step 508, the loss is 197660.34375\n",
            "in training loop, epoch 2, step 509, the loss is 226017.09375\n",
            "in training loop, epoch 2, step 510, the loss is 195325.296875\n",
            "in training loop, epoch 2, step 511, the loss is 246116.828125\n",
            "in training loop, epoch 2, step 512, the loss is 362579.0625\n",
            "in training loop, epoch 2, step 513, the loss is 312751.96875\n",
            "in training loop, epoch 2, step 514, the loss is 166187.0625\n",
            "in training loop, epoch 2, step 515, the loss is 302271.3125\n",
            "in training loop, epoch 2, step 516, the loss is 264520.9375\n",
            "in training loop, epoch 2, step 517, the loss is 394734.0\n",
            "in training loop, epoch 2, step 518, the loss is 207724.59375\n",
            "in training loop, epoch 2, step 519, the loss is 203568.4375\n",
            "in training loop, epoch 2, step 520, the loss is 249332.640625\n",
            "in training loop, epoch 2, step 521, the loss is 162133.0\n",
            "in training loop, epoch 2, step 522, the loss is 345931.125\n",
            "in training loop, epoch 2, step 523, the loss is 272273.125\n",
            "in training loop, epoch 2, step 524, the loss is 174123.203125\n",
            "in training loop, epoch 2, step 525, the loss is 202380.140625\n",
            "in training loop, epoch 2, step 526, the loss is 309165.84375\n",
            "in training loop, epoch 2, step 527, the loss is 353513.59375\n",
            "in training loop, epoch 2, step 528, the loss is 512402.125\n",
            "in training loop, epoch 2, step 529, the loss is 286117.28125\n",
            "in training loop, epoch 2, step 530, the loss is 171762.234375\n",
            "in training loop, epoch 2, step 531, the loss is 254319.1875\n",
            "in training loop, epoch 2, step 532, the loss is 290655.4375\n",
            "in training loop, epoch 2, step 533, the loss is 209923.9375\n",
            "in training loop, epoch 2, step 534, the loss is 451621.1875\n",
            "in training loop, epoch 2, step 535, the loss is 274399.8125\n",
            "in training loop, epoch 2, step 536, the loss is 108966.5625\n",
            "in training loop, epoch 2, step 537, the loss is 317817.6875\n",
            "in training loop, epoch 2, step 538, the loss is 274935.53125\n",
            "in training loop, epoch 2, step 539, the loss is 156875.125\n",
            "in training loop, epoch 2, step 540, the loss is 397709.5\n",
            "in training loop, epoch 2, step 541, the loss is 377212.03125\n",
            "in training loop, epoch 2, step 542, the loss is 274929.84375\n",
            "in training loop, epoch 2, step 543, the loss is 529641.25\n",
            "in training loop, epoch 2, step 544, the loss is 418718.8125\n",
            "in training loop, epoch 2, step 545, the loss is 237759.078125\n",
            "in training loop, epoch 2, step 546, the loss is 222741.046875\n",
            "in training loop, epoch 2, step 547, the loss is 232943.890625\n",
            "in training loop, epoch 2, step 548, the loss is 341120.90625\n",
            "in training loop, epoch 2, step 549, the loss is 395826.15625\n",
            "in training loop, epoch 2, step 550, the loss is 298376.84375\n",
            "in training loop, epoch 2, step 551, the loss is 336057.46875\n",
            "in training loop, epoch 2, step 552, the loss is 162399.9375\n",
            "in training loop, epoch 2, step 553, the loss is 142942.0625\n",
            "in training loop, epoch 2, step 554, the loss is 169068.34375\n",
            "in training loop, epoch 2, step 555, the loss is 285859.96875\n",
            "in training loop, epoch 2, step 556, the loss is 278366.9375\n",
            "in training loop, epoch 2, step 557, the loss is 229445.96875\n",
            "in training loop, epoch 2, step 558, the loss is 228024.6875\n",
            "in training loop, epoch 2, step 559, the loss is 317442.09375\n",
            "in training loop, epoch 2, step 560, the loss is 203412.78125\n",
            "in training loop, epoch 2, step 561, the loss is 241029.125\n",
            "in training loop, epoch 2, step 562, the loss is 265489.40625\n",
            "in training loop, epoch 2, step 563, the loss is 174045.953125\n",
            "in training loop, epoch 2, step 564, the loss is 307550.5625\n",
            "in training loop, epoch 2, step 565, the loss is 513642.5\n",
            "in training loop, epoch 2, step 566, the loss is 264985.34375\n",
            "in training loop, epoch 2, step 567, the loss is 205909.140625\n",
            "in training loop, epoch 2, step 568, the loss is 330669.1875\n",
            "in training loop, epoch 2, step 569, the loss is 285051.0625\n",
            "in training loop, epoch 2, step 570, the loss is 193237.953125\n",
            "in training loop, epoch 2, step 571, the loss is 220956.109375\n",
            "in training loop, epoch 2, step 572, the loss is 172196.59375\n",
            "in training loop, epoch 2, step 573, the loss is 636215.3125\n",
            "in training loop, epoch 2, step 574, the loss is 197191.015625\n",
            "in training loop, epoch 2, step 575, the loss is 210981.328125\n",
            "in training loop, epoch 2, step 576, the loss is 186430.953125\n",
            "in training loop, epoch 2, step 577, the loss is 332162.15625\n",
            "in training loop, epoch 2, step 578, the loss is 218582.78125\n",
            "in training loop, epoch 2, step 579, the loss is 211104.84375\n",
            "in training loop, epoch 2, step 580, the loss is 213046.21875\n",
            "in training loop, epoch 2, step 581, the loss is 314207.6875\n",
            "in training loop, epoch 2, step 582, the loss is 270123.03125\n",
            "in training loop, epoch 2, step 583, the loss is 180155.75\n",
            "in training loop, epoch 2, step 584, the loss is 296154.15625\n",
            "in training loop, epoch 2, step 585, the loss is 171432.421875\n",
            "in training loop, epoch 2, step 586, the loss is 276291.0\n",
            "in training loop, epoch 2, step 587, the loss is 276815.84375\n",
            "in training loop, epoch 2, step 588, the loss is 172326.984375\n",
            "in training loop, epoch 2, step 589, the loss is 114737.375\n",
            "in training loop, epoch 2, step 590, the loss is 404788.5\n",
            "in training loop, epoch 2, step 591, the loss is 374778.40625\n",
            "in training loop, epoch 2, step 592, the loss is 131647.15625\n",
            "in training loop, epoch 2, step 593, the loss is 234721.78125\n",
            "in training loop, epoch 2, step 594, the loss is 273001.5\n",
            "in training loop, epoch 2, step 595, the loss is 208308.5\n",
            "in training loop, epoch 2, step 596, the loss is 127597.359375\n",
            "in training loop, epoch 2, step 597, the loss is 155883.3125\n",
            "in training loop, epoch 2, step 598, the loss is 207610.5\n",
            "in training loop, epoch 2, step 599, the loss is 352942.75\n",
            "in training loop, epoch 2, step 600, the loss is 181792.1875\n",
            "in training loop, epoch 2, step 601, the loss is 192175.1875\n",
            "in training loop, epoch 2, step 602, the loss is 217510.765625\n",
            "in training loop, epoch 2, step 603, the loss is 389338.75\n",
            "in training loop, epoch 2, step 604, the loss is 252813.03125\n",
            "in training loop, epoch 2, step 605, the loss is 200969.390625\n",
            "in training loop, epoch 2, step 606, the loss is 267259.3125\n",
            "in training loop, epoch 2, step 607, the loss is 182198.59375\n",
            "in training loop, epoch 2, step 608, the loss is 577495.1875\n",
            "in training loop, epoch 2, step 609, the loss is 283200.15625\n",
            "in training loop, epoch 2, step 610, the loss is 478378.46875\n",
            "in training loop, epoch 2, step 611, the loss is 297332.1875\n",
            "in training loop, epoch 2, step 612, the loss is 554736.75\n",
            "in training loop, epoch 2, step 613, the loss is 322398.28125\n",
            "in training loop, epoch 2, step 614, the loss is 109210.515625\n",
            "in training loop, epoch 2, step 615, the loss is 215915.65625\n",
            "in training loop, epoch 2, step 616, the loss is 222170.21875\n",
            "in training loop, epoch 2, step 617, the loss is 344785.53125\n",
            "in training loop, epoch 2, step 618, the loss is 285105.125\n",
            "in training loop, epoch 2, step 619, the loss is 202038.53125\n",
            "in training loop, epoch 2, step 620, the loss is 271981.3125\n",
            "in training loop, epoch 2, step 621, the loss is 256669.828125\n",
            "in training loop, epoch 2, step 622, the loss is 270160.3125\n",
            "in training loop, epoch 2, step 623, the loss is 148307.59375\n",
            "in training loop, epoch 2, step 624, the loss is 273626.0\n",
            "in training loop, epoch 2, step 625, the loss is 250637.84375\n",
            "in training loop, epoch 2, step 626, the loss is 208252.90625\n",
            "in training loop, epoch 2, step 627, the loss is 227812.984375\n",
            "in training loop, epoch 2, step 628, the loss is 261021.15625\n",
            "in training loop, epoch 2, step 629, the loss is 208261.0625\n",
            "in training loop, epoch 2, step 630, the loss is 152828.09375\n",
            "in training loop, epoch 2, step 631, the loss is 244358.46875\n",
            "in training loop, epoch 2, step 632, the loss is 167337.671875\n",
            "in training loop, epoch 2, step 633, the loss is 256457.34375\n",
            "in training loop, epoch 2, step 634, the loss is 501632.1875\n",
            "in training loop, epoch 2, step 635, the loss is 461823.09375\n",
            "in training loop, epoch 2, step 636, the loss is 256780.5\n",
            "in training loop, epoch 2, step 637, the loss is 323509.53125\n",
            "in training loop, epoch 2, step 638, the loss is 323409.25\n",
            "in training loop, epoch 2, step 639, the loss is 234791.140625\n",
            "in training loop, epoch 2, step 640, the loss is 156934.078125\n",
            "in training loop, epoch 2, step 641, the loss is 147291.390625\n",
            "in training loop, epoch 2, step 642, the loss is 221619.625\n",
            "in training loop, epoch 2, step 643, the loss is 266568.4375\n",
            "in training loop, epoch 2, step 644, the loss is 267907.0\n",
            "in training loop, epoch 2, step 645, the loss is 336253.75\n",
            "in training loop, epoch 2, step 646, the loss is 256339.21875\n",
            "in training loop, epoch 2, step 647, the loss is 207974.234375\n",
            "in training loop, epoch 2, step 648, the loss is 343701.65625\n",
            "in training loop, epoch 2, step 649, the loss is 208639.0625\n",
            "in training loop, epoch 2, step 650, the loss is 101415.4296875\n",
            "in training loop, epoch 2, step 651, the loss is 171962.203125\n",
            "in training loop, epoch 2, step 652, the loss is 270218.4375\n",
            "in training loop, epoch 2, step 653, the loss is 175438.453125\n",
            "in training loop, epoch 2, step 654, the loss is 568237.6875\n",
            "in training loop, epoch 2, step 655, the loss is 199306.53125\n",
            "in training loop, epoch 2, step 656, the loss is 279994.65625\n",
            "in training loop, epoch 2, step 657, the loss is 202243.515625\n",
            "in training loop, epoch 2, step 658, the loss is 115076.5390625\n",
            "in training loop, epoch 2, step 659, the loss is 139822.765625\n",
            "in training loop, epoch 2, step 660, the loss is 293491.625\n",
            "in training loop, epoch 2, step 661, the loss is 193719.96875\n",
            "in training loop, epoch 2, step 662, the loss is 240110.125\n",
            "in training loop, epoch 2, step 663, the loss is 180163.171875\n",
            "in training loop, epoch 2, step 664, the loss is 334664.625\n",
            "in training loop, epoch 2, step 665, the loss is 257168.71875\n",
            "in training loop, epoch 2, step 666, the loss is 221332.03125\n",
            "in training loop, epoch 2, step 667, the loss is 169830.4375\n",
            "in training loop, epoch 2, step 668, the loss is 262524.125\n",
            "in training loop, epoch 2, step 669, the loss is 318223.75\n",
            "in training loop, epoch 2, step 670, the loss is 239526.234375\n",
            "in training loop, epoch 2, step 671, the loss is 314273.09375\n",
            "in training loop, epoch 2, step 672, the loss is 210429.75\n",
            "in training loop, epoch 2, step 673, the loss is 215105.9375\n",
            "in training loop, epoch 2, step 674, the loss is 427182.09375\n",
            "in training loop, epoch 2, step 675, the loss is 223576.8125\n",
            "in training loop, epoch 2, step 676, the loss is 148691.90625\n",
            "in training loop, epoch 2, step 677, the loss is 322015.125\n",
            "in training loop, epoch 2, step 678, the loss is 169412.90625\n",
            "in training loop, epoch 2, step 679, the loss is 268707.3125\n",
            "in training loop, epoch 2, step 680, the loss is 236966.78125\n",
            "in training loop, epoch 2, step 681, the loss is 259358.796875\n",
            "in training loop, epoch 2, step 682, the loss is 168985.859375\n",
            "in training loop, epoch 2, step 683, the loss is 316193.5625\n",
            "in training loop, epoch 2, step 684, the loss is 276526.4375\n",
            "in training loop, epoch 2, step 685, the loss is 457870.25\n",
            "in training loop, epoch 2, step 686, the loss is 178453.15625\n",
            "in training loop, epoch 2, step 687, the loss is 277055.71875\n",
            "in training loop, epoch 2, step 688, the loss is 488480.84375\n",
            "in training loop, epoch 2, step 689, the loss is 240327.34375\n",
            "in training loop, epoch 2, step 690, the loss is 311080.90625\n",
            "in training loop, epoch 2, step 691, the loss is 261206.140625\n",
            "in training loop, epoch 2, step 692, the loss is 264246.75\n",
            "in training loop, epoch 2, step 693, the loss is 214188.15625\n",
            "in training loop, epoch 2, step 694, the loss is 229038.53125\n",
            "in training loop, epoch 2, step 695, the loss is 204285.6875\n",
            "in training loop, epoch 2, step 696, the loss is 232761.359375\n",
            "in training loop, epoch 2, step 697, the loss is 242590.8125\n",
            "in training loop, epoch 2, step 698, the loss is 210645.984375\n",
            "in training loop, epoch 2, step 699, the loss is 254759.0625\n",
            "in training loop, epoch 2, step 700, the loss is 220841.140625\n",
            "in training loop, epoch 2, step 701, the loss is 349686.1875\n",
            "in training loop, epoch 2, step 702, the loss is 212377.984375\n",
            "in training loop, epoch 2, step 703, the loss is 323087.125\n",
            "in training loop, epoch 2, step 704, the loss is 283607.34375\n",
            "in training loop, epoch 2, step 705, the loss is 180614.6875\n",
            "in training loop, epoch 2, step 706, the loss is 350024.25\n",
            "in training loop, epoch 2, step 707, the loss is 146116.1875\n",
            "in training loop, epoch 2, step 708, the loss is 161218.125\n",
            "in training loop, epoch 2, step 709, the loss is 324634.21875\n",
            "in training loop, epoch 2, step 710, the loss is 103444.3203125\n",
            "in training loop, epoch 2, step 711, the loss is 289522.9375\n",
            "in training loop, epoch 2, step 712, the loss is 264365.78125\n",
            "in training loop, epoch 2, step 713, the loss is 291230.40625\n",
            "in training loop, epoch 2, step 714, the loss is 193963.484375\n",
            "in training loop, epoch 2, step 715, the loss is 308271.65625\n",
            "in training loop, epoch 2, step 716, the loss is 256146.625\n",
            "in training loop, epoch 2, step 717, the loss is 320956.25\n",
            "in training loop, epoch 2, step 718, the loss is 190501.21875\n",
            "in training loop, epoch 2, step 719, the loss is 173852.640625\n",
            "in training loop, epoch 2, step 720, the loss is 267635.59375\n",
            "in training loop, epoch 2, step 721, the loss is 216102.21875\n",
            "in training loop, epoch 2, step 722, the loss is 215244.015625\n",
            "in training loop, epoch 2, step 723, the loss is 220975.1875\n",
            "in training loop, epoch 2, step 724, the loss is 155665.953125\n",
            "in training loop, epoch 2, step 725, the loss is 256544.625\n",
            "in training loop, epoch 2, step 726, the loss is 141528.4375\n",
            "in training loop, epoch 2, step 727, the loss is 155172.390625\n",
            "in training loop, epoch 2, step 728, the loss is 152764.28125\n",
            "in training loop, epoch 2, step 729, the loss is 261859.65625\n",
            "in training loop, epoch 2, step 730, the loss is 158712.375\n",
            "in training loop, epoch 2, step 731, the loss is 138826.046875\n",
            "in training loop, epoch 2, step 732, the loss is 256511.984375\n",
            "in training loop, epoch 2, step 733, the loss is 289407.59375\n",
            "in training loop, epoch 2, step 734, the loss is 164726.234375\n",
            "in training loop, epoch 2, step 735, the loss is 225335.34375\n",
            "in training loop, epoch 2, step 736, the loss is 251398.34375\n",
            "in training loop, epoch 2, step 737, the loss is 207631.484375\n",
            "in training loop, epoch 2, step 738, the loss is 151792.359375\n",
            "in training loop, epoch 2, step 739, the loss is 260716.859375\n",
            "in training loop, epoch 2, step 740, the loss is 305230.5625\n",
            "in training loop, epoch 2, step 741, the loss is 201438.890625\n",
            "in training loop, epoch 2, step 742, the loss is 148983.0\n",
            "in training loop, epoch 2, step 743, the loss is 287587.5\n",
            "in training loop, epoch 2, step 744, the loss is 465930.15625\n",
            "in training loop, epoch 2, step 745, the loss is 280747.4375\n",
            "in training loop, epoch 2, step 746, the loss is 260570.5625\n",
            "in training loop, epoch 2, step 747, the loss is 178342.015625\n",
            "in training loop, epoch 2, step 748, the loss is 308609.6875\n",
            "in training loop, epoch 2, step 749, the loss is 312900.125\n",
            "in training loop, epoch 2, step 750, the loss is 147479.15625\n",
            "in training loop, epoch 2, step 751, the loss is 113729.78125\n",
            "in training loop, epoch 2, step 752, the loss is 283365.5\n",
            "in training loop, epoch 2, step 753, the loss is 106854.8046875\n",
            "in training loop, epoch 2, step 754, the loss is 233764.546875\n",
            "in training loop, epoch 2, step 755, the loss is 218940.09375\n",
            "in training loop, epoch 2, step 756, the loss is 219895.1875\n",
            "in training loop, epoch 2, step 757, the loss is 192922.03125\n",
            "in training loop, epoch 2, step 758, the loss is 186543.984375\n",
            "in training loop, epoch 2, step 759, the loss is 227316.6875\n",
            "in training loop, epoch 2, step 760, the loss is 227008.640625\n",
            "in training loop, epoch 2, step 761, the loss is 301113.5\n",
            "in training loop, epoch 2, step 762, the loss is 217432.3125\n",
            "in training loop, epoch 2, step 763, the loss is 209954.21875\n",
            "in training loop, epoch 2, step 764, the loss is 230880.78125\n",
            "in training loop, epoch 2, step 765, the loss is 278193.03125\n",
            "in training loop, epoch 2, step 766, the loss is 255131.3125\n",
            "in training loop, epoch 2, step 767, the loss is 220475.71875\n",
            "in training loop, epoch 2, step 768, the loss is 227417.53125\n",
            "in training loop, epoch 2, step 769, the loss is 251153.40625\n",
            "in training loop, epoch 2, step 770, the loss is 216722.0625\n",
            "in training loop, epoch 2, step 771, the loss is 294803.15625\n",
            "in training loop, epoch 2, step 772, the loss is 188418.21875\n",
            "in training loop, epoch 2, step 773, the loss is 196823.171875\n",
            "in training loop, epoch 2, step 774, the loss is 147700.96875\n",
            "in training loop, epoch 2, step 775, the loss is 265445.9375\n",
            "in training loop, epoch 2, step 776, the loss is 251060.9375\n",
            "in training loop, epoch 2, step 777, the loss is 226299.03125\n",
            "in training loop, epoch 2, step 778, the loss is 262438.0625\n",
            "in training loop, epoch 2, step 779, the loss is 172793.671875\n",
            "in training loop, epoch 2, step 780, the loss is 258586.40625\n",
            "in training loop, epoch 2, step 781, the loss is 220960.0625\n",
            "in training loop, epoch 2, step 782, the loss is 180470.65625\n",
            "in training loop, epoch 2, step 783, the loss is 137732.75\n",
            "in training loop, epoch 2, step 784, the loss is 216019.84375\n",
            "in training loop, epoch 2, step 785, the loss is 269235.71875\n",
            "in training loop, epoch 2, step 786, the loss is 267316.53125\n",
            "in training loop, epoch 2, step 787, the loss is 214421.515625\n",
            "in training loop, epoch 2, step 788, the loss is 118225.1875\n",
            "in training loop, epoch 2, step 789, the loss is 258131.4375\n",
            "in training loop, epoch 2, step 790, the loss is 250837.0625\n",
            "in training loop, epoch 2, step 791, the loss is 161088.875\n",
            "in training loop, epoch 2, step 792, the loss is 380459.5\n",
            "in training loop, epoch 2, step 793, the loss is 174877.875\n",
            "in training loop, epoch 2, step 794, the loss is 333674.21875\n",
            "in training loop, epoch 2, step 795, the loss is 366120.71875\n",
            "in training loop, epoch 2, step 796, the loss is 185414.65625\n",
            "in training loop, epoch 2, step 797, the loss is 579549.0625\n",
            "in training loop, epoch 2, step 798, the loss is 175710.34375\n",
            "in training loop, epoch 2, step 799, the loss is 162984.125\n",
            "in training loop, epoch 2, step 800, the loss is 267739.65625\n",
            "in training loop, epoch 2, step 801, the loss is 356203.03125\n",
            "in training loop, epoch 2, step 802, the loss is 264225.28125\n",
            "in training loop, epoch 2, step 803, the loss is 152395.515625\n",
            "in training loop, epoch 2, step 804, the loss is 150696.75\n",
            "in training loop, epoch 2, step 805, the loss is 207654.109375\n",
            "in training loop, epoch 2, step 806, the loss is 305876.4375\n",
            "in training loop, epoch 2, step 807, the loss is 200116.5625\n",
            "in training loop, epoch 2, step 808, the loss is 287170.59375\n",
            "in training loop, epoch 2, step 809, the loss is 295408.875\n",
            "in training loop, epoch 2, step 810, the loss is 135466.34375\n",
            "in training loop, epoch 2, step 811, the loss is 405851.1875\n",
            "in training loop, epoch 2, step 812, the loss is 219628.515625\n",
            "in training loop, epoch 2, step 813, the loss is 271486.1875\n",
            "in training loop, epoch 2, step 814, the loss is 186621.46875\n",
            "in training loop, epoch 2, step 815, the loss is 380743.46875\n",
            "in training loop, epoch 2, step 816, the loss is 276895.1875\n",
            "in training loop, epoch 2, step 817, the loss is 168293.46875\n",
            "in training loop, epoch 2, step 818, the loss is 207184.609375\n",
            "in training loop, epoch 2, step 819, the loss is 361763.9375\n",
            "in training loop, epoch 2, step 820, the loss is 258427.09375\n",
            "in training loop, epoch 2, step 821, the loss is 307025.6875\n",
            "in training loop, epoch 2, step 822, the loss is 297669.25\n",
            "in training loop, epoch 2, step 823, the loss is 281047.46875\n",
            "in training loop, epoch 2, step 824, the loss is 300072.96875\n",
            "in training loop, epoch 2, step 825, the loss is 323810.34375\n",
            "in training loop, epoch 2, step 826, the loss is 330377.1875\n",
            "in training loop, epoch 2, step 827, the loss is 406439.75\n",
            "in training loop, epoch 2, step 828, the loss is 383896.34375\n",
            "in training loop, epoch 2, step 829, the loss is 285609.21875\n",
            "in training loop, epoch 2, step 830, the loss is 409722.8125\n",
            "in training loop, epoch 2, step 831, the loss is 258346.8125\n",
            "in training loop, epoch 2, step 832, the loss is 181214.546875\n",
            "in training loop, epoch 2, step 833, the loss is 309858.0625\n",
            "in training loop, epoch 2, step 834, the loss is 374020.75\n",
            "in training loop, epoch 2, step 835, the loss is 320440.9375\n",
            "in training loop, epoch 2, step 836, the loss is 217401.03125\n",
            "in training loop, epoch 2, step 837, the loss is 211325.75\n",
            "in training loop, epoch 2, step 838, the loss is 123917.5078125\n",
            "in training loop, epoch 2, step 839, the loss is 339358.40625\n",
            "in training loop, epoch 2, step 840, the loss is 354209.6875\n",
            "in training loop, epoch 2, step 841, the loss is 190686.578125\n",
            "in training loop, epoch 2, step 842, the loss is 359844.1875\n",
            "in training loop, epoch 2, step 843, the loss is 98055.296875\n",
            "in training loop, epoch 2, step 844, the loss is 187600.96875\n",
            "in training loop, epoch 2, step 845, the loss is 179287.453125\n",
            "in training loop, epoch 2, step 846, the loss is 143932.96875\n",
            "in training loop, epoch 2, step 847, the loss is 175112.8125\n",
            "in training loop, epoch 2, step 848, the loss is 198304.078125\n",
            "in training loop, epoch 2, step 849, the loss is 247097.25\n",
            "in training loop, epoch 2, step 850, the loss is 269909.21875\n",
            "in training loop, epoch 2, step 851, the loss is 164502.5625\n",
            "in training loop, epoch 2, step 852, the loss is 317225.625\n",
            "in training loop, epoch 2, step 853, the loss is 229927.3125\n",
            "in training loop, epoch 2, step 854, the loss is 190009.03125\n",
            "in training loop, epoch 2, step 855, the loss is 150776.28125\n",
            "in training loop, epoch 2, step 856, the loss is 272591.125\n",
            "in training loop, epoch 2, step 857, the loss is 172210.765625\n",
            "in training loop, epoch 2, step 858, the loss is 220293.09375\n",
            "in training loop, epoch 2, step 859, the loss is 457315.40625\n",
            "in training loop, epoch 2, step 860, the loss is 82494.15625\n",
            "in training loop, epoch 2, step 861, the loss is 152927.03125\n",
            "in training loop, epoch 2, step 862, the loss is 163200.140625\n",
            "in training loop, epoch 2, step 863, the loss is 199922.671875\n",
            "in training loop, epoch 2, step 864, the loss is 133618.1875\n",
            "in training loop, epoch 2, step 865, the loss is 122114.765625\n",
            "in training loop, epoch 2, step 866, the loss is 116822.515625\n",
            "in training loop, epoch 2, step 867, the loss is 143266.96875\n",
            "in training loop, epoch 2, step 868, the loss is 260766.359375\n",
            "in training loop, epoch 2, step 869, the loss is 295561.34375\n",
            "in training loop, epoch 2, step 870, the loss is 180677.046875\n",
            "in training loop, epoch 2, step 871, the loss is 185464.84375\n",
            "in training loop, epoch 2, step 872, the loss is 280533.09375\n",
            "in training loop, epoch 2, step 873, the loss is 200791.984375\n",
            "in training loop, epoch 2, step 874, the loss is 210025.296875\n",
            "in training loop, epoch 2, step 875, the loss is 178296.0625\n",
            "in training loop, epoch 2, step 876, the loss is 203034.3125\n",
            "in training loop, epoch 2, step 877, the loss is 346747.53125\n",
            "in training loop, epoch 2, step 878, the loss is 152296.203125\n",
            "in training loop, epoch 2, step 879, the loss is 211671.75\n",
            "in training loop, epoch 2, step 880, the loss is 246998.15625\n",
            "in training loop, epoch 2, step 881, the loss is 187416.109375\n",
            "in training loop, epoch 2, step 882, the loss is 146724.0625\n",
            "in training loop, epoch 2, step 883, the loss is 348057.71875\n",
            "in training loop, epoch 2, step 884, the loss is 128126.8984375\n",
            "in training loop, epoch 2, step 885, the loss is 348735.3125\n",
            "in training loop, epoch 2, step 886, the loss is 373990.9375\n",
            "in training loop, epoch 2, step 887, the loss is 213509.09375\n",
            "in training loop, epoch 2, step 888, the loss is 309792.96875\n",
            "in training loop, epoch 2, step 889, the loss is 220313.109375\n",
            "in training loop, epoch 2, step 890, the loss is 117440.796875\n",
            "in training loop, epoch 2, step 891, the loss is 196609.15625\n",
            "in training loop, epoch 2, step 892, the loss is 196833.59375\n",
            "in training loop, epoch 2, step 893, the loss is 140804.609375\n",
            "in training loop, epoch 2, step 894, the loss is 267784.78125\n",
            "in training loop, epoch 2, step 895, the loss is 271353.3125\n",
            "in training loop, epoch 2, step 896, the loss is 232407.5625\n",
            "in training loop, epoch 2, step 897, the loss is 295812.125\n",
            "in training loop, epoch 2, step 898, the loss is 191494.65625\n",
            "in training loop, epoch 2, step 899, the loss is 190399.546875\n",
            "in training loop, epoch 2, step 900, the loss is 306457.5625\n",
            "in training loop, epoch 2, step 901, the loss is 396772.0625\n",
            "in training loop, epoch 2, step 902, the loss is 161205.125\n",
            "in training loop, epoch 2, step 903, the loss is 91166.4765625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "k-fold 0:: Epoch 2: train loss 261561.0876486449 val loss 299039.18456064357\n",
            "in training loop, epoch 3, step 0, the loss is 196793.421875\n",
            "in training loop, epoch 3, step 1, the loss is 227338.484375\n",
            "in training loop, epoch 3, step 2, the loss is 236931.5\n",
            "in training loop, epoch 3, step 3, the loss is 136745.578125\n",
            "in training loop, epoch 3, step 4, the loss is 213474.484375\n",
            "in training loop, epoch 3, step 5, the loss is 218670.15625\n",
            "in training loop, epoch 3, step 6, the loss is 182769.203125\n",
            "in training loop, epoch 3, step 7, the loss is 283146.25\n",
            "in training loop, epoch 3, step 8, the loss is 176081.71875\n",
            "in training loop, epoch 3, step 9, the loss is 146946.71875\n",
            "in training loop, epoch 3, step 10, the loss is 203788.171875\n",
            "in training loop, epoch 3, step 11, the loss is 226105.578125\n",
            "in training loop, epoch 3, step 12, the loss is 240257.796875\n",
            "in training loop, epoch 3, step 13, the loss is 162433.34375\n",
            "in training loop, epoch 3, step 14, the loss is 214638.484375\n",
            "in training loop, epoch 3, step 15, the loss is 222336.140625\n",
            "in training loop, epoch 3, step 16, the loss is 212690.796875\n",
            "in training loop, epoch 3, step 17, the loss is 227485.734375\n",
            "in training loop, epoch 3, step 18, the loss is 209815.6875\n",
            "in training loop, epoch 3, step 19, the loss is 191704.84375\n",
            "in training loop, epoch 3, step 20, the loss is 156592.234375\n",
            "in training loop, epoch 3, step 21, the loss is 141357.5\n",
            "in training loop, epoch 3, step 22, the loss is 138737.109375\n",
            "in training loop, epoch 3, step 23, the loss is 201785.1875\n",
            "in training loop, epoch 3, step 24, the loss is 179543.015625\n",
            "in training loop, epoch 3, step 25, the loss is 253470.9375\n",
            "in training loop, epoch 3, step 26, the loss is 157902.921875\n",
            "in training loop, epoch 3, step 27, the loss is 248064.8125\n",
            "in training loop, epoch 3, step 28, the loss is 158205.28125\n",
            "in training loop, epoch 3, step 29, the loss is 251403.4375\n",
            "in training loop, epoch 3, step 30, the loss is 213187.765625\n",
            "in training loop, epoch 3, step 31, the loss is 214939.859375\n",
            "in training loop, epoch 3, step 32, the loss is 197111.609375\n",
            "in training loop, epoch 3, step 33, the loss is 192242.28125\n",
            "in training loop, epoch 3, step 34, the loss is 222286.0\n",
            "in training loop, epoch 3, step 35, the loss is 118782.46875\n",
            "in training loop, epoch 3, step 36, the loss is 178909.953125\n",
            "in training loop, epoch 3, step 37, the loss is 207247.0\n",
            "in training loop, epoch 3, step 38, the loss is 236224.5\n",
            "in training loop, epoch 3, step 39, the loss is 192937.328125\n",
            "in training loop, epoch 3, step 40, the loss is 168756.0\n",
            "in training loop, epoch 3, step 41, the loss is 188420.53125\n",
            "in training loop, epoch 3, step 42, the loss is 318500.5625\n",
            "in training loop, epoch 3, step 43, the loss is 221117.6875\n",
            "in training loop, epoch 3, step 44, the loss is 159147.34375\n",
            "in training loop, epoch 3, step 45, the loss is 232491.59375\n",
            "in training loop, epoch 3, step 46, the loss is 177819.484375\n",
            "in training loop, epoch 3, step 47, the loss is 136902.9375\n",
            "in training loop, epoch 3, step 48, the loss is 175132.28125\n",
            "in training loop, epoch 3, step 49, the loss is 137610.59375\n",
            "in training loop, epoch 3, step 50, the loss is 427047.3125\n",
            "in training loop, epoch 3, step 51, the loss is 166357.625\n",
            "in training loop, epoch 3, step 52, the loss is 128672.0390625\n",
            "in training loop, epoch 3, step 53, the loss is 218582.546875\n",
            "in training loop, epoch 3, step 54, the loss is 133939.59375\n",
            "in training loop, epoch 3, step 55, the loss is 213320.71875\n",
            "in training loop, epoch 3, step 56, the loss is 118681.8046875\n",
            "in training loop, epoch 3, step 57, the loss is 198708.0\n",
            "in training loop, epoch 3, step 58, the loss is 250508.0625\n",
            "in training loop, epoch 3, step 59, the loss is 250950.109375\n",
            "in training loop, epoch 3, step 60, the loss is 221548.0625\n",
            "in training loop, epoch 3, step 61, the loss is 282001.0\n",
            "in training loop, epoch 3, step 62, the loss is 121563.34375\n",
            "in training loop, epoch 3, step 63, the loss is 212875.125\n",
            "in training loop, epoch 3, step 64, the loss is 280749.15625\n",
            "in training loop, epoch 3, step 65, the loss is 250365.40625\n",
            "in training loop, epoch 3, step 66, the loss is 292558.25\n",
            "in training loop, epoch 3, step 67, the loss is 152667.109375\n",
            "in training loop, epoch 3, step 68, the loss is 217722.921875\n",
            "in training loop, epoch 3, step 69, the loss is 259210.234375\n",
            "in training loop, epoch 3, step 70, the loss is 201753.015625\n",
            "in training loop, epoch 3, step 71, the loss is 188702.875\n",
            "in training loop, epoch 3, step 72, the loss is 242266.625\n",
            "in training loop, epoch 3, step 73, the loss is 184852.46875\n",
            "in training loop, epoch 3, step 74, the loss is 223901.28125\n",
            "in training loop, epoch 3, step 75, the loss is 198516.203125\n",
            "in training loop, epoch 3, step 76, the loss is 194567.6875\n",
            "in training loop, epoch 3, step 77, the loss is 145615.078125\n",
            "in training loop, epoch 3, step 78, the loss is 169899.875\n",
            "in training loop, epoch 3, step 79, the loss is 215653.15625\n",
            "in training loop, epoch 3, step 80, the loss is 171912.875\n",
            "in training loop, epoch 3, step 81, the loss is 216612.375\n",
            "in training loop, epoch 3, step 82, the loss is 229837.796875\n",
            "in training loop, epoch 3, step 83, the loss is 277124.1875\n",
            "in training loop, epoch 3, step 84, the loss is 151918.4375\n",
            "in training loop, epoch 3, step 85, the loss is 162722.546875\n",
            "in training loop, epoch 3, step 86, the loss is 179845.875\n",
            "in training loop, epoch 3, step 87, the loss is 216559.3125\n",
            "in training loop, epoch 3, step 88, the loss is 213867.5625\n",
            "in training loop, epoch 3, step 89, the loss is 114689.5234375\n",
            "in training loop, epoch 3, step 90, the loss is 230045.40625\n",
            "in training loop, epoch 3, step 91, the loss is 215496.53125\n",
            "in training loop, epoch 3, step 92, the loss is 205896.21875\n",
            "in training loop, epoch 3, step 93, the loss is 197720.125\n",
            "in training loop, epoch 3, step 94, the loss is 210269.171875\n",
            "in training loop, epoch 3, step 95, the loss is 193402.078125\n",
            "in training loop, epoch 3, step 96, the loss is 157798.390625\n",
            "in training loop, epoch 3, step 97, the loss is 249631.09375\n",
            "in training loop, epoch 3, step 98, the loss is 258702.421875\n",
            "in training loop, epoch 3, step 99, the loss is 134949.890625\n",
            "in training loop, epoch 3, step 100, the loss is 143506.578125\n",
            "in training loop, epoch 3, step 101, the loss is 150619.25\n",
            "in training loop, epoch 3, step 102, the loss is 124373.1640625\n",
            "in training loop, epoch 3, step 103, the loss is 241633.71875\n",
            "in training loop, epoch 3, step 104, the loss is 127674.171875\n",
            "in training loop, epoch 3, step 105, the loss is 309387.25\n",
            "in training loop, epoch 3, step 106, the loss is 160166.796875\n",
            "in training loop, epoch 3, step 107, the loss is 160099.28125\n",
            "in training loop, epoch 3, step 108, the loss is 169719.265625\n",
            "in training loop, epoch 3, step 109, the loss is 214615.96875\n",
            "in training loop, epoch 3, step 110, the loss is 107913.4921875\n",
            "in training loop, epoch 3, step 111, the loss is 287625.46875\n",
            "in training loop, epoch 3, step 112, the loss is 109097.5\n",
            "in training loop, epoch 3, step 113, the loss is 176641.0625\n",
            "in training loop, epoch 3, step 114, the loss is 265865.125\n",
            "in training loop, epoch 3, step 115, the loss is 132171.21875\n",
            "in training loop, epoch 3, step 116, the loss is 221642.09375\n",
            "in training loop, epoch 3, step 117, the loss is 198614.859375\n",
            "in training loop, epoch 3, step 118, the loss is 111456.40625\n",
            "in training loop, epoch 3, step 119, the loss is 109379.5703125\n",
            "in training loop, epoch 3, step 120, the loss is 362781.40625\n",
            "in training loop, epoch 3, step 121, the loss is 195356.40625\n",
            "in training loop, epoch 3, step 122, the loss is 403791.5625\n",
            "in training loop, epoch 3, step 123, the loss is 271358.1875\n",
            "in training loop, epoch 3, step 124, the loss is 213804.03125\n",
            "in training loop, epoch 3, step 125, the loss is 198024.546875\n",
            "in training loop, epoch 3, step 126, the loss is 325949.0\n",
            "in training loop, epoch 3, step 127, the loss is 259244.28125\n",
            "in training loop, epoch 3, step 128, the loss is 282018.5625\n",
            "in training loop, epoch 3, step 129, the loss is 168978.8125\n",
            "in training loop, epoch 3, step 130, the loss is 195466.859375\n",
            "in training loop, epoch 3, step 131, the loss is 278964.625\n",
            "in training loop, epoch 3, step 132, the loss is 298120.15625\n",
            "in training loop, epoch 3, step 133, the loss is 214164.3125\n",
            "in training loop, epoch 3, step 134, the loss is 152552.265625\n",
            "in training loop, epoch 3, step 135, the loss is 116700.2421875\n",
            "in training loop, epoch 3, step 136, the loss is 138699.984375\n",
            "in training loop, epoch 3, step 137, the loss is 215863.046875\n",
            "in training loop, epoch 3, step 138, the loss is 446546.78125\n",
            "in training loop, epoch 3, step 139, the loss is 182250.9375\n",
            "in training loop, epoch 3, step 140, the loss is 235637.0625\n",
            "in training loop, epoch 3, step 141, the loss is 362626.125\n",
            "in training loop, epoch 3, step 142, the loss is 259455.015625\n",
            "in training loop, epoch 3, step 143, the loss is 264479.53125\n",
            "in training loop, epoch 3, step 144, the loss is 170612.53125\n",
            "in training loop, epoch 3, step 145, the loss is 245715.609375\n",
            "in training loop, epoch 3, step 146, the loss is 248759.1875\n",
            "in training loop, epoch 3, step 147, the loss is 220501.890625\n",
            "in training loop, epoch 3, step 148, the loss is 210814.578125\n",
            "in training loop, epoch 3, step 149, the loss is 305069.03125\n",
            "in training loop, epoch 3, step 150, the loss is 265063.40625\n",
            "in training loop, epoch 3, step 151, the loss is 336680.4375\n",
            "in training loop, epoch 3, step 152, the loss is 330542.34375\n",
            "in training loop, epoch 3, step 153, the loss is 241832.1875\n",
            "in training loop, epoch 3, step 154, the loss is 144968.25\n",
            "in training loop, epoch 3, step 155, the loss is 189898.75\n",
            "in training loop, epoch 3, step 156, the loss is 195181.671875\n",
            "in training loop, epoch 3, step 157, the loss is 224652.265625\n",
            "in training loop, epoch 3, step 158, the loss is 310610.6875\n",
            "in training loop, epoch 3, step 159, the loss is 308505.6875\n",
            "in training loop, epoch 3, step 160, the loss is 327654.96875\n",
            "in training loop, epoch 3, step 161, the loss is 190634.640625\n",
            "in training loop, epoch 3, step 162, the loss is 256306.5\n",
            "in training loop, epoch 3, step 163, the loss is 246360.734375\n",
            "in training loop, epoch 3, step 164, the loss is 208530.1875\n",
            "in training loop, epoch 3, step 165, the loss is 223202.359375\n",
            "in training loop, epoch 3, step 166, the loss is 150518.3125\n",
            "in training loop, epoch 3, step 167, the loss is 200282.640625\n",
            "in training loop, epoch 3, step 168, the loss is 312155.1875\n",
            "in training loop, epoch 3, step 169, the loss is 316833.75\n",
            "in training loop, epoch 3, step 170, the loss is 108162.890625\n",
            "in training loop, epoch 3, step 171, the loss is 235284.140625\n",
            "in training loop, epoch 3, step 172, the loss is 195734.09375\n",
            "in training loop, epoch 3, step 173, the loss is 185295.140625\n",
            "in training loop, epoch 3, step 174, the loss is 262618.65625\n",
            "in training loop, epoch 3, step 175, the loss is 314663.03125\n",
            "in training loop, epoch 3, step 176, the loss is 226248.84375\n",
            "in training loop, epoch 3, step 177, the loss is 349926.65625\n",
            "in training loop, epoch 3, step 178, the loss is 242094.609375\n",
            "in training loop, epoch 3, step 179, the loss is 188180.921875\n",
            "in training loop, epoch 3, step 180, the loss is 332620.9375\n",
            "in training loop, epoch 3, step 181, the loss is 233382.4375\n",
            "in training loop, epoch 3, step 182, the loss is 227459.734375\n",
            "in training loop, epoch 3, step 183, the loss is 242266.875\n",
            "in training loop, epoch 3, step 184, the loss is 199409.375\n",
            "in training loop, epoch 3, step 185, the loss is 218469.078125\n",
            "in training loop, epoch 3, step 186, the loss is 246619.421875\n",
            "in training loop, epoch 3, step 187, the loss is 379548.0\n",
            "in training loop, epoch 3, step 188, the loss is 228173.421875\n",
            "in training loop, epoch 3, step 189, the loss is 197038.203125\n",
            "in training loop, epoch 3, step 190, the loss is 174340.078125\n",
            "in training loop, epoch 3, step 191, the loss is 290632.875\n",
            "in training loop, epoch 3, step 192, the loss is 228904.421875\n",
            "in training loop, epoch 3, step 193, the loss is 249331.765625\n",
            "in training loop, epoch 3, step 194, the loss is 203180.96875\n",
            "in training loop, epoch 3, step 195, the loss is 177324.53125\n",
            "in training loop, epoch 3, step 196, the loss is 314836.09375\n",
            "in training loop, epoch 3, step 197, the loss is 262748.625\n",
            "in training loop, epoch 3, step 198, the loss is 224786.625\n",
            "in training loop, epoch 3, step 199, the loss is 245247.21875\n",
            "in training loop, epoch 3, step 200, the loss is 247540.546875\n",
            "in training loop, epoch 3, step 201, the loss is 246396.0625\n",
            "in training loop, epoch 3, step 202, the loss is 199191.859375\n",
            "in training loop, epoch 3, step 203, the loss is 310784.4375\n",
            "in training loop, epoch 3, step 204, the loss is 145057.0\n",
            "in training loop, epoch 3, step 205, the loss is 234757.375\n",
            "in training loop, epoch 3, step 206, the loss is 180439.5625\n",
            "in training loop, epoch 3, step 207, the loss is 316131.4375\n",
            "in training loop, epoch 3, step 208, the loss is 259698.125\n",
            "in training loop, epoch 3, step 209, the loss is 191440.28125\n",
            "in training loop, epoch 3, step 210, the loss is 303134.1875\n",
            "in training loop, epoch 3, step 211, the loss is 213295.75\n",
            "in training loop, epoch 3, step 212, the loss is 289883.53125\n",
            "in training loop, epoch 3, step 213, the loss is 209845.0625\n",
            "in training loop, epoch 3, step 214, the loss is 184015.234375\n",
            "in training loop, epoch 3, step 215, the loss is 275574.28125\n",
            "in training loop, epoch 3, step 216, the loss is 176329.3125\n",
            "in training loop, epoch 3, step 217, the loss is 195324.140625\n",
            "in training loop, epoch 3, step 218, the loss is 250537.25\n",
            "in training loop, epoch 3, step 219, the loss is 228112.125\n",
            "in training loop, epoch 3, step 220, the loss is 241659.109375\n",
            "in training loop, epoch 3, step 221, the loss is 171076.03125\n",
            "in training loop, epoch 3, step 222, the loss is 274246.9375\n",
            "in training loop, epoch 3, step 223, the loss is 139417.78125\n",
            "in training loop, epoch 3, step 224, the loss is 171641.0\n",
            "in training loop, epoch 3, step 225, the loss is 274509.5625\n",
            "in training loop, epoch 3, step 226, the loss is 224104.75\n",
            "in training loop, epoch 3, step 227, the loss is 231816.890625\n",
            "in training loop, epoch 3, step 228, the loss is 270609.25\n",
            "in training loop, epoch 3, step 229, the loss is 189406.0\n",
            "in training loop, epoch 3, step 230, the loss is 150114.921875\n",
            "in training loop, epoch 3, step 231, the loss is 302280.125\n",
            "in training loop, epoch 3, step 232, the loss is 142942.359375\n",
            "in training loop, epoch 3, step 233, the loss is 206055.203125\n",
            "in training loop, epoch 3, step 234, the loss is 208488.890625\n",
            "in training loop, epoch 3, step 235, the loss is 149957.765625\n",
            "in training loop, epoch 3, step 236, the loss is 125972.0\n",
            "in training loop, epoch 3, step 237, the loss is 239053.28125\n",
            "in training loop, epoch 3, step 238, the loss is 331347.6875\n",
            "in training loop, epoch 3, step 239, the loss is 279836.75\n",
            "in training loop, epoch 3, step 240, the loss is 214528.84375\n",
            "in training loop, epoch 3, step 241, the loss is 155677.046875\n",
            "in training loop, epoch 3, step 242, the loss is 328235.5625\n",
            "in training loop, epoch 3, step 243, the loss is 145823.46875\n",
            "in training loop, epoch 3, step 244, the loss is 231685.5\n",
            "in training loop, epoch 3, step 245, the loss is 218978.546875\n",
            "in training loop, epoch 3, step 246, the loss is 166866.875\n",
            "in training loop, epoch 3, step 247, the loss is 166104.0625\n",
            "in training loop, epoch 3, step 248, the loss is 167086.40625\n",
            "in training loop, epoch 3, step 249, the loss is 147980.140625\n",
            "in training loop, epoch 3, step 250, the loss is 159458.15625\n",
            "in training loop, epoch 3, step 251, the loss is 185405.765625\n",
            "in training loop, epoch 3, step 252, the loss is 190646.71875\n",
            "in training loop, epoch 3, step 253, the loss is 154916.0\n",
            "in training loop, epoch 3, step 254, the loss is 293631.4375\n",
            "in training loop, epoch 3, step 255, the loss is 132735.296875\n",
            "in training loop, epoch 3, step 256, the loss is 320634.5\n",
            "in training loop, epoch 3, step 257, the loss is 193587.0625\n",
            "in training loop, epoch 3, step 258, the loss is 258796.9375\n",
            "in training loop, epoch 3, step 259, the loss is 298492.21875\n",
            "in training loop, epoch 3, step 260, the loss is 170100.125\n",
            "in training loop, epoch 3, step 261, the loss is 220744.421875\n",
            "in training loop, epoch 3, step 262, the loss is 238539.8125\n",
            "in training loop, epoch 3, step 263, the loss is 302048.0625\n",
            "in training loop, epoch 3, step 264, the loss is 155575.890625\n",
            "in training loop, epoch 3, step 265, the loss is 133158.703125\n",
            "in training loop, epoch 3, step 266, the loss is 261291.140625\n",
            "in training loop, epoch 3, step 267, the loss is 136696.8125\n",
            "in training loop, epoch 3, step 268, the loss is 218349.75\n",
            "in training loop, epoch 3, step 269, the loss is 301621.4375\n",
            "in training loop, epoch 3, step 270, the loss is 219116.015625\n",
            "in training loop, epoch 3, step 271, the loss is 295389.625\n",
            "in training loop, epoch 3, step 272, the loss is 327109.71875\n",
            "in training loop, epoch 3, step 273, the loss is 295341.0625\n",
            "in training loop, epoch 3, step 274, the loss is 218476.59375\n",
            "in training loop, epoch 3, step 275, the loss is 237090.921875\n",
            "in training loop, epoch 3, step 276, the loss is 297325.96875\n",
            "in training loop, epoch 3, step 277, the loss is 192696.484375\n",
            "in training loop, epoch 3, step 278, the loss is 289606.71875\n",
            "in training loop, epoch 3, step 279, the loss is 316563.4375\n",
            "in training loop, epoch 3, step 280, the loss is 254645.65625\n",
            "in training loop, epoch 3, step 281, the loss is 161472.484375\n",
            "in training loop, epoch 3, step 282, the loss is 191572.515625\n",
            "in training loop, epoch 3, step 283, the loss is 222428.796875\n",
            "in training loop, epoch 3, step 284, the loss is 202366.34375\n",
            "in training loop, epoch 3, step 285, the loss is 169310.15625\n",
            "in training loop, epoch 3, step 286, the loss is 279142.9375\n",
            "in training loop, epoch 3, step 287, the loss is 362331.1875\n",
            "in training loop, epoch 3, step 288, the loss is 206804.390625\n",
            "in training loop, epoch 3, step 289, the loss is 311450.09375\n",
            "in training loop, epoch 3, step 290, the loss is 227005.015625\n",
            "in training loop, epoch 3, step 291, the loss is 168478.90625\n",
            "in training loop, epoch 3, step 292, the loss is 155127.484375\n",
            "in training loop, epoch 3, step 293, the loss is 239388.0\n",
            "in training loop, epoch 3, step 294, the loss is 215098.140625\n",
            "in training loop, epoch 3, step 295, the loss is 280255.84375\n",
            "in training loop, epoch 3, step 296, the loss is 328796.625\n",
            "in training loop, epoch 3, step 297, the loss is 303678.40625\n",
            "in training loop, epoch 3, step 298, the loss is 256244.59375\n",
            "in training loop, epoch 3, step 299, the loss is 271511.65625\n",
            "in training loop, epoch 3, step 300, the loss is 234318.34375\n",
            "in training loop, epoch 3, step 301, the loss is 271323.625\n",
            "in training loop, epoch 3, step 302, the loss is 298560.0625\n",
            "in training loop, epoch 3, step 303, the loss is 163060.359375\n",
            "in training loop, epoch 3, step 304, the loss is 407392.84375\n",
            "in training loop, epoch 3, step 305, the loss is 271167.90625\n",
            "in training loop, epoch 3, step 306, the loss is 203917.6875\n",
            "in training loop, epoch 3, step 307, the loss is 237320.765625\n",
            "in training loop, epoch 3, step 308, the loss is 210339.375\n",
            "in training loop, epoch 3, step 309, the loss is 265811.03125\n",
            "in training loop, epoch 3, step 310, the loss is 205527.75\n",
            "in training loop, epoch 3, step 311, the loss is 251544.03125\n",
            "in training loop, epoch 3, step 312, the loss is 202545.984375\n",
            "in training loop, epoch 3, step 313, the loss is 215039.921875\n",
            "in training loop, epoch 3, step 314, the loss is 157548.515625\n",
            "in training loop, epoch 3, step 315, the loss is 297561.28125\n",
            "in training loop, epoch 3, step 316, the loss is 196873.578125\n",
            "in training loop, epoch 3, step 317, the loss is 280675.875\n",
            "in training loop, epoch 3, step 318, the loss is 152919.921875\n",
            "in training loop, epoch 3, step 319, the loss is 228111.625\n",
            "in training loop, epoch 3, step 320, the loss is 264170.53125\n",
            "in training loop, epoch 3, step 321, the loss is 237435.5625\n",
            "in training loop, epoch 3, step 322, the loss is 157178.015625\n",
            "in training loop, epoch 3, step 323, the loss is 221669.28125\n",
            "in training loop, epoch 3, step 324, the loss is 175247.21875\n",
            "in training loop, epoch 3, step 325, the loss is 256666.234375\n",
            "in training loop, epoch 3, step 326, the loss is 314633.53125\n",
            "in training loop, epoch 3, step 327, the loss is 269723.21875\n",
            "in training loop, epoch 3, step 328, the loss is 124285.9375\n",
            "in training loop, epoch 3, step 329, the loss is 217932.296875\n",
            "in training loop, epoch 3, step 330, the loss is 192720.5\n",
            "in training loop, epoch 3, step 331, the loss is 153267.21875\n",
            "in training loop, epoch 3, step 332, the loss is 270656.1875\n",
            "in training loop, epoch 3, step 333, the loss is 128984.1796875\n",
            "in training loop, epoch 3, step 334, the loss is 343850.96875\n",
            "in training loop, epoch 3, step 335, the loss is 242337.34375\n",
            "in training loop, epoch 3, step 336, the loss is 269396.5625\n",
            "in training loop, epoch 3, step 337, the loss is 507548.5625\n",
            "in training loop, epoch 3, step 338, the loss is 219588.1875\n",
            "in training loop, epoch 3, step 339, the loss is 205340.921875\n",
            "in training loop, epoch 3, step 340, the loss is 171832.359375\n",
            "in training loop, epoch 3, step 341, the loss is 164387.71875\n",
            "in training loop, epoch 3, step 342, the loss is 160781.40625\n",
            "in training loop, epoch 3, step 343, the loss is 268010.9375\n",
            "in training loop, epoch 3, step 344, the loss is 179346.125\n",
            "in training loop, epoch 3, step 345, the loss is 207837.015625\n",
            "in training loop, epoch 3, step 346, the loss is 135802.953125\n",
            "in training loop, epoch 3, step 347, the loss is 173009.71875\n",
            "in training loop, epoch 3, step 348, the loss is 219597.546875\n",
            "in training loop, epoch 3, step 349, the loss is 412234.4375\n",
            "in training loop, epoch 3, step 350, the loss is 429529.65625\n",
            "in training loop, epoch 3, step 351, the loss is 230058.84375\n",
            "in training loop, epoch 3, step 352, the loss is 263119.6875\n",
            "in training loop, epoch 3, step 353, the loss is 211361.078125\n",
            "in training loop, epoch 3, step 354, the loss is 166042.53125\n",
            "in training loop, epoch 3, step 355, the loss is 265502.09375\n",
            "in training loop, epoch 3, step 356, the loss is 193078.703125\n",
            "in training loop, epoch 3, step 357, the loss is 188204.21875\n",
            "in training loop, epoch 3, step 358, the loss is 150708.78125\n",
            "in training loop, epoch 3, step 359, the loss is 171289.921875\n",
            "in training loop, epoch 3, step 360, the loss is 220042.875\n",
            "in training loop, epoch 3, step 361, the loss is 207935.0625\n",
            "in training loop, epoch 3, step 362, the loss is 326569.90625\n",
            "in training loop, epoch 3, step 363, the loss is 265629.46875\n",
            "in training loop, epoch 3, step 364, the loss is 235268.4375\n",
            "in training loop, epoch 3, step 365, the loss is 348375.25\n",
            "in training loop, epoch 3, step 366, the loss is 415614.21875\n",
            "in training loop, epoch 3, step 367, the loss is 193708.21875\n",
            "in training loop, epoch 3, step 368, the loss is 247263.703125\n",
            "in training loop, epoch 3, step 369, the loss is 183209.59375\n",
            "in training loop, epoch 3, step 370, the loss is 183839.0625\n",
            "in training loop, epoch 3, step 371, the loss is 207627.890625\n",
            "in training loop, epoch 3, step 372, the loss is 237648.71875\n",
            "in training loop, epoch 3, step 373, the loss is 502850.71875\n",
            "in training loop, epoch 3, step 374, the loss is 232178.5625\n",
            "in training loop, epoch 3, step 375, the loss is 261998.5\n",
            "in training loop, epoch 3, step 376, the loss is 217713.34375\n",
            "in training loop, epoch 3, step 377, the loss is 274299.3125\n",
            "in training loop, epoch 3, step 378, the loss is 186679.65625\n",
            "in training loop, epoch 3, step 379, the loss is 230932.546875\n",
            "in training loop, epoch 3, step 380, the loss is 258691.390625\n",
            "in training loop, epoch 3, step 381, the loss is 285397.6875\n",
            "in training loop, epoch 3, step 382, the loss is 156630.9375\n",
            "in training loop, epoch 3, step 383, the loss is 221542.3125\n",
            "in training loop, epoch 3, step 384, the loss is 153194.703125\n",
            "in training loop, epoch 3, step 385, the loss is 209698.296875\n",
            "in training loop, epoch 3, step 386, the loss is 187818.125\n",
            "in training loop, epoch 3, step 387, the loss is 230431.1875\n",
            "in training loop, epoch 3, step 388, the loss is 192560.34375\n",
            "in training loop, epoch 3, step 389, the loss is 223601.875\n",
            "in training loop, epoch 3, step 390, the loss is 88150.390625\n",
            "in training loop, epoch 3, step 391, the loss is 247099.703125\n",
            "in training loop, epoch 3, step 392, the loss is 147120.0\n",
            "in training loop, epoch 3, step 393, the loss is 226434.140625\n",
            "in training loop, epoch 3, step 394, the loss is 348105.875\n",
            "in training loop, epoch 3, step 395, the loss is 256390.671875\n",
            "in training loop, epoch 3, step 396, the loss is 228522.640625\n",
            "in training loop, epoch 3, step 397, the loss is 157025.3125\n",
            "in training loop, epoch 3, step 398, the loss is 303564.09375\n",
            "in training loop, epoch 3, step 399, the loss is 348143.625\n",
            "in training loop, epoch 3, step 400, the loss is 123827.828125\n",
            "in training loop, epoch 3, step 401, the loss is 219808.1875\n",
            "in training loop, epoch 3, step 402, the loss is 252031.8125\n",
            "in training loop, epoch 3, step 403, the loss is 462241.75\n",
            "in training loop, epoch 3, step 404, the loss is 304277.40625\n",
            "in training loop, epoch 3, step 405, the loss is 285245.875\n",
            "in training loop, epoch 3, step 406, the loss is 230003.703125\n",
            "in training loop, epoch 3, step 407, the loss is 328651.84375\n",
            "in training loop, epoch 3, step 408, the loss is 341870.25\n",
            "in training loop, epoch 3, step 409, the loss is 408749.46875\n",
            "in training loop, epoch 3, step 410, the loss is 267795.5\n",
            "in training loop, epoch 3, step 411, the loss is 266434.875\n",
            "in training loop, epoch 3, step 412, the loss is 651299.75\n",
            "in training loop, epoch 3, step 413, the loss is 365624.6875\n",
            "in training loop, epoch 3, step 414, the loss is 263965.15625\n",
            "in training loop, epoch 3, step 415, the loss is 204022.9375\n",
            "in training loop, epoch 3, step 416, the loss is 339150.96875\n",
            "in training loop, epoch 3, step 417, the loss is 383942.0\n",
            "in training loop, epoch 3, step 418, the loss is 290383.375\n",
            "in training loop, epoch 3, step 419, the loss is 286171.6875\n",
            "in training loop, epoch 3, step 420, the loss is 218251.234375\n",
            "in training loop, epoch 3, step 421, the loss is 208522.671875\n",
            "in training loop, epoch 3, step 422, the loss is 193303.265625\n",
            "in training loop, epoch 3, step 423, the loss is 302735.875\n",
            "in training loop, epoch 3, step 424, the loss is 191827.75\n",
            "in training loop, epoch 3, step 425, the loss is 160898.234375\n",
            "in training loop, epoch 3, step 426, the loss is 91749.234375\n",
            "in training loop, epoch 3, step 427, the loss is 263074.8125\n",
            "in training loop, epoch 3, step 428, the loss is 137284.78125\n",
            "in training loop, epoch 3, step 429, the loss is 189071.546875\n",
            "in training loop, epoch 3, step 430, the loss is 280235.59375\n",
            "in training loop, epoch 3, step 431, the loss is 215277.15625\n",
            "in training loop, epoch 3, step 432, the loss is 329156.21875\n",
            "in training loop, epoch 3, step 433, the loss is 226864.6875\n",
            "in training loop, epoch 3, step 434, the loss is 487662.25\n",
            "in training loop, epoch 3, step 435, the loss is 91406.7578125\n",
            "in training loop, epoch 3, step 436, the loss is 216594.859375\n",
            "in training loop, epoch 3, step 437, the loss is 193692.21875\n",
            "in training loop, epoch 3, step 438, the loss is 287281.46875\n",
            "in training loop, epoch 3, step 439, the loss is 308568.78125\n",
            "in training loop, epoch 3, step 440, the loss is 306675.59375\n",
            "in training loop, epoch 3, step 441, the loss is 146416.125\n",
            "in training loop, epoch 3, step 442, the loss is 119493.4609375\n",
            "in training loop, epoch 3, step 443, the loss is 295749.625\n",
            "in training loop, epoch 3, step 444, the loss is 219589.515625\n",
            "in training loop, epoch 3, step 445, the loss is 243920.875\n",
            "in training loop, epoch 3, step 446, the loss is 123957.46875\n",
            "in training loop, epoch 3, step 447, the loss is 145235.609375\n",
            "in training loop, epoch 3, step 448, the loss is 263726.59375\n",
            "in training loop, epoch 3, step 449, the loss is 283011.9375\n",
            "in training loop, epoch 3, step 450, the loss is 129527.34375\n",
            "in training loop, epoch 3, step 451, the loss is 238095.53125\n",
            "in training loop, epoch 3, step 452, the loss is 203110.96875\n",
            "in training loop, epoch 3, step 453, the loss is 215008.546875\n",
            "in training loop, epoch 3, step 454, the loss is 279540.0625\n",
            "in training loop, epoch 3, step 455, the loss is 276765.09375\n",
            "in training loop, epoch 3, step 456, the loss is 243028.515625\n",
            "in training loop, epoch 3, step 457, the loss is 177474.171875\n",
            "in training loop, epoch 3, step 458, the loss is 342770.34375\n",
            "in training loop, epoch 3, step 459, the loss is 278458.21875\n",
            "in training loop, epoch 3, step 460, the loss is 219303.296875\n",
            "in training loop, epoch 3, step 461, the loss is 149127.5\n",
            "in training loop, epoch 3, step 462, the loss is 258047.015625\n",
            "in training loop, epoch 3, step 463, the loss is 128184.453125\n",
            "in training loop, epoch 3, step 464, the loss is 161873.5625\n",
            "in training loop, epoch 3, step 465, the loss is 295960.46875\n",
            "in training loop, epoch 3, step 466, the loss is 178691.875\n",
            "in training loop, epoch 3, step 467, the loss is 275896.25\n",
            "in training loop, epoch 3, step 468, the loss is 246930.125\n",
            "in training loop, epoch 3, step 469, the loss is 139012.734375\n",
            "in training loop, epoch 3, step 470, the loss is 320155.625\n",
            "in training loop, epoch 3, step 471, the loss is 369273.625\n",
            "in training loop, epoch 3, step 472, the loss is 121988.0390625\n",
            "in training loop, epoch 3, step 473, the loss is 218426.796875\n",
            "in training loop, epoch 3, step 474, the loss is 207268.296875\n",
            "in training loop, epoch 3, step 475, the loss is 353504.875\n",
            "in training loop, epoch 3, step 476, the loss is 247135.109375\n",
            "in training loop, epoch 3, step 477, the loss is 233776.859375\n",
            "in training loop, epoch 3, step 478, the loss is 146614.953125\n",
            "in training loop, epoch 3, step 479, the loss is 178894.453125\n",
            "in training loop, epoch 3, step 480, the loss is 182307.03125\n",
            "in training loop, epoch 3, step 481, the loss is 183337.640625\n",
            "in training loop, epoch 3, step 482, the loss is 349062.875\n",
            "in training loop, epoch 3, step 483, the loss is 229715.390625\n",
            "in training loop, epoch 3, step 484, the loss is 152035.75\n",
            "in training loop, epoch 3, step 485, the loss is 129774.265625\n",
            "in training loop, epoch 3, step 486, the loss is 309381.1875\n",
            "in training loop, epoch 3, step 487, the loss is 206683.96875\n",
            "in training loop, epoch 3, step 488, the loss is 155187.8125\n",
            "in training loop, epoch 3, step 489, the loss is 255910.359375\n",
            "in training loop, epoch 3, step 490, the loss is 264869.875\n",
            "in training loop, epoch 3, step 491, the loss is 167341.875\n",
            "in training loop, epoch 3, step 492, the loss is 246276.515625\n",
            "in training loop, epoch 3, step 493, the loss is 285275.90625\n",
            "in training loop, epoch 3, step 494, the loss is 230077.5\n",
            "in training loop, epoch 3, step 495, the loss is 239802.171875\n",
            "in training loop, epoch 3, step 496, the loss is 180726.5\n",
            "in training loop, epoch 3, step 497, the loss is 170659.890625\n",
            "in training loop, epoch 3, step 498, the loss is 158202.359375\n",
            "in training loop, epoch 3, step 499, the loss is 189346.296875\n",
            "in training loop, epoch 3, step 500, the loss is 149738.28125\n",
            "in training loop, epoch 3, step 501, the loss is 136232.890625\n",
            "in training loop, epoch 3, step 502, the loss is 408354.78125\n",
            "in training loop, epoch 3, step 503, the loss is 199238.109375\n",
            "in training loop, epoch 3, step 504, the loss is 155617.65625\n",
            "in training loop, epoch 3, step 505, the loss is 176722.15625\n",
            "in training loop, epoch 3, step 506, the loss is 163204.8125\n",
            "in training loop, epoch 3, step 507, the loss is 215390.640625\n",
            "in training loop, epoch 3, step 508, the loss is 122357.34375\n",
            "in training loop, epoch 3, step 509, the loss is 212016.0625\n",
            "in training loop, epoch 3, step 510, the loss is 159447.078125\n",
            "in training loop, epoch 3, step 511, the loss is 255181.28125\n",
            "in training loop, epoch 3, step 512, the loss is 132676.53125\n",
            "in training loop, epoch 3, step 513, the loss is 224187.71875\n",
            "in training loop, epoch 3, step 514, the loss is 164529.8125\n",
            "in training loop, epoch 3, step 515, the loss is 245855.28125\n",
            "in training loop, epoch 3, step 516, the loss is 216118.359375\n",
            "in training loop, epoch 3, step 517, the loss is 152739.03125\n",
            "in training loop, epoch 3, step 518, the loss is 263715.125\n",
            "in training loop, epoch 3, step 519, the loss is 124356.8515625\n",
            "in training loop, epoch 3, step 520, the loss is 100841.640625\n",
            "in training loop, epoch 3, step 521, the loss is 159273.171875\n",
            "in training loop, epoch 3, step 522, the loss is 197661.3125\n",
            "in training loop, epoch 3, step 523, the loss is 251141.78125\n",
            "in training loop, epoch 3, step 524, the loss is 491264.0\n",
            "in training loop, epoch 3, step 525, the loss is 415312.53125\n",
            "in training loop, epoch 3, step 526, the loss is 266927.34375\n",
            "in training loop, epoch 3, step 527, the loss is 252888.671875\n",
            "in training loop, epoch 3, step 528, the loss is 90996.2421875\n",
            "in training loop, epoch 3, step 529, the loss is 383173.1875\n",
            "in training loop, epoch 3, step 530, the loss is 192786.671875\n",
            "in training loop, epoch 3, step 531, the loss is 250244.234375\n",
            "in training loop, epoch 3, step 532, the loss is 250166.46875\n",
            "in training loop, epoch 3, step 533, the loss is 356248.21875\n",
            "in training loop, epoch 3, step 534, the loss is 203427.71875\n",
            "in training loop, epoch 3, step 535, the loss is 228131.046875\n",
            "in training loop, epoch 3, step 536, the loss is 200254.875\n",
            "in training loop, epoch 3, step 537, the loss is 348778.0625\n",
            "in training loop, epoch 3, step 538, the loss is 211030.21875\n",
            "in training loop, epoch 3, step 539, the loss is 311400.03125\n",
            "in training loop, epoch 3, step 540, the loss is 261840.296875\n",
            "in training loop, epoch 3, step 541, the loss is 310527.875\n",
            "in training loop, epoch 3, step 542, the loss is 203749.71875\n",
            "in training loop, epoch 3, step 543, the loss is 224257.140625\n",
            "in training loop, epoch 3, step 544, the loss is 281143.96875\n",
            "in training loop, epoch 3, step 545, the loss is 516706.0\n",
            "in training loop, epoch 3, step 546, the loss is 241926.203125\n",
            "in training loop, epoch 3, step 547, the loss is 157159.625\n",
            "in training loop, epoch 3, step 548, the loss is 225008.625\n",
            "in training loop, epoch 3, step 549, the loss is 228266.25\n",
            "in training loop, epoch 3, step 550, the loss is 324425.9375\n",
            "in training loop, epoch 3, step 551, the loss is 110988.9765625\n",
            "in training loop, epoch 3, step 552, the loss is 231864.609375\n",
            "in training loop, epoch 3, step 553, the loss is 185992.265625\n",
            "in training loop, epoch 3, step 554, the loss is 212252.5\n",
            "in training loop, epoch 3, step 555, the loss is 271980.59375\n",
            "in training loop, epoch 3, step 556, the loss is 211571.921875\n",
            "in training loop, epoch 3, step 557, the loss is 286463.0625\n",
            "in training loop, epoch 3, step 558, the loss is 187290.421875\n",
            "in training loop, epoch 3, step 559, the loss is 234287.34375\n",
            "in training loop, epoch 3, step 560, the loss is 247196.0625\n",
            "in training loop, epoch 3, step 561, the loss is 161205.0625\n",
            "in training loop, epoch 3, step 562, the loss is 159695.015625\n",
            "in training loop, epoch 3, step 563, the loss is 219418.765625\n",
            "in training loop, epoch 3, step 564, the loss is 176268.109375\n",
            "in training loop, epoch 3, step 565, the loss is 451257.625\n",
            "in training loop, epoch 3, step 566, the loss is 331557.09375\n",
            "in training loop, epoch 3, step 567, the loss is 156553.75\n",
            "in training loop, epoch 3, step 568, the loss is 173992.125\n",
            "in training loop, epoch 3, step 569, the loss is 329516.9375\n",
            "in training loop, epoch 3, step 570, the loss is 281134.15625\n",
            "in training loop, epoch 3, step 571, the loss is 291603.5\n",
            "in training loop, epoch 3, step 572, the loss is 196767.453125\n",
            "in training loop, epoch 3, step 573, the loss is 257454.875\n",
            "in training loop, epoch 3, step 574, the loss is 180393.65625\n",
            "in training loop, epoch 3, step 575, the loss is 341907.0\n",
            "in training loop, epoch 3, step 576, the loss is 349468.59375\n",
            "in training loop, epoch 3, step 577, the loss is 217391.0625\n",
            "in training loop, epoch 3, step 578, the loss is 301137.53125\n",
            "in training loop, epoch 3, step 579, the loss is 270848.84375\n",
            "in training loop, epoch 3, step 580, the loss is 374442.625\n",
            "in training loop, epoch 3, step 581, the loss is 211290.96875\n",
            "in training loop, epoch 3, step 582, the loss is 395016.25\n",
            "in training loop, epoch 3, step 583, the loss is 197963.953125\n",
            "in training loop, epoch 3, step 584, the loss is 220843.828125\n",
            "in training loop, epoch 3, step 585, the loss is 243898.421875\n",
            "in training loop, epoch 3, step 586, the loss is 323201.25\n",
            "in training loop, epoch 3, step 587, the loss is 212659.6875\n",
            "in training loop, epoch 3, step 588, the loss is 469985.34375\n",
            "in training loop, epoch 3, step 589, the loss is 208165.03125\n",
            "in training loop, epoch 3, step 590, the loss is 250480.703125\n",
            "in training loop, epoch 3, step 591, the loss is 269140.78125\n",
            "in training loop, epoch 3, step 592, the loss is 170534.6875\n",
            "in training loop, epoch 3, step 593, the loss is 412746.625\n",
            "in training loop, epoch 3, step 594, the loss is 176235.453125\n",
            "in training loop, epoch 3, step 595, the loss is 273777.9375\n",
            "in training loop, epoch 3, step 596, the loss is 247042.75\n",
            "in training loop, epoch 3, step 597, the loss is 194039.828125\n",
            "in training loop, epoch 3, step 598, the loss is 317318.0\n",
            "in training loop, epoch 3, step 599, the loss is 238816.515625\n",
            "in training loop, epoch 3, step 600, the loss is 256074.53125\n",
            "in training loop, epoch 3, step 601, the loss is 208778.796875\n",
            "in training loop, epoch 3, step 602, the loss is 203733.9375\n",
            "in training loop, epoch 3, step 603, the loss is 307787.28125\n",
            "in training loop, epoch 3, step 604, the loss is 156691.296875\n",
            "in training loop, epoch 3, step 605, the loss is 216729.78125\n",
            "in training loop, epoch 3, step 606, the loss is 201338.59375\n",
            "in training loop, epoch 3, step 607, the loss is 185118.296875\n",
            "in training loop, epoch 3, step 608, the loss is 205829.09375\n",
            "in training loop, epoch 3, step 609, the loss is 151890.5625\n",
            "in training loop, epoch 3, step 610, the loss is 161689.84375\n",
            "in training loop, epoch 3, step 611, the loss is 208055.640625\n",
            "in training loop, epoch 3, step 612, the loss is 195627.640625\n",
            "in training loop, epoch 3, step 613, the loss is 191999.125\n",
            "in training loop, epoch 3, step 614, the loss is 327669.5625\n",
            "in training loop, epoch 3, step 615, the loss is 199201.09375\n",
            "in training loop, epoch 3, step 616, the loss is 126444.828125\n",
            "in training loop, epoch 3, step 617, the loss is 153105.03125\n",
            "in training loop, epoch 3, step 618, the loss is 134240.609375\n",
            "in training loop, epoch 3, step 619, the loss is 257920.5\n",
            "in training loop, epoch 3, step 620, the loss is 283431.71875\n",
            "in training loop, epoch 3, step 621, the loss is 223379.59375\n",
            "in training loop, epoch 3, step 622, the loss is 217688.375\n",
            "in training loop, epoch 3, step 623, the loss is 300629.75\n",
            "in training loop, epoch 3, step 624, the loss is 294150.46875\n",
            "in training loop, epoch 3, step 625, the loss is 226579.328125\n",
            "in training loop, epoch 3, step 626, the loss is 286398.84375\n",
            "in training loop, epoch 3, step 627, the loss is 274578.28125\n",
            "in training loop, epoch 3, step 628, the loss is 225176.96875\n",
            "in training loop, epoch 3, step 629, the loss is 297617.59375\n",
            "in training loop, epoch 3, step 630, the loss is 421447.1875\n",
            "in training loop, epoch 3, step 631, the loss is 345384.28125\n",
            "in training loop, epoch 3, step 632, the loss is 215113.15625\n",
            "in training loop, epoch 3, step 633, the loss is 144612.1875\n",
            "in training loop, epoch 3, step 634, the loss is 226612.09375\n",
            "in training loop, epoch 3, step 635, the loss is 294637.0\n",
            "in training loop, epoch 3, step 636, the loss is 117108.3671875\n",
            "in training loop, epoch 3, step 637, the loss is 195553.71875\n",
            "in training loop, epoch 3, step 638, the loss is 295663.5\n",
            "in training loop, epoch 3, step 639, the loss is 160808.5625\n",
            "in training loop, epoch 3, step 640, the loss is 208529.203125\n",
            "in training loop, epoch 3, step 641, the loss is 169567.5625\n",
            "in training loop, epoch 3, step 642, the loss is 249868.59375\n",
            "in training loop, epoch 3, step 643, the loss is 146527.578125\n",
            "in training loop, epoch 3, step 644, the loss is 195994.890625\n",
            "in training loop, epoch 3, step 645, the loss is 288006.9375\n",
            "in training loop, epoch 3, step 646, the loss is 208438.9375\n",
            "in training loop, epoch 3, step 647, the loss is 300823.25\n",
            "in training loop, epoch 3, step 648, the loss is 224581.359375\n",
            "in training loop, epoch 3, step 649, the loss is 473867.40625\n",
            "in training loop, epoch 3, step 650, the loss is 142506.640625\n",
            "in training loop, epoch 3, step 651, the loss is 285736.96875\n",
            "in training loop, epoch 3, step 652, the loss is 228192.234375\n",
            "in training loop, epoch 3, step 653, the loss is 161796.59375\n",
            "in training loop, epoch 3, step 654, the loss is 116713.859375\n",
            "in training loop, epoch 3, step 655, the loss is 287834.3125\n",
            "in training loop, epoch 3, step 656, the loss is 315473.90625\n",
            "in training loop, epoch 3, step 657, the loss is 127001.1796875\n",
            "in training loop, epoch 3, step 658, the loss is 228773.671875\n",
            "in training loop, epoch 3, step 659, the loss is 250249.515625\n",
            "in training loop, epoch 3, step 660, the loss is 744186.4375\n",
            "in training loop, epoch 3, step 661, the loss is 268517.875\n",
            "in training loop, epoch 3, step 662, the loss is 197526.140625\n",
            "in training loop, epoch 3, step 663, the loss is 253375.59375\n",
            "in training loop, epoch 3, step 664, the loss is 324827.0625\n",
            "in training loop, epoch 3, step 665, the loss is 160212.796875\n",
            "in training loop, epoch 3, step 666, the loss is 235388.9375\n",
            "in training loop, epoch 3, step 667, the loss is 184817.0\n",
            "in training loop, epoch 3, step 668, the loss is 119863.2890625\n",
            "in training loop, epoch 3, step 669, the loss is 132887.90625\n",
            "in training loop, epoch 3, step 670, the loss is 188286.46875\n",
            "in training loop, epoch 3, step 671, the loss is 157467.53125\n",
            "in training loop, epoch 3, step 672, the loss is 305736.3125\n",
            "in training loop, epoch 3, step 673, the loss is 217770.59375\n",
            "in training loop, epoch 3, step 674, the loss is 181889.5\n",
            "in training loop, epoch 3, step 675, the loss is 163764.296875\n",
            "in training loop, epoch 3, step 676, the loss is 209505.109375\n",
            "in training loop, epoch 3, step 677, the loss is 265224.6875\n",
            "in training loop, epoch 3, step 678, the loss is 198221.796875\n",
            "in training loop, epoch 3, step 679, the loss is 526711.0625\n",
            "in training loop, epoch 3, step 680, the loss is 296606.5\n",
            "in training loop, epoch 3, step 681, the loss is 255313.6875\n",
            "in training loop, epoch 3, step 682, the loss is 127863.046875\n",
            "in training loop, epoch 3, step 683, the loss is 188798.34375\n",
            "in training loop, epoch 3, step 684, the loss is 343802.34375\n",
            "in training loop, epoch 3, step 685, the loss is 216162.578125\n",
            "in training loop, epoch 3, step 686, the loss is 248509.9375\n",
            "in training loop, epoch 3, step 687, the loss is 261883.078125\n",
            "in training loop, epoch 3, step 688, the loss is 188826.78125\n",
            "in training loop, epoch 3, step 689, the loss is 147490.984375\n",
            "in training loop, epoch 3, step 690, the loss is 431668.1875\n",
            "in training loop, epoch 3, step 691, the loss is 557381.8125\n",
            "in training loop, epoch 3, step 692, the loss is 250740.9375\n",
            "in training loop, epoch 3, step 693, the loss is 217450.625\n",
            "in training loop, epoch 3, step 694, the loss is 376674.21875\n",
            "in training loop, epoch 3, step 695, the loss is 382379.96875\n",
            "in training loop, epoch 3, step 696, the loss is 262638.25\n",
            "in training loop, epoch 3, step 697, the loss is 572689.875\n",
            "in training loop, epoch 3, step 698, the loss is 247587.5625\n",
            "in training loop, epoch 3, step 699, the loss is 202589.90625\n",
            "in training loop, epoch 3, step 700, the loss is 227363.703125\n",
            "in training loop, epoch 3, step 701, the loss is 323602.53125\n",
            "in training loop, epoch 3, step 702, the loss is 211841.046875\n",
            "in training loop, epoch 3, step 703, the loss is 262225.75\n",
            "in training loop, epoch 3, step 704, the loss is 189889.859375\n",
            "in training loop, epoch 3, step 705, the loss is 250467.46875\n",
            "in training loop, epoch 3, step 706, the loss is 299907.90625\n",
            "in training loop, epoch 3, step 707, the loss is 361537.125\n",
            "in training loop, epoch 3, step 708, the loss is 175579.234375\n",
            "in training loop, epoch 3, step 709, the loss is 409138.625\n",
            "in training loop, epoch 3, step 710, the loss is 370101.375\n",
            "in training loop, epoch 3, step 711, the loss is 358557.5\n",
            "in training loop, epoch 3, step 712, the loss is 291477.3125\n",
            "in training loop, epoch 3, step 713, the loss is 358406.59375\n",
            "in training loop, epoch 3, step 714, the loss is 301921.03125\n",
            "in training loop, epoch 3, step 715, the loss is 257785.28125\n",
            "in training loop, epoch 3, step 716, the loss is 359565.28125\n",
            "in training loop, epoch 3, step 717, the loss is 227790.609375\n",
            "in training loop, epoch 3, step 718, the loss is 242700.5\n",
            "in training loop, epoch 3, step 719, the loss is 730782.3125\n",
            "in training loop, epoch 3, step 720, the loss is 212607.71875\n",
            "in training loop, epoch 3, step 721, the loss is 256727.4375\n",
            "in training loop, epoch 3, step 722, the loss is 244299.25\n",
            "in training loop, epoch 3, step 723, the loss is 558263.875\n",
            "in training loop, epoch 3, step 724, the loss is 297275.8125\n",
            "in training loop, epoch 3, step 725, the loss is 284062.78125\n",
            "in training loop, epoch 3, step 726, the loss is 248848.703125\n",
            "in training loop, epoch 3, step 727, the loss is 376668.84375\n",
            "in training loop, epoch 3, step 728, the loss is 138736.078125\n",
            "in training loop, epoch 3, step 729, the loss is 319124.375\n",
            "in training loop, epoch 3, step 730, the loss is 360652.25\n",
            "in training loop, epoch 3, step 731, the loss is 885146.25\n",
            "in training loop, epoch 3, step 732, the loss is 320500.625\n",
            "in training loop, epoch 3, step 733, the loss is 281356.5625\n",
            "in training loop, epoch 3, step 734, the loss is 224136.203125\n",
            "in training loop, epoch 3, step 735, the loss is 358482.3125\n",
            "in training loop, epoch 3, step 736, the loss is 165081.75\n",
            "in training loop, epoch 3, step 737, the loss is 291485.875\n",
            "in training loop, epoch 3, step 738, the loss is 209719.078125\n",
            "in training loop, epoch 3, step 739, the loss is 254476.375\n",
            "in training loop, epoch 3, step 740, the loss is 436446.90625\n",
            "in training loop, epoch 3, step 741, the loss is 182761.546875\n",
            "in training loop, epoch 3, step 742, the loss is 269672.71875\n",
            "in training loop, epoch 3, step 743, the loss is 228271.96875\n",
            "in training loop, epoch 3, step 744, the loss is 145158.34375\n",
            "in training loop, epoch 3, step 745, the loss is 337523.25\n",
            "in training loop, epoch 3, step 746, the loss is 172837.421875\n",
            "in training loop, epoch 3, step 747, the loss is 203906.890625\n",
            "in training loop, epoch 3, step 748, the loss is 218270.8125\n",
            "in training loop, epoch 3, step 749, the loss is 240728.296875\n",
            "in training loop, epoch 3, step 750, the loss is 366772.0625\n",
            "in training loop, epoch 3, step 751, the loss is 157906.921875\n",
            "in training loop, epoch 3, step 752, the loss is 182735.875\n",
            "in training loop, epoch 3, step 753, the loss is 274485.28125\n",
            "in training loop, epoch 3, step 754, the loss is 336781.0\n",
            "in training loop, epoch 3, step 755, the loss is 336202.125\n",
            "in training loop, epoch 3, step 756, the loss is 194649.671875\n",
            "in training loop, epoch 3, step 757, the loss is 268587.3125\n",
            "in training loop, epoch 3, step 758, the loss is 227511.171875\n",
            "in training loop, epoch 3, step 759, the loss is 164599.625\n",
            "in training loop, epoch 3, step 760, the loss is 216813.9375\n",
            "in training loop, epoch 3, step 761, the loss is 264716.28125\n",
            "in training loop, epoch 3, step 762, the loss is 398536.34375\n",
            "in training loop, epoch 3, step 763, the loss is 218466.015625\n",
            "in training loop, epoch 3, step 764, the loss is 198290.921875\n",
            "in training loop, epoch 3, step 765, the loss is 166728.390625\n",
            "in training loop, epoch 3, step 766, the loss is 170487.421875\n",
            "in training loop, epoch 3, step 767, the loss is 190984.734375\n",
            "in training loop, epoch 3, step 768, the loss is 260659.84375\n",
            "in training loop, epoch 3, step 769, the loss is 213785.1875\n",
            "in training loop, epoch 3, step 770, the loss is 119625.59375\n",
            "in training loop, epoch 3, step 771, the loss is 258670.40625\n",
            "in training loop, epoch 3, step 772, the loss is 249236.8125\n",
            "in training loop, epoch 3, step 773, the loss is 276242.53125\n",
            "in training loop, epoch 3, step 774, the loss is 229217.890625\n",
            "in training loop, epoch 3, step 775, the loss is 254181.265625\n",
            "in training loop, epoch 3, step 776, the loss is 322811.0625\n",
            "in training loop, epoch 3, step 777, the loss is 323614.84375\n",
            "in training loop, epoch 3, step 778, the loss is 244357.5625\n",
            "in training loop, epoch 3, step 779, the loss is 346996.53125\n",
            "in training loop, epoch 3, step 780, the loss is 163877.8125\n",
            "in training loop, epoch 3, step 781, the loss is 185799.640625\n",
            "in training loop, epoch 3, step 782, the loss is 210346.96875\n",
            "in training loop, epoch 3, step 783, the loss is 354347.90625\n",
            "in training loop, epoch 3, step 784, the loss is 191772.703125\n",
            "in training loop, epoch 3, step 785, the loss is 393501.4375\n",
            "in training loop, epoch 3, step 786, the loss is 253269.421875\n",
            "in training loop, epoch 3, step 787, the loss is 167134.40625\n",
            "in training loop, epoch 3, step 788, the loss is 240707.109375\n",
            "in training loop, epoch 3, step 789, the loss is 185967.890625\n",
            "in training loop, epoch 3, step 790, the loss is 215033.796875\n",
            "in training loop, epoch 3, step 791, the loss is 276731.625\n",
            "in training loop, epoch 3, step 792, the loss is 255506.265625\n",
            "in training loop, epoch 3, step 793, the loss is 343398.75\n",
            "in training loop, epoch 3, step 794, the loss is 253818.8125\n",
            "in training loop, epoch 3, step 795, the loss is 167315.0625\n",
            "in training loop, epoch 3, step 796, the loss is 217108.5625\n",
            "in training loop, epoch 3, step 797, the loss is 139266.484375\n",
            "in training loop, epoch 3, step 798, the loss is 160320.640625\n",
            "in training loop, epoch 3, step 799, the loss is 314098.1875\n",
            "in training loop, epoch 3, step 800, the loss is 401262.0\n",
            "in training loop, epoch 3, step 801, the loss is 217561.4375\n",
            "in training loop, epoch 3, step 802, the loss is 152898.9375\n",
            "in training loop, epoch 3, step 803, the loss is 242948.84375\n",
            "in training loop, epoch 3, step 804, the loss is 229453.859375\n",
            "in training loop, epoch 3, step 805, the loss is 200424.765625\n",
            "in training loop, epoch 3, step 806, the loss is 183560.421875\n",
            "in training loop, epoch 3, step 807, the loss is 256345.265625\n",
            "in training loop, epoch 3, step 808, the loss is 259960.03125\n",
            "in training loop, epoch 3, step 809, the loss is 259861.1875\n",
            "in training loop, epoch 3, step 810, the loss is 325142.65625\n",
            "in training loop, epoch 3, step 811, the loss is 190281.34375\n",
            "in training loop, epoch 3, step 812, the loss is 254922.171875\n",
            "in training loop, epoch 3, step 813, the loss is 220558.296875\n",
            "in training loop, epoch 3, step 814, the loss is 218162.3125\n",
            "in training loop, epoch 3, step 815, the loss is 356379.25\n",
            "in training loop, epoch 3, step 816, the loss is 270277.25\n",
            "in training loop, epoch 3, step 817, the loss is 102965.296875\n",
            "in training loop, epoch 3, step 818, the loss is 187053.25\n",
            "in training loop, epoch 3, step 819, the loss is 480632.21875\n",
            "in training loop, epoch 3, step 820, the loss is 115042.6484375\n",
            "in training loop, epoch 3, step 821, the loss is 244292.90625\n",
            "in training loop, epoch 3, step 822, the loss is 358658.9375\n",
            "in training loop, epoch 3, step 823, the loss is 257447.34375\n",
            "in training loop, epoch 3, step 824, the loss is 220325.625\n",
            "in training loop, epoch 3, step 825, the loss is 199228.28125\n",
            "in training loop, epoch 3, step 826, the loss is 196196.03125\n",
            "in training loop, epoch 3, step 827, the loss is 191393.671875\n",
            "in training loop, epoch 3, step 828, the loss is 281293.5\n",
            "in training loop, epoch 3, step 829, the loss is 103231.625\n",
            "in training loop, epoch 3, step 830, the loss is 130366.9140625\n",
            "in training loop, epoch 3, step 831, the loss is 218696.25\n",
            "in training loop, epoch 3, step 832, the loss is 223546.6875\n",
            "in training loop, epoch 3, step 833, the loss is 154208.4375\n",
            "in training loop, epoch 3, step 834, the loss is 229038.09375\n",
            "in training loop, epoch 3, step 835, the loss is 208737.109375\n",
            "in training loop, epoch 3, step 836, the loss is 193454.6875\n",
            "in training loop, epoch 3, step 837, the loss is 281023.40625\n",
            "in training loop, epoch 3, step 838, the loss is 164631.375\n",
            "in training loop, epoch 3, step 839, the loss is 319935.40625\n",
            "in training loop, epoch 3, step 840, the loss is 161227.75\n",
            "in training loop, epoch 3, step 841, the loss is 254205.328125\n",
            "in training loop, epoch 3, step 842, the loss is 198937.15625\n",
            "in training loop, epoch 3, step 843, the loss is 200505.40625\n",
            "in training loop, epoch 3, step 844, the loss is 219972.59375\n",
            "in training loop, epoch 3, step 845, the loss is 273544.3125\n",
            "in training loop, epoch 3, step 846, the loss is 192500.984375\n",
            "in training loop, epoch 3, step 847, the loss is 373857.5625\n",
            "in training loop, epoch 3, step 848, the loss is 327429.71875\n",
            "in training loop, epoch 3, step 849, the loss is 310374.4375\n",
            "in training loop, epoch 3, step 850, the loss is 420595.5625\n",
            "in training loop, epoch 3, step 851, the loss is 176433.296875\n",
            "in training loop, epoch 3, step 852, the loss is 309855.5625\n",
            "in training loop, epoch 3, step 853, the loss is 249838.84375\n",
            "in training loop, epoch 3, step 854, the loss is 290447.46875\n",
            "in training loop, epoch 3, step 855, the loss is 168101.5625\n",
            "in training loop, epoch 3, step 856, the loss is 191212.453125\n",
            "in training loop, epoch 3, step 857, the loss is 89933.546875\n",
            "in training loop, epoch 3, step 858, the loss is 262156.96875\n",
            "in training loop, epoch 3, step 859, the loss is 134189.296875\n",
            "in training loop, epoch 3, step 860, the loss is 252556.40625\n",
            "in training loop, epoch 3, step 861, the loss is 152607.96875\n",
            "in training loop, epoch 3, step 862, the loss is 177377.765625\n",
            "in training loop, epoch 3, step 863, the loss is 265823.125\n",
            "in training loop, epoch 3, step 864, the loss is 261219.96875\n",
            "in training loop, epoch 3, step 865, the loss is 226345.828125\n",
            "in training loop, epoch 3, step 866, the loss is 241688.96875\n",
            "in training loop, epoch 3, step 867, the loss is 203965.25\n",
            "in training loop, epoch 3, step 868, the loss is 225622.625\n",
            "in training loop, epoch 3, step 869, the loss is 109268.15625\n",
            "in training loop, epoch 3, step 870, the loss is 266134.5\n",
            "in training loop, epoch 3, step 871, the loss is 243265.125\n",
            "in training loop, epoch 3, step 872, the loss is 177112.640625\n",
            "in training loop, epoch 3, step 873, the loss is 266729.90625\n",
            "in training loop, epoch 3, step 874, the loss is 324135.375\n",
            "in training loop, epoch 3, step 875, the loss is 228011.484375\n",
            "in training loop, epoch 3, step 876, the loss is 188691.984375\n",
            "in training loop, epoch 3, step 877, the loss is 337442.9375\n",
            "in training loop, epoch 3, step 878, the loss is 213318.640625\n",
            "in training loop, epoch 3, step 879, the loss is 301463.5625\n",
            "in training loop, epoch 3, step 880, the loss is 143436.71875\n",
            "in training loop, epoch 3, step 881, the loss is 132853.46875\n",
            "in training loop, epoch 3, step 882, the loss is 210381.359375\n",
            "in training loop, epoch 3, step 883, the loss is 233815.625\n",
            "in training loop, epoch 3, step 884, the loss is 292331.34375\n",
            "in training loop, epoch 3, step 885, the loss is 231544.1875\n",
            "in training loop, epoch 3, step 886, the loss is 169927.71875\n",
            "in training loop, epoch 3, step 887, the loss is 165957.125\n",
            "in training loop, epoch 3, step 888, the loss is 325157.03125\n",
            "in training loop, epoch 3, step 889, the loss is 489053.6875\n",
            "in training loop, epoch 3, step 890, the loss is 219464.3125\n",
            "in training loop, epoch 3, step 891, the loss is 129870.984375\n",
            "in training loop, epoch 3, step 892, the loss is 232850.875\n",
            "in training loop, epoch 3, step 893, the loss is 185156.375\n",
            "in training loop, epoch 3, step 894, the loss is 171059.625\n",
            "in training loop, epoch 3, step 895, the loss is 429713.625\n",
            "in training loop, epoch 3, step 896, the loss is 182201.609375\n",
            "in training loop, epoch 3, step 897, the loss is 233524.984375\n",
            "in training loop, epoch 3, step 898, the loss is 210844.34375\n",
            "in training loop, epoch 3, step 899, the loss is 169872.84375\n",
            "in training loop, epoch 3, step 900, the loss is 488601.1875\n",
            "in training loop, epoch 3, step 901, the loss is 89884.5546875\n",
            "in training loop, epoch 3, step 902, the loss is 161157.96875\n",
            "in training loop, epoch 3, step 903, the loss is 110299.828125\n",
            "k-fold 0:: Epoch 3: train loss 236744.10011061947 val loss 313402.7904548267\n",
            "in training loop, epoch 4, step 0, the loss is 83795.296875\n",
            "in training loop, epoch 4, step 1, the loss is 199612.21875\n",
            "in training loop, epoch 4, step 2, the loss is 158718.640625\n",
            "in training loop, epoch 4, step 3, the loss is 205064.984375\n",
            "in training loop, epoch 4, step 4, the loss is 189363.796875\n",
            "in training loop, epoch 4, step 5, the loss is 263908.0625\n",
            "in training loop, epoch 4, step 6, the loss is 185311.796875\n",
            "in training loop, epoch 4, step 7, the loss is 184006.5\n",
            "in training loop, epoch 4, step 8, the loss is 207165.234375\n",
            "in training loop, epoch 4, step 9, the loss is 263989.90625\n",
            "in training loop, epoch 4, step 10, the loss is 209338.40625\n",
            "in training loop, epoch 4, step 11, the loss is 157439.90625\n",
            "in training loop, epoch 4, step 12, the loss is 194711.9375\n",
            "in training loop, epoch 4, step 13, the loss is 196001.90625\n",
            "in training loop, epoch 4, step 14, the loss is 196690.859375\n",
            "in training loop, epoch 4, step 15, the loss is 195904.1875\n",
            "in training loop, epoch 4, step 16, the loss is 163692.65625\n",
            "in training loop, epoch 4, step 17, the loss is 168409.125\n",
            "in training loop, epoch 4, step 18, the loss is 217844.265625\n",
            "in training loop, epoch 4, step 19, the loss is 212725.671875\n",
            "in training loop, epoch 4, step 20, the loss is 132177.796875\n",
            "in training loop, epoch 4, step 21, the loss is 211972.546875\n",
            "in training loop, epoch 4, step 22, the loss is 214456.34375\n",
            "in training loop, epoch 4, step 23, the loss is 177138.265625\n",
            "in training loop, epoch 4, step 24, the loss is 175483.8125\n",
            "in training loop, epoch 4, step 25, the loss is 178133.375\n",
            "in training loop, epoch 4, step 26, the loss is 117978.640625\n",
            "in training loop, epoch 4, step 27, the loss is 289171.34375\n",
            "in training loop, epoch 4, step 28, the loss is 206758.96875\n",
            "in training loop, epoch 4, step 29, the loss is 276043.875\n",
            "in training loop, epoch 4, step 30, the loss is 240366.53125\n",
            "in training loop, epoch 4, step 31, the loss is 256268.1875\n",
            "in training loop, epoch 4, step 32, the loss is 149290.109375\n",
            "in training loop, epoch 4, step 33, the loss is 149042.40625\n",
            "in training loop, epoch 4, step 34, the loss is 158332.5625\n",
            "in training loop, epoch 4, step 35, the loss is 185878.484375\n",
            "in training loop, epoch 4, step 36, the loss is 240064.328125\n",
            "in training loop, epoch 4, step 37, the loss is 323698.0625\n",
            "in training loop, epoch 4, step 38, the loss is 231698.734375\n",
            "in training loop, epoch 4, step 39, the loss is 181212.8125\n",
            "in training loop, epoch 4, step 40, the loss is 186348.40625\n",
            "in training loop, epoch 4, step 41, the loss is 195689.09375\n",
            "in training loop, epoch 4, step 42, the loss is 168116.96875\n",
            "in training loop, epoch 4, step 43, the loss is 167571.703125\n",
            "in training loop, epoch 4, step 44, the loss is 153089.8125\n",
            "in training loop, epoch 4, step 45, the loss is 157091.484375\n",
            "in training loop, epoch 4, step 46, the loss is 154182.78125\n",
            "in training loop, epoch 4, step 47, the loss is 188171.890625\n",
            "in training loop, epoch 4, step 48, the loss is 196190.3125\n",
            "in training loop, epoch 4, step 49, the loss is 133825.390625\n",
            "in training loop, epoch 4, step 50, the loss is 201739.015625\n",
            "in training loop, epoch 4, step 51, the loss is 129108.703125\n",
            "in training loop, epoch 4, step 52, the loss is 258614.46875\n",
            "in training loop, epoch 4, step 53, the loss is 167799.3125\n",
            "in training loop, epoch 4, step 54, the loss is 184547.296875\n",
            "in training loop, epoch 4, step 55, the loss is 272725.9375\n",
            "in training loop, epoch 4, step 56, the loss is 243037.046875\n",
            "in training loop, epoch 4, step 57, the loss is 121508.3359375\n",
            "in training loop, epoch 4, step 58, the loss is 338808.3125\n",
            "in training loop, epoch 4, step 59, the loss is 206169.59375\n",
            "in training loop, epoch 4, step 60, the loss is 211618.671875\n",
            "in training loop, epoch 4, step 61, the loss is 148613.5625\n",
            "in training loop, epoch 4, step 62, the loss is 116781.3828125\n",
            "in training loop, epoch 4, step 63, the loss is 257839.0\n",
            "in training loop, epoch 4, step 64, the loss is 152467.5625\n",
            "in training loop, epoch 4, step 65, the loss is 150284.953125\n",
            "in training loop, epoch 4, step 66, the loss is 215887.328125\n",
            "in training loop, epoch 4, step 67, the loss is 253499.90625\n",
            "in training loop, epoch 4, step 68, the loss is 156495.640625\n",
            "in training loop, epoch 4, step 69, the loss is 236198.125\n",
            "in training loop, epoch 4, step 70, the loss is 162782.703125\n",
            "in training loop, epoch 4, step 71, the loss is 158726.3125\n",
            "in training loop, epoch 4, step 72, the loss is 178109.3125\n",
            "in training loop, epoch 4, step 73, the loss is 211904.0\n",
            "in training loop, epoch 4, step 74, the loss is 255170.25\n",
            "in training loop, epoch 4, step 75, the loss is 111642.109375\n",
            "in training loop, epoch 4, step 76, the loss is 115544.78125\n",
            "in training loop, epoch 4, step 77, the loss is 261257.34375\n",
            "in training loop, epoch 4, step 78, the loss is 263912.75\n",
            "in training loop, epoch 4, step 79, the loss is 133006.4375\n",
            "in training loop, epoch 4, step 80, the loss is 198086.21875\n",
            "in training loop, epoch 4, step 81, the loss is 178119.78125\n",
            "in training loop, epoch 4, step 82, the loss is 146011.4375\n",
            "in training loop, epoch 4, step 83, the loss is 153869.234375\n",
            "in training loop, epoch 4, step 84, the loss is 208760.140625\n",
            "in training loop, epoch 4, step 85, the loss is 209902.21875\n",
            "in training loop, epoch 4, step 86, the loss is 140103.859375\n",
            "in training loop, epoch 4, step 87, the loss is 174547.359375\n",
            "in training loop, epoch 4, step 88, the loss is 176694.234375\n",
            "in training loop, epoch 4, step 89, the loss is 224258.75\n",
            "in training loop, epoch 4, step 90, the loss is 169631.609375\n",
            "in training loop, epoch 4, step 91, the loss is 250414.796875\n",
            "in training loop, epoch 4, step 92, the loss is 205230.84375\n",
            "in training loop, epoch 4, step 93, the loss is 552579.0\n",
            "in training loop, epoch 4, step 94, the loss is 226448.5625\n",
            "in training loop, epoch 4, step 95, the loss is 276251.4375\n",
            "in training loop, epoch 4, step 96, the loss is 221164.390625\n",
            "in training loop, epoch 4, step 97, the loss is 209312.53125\n",
            "in training loop, epoch 4, step 98, the loss is 350030.125\n",
            "in training loop, epoch 4, step 99, the loss is 170786.59375\n",
            "in training loop, epoch 4, step 100, the loss is 194553.28125\n",
            "in training loop, epoch 4, step 101, the loss is 206975.484375\n",
            "in training loop, epoch 4, step 102, the loss is 175132.640625\n",
            "in training loop, epoch 4, step 103, the loss is 162008.8125\n",
            "in training loop, epoch 4, step 104, the loss is 150193.046875\n",
            "in training loop, epoch 4, step 105, the loss is 109578.0078125\n",
            "in training loop, epoch 4, step 106, the loss is 160924.46875\n",
            "in training loop, epoch 4, step 107, the loss is 135745.359375\n",
            "in training loop, epoch 4, step 108, the loss is 234038.8125\n",
            "in training loop, epoch 4, step 109, the loss is 237187.921875\n",
            "in training loop, epoch 4, step 110, the loss is 165600.21875\n",
            "in training loop, epoch 4, step 111, the loss is 171181.40625\n",
            "in training loop, epoch 4, step 112, the loss is 270297.78125\n",
            "in training loop, epoch 4, step 113, the loss is 229535.65625\n",
            "in training loop, epoch 4, step 114, the loss is 173413.03125\n",
            "in training loop, epoch 4, step 115, the loss is 151921.84375\n",
            "in training loop, epoch 4, step 116, the loss is 184114.59375\n",
            "in training loop, epoch 4, step 117, the loss is 125080.515625\n",
            "in training loop, epoch 4, step 118, the loss is 286106.875\n",
            "in training loop, epoch 4, step 119, the loss is 302187.90625\n",
            "in training loop, epoch 4, step 120, the loss is 253367.5\n",
            "in training loop, epoch 4, step 121, the loss is 278574.96875\n",
            "in training loop, epoch 4, step 122, the loss is 345968.125\n",
            "in training loop, epoch 4, step 123, the loss is 319678.34375\n",
            "in training loop, epoch 4, step 124, the loss is 271564.625\n",
            "in training loop, epoch 4, step 125, the loss is 149991.84375\n",
            "in training loop, epoch 4, step 126, the loss is 214618.421875\n",
            "in training loop, epoch 4, step 127, the loss is 296867.6875\n",
            "in training loop, epoch 4, step 128, the loss is 351347.28125\n",
            "in training loop, epoch 4, step 129, the loss is 192257.59375\n",
            "in training loop, epoch 4, step 130, the loss is 245531.71875\n",
            "in training loop, epoch 4, step 131, the loss is 241121.515625\n",
            "in training loop, epoch 4, step 132, the loss is 200163.71875\n",
            "in training loop, epoch 4, step 133, the loss is 292025.9375\n",
            "in training loop, epoch 4, step 134, the loss is 284802.625\n",
            "in training loop, epoch 4, step 135, the loss is 119615.28125\n",
            "in training loop, epoch 4, step 136, the loss is 229916.78125\n",
            "in training loop, epoch 4, step 137, the loss is 202565.21875\n",
            "in training loop, epoch 4, step 138, the loss is 243562.921875\n",
            "in training loop, epoch 4, step 139, the loss is 246212.78125\n",
            "in training loop, epoch 4, step 140, the loss is 246502.203125\n",
            "in training loop, epoch 4, step 141, the loss is 290562.75\n",
            "in training loop, epoch 4, step 142, the loss is 253756.875\n",
            "in training loop, epoch 4, step 143, the loss is 295644.4375\n",
            "in training loop, epoch 4, step 144, the loss is 210182.0625\n",
            "in training loop, epoch 4, step 145, the loss is 225679.625\n",
            "in training loop, epoch 4, step 146, the loss is 181252.03125\n",
            "in training loop, epoch 4, step 147, the loss is 178882.875\n",
            "in training loop, epoch 4, step 148, the loss is 190292.234375\n",
            "in training loop, epoch 4, step 149, the loss is 166819.4375\n",
            "in training loop, epoch 4, step 150, the loss is 237446.5625\n",
            "in training loop, epoch 4, step 151, the loss is 131118.453125\n",
            "in training loop, epoch 4, step 152, the loss is 145871.8125\n",
            "in training loop, epoch 4, step 153, the loss is 211365.078125\n",
            "in training loop, epoch 4, step 154, the loss is 279612.96875\n",
            "in training loop, epoch 4, step 155, the loss is 295319.65625\n",
            "in training loop, epoch 4, step 156, the loss is 302071.125\n",
            "in training loop, epoch 4, step 157, the loss is 326000.96875\n",
            "in training loop, epoch 4, step 158, the loss is 269007.90625\n",
            "in training loop, epoch 4, step 159, the loss is 232606.859375\n",
            "in training loop, epoch 4, step 160, the loss is 177001.984375\n",
            "in training loop, epoch 4, step 161, the loss is 189212.390625\n",
            "in training loop, epoch 4, step 162, the loss is 244186.03125\n",
            "in training loop, epoch 4, step 163, the loss is 165142.109375\n",
            "in training loop, epoch 4, step 164, the loss is 209206.21875\n",
            "in training loop, epoch 4, step 165, the loss is 190569.515625\n",
            "in training loop, epoch 4, step 166, the loss is 201240.703125\n",
            "in training loop, epoch 4, step 167, the loss is 265156.21875\n",
            "in training loop, epoch 4, step 168, the loss is 144049.125\n",
            "in training loop, epoch 4, step 169, the loss is 249108.8125\n",
            "in training loop, epoch 4, step 170, the loss is 172298.8125\n",
            "in training loop, epoch 4, step 171, the loss is 166352.859375\n",
            "in training loop, epoch 4, step 172, the loss is 193544.34375\n",
            "in training loop, epoch 4, step 173, the loss is 321391.625\n",
            "in training loop, epoch 4, step 174, the loss is 191698.78125\n",
            "in training loop, epoch 4, step 175, the loss is 149466.90625\n",
            "in training loop, epoch 4, step 176, the loss is 202385.921875\n",
            "in training loop, epoch 4, step 177, the loss is 306737.09375\n",
            "in training loop, epoch 4, step 178, the loss is 340166.34375\n",
            "in training loop, epoch 4, step 179, the loss is 143694.546875\n",
            "in training loop, epoch 4, step 180, the loss is 152620.296875\n",
            "in training loop, epoch 4, step 181, the loss is 187444.578125\n",
            "in training loop, epoch 4, step 182, the loss is 209505.078125\n",
            "in training loop, epoch 4, step 183, the loss is 185865.546875\n",
            "in training loop, epoch 4, step 184, the loss is 176716.515625\n",
            "in training loop, epoch 4, step 185, the loss is 255421.625\n",
            "in training loop, epoch 4, step 186, the loss is 210427.234375\n",
            "in training loop, epoch 4, step 187, the loss is 197164.671875\n",
            "in training loop, epoch 4, step 188, the loss is 173429.609375\n",
            "in training loop, epoch 4, step 189, the loss is 145274.046875\n",
            "in training loop, epoch 4, step 190, the loss is 268966.625\n",
            "in training loop, epoch 4, step 191, the loss is 172861.65625\n",
            "in training loop, epoch 4, step 192, the loss is 177741.953125\n",
            "in training loop, epoch 4, step 193, the loss is 286880.6875\n",
            "in training loop, epoch 4, step 194, the loss is 221069.296875\n",
            "in training loop, epoch 4, step 195, the loss is 188618.515625\n",
            "in training loop, epoch 4, step 196, the loss is 206150.59375\n",
            "in training loop, epoch 4, step 197, the loss is 119542.0234375\n",
            "in training loop, epoch 4, step 198, the loss is 142544.125\n",
            "in training loop, epoch 4, step 199, the loss is 212927.65625\n",
            "in training loop, epoch 4, step 200, the loss is 210888.609375\n",
            "in training loop, epoch 4, step 201, the loss is 178806.703125\n",
            "in training loop, epoch 4, step 202, the loss is 206708.1875\n",
            "in training loop, epoch 4, step 203, the loss is 149824.609375\n",
            "in training loop, epoch 4, step 204, the loss is 230173.390625\n",
            "in training loop, epoch 4, step 205, the loss is 169808.9375\n",
            "in training loop, epoch 4, step 206, the loss is 206216.03125\n",
            "in training loop, epoch 4, step 207, the loss is 175801.09375\n",
            "in training loop, epoch 4, step 208, the loss is 134677.375\n",
            "in training loop, epoch 4, step 209, the loss is 192865.0\n",
            "in training loop, epoch 4, step 210, the loss is 184641.5\n",
            "in training loop, epoch 4, step 211, the loss is 167170.171875\n",
            "in training loop, epoch 4, step 212, the loss is 243866.75\n",
            "in training loop, epoch 4, step 213, the loss is 137385.1875\n",
            "in training loop, epoch 4, step 214, the loss is 132955.515625\n",
            "in training loop, epoch 4, step 215, the loss is 178028.046875\n",
            "in training loop, epoch 4, step 216, the loss is 176710.9375\n",
            "in training loop, epoch 4, step 217, the loss is 110063.1171875\n",
            "in training loop, epoch 4, step 218, the loss is 88590.9765625\n",
            "in training loop, epoch 4, step 219, the loss is 159872.109375\n",
            "in training loop, epoch 4, step 220, the loss is 168813.375\n",
            "in training loop, epoch 4, step 221, the loss is 172154.65625\n",
            "in training loop, epoch 4, step 222, the loss is 239850.71875\n",
            "in training loop, epoch 4, step 223, the loss is 264430.375\n",
            "in training loop, epoch 4, step 224, the loss is 172576.5\n",
            "in training loop, epoch 4, step 225, the loss is 135237.71875\n",
            "in training loop, epoch 4, step 226, the loss is 186450.5\n",
            "in training loop, epoch 4, step 227, the loss is 166976.984375\n",
            "in training loop, epoch 4, step 228, the loss is 248823.09375\n",
            "in training loop, epoch 4, step 229, the loss is 210529.90625\n",
            "in training loop, epoch 4, step 230, the loss is 138303.515625\n",
            "in training loop, epoch 4, step 231, the loss is 151849.4375\n",
            "in training loop, epoch 4, step 232, the loss is 138818.28125\n",
            "in training loop, epoch 4, step 233, the loss is 150179.109375\n",
            "in training loop, epoch 4, step 234, the loss is 158706.6875\n",
            "in training loop, epoch 4, step 235, the loss is 237539.640625\n",
            "in training loop, epoch 4, step 236, the loss is 241498.265625\n",
            "in training loop, epoch 4, step 237, the loss is 153062.03125\n",
            "in training loop, epoch 4, step 238, the loss is 234371.4375\n",
            "in training loop, epoch 4, step 239, the loss is 198194.171875\n",
            "in training loop, epoch 4, step 240, the loss is 220410.453125\n",
            "in training loop, epoch 4, step 241, the loss is 122914.734375\n",
            "in training loop, epoch 4, step 242, the loss is 236470.40625\n",
            "in training loop, epoch 4, step 243, the loss is 143571.9375\n",
            "in training loop, epoch 4, step 244, the loss is 284960.53125\n",
            "in training loop, epoch 4, step 245, the loss is 223067.21875\n",
            "in training loop, epoch 4, step 246, the loss is 153269.203125\n",
            "in training loop, epoch 4, step 247, the loss is 187920.421875\n",
            "in training loop, epoch 4, step 248, the loss is 165422.65625\n",
            "in training loop, epoch 4, step 249, the loss is 166812.25\n",
            "in training loop, epoch 4, step 250, the loss is 143626.5625\n",
            "in training loop, epoch 4, step 251, the loss is 165662.078125\n",
            "in training loop, epoch 4, step 252, the loss is 185239.578125\n",
            "in training loop, epoch 4, step 253, the loss is 195546.15625\n",
            "in training loop, epoch 4, step 254, the loss is 179165.765625\n",
            "in training loop, epoch 4, step 255, the loss is 191134.71875\n",
            "in training loop, epoch 4, step 256, the loss is 192667.9375\n",
            "in training loop, epoch 4, step 257, the loss is 212121.578125\n",
            "in training loop, epoch 4, step 258, the loss is 127461.8828125\n",
            "in training loop, epoch 4, step 259, the loss is 187022.828125\n",
            "in training loop, epoch 4, step 260, the loss is 284282.9375\n",
            "in training loop, epoch 4, step 261, the loss is 237564.25\n",
            "in training loop, epoch 4, step 262, the loss is 241638.34375\n",
            "in training loop, epoch 4, step 263, the loss is 210260.796875\n",
            "in training loop, epoch 4, step 264, the loss is 162330.390625\n",
            "in training loop, epoch 4, step 265, the loss is 353734.3125\n",
            "in training loop, epoch 4, step 266, the loss is 205049.625\n",
            "in training loop, epoch 4, step 267, the loss is 156696.046875\n",
            "in training loop, epoch 4, step 268, the loss is 233233.8125\n",
            "in training loop, epoch 4, step 269, the loss is 258507.71875\n",
            "in training loop, epoch 4, step 270, the loss is 152135.265625\n",
            "in training loop, epoch 4, step 271, the loss is 167262.109375\n",
            "in training loop, epoch 4, step 272, the loss is 184261.28125\n",
            "in training loop, epoch 4, step 273, the loss is 201880.890625\n",
            "in training loop, epoch 4, step 274, the loss is 200078.21875\n",
            "in training loop, epoch 4, step 275, the loss is 283701.5\n",
            "in training loop, epoch 4, step 276, the loss is 196479.71875\n",
            "in training loop, epoch 4, step 277, the loss is 238166.734375\n",
            "in training loop, epoch 4, step 278, the loss is 120476.1171875\n",
            "in training loop, epoch 4, step 279, the loss is 234017.96875\n",
            "in training loop, epoch 4, step 280, the loss is 199240.203125\n",
            "in training loop, epoch 4, step 281, the loss is 214716.484375\n",
            "in training loop, epoch 4, step 282, the loss is 209225.9375\n",
            "in training loop, epoch 4, step 283, the loss is 243623.34375\n",
            "in training loop, epoch 4, step 284, the loss is 222792.796875\n",
            "in training loop, epoch 4, step 285, the loss is 221760.078125\n",
            "in training loop, epoch 4, step 286, the loss is 200195.109375\n",
            "in training loop, epoch 4, step 287, the loss is 314171.71875\n",
            "in training loop, epoch 4, step 288, the loss is 197300.125\n",
            "in training loop, epoch 4, step 289, the loss is 251471.171875\n",
            "in training loop, epoch 4, step 290, the loss is 151135.65625\n",
            "in training loop, epoch 4, step 291, the loss is 229671.671875\n",
            "in training loop, epoch 4, step 292, the loss is 150022.828125\n",
            "in training loop, epoch 4, step 293, the loss is 265121.96875\n",
            "in training loop, epoch 4, step 294, the loss is 345097.8125\n",
            "in training loop, epoch 4, step 295, the loss is 221716.0\n",
            "in training loop, epoch 4, step 296, the loss is 168482.703125\n",
            "in training loop, epoch 4, step 297, the loss is 296517.0625\n",
            "in training loop, epoch 4, step 298, the loss is 146860.078125\n",
            "in training loop, epoch 4, step 299, the loss is 219374.15625\n",
            "in training loop, epoch 4, step 300, the loss is 196847.421875\n",
            "in training loop, epoch 4, step 301, the loss is 189197.65625\n",
            "in training loop, epoch 4, step 302, the loss is 212910.125\n",
            "in training loop, epoch 4, step 303, the loss is 234622.203125\n",
            "in training loop, epoch 4, step 304, the loss is 169811.71875\n",
            "in training loop, epoch 4, step 305, the loss is 234931.234375\n",
            "in training loop, epoch 4, step 306, the loss is 234650.09375\n",
            "in training loop, epoch 4, step 307, the loss is 228352.640625\n",
            "in training loop, epoch 4, step 308, the loss is 210558.578125\n",
            "in training loop, epoch 4, step 309, the loss is 110160.421875\n",
            "in training loop, epoch 4, step 310, the loss is 203124.140625\n",
            "in training loop, epoch 4, step 311, the loss is 292860.3125\n",
            "in training loop, epoch 4, step 312, the loss is 219390.859375\n",
            "in training loop, epoch 4, step 313, the loss is 228542.953125\n",
            "in training loop, epoch 4, step 314, the loss is 177005.96875\n",
            "in training loop, epoch 4, step 315, the loss is 116515.7578125\n",
            "in training loop, epoch 4, step 316, the loss is 85366.6953125\n",
            "in training loop, epoch 4, step 317, the loss is 252020.5625\n",
            "in training loop, epoch 4, step 318, the loss is 286492.40625\n",
            "in training loop, epoch 4, step 319, the loss is 191067.140625\n",
            "in training loop, epoch 4, step 320, the loss is 204258.984375\n",
            "in training loop, epoch 4, step 321, the loss is 143871.953125\n",
            "in training loop, epoch 4, step 322, the loss is 120545.4453125\n",
            "in training loop, epoch 4, step 323, the loss is 202838.5625\n",
            "in training loop, epoch 4, step 324, the loss is 162340.171875\n",
            "in training loop, epoch 4, step 325, the loss is 191962.015625\n",
            "in training loop, epoch 4, step 326, the loss is 157641.375\n",
            "in training loop, epoch 4, step 327, the loss is 97463.546875\n",
            "in training loop, epoch 4, step 328, the loss is 194140.75\n",
            "in training loop, epoch 4, step 329, the loss is 142465.109375\n",
            "in training loop, epoch 4, step 330, the loss is 153450.25\n",
            "in training loop, epoch 4, step 331, the loss is 138005.515625\n",
            "in training loop, epoch 4, step 332, the loss is 244509.03125\n",
            "in training loop, epoch 4, step 333, the loss is 220357.0\n",
            "in training loop, epoch 4, step 334, the loss is 179610.234375\n",
            "in training loop, epoch 4, step 335, the loss is 230725.0\n",
            "in training loop, epoch 4, step 336, the loss is 185609.65625\n",
            "in training loop, epoch 4, step 337, the loss is 225903.375\n",
            "in training loop, epoch 4, step 338, the loss is 140970.296875\n",
            "in training loop, epoch 4, step 339, the loss is 206828.171875\n",
            "in training loop, epoch 4, step 340, the loss is 202800.8125\n",
            "in training loop, epoch 4, step 341, the loss is 233936.375\n",
            "in training loop, epoch 4, step 342, the loss is 200101.640625\n",
            "in training loop, epoch 4, step 343, the loss is 206956.75\n",
            "in training loop, epoch 4, step 344, the loss is 226870.453125\n",
            "in training loop, epoch 4, step 345, the loss is 79190.2734375\n",
            "in training loop, epoch 4, step 346, the loss is 190564.46875\n",
            "in training loop, epoch 4, step 347, the loss is 174114.421875\n",
            "in training loop, epoch 4, step 348, the loss is 156861.75\n",
            "in training loop, epoch 4, step 349, the loss is 177639.96875\n",
            "in training loop, epoch 4, step 350, the loss is 204736.0\n",
            "in training loop, epoch 4, step 351, the loss is 179893.0625\n",
            "in training loop, epoch 4, step 352, the loss is 244207.09375\n",
            "in training loop, epoch 4, step 353, the loss is 256952.96875\n",
            "in training loop, epoch 4, step 354, the loss is 236755.96875\n",
            "in training loop, epoch 4, step 355, the loss is 135471.78125\n",
            "in training loop, epoch 4, step 356, the loss is 113020.78125\n",
            "in training loop, epoch 4, step 357, the loss is 146466.28125\n",
            "in training loop, epoch 4, step 358, the loss is 464755.4375\n",
            "in training loop, epoch 4, step 359, the loss is 137329.546875\n",
            "in training loop, epoch 4, step 360, the loss is 255623.9375\n",
            "in training loop, epoch 4, step 361, the loss is 252300.765625\n",
            "in training loop, epoch 4, step 362, the loss is 252725.109375\n",
            "in training loop, epoch 4, step 363, the loss is 69793.359375\n",
            "in training loop, epoch 4, step 364, the loss is 188344.1875\n",
            "in training loop, epoch 4, step 365, the loss is 202087.78125\n",
            "in training loop, epoch 4, step 366, the loss is 239235.375\n",
            "in training loop, epoch 4, step 367, the loss is 162480.15625\n",
            "in training loop, epoch 4, step 368, the loss is 175433.140625\n",
            "in training loop, epoch 4, step 369, the loss is 204820.703125\n",
            "in training loop, epoch 4, step 370, the loss is 241200.5\n",
            "in training loop, epoch 4, step 371, the loss is 155813.03125\n",
            "in training loop, epoch 4, step 372, the loss is 254347.75\n",
            "in training loop, epoch 4, step 373, the loss is 197325.34375\n",
            "in training loop, epoch 4, step 374, the loss is 198132.375\n",
            "in training loop, epoch 4, step 375, the loss is 254483.1875\n",
            "in training loop, epoch 4, step 376, the loss is 172967.34375\n",
            "in training loop, epoch 4, step 377, the loss is 167381.25\n",
            "in training loop, epoch 4, step 378, the loss is 180573.515625\n",
            "in training loop, epoch 4, step 379, the loss is 241581.8125\n",
            "in training loop, epoch 4, step 380, the loss is 121057.09375\n",
            "in training loop, epoch 4, step 381, the loss is 135359.40625\n",
            "in training loop, epoch 4, step 382, the loss is 158829.09375\n",
            "in training loop, epoch 4, step 383, the loss is 233860.890625\n",
            "in training loop, epoch 4, step 384, the loss is 113335.6640625\n",
            "in training loop, epoch 4, step 385, the loss is 149142.0625\n",
            "in training loop, epoch 4, step 386, the loss is 201464.640625\n",
            "in training loop, epoch 4, step 387, the loss is 222943.25\n",
            "in training loop, epoch 4, step 388, the loss is 161412.0\n",
            "in training loop, epoch 4, step 389, the loss is 187271.484375\n",
            "in training loop, epoch 4, step 390, the loss is 227407.796875\n",
            "in training loop, epoch 4, step 391, the loss is 211697.859375\n",
            "in training loop, epoch 4, step 392, the loss is 232177.484375\n",
            "in training loop, epoch 4, step 393, the loss is 178997.921875\n",
            "in training loop, epoch 4, step 394, the loss is 196418.5\n",
            "in training loop, epoch 4, step 395, the loss is 134378.921875\n",
            "in training loop, epoch 4, step 396, the loss is 381136.46875\n",
            "in training loop, epoch 4, step 397, the loss is 109520.6875\n",
            "in training loop, epoch 4, step 398, the loss is 190742.25\n",
            "in training loop, epoch 4, step 399, the loss is 102955.40625\n",
            "in training loop, epoch 4, step 400, the loss is 151476.59375\n",
            "in training loop, epoch 4, step 401, the loss is 205571.671875\n",
            "in training loop, epoch 4, step 402, the loss is 135478.578125\n",
            "in training loop, epoch 4, step 403, the loss is 136993.171875\n",
            "in training loop, epoch 4, step 404, the loss is 110045.125\n",
            "in training loop, epoch 4, step 405, the loss is 168499.171875\n",
            "in training loop, epoch 4, step 406, the loss is 214521.078125\n",
            "in training loop, epoch 4, step 407, the loss is 185925.25\n",
            "in training loop, epoch 4, step 408, the loss is 260092.21875\n",
            "in training loop, epoch 4, step 409, the loss is 243681.40625\n",
            "in training loop, epoch 4, step 410, the loss is 144316.375\n",
            "in training loop, epoch 4, step 411, the loss is 148718.765625\n",
            "in training loop, epoch 4, step 412, the loss is 324587.90625\n",
            "in training loop, epoch 4, step 413, the loss is 228107.71875\n",
            "in training loop, epoch 4, step 414, the loss is 144357.453125\n",
            "in training loop, epoch 4, step 415, the loss is 160165.40625\n",
            "in training loop, epoch 4, step 416, the loss is 186080.515625\n",
            "in training loop, epoch 4, step 417, the loss is 155891.953125\n",
            "in training loop, epoch 4, step 418, the loss is 138114.359375\n",
            "in training loop, epoch 4, step 419, the loss is 214468.140625\n",
            "in training loop, epoch 4, step 420, the loss is 176889.234375\n",
            "in training loop, epoch 4, step 421, the loss is 301816.375\n",
            "in training loop, epoch 4, step 422, the loss is 237968.953125\n",
            "in training loop, epoch 4, step 423, the loss is 246589.8125\n",
            "in training loop, epoch 4, step 424, the loss is 211861.359375\n",
            "in training loop, epoch 4, step 425, the loss is 146184.03125\n",
            "in training loop, epoch 4, step 426, the loss is 166294.828125\n",
            "in training loop, epoch 4, step 427, the loss is 163763.546875\n",
            "in training loop, epoch 4, step 428, the loss is 241516.78125\n",
            "in training loop, epoch 4, step 429, the loss is 210038.40625\n",
            "in training loop, epoch 4, step 430, the loss is 126734.796875\n",
            "in training loop, epoch 4, step 431, the loss is 246547.09375\n",
            "in training loop, epoch 4, step 432, the loss is 281909.96875\n",
            "in training loop, epoch 4, step 433, the loss is 149402.96875\n",
            "in training loop, epoch 4, step 434, the loss is 95416.8984375\n",
            "in training loop, epoch 4, step 435, the loss is 250202.890625\n",
            "in training loop, epoch 4, step 436, the loss is 385346.1875\n",
            "in training loop, epoch 4, step 437, the loss is 177525.40625\n",
            "in training loop, epoch 4, step 438, the loss is 244793.34375\n",
            "in training loop, epoch 4, step 439, the loss is 329265.65625\n",
            "in training loop, epoch 4, step 440, the loss is 175339.03125\n",
            "in training loop, epoch 4, step 441, the loss is 154120.625\n",
            "in training loop, epoch 4, step 442, the loss is 259659.9375\n",
            "in training loop, epoch 4, step 443, the loss is 151190.125\n",
            "in training loop, epoch 4, step 444, the loss is 158219.46875\n",
            "in training loop, epoch 4, step 445, the loss is 243272.828125\n",
            "in training loop, epoch 4, step 446, the loss is 255018.0\n",
            "in training loop, epoch 4, step 447, the loss is 205612.109375\n",
            "in training loop, epoch 4, step 448, the loss is 168817.65625\n",
            "in training loop, epoch 4, step 449, the loss is 179638.21875\n",
            "in training loop, epoch 4, step 450, the loss is 101442.34375\n",
            "in training loop, epoch 4, step 451, the loss is 182337.28125\n",
            "in training loop, epoch 4, step 452, the loss is 181328.359375\n",
            "in training loop, epoch 4, step 453, the loss is 317782.65625\n",
            "in training loop, epoch 4, step 454, the loss is 171409.96875\n",
            "in training loop, epoch 4, step 455, the loss is 197354.875\n",
            "in training loop, epoch 4, step 456, the loss is 225139.8125\n",
            "in training loop, epoch 4, step 457, the loss is 172195.0\n",
            "in training loop, epoch 4, step 458, the loss is 183576.921875\n",
            "in training loop, epoch 4, step 459, the loss is 232334.734375\n",
            "in training loop, epoch 4, step 460, the loss is 200784.984375\n",
            "in training loop, epoch 4, step 461, the loss is 94369.7734375\n",
            "in training loop, epoch 4, step 462, the loss is 195278.71875\n",
            "in training loop, epoch 4, step 463, the loss is 195783.921875\n",
            "in training loop, epoch 4, step 464, the loss is 180056.28125\n",
            "in training loop, epoch 4, step 465, the loss is 163937.625\n",
            "in training loop, epoch 4, step 466, the loss is 252073.34375\n",
            "in training loop, epoch 4, step 467, the loss is 154300.921875\n",
            "in training loop, epoch 4, step 468, the loss is 157768.734375\n",
            "in training loop, epoch 4, step 469, the loss is 277097.40625\n",
            "in training loop, epoch 4, step 470, the loss is 146886.28125\n",
            "in training loop, epoch 4, step 471, the loss is 234835.21875\n",
            "in training loop, epoch 4, step 472, the loss is 198643.53125\n",
            "in training loop, epoch 4, step 473, the loss is 255800.21875\n",
            "in training loop, epoch 4, step 474, the loss is 165905.65625\n",
            "in training loop, epoch 4, step 475, the loss is 214242.1875\n",
            "in training loop, epoch 4, step 476, the loss is 145285.15625\n",
            "in training loop, epoch 4, step 477, the loss is 198482.1875\n",
            "in training loop, epoch 4, step 478, the loss is 290543.96875\n",
            "in training loop, epoch 4, step 479, the loss is 134886.75\n",
            "in training loop, epoch 4, step 480, the loss is 166055.90625\n",
            "in training loop, epoch 4, step 481, the loss is 292822.40625\n",
            "in training loop, epoch 4, step 482, the loss is 248472.140625\n",
            "in training loop, epoch 4, step 483, the loss is 212945.390625\n",
            "in training loop, epoch 4, step 484, the loss is 191607.03125\n",
            "in training loop, epoch 4, step 485, the loss is 166334.875\n",
            "in training loop, epoch 4, step 486, the loss is 161533.578125\n",
            "in training loop, epoch 4, step 487, the loss is 184945.671875\n",
            "in training loop, epoch 4, step 488, the loss is 211134.609375\n",
            "in training loop, epoch 4, step 489, the loss is 162887.96875\n",
            "in training loop, epoch 4, step 490, the loss is 261392.8125\n",
            "in training loop, epoch 4, step 491, the loss is 245277.21875\n",
            "in training loop, epoch 4, step 492, the loss is 209236.46875\n",
            "in training loop, epoch 4, step 493, the loss is 226688.75\n",
            "in training loop, epoch 4, step 494, the loss is 106570.65625\n",
            "in training loop, epoch 4, step 495, the loss is 228993.75\n",
            "in training loop, epoch 4, step 496, the loss is 214883.78125\n",
            "in training loop, epoch 4, step 497, the loss is 257130.03125\n",
            "in training loop, epoch 4, step 498, the loss is 243240.203125\n",
            "in training loop, epoch 4, step 499, the loss is 168692.515625\n",
            "in training loop, epoch 4, step 500, the loss is 283737.84375\n",
            "in training loop, epoch 4, step 501, the loss is 227060.46875\n",
            "in training loop, epoch 4, step 502, the loss is 233932.84375\n",
            "in training loop, epoch 4, step 503, the loss is 184839.515625\n",
            "in training loop, epoch 4, step 504, the loss is 185618.0\n",
            "in training loop, epoch 4, step 505, the loss is 135767.5\n",
            "in training loop, epoch 4, step 506, the loss is 251646.34375\n",
            "in training loop, epoch 4, step 507, the loss is 149623.703125\n",
            "in training loop, epoch 4, step 508, the loss is 341543.8125\n",
            "in training loop, epoch 4, step 509, the loss is 233695.78125\n",
            "in training loop, epoch 4, step 510, the loss is 180485.5625\n",
            "in training loop, epoch 4, step 511, the loss is 203887.1875\n",
            "in training loop, epoch 4, step 512, the loss is 285976.65625\n",
            "in training loop, epoch 4, step 513, the loss is 237518.65625\n",
            "in training loop, epoch 4, step 514, the loss is 156731.0625\n",
            "in training loop, epoch 4, step 515, the loss is 217117.03125\n",
            "in training loop, epoch 4, step 516, the loss is 211350.5\n",
            "in training loop, epoch 4, step 517, the loss is 217916.40625\n",
            "in training loop, epoch 4, step 518, the loss is 177084.921875\n",
            "in training loop, epoch 4, step 519, the loss is 135389.5\n",
            "in training loop, epoch 4, step 520, the loss is 164908.4375\n",
            "in training loop, epoch 4, step 521, the loss is 222776.25\n",
            "in training loop, epoch 4, step 522, the loss is 114788.28125\n",
            "in training loop, epoch 4, step 523, the loss is 152755.078125\n",
            "in training loop, epoch 4, step 524, the loss is 268812.78125\n",
            "in training loop, epoch 4, step 525, the loss is 194076.9375\n",
            "in training loop, epoch 4, step 526, the loss is 176121.03125\n",
            "in training loop, epoch 4, step 527, the loss is 166827.71875\n",
            "in training loop, epoch 4, step 528, the loss is 192860.53125\n",
            "in training loop, epoch 4, step 529, the loss is 195299.21875\n",
            "in training loop, epoch 4, step 530, the loss is 271946.46875\n",
            "in training loop, epoch 4, step 531, the loss is 132723.546875\n",
            "in training loop, epoch 4, step 532, the loss is 123515.640625\n",
            "in training loop, epoch 4, step 533, the loss is 118355.7109375\n",
            "in training loop, epoch 4, step 534, the loss is 266016.96875\n",
            "in training loop, epoch 4, step 535, the loss is 233897.703125\n",
            "in training loop, epoch 4, step 536, the loss is 201666.3125\n",
            "in training loop, epoch 4, step 537, the loss is 199701.75\n",
            "in training loop, epoch 4, step 538, the loss is 212792.078125\n",
            "in training loop, epoch 4, step 539, the loss is 199269.40625\n",
            "in training loop, epoch 4, step 540, the loss is 213600.765625\n",
            "in training loop, epoch 4, step 541, the loss is 101937.2578125\n",
            "in training loop, epoch 4, step 542, the loss is 170444.78125\n",
            "in training loop, epoch 4, step 543, the loss is 238915.078125\n",
            "in training loop, epoch 4, step 544, the loss is 192723.8125\n",
            "in training loop, epoch 4, step 545, the loss is 224881.234375\n",
            "in training loop, epoch 4, step 546, the loss is 207858.3125\n",
            "in training loop, epoch 4, step 547, the loss is 210743.28125\n",
            "in training loop, epoch 4, step 548, the loss is 114968.28125\n",
            "in training loop, epoch 4, step 549, the loss is 194619.921875\n",
            "in training loop, epoch 4, step 550, the loss is 236707.875\n",
            "in training loop, epoch 4, step 551, the loss is 260167.75\n",
            "in training loop, epoch 4, step 552, the loss is 314123.15625\n",
            "in training loop, epoch 4, step 553, the loss is 180987.65625\n",
            "in training loop, epoch 4, step 554, the loss is 156301.140625\n",
            "in training loop, epoch 4, step 555, the loss is 175277.625\n",
            "in training loop, epoch 4, step 556, the loss is 177200.703125\n",
            "in training loop, epoch 4, step 557, the loss is 179494.984375\n",
            "in training loop, epoch 4, step 558, the loss is 236242.03125\n",
            "in training loop, epoch 4, step 559, the loss is 204310.453125\n",
            "in training loop, epoch 4, step 560, the loss is 263682.0625\n",
            "in training loop, epoch 4, step 561, the loss is 227274.25\n",
            "in training loop, epoch 4, step 562, the loss is 207953.15625\n",
            "in training loop, epoch 4, step 563, the loss is 270567.15625\n",
            "in training loop, epoch 4, step 564, the loss is 189353.703125\n",
            "in training loop, epoch 4, step 565, the loss is 264629.71875\n",
            "in training loop, epoch 4, step 566, the loss is 205328.484375\n",
            "in training loop, epoch 4, step 567, the loss is 208815.46875\n",
            "in training loop, epoch 4, step 568, the loss is 312391.5625\n",
            "in training loop, epoch 4, step 569, the loss is 257945.828125\n",
            "in training loop, epoch 4, step 570, the loss is 302397.3125\n",
            "in training loop, epoch 4, step 571, the loss is 218511.734375\n",
            "in training loop, epoch 4, step 572, the loss is 219437.234375\n",
            "in training loop, epoch 4, step 573, the loss is 199648.0625\n",
            "in training loop, epoch 4, step 574, the loss is 183820.921875\n",
            "in training loop, epoch 4, step 575, the loss is 193649.03125\n",
            "in training loop, epoch 4, step 576, the loss is 179821.359375\n",
            "in training loop, epoch 4, step 577, the loss is 188992.390625\n",
            "in training loop, epoch 4, step 578, the loss is 243860.265625\n",
            "in training loop, epoch 4, step 579, the loss is 222505.140625\n",
            "in training loop, epoch 4, step 580, the loss is 138278.4375\n",
            "in training loop, epoch 4, step 581, the loss is 143744.515625\n",
            "in training loop, epoch 4, step 582, the loss is 144821.4375\n",
            "in training loop, epoch 4, step 583, the loss is 203694.265625\n",
            "in training loop, epoch 4, step 584, the loss is 165726.15625\n",
            "in training loop, epoch 4, step 585, the loss is 204172.90625\n",
            "in training loop, epoch 4, step 586, the loss is 210883.640625\n",
            "in training loop, epoch 4, step 587, the loss is 206686.71875\n",
            "in training loop, epoch 4, step 588, the loss is 198633.53125\n",
            "in training loop, epoch 4, step 589, the loss is 303524.125\n",
            "in training loop, epoch 4, step 590, the loss is 255333.734375\n",
            "in training loop, epoch 4, step 591, the loss is 232551.5625\n",
            "in training loop, epoch 4, step 592, the loss is 129854.3671875\n",
            "in training loop, epoch 4, step 593, the loss is 157624.34375\n",
            "in training loop, epoch 4, step 594, the loss is 150100.359375\n",
            "in training loop, epoch 4, step 595, the loss is 153863.4375\n",
            "in training loop, epoch 4, step 596, the loss is 222755.65625\n",
            "in training loop, epoch 4, step 597, the loss is 183292.234375\n",
            "in training loop, epoch 4, step 598, the loss is 149425.6875\n",
            "in training loop, epoch 4, step 599, the loss is 162005.015625\n",
            "in training loop, epoch 4, step 600, the loss is 237790.703125\n",
            "in training loop, epoch 4, step 601, the loss is 150306.328125\n",
            "in training loop, epoch 4, step 602, the loss is 179523.53125\n",
            "in training loop, epoch 4, step 603, the loss is 145965.921875\n",
            "in training loop, epoch 4, step 604, the loss is 192062.0625\n",
            "in training loop, epoch 4, step 605, the loss is 223841.03125\n",
            "in training loop, epoch 4, step 606, the loss is 166310.90625\n",
            "in training loop, epoch 4, step 607, the loss is 198343.828125\n",
            "in training loop, epoch 4, step 608, the loss is 200574.625\n",
            "in training loop, epoch 4, step 609, the loss is 139455.0\n",
            "in training loop, epoch 4, step 610, the loss is 183498.703125\n",
            "in training loop, epoch 4, step 611, the loss is 202559.125\n",
            "in training loop, epoch 4, step 612, the loss is 246936.859375\n",
            "in training loop, epoch 4, step 613, the loss is 120971.5625\n",
            "in training loop, epoch 4, step 614, the loss is 259995.46875\n",
            "in training loop, epoch 4, step 615, the loss is 185584.484375\n",
            "in training loop, epoch 4, step 616, the loss is 124319.625\n",
            "in training loop, epoch 4, step 617, the loss is 186947.546875\n",
            "in training loop, epoch 4, step 618, the loss is 178984.75\n",
            "in training loop, epoch 4, step 619, the loss is 189313.328125\n",
            "in training loop, epoch 4, step 620, the loss is 160070.53125\n",
            "in training loop, epoch 4, step 621, the loss is 189138.1875\n",
            "in training loop, epoch 4, step 622, the loss is 159265.609375\n",
            "in training loop, epoch 4, step 623, the loss is 216954.03125\n",
            "in training loop, epoch 4, step 624, the loss is 181377.734375\n",
            "in training loop, epoch 4, step 625, the loss is 315850.5\n",
            "in training loop, epoch 4, step 626, the loss is 123193.8671875\n",
            "in training loop, epoch 4, step 627, the loss is 154363.03125\n",
            "in training loop, epoch 4, step 628, the loss is 268171.96875\n",
            "in training loop, epoch 4, step 629, the loss is 208416.75\n",
            "in training loop, epoch 4, step 630, the loss is 278644.125\n",
            "in training loop, epoch 4, step 631, the loss is 159468.984375\n",
            "in training loop, epoch 4, step 632, the loss is 120637.8671875\n",
            "in training loop, epoch 4, step 633, the loss is 168809.875\n",
            "in training loop, epoch 4, step 634, the loss is 182376.125\n",
            "in training loop, epoch 4, step 635, the loss is 182431.0\n",
            "in training loop, epoch 4, step 636, the loss is 169759.40625\n",
            "in training loop, epoch 4, step 637, the loss is 156913.09375\n",
            "in training loop, epoch 4, step 638, the loss is 227706.78125\n",
            "in training loop, epoch 4, step 639, the loss is 132845.71875\n",
            "in training loop, epoch 4, step 640, the loss is 175006.515625\n",
            "in training loop, epoch 4, step 641, the loss is 200875.65625\n",
            "in training loop, epoch 4, step 642, the loss is 134460.265625\n",
            "in training loop, epoch 4, step 643, the loss is 248006.765625\n",
            "in training loop, epoch 4, step 644, the loss is 202223.671875\n",
            "in training loop, epoch 4, step 645, the loss is 216565.9375\n",
            "in training loop, epoch 4, step 646, the loss is 104297.015625\n",
            "in training loop, epoch 4, step 647, the loss is 335469.3125\n",
            "in training loop, epoch 4, step 648, the loss is 216957.3125\n",
            "in training loop, epoch 4, step 649, the loss is 120170.890625\n",
            "in training loop, epoch 4, step 650, the loss is 176650.375\n",
            "in training loop, epoch 4, step 651, the loss is 223816.4375\n",
            "in training loop, epoch 4, step 652, the loss is 129729.328125\n",
            "in training loop, epoch 4, step 653, the loss is 115196.25\n",
            "in training loop, epoch 4, step 654, the loss is 141710.890625\n",
            "in training loop, epoch 4, step 655, the loss is 198400.3125\n",
            "in training loop, epoch 4, step 656, the loss is 159023.21875\n",
            "in training loop, epoch 4, step 657, the loss is 81162.1015625\n",
            "in training loop, epoch 4, step 658, the loss is 218787.390625\n",
            "in training loop, epoch 4, step 659, the loss is 200485.234375\n",
            "in training loop, epoch 4, step 660, the loss is 187827.421875\n",
            "in training loop, epoch 4, step 661, the loss is 175800.015625\n",
            "in training loop, epoch 4, step 662, the loss is 171953.890625\n",
            "in training loop, epoch 4, step 663, the loss is 124275.34375\n",
            "in training loop, epoch 4, step 664, the loss is 212019.28125\n",
            "in training loop, epoch 4, step 665, the loss is 174604.953125\n",
            "in training loop, epoch 4, step 666, the loss is 166926.03125\n",
            "in training loop, epoch 4, step 667, the loss is 86125.203125\n",
            "in training loop, epoch 4, step 668, the loss is 146972.421875\n",
            "in training loop, epoch 4, step 669, the loss is 153217.21875\n",
            "in training loop, epoch 4, step 670, the loss is 136025.375\n",
            "in training loop, epoch 4, step 671, the loss is 137427.40625\n",
            "in training loop, epoch 4, step 672, the loss is 75974.703125\n",
            "in training loop, epoch 4, step 673, the loss is 241061.171875\n",
            "in training loop, epoch 4, step 674, the loss is 149679.71875\n",
            "in training loop, epoch 4, step 675, the loss is 219767.5625\n",
            "in training loop, epoch 4, step 676, the loss is 227705.078125\n",
            "in training loop, epoch 4, step 677, the loss is 159423.015625\n",
            "in training loop, epoch 4, step 678, the loss is 126983.1328125\n",
            "in training loop, epoch 4, step 679, the loss is 229566.53125\n",
            "in training loop, epoch 4, step 680, the loss is 231167.078125\n",
            "in training loop, epoch 4, step 681, the loss is 99033.609375\n",
            "in training loop, epoch 4, step 682, the loss is 313600.25\n",
            "in training loop, epoch 4, step 683, the loss is 147138.09375\n",
            "in training loop, epoch 4, step 684, the loss is 272044.0625\n",
            "in training loop, epoch 4, step 685, the loss is 123014.5078125\n",
            "in training loop, epoch 4, step 686, the loss is 175638.3125\n",
            "in training loop, epoch 4, step 687, the loss is 198280.8125\n",
            "in training loop, epoch 4, step 688, the loss is 183754.640625\n",
            "in training loop, epoch 4, step 689, the loss is 172106.75\n",
            "in training loop, epoch 4, step 690, the loss is 266836.46875\n",
            "in training loop, epoch 4, step 691, the loss is 120095.4140625\n",
            "in training loop, epoch 4, step 692, the loss is 118010.0859375\n",
            "in training loop, epoch 4, step 693, the loss is 193566.25\n",
            "in training loop, epoch 4, step 694, the loss is 282681.90625\n",
            "in training loop, epoch 4, step 695, the loss is 205462.0625\n",
            "in training loop, epoch 4, step 696, the loss is 190863.8125\n",
            "in training loop, epoch 4, step 697, the loss is 219830.390625\n",
            "in training loop, epoch 4, step 698, the loss is 270934.78125\n",
            "in training loop, epoch 4, step 699, the loss is 226230.015625\n",
            "in training loop, epoch 4, step 700, the loss is 238690.859375\n",
            "in training loop, epoch 4, step 701, the loss is 177781.53125\n",
            "in training loop, epoch 4, step 702, the loss is 156423.0625\n",
            "in training loop, epoch 4, step 703, the loss is 289170.09375\n",
            "in training loop, epoch 4, step 704, the loss is 307410.5625\n",
            "in training loop, epoch 4, step 705, the loss is 226384.71875\n",
            "in training loop, epoch 4, step 706, the loss is 192906.546875\n",
            "in training loop, epoch 4, step 707, the loss is 143483.28125\n",
            "in training loop, epoch 4, step 708, the loss is 285826.8125\n",
            "in training loop, epoch 4, step 709, the loss is 357887.9375\n",
            "in training loop, epoch 4, step 710, the loss is 255495.40625\n",
            "in training loop, epoch 4, step 711, the loss is 245801.921875\n",
            "in training loop, epoch 4, step 712, the loss is 306663.75\n",
            "in training loop, epoch 4, step 713, the loss is 269560.34375\n",
            "in training loop, epoch 4, step 714, the loss is 302996.8125\n",
            "in training loop, epoch 4, step 715, the loss is 236388.09375\n",
            "in training loop, epoch 4, step 716, the loss is 331082.34375\n",
            "in training loop, epoch 4, step 717, the loss is 291832.1875\n",
            "in training loop, epoch 4, step 718, the loss is 181367.03125\n",
            "in training loop, epoch 4, step 719, the loss is 130751.2890625\n",
            "in training loop, epoch 4, step 720, the loss is 206759.453125\n",
            "in training loop, epoch 4, step 721, the loss is 231558.890625\n",
            "in training loop, epoch 4, step 722, the loss is 235578.90625\n",
            "in training loop, epoch 4, step 723, the loss is 205221.203125\n",
            "in training loop, epoch 4, step 724, the loss is 230538.125\n",
            "in training loop, epoch 4, step 725, the loss is 211675.375\n",
            "in training loop, epoch 4, step 726, the loss is 279288.53125\n",
            "in training loop, epoch 4, step 727, the loss is 214395.65625\n",
            "in training loop, epoch 4, step 728, the loss is 147109.890625\n",
            "in training loop, epoch 4, step 729, the loss is 235545.1875\n",
            "in training loop, epoch 4, step 730, the loss is 219851.375\n",
            "in training loop, epoch 4, step 731, the loss is 300962.375\n",
            "in training loop, epoch 4, step 732, the loss is 304940.78125\n",
            "in training loop, epoch 4, step 733, the loss is 223475.34375\n",
            "in training loop, epoch 4, step 734, the loss is 200492.984375\n",
            "in training loop, epoch 4, step 735, the loss is 138027.640625\n",
            "in training loop, epoch 4, step 736, the loss is 287616.0\n",
            "in training loop, epoch 4, step 737, the loss is 306216.375\n",
            "in training loop, epoch 4, step 738, the loss is 149191.96875\n",
            "in training loop, epoch 4, step 739, the loss is 143062.96875\n",
            "in training loop, epoch 4, step 740, the loss is 385820.25\n",
            "in training loop, epoch 4, step 741, the loss is 230356.375\n",
            "in training loop, epoch 4, step 742, the loss is 294348.15625\n",
            "in training loop, epoch 4, step 743, the loss is 224964.71875\n",
            "in training loop, epoch 4, step 744, the loss is 253635.28125\n",
            "in training loop, epoch 4, step 745, the loss is 225049.109375\n",
            "in training loop, epoch 4, step 746, the loss is 646091.75\n",
            "in training loop, epoch 4, step 747, the loss is 215749.171875\n",
            "in training loop, epoch 4, step 748, the loss is 260209.453125\n",
            "in training loop, epoch 4, step 749, the loss is 206179.71875\n",
            "in training loop, epoch 4, step 750, the loss is 204880.53125\n",
            "in training loop, epoch 4, step 751, the loss is 350821.8125\n",
            "in training loop, epoch 4, step 752, the loss is 251172.1875\n",
            "in training loop, epoch 4, step 753, the loss is 184636.1875\n",
            "in training loop, epoch 4, step 754, the loss is 153149.84375\n",
            "in training loop, epoch 4, step 755, the loss is 205094.9375\n",
            "in training loop, epoch 4, step 756, the loss is 242990.46875\n",
            "in training loop, epoch 4, step 757, the loss is 191916.515625\n",
            "in training loop, epoch 4, step 758, the loss is 184019.46875\n",
            "in training loop, epoch 4, step 759, the loss is 225340.984375\n",
            "in training loop, epoch 4, step 760, the loss is 205840.65625\n",
            "in training loop, epoch 4, step 761, the loss is 292607.3125\n",
            "in training loop, epoch 4, step 762, the loss is 170021.5625\n",
            "in training loop, epoch 4, step 763, the loss is 303767.4375\n",
            "in training loop, epoch 4, step 764, the loss is 248852.609375\n",
            "in training loop, epoch 4, step 765, the loss is 187091.453125\n",
            "in training loop, epoch 4, step 766, the loss is 153252.890625\n",
            "in training loop, epoch 4, step 767, the loss is 555822.125\n",
            "in training loop, epoch 4, step 768, the loss is 355997.125\n",
            "in training loop, epoch 4, step 769, the loss is 269642.25\n",
            "in training loop, epoch 4, step 770, the loss is 205351.46875\n",
            "in training loop, epoch 4, step 771, the loss is 248672.34375\n",
            "in training loop, epoch 4, step 772, the loss is 177236.96875\n",
            "in training loop, epoch 4, step 773, the loss is 109041.3203125\n",
            "in training loop, epoch 4, step 774, the loss is 164111.6875\n",
            "in training loop, epoch 4, step 775, the loss is 323834.71875\n",
            "in training loop, epoch 4, step 776, the loss is 161441.84375\n",
            "in training loop, epoch 4, step 777, the loss is 275046.40625\n",
            "in training loop, epoch 4, step 778, the loss is 194959.65625\n",
            "in training loop, epoch 4, step 779, the loss is 901290.75\n",
            "in training loop, epoch 4, step 780, the loss is 249091.15625\n",
            "in training loop, epoch 4, step 781, the loss is 184547.796875\n",
            "in training loop, epoch 4, step 782, the loss is 222435.734375\n",
            "in training loop, epoch 4, step 783, the loss is 521164.25\n",
            "in training loop, epoch 4, step 784, the loss is 245894.171875\n",
            "in training loop, epoch 4, step 785, the loss is 292864.375\n",
            "in training loop, epoch 4, step 786, the loss is 404872.3125\n",
            "in training loop, epoch 4, step 787, the loss is 189587.75\n",
            "in training loop, epoch 4, step 788, the loss is 222496.96875\n",
            "in training loop, epoch 4, step 789, the loss is 324476.71875\n",
            "in training loop, epoch 4, step 790, the loss is 294016.875\n",
            "in training loop, epoch 4, step 791, the loss is 281120.9375\n",
            "in training loop, epoch 4, step 792, the loss is 337453.5625\n",
            "in training loop, epoch 4, step 793, the loss is 283751.40625\n",
            "in training loop, epoch 4, step 794, the loss is 229786.59375\n",
            "in training loop, epoch 4, step 795, the loss is 371931.90625\n",
            "in training loop, epoch 4, step 796, the loss is 379457.4375\n",
            "in training loop, epoch 4, step 797, the loss is 271151.96875\n",
            "in training loop, epoch 4, step 798, the loss is 219587.140625\n",
            "in training loop, epoch 4, step 799, the loss is 346452.0625\n",
            "in training loop, epoch 4, step 800, the loss is 229996.765625\n",
            "in training loop, epoch 4, step 801, the loss is 362915.09375\n",
            "in training loop, epoch 4, step 802, the loss is 314102.71875\n",
            "in training loop, epoch 4, step 803, the loss is 294020.59375\n",
            "in training loop, epoch 4, step 804, the loss is 138816.46875\n",
            "in training loop, epoch 4, step 805, the loss is 174934.21875\n",
            "in training loop, epoch 4, step 806, the loss is 330648.5\n",
            "in training loop, epoch 4, step 807, the loss is 275082.0\n",
            "in training loop, epoch 4, step 808, the loss is 208268.53125\n",
            "in training loop, epoch 4, step 809, the loss is 180200.6875\n",
            "in training loop, epoch 4, step 810, the loss is 228455.203125\n",
            "in training loop, epoch 4, step 811, the loss is 269504.1875\n",
            "in training loop, epoch 4, step 812, the loss is 366752.5\n",
            "in training loop, epoch 4, step 813, the loss is 280753.0\n",
            "in training loop, epoch 4, step 814, the loss is 235492.875\n",
            "in training loop, epoch 4, step 815, the loss is 387670.28125\n",
            "in training loop, epoch 4, step 816, the loss is 212860.21875\n",
            "in training loop, epoch 4, step 817, the loss is 179199.234375\n",
            "in training loop, epoch 4, step 818, the loss is 254690.125\n",
            "in training loop, epoch 4, step 819, the loss is 162035.1875\n",
            "in training loop, epoch 4, step 820, the loss is 302399.0625\n",
            "in training loop, epoch 4, step 821, the loss is 200485.765625\n",
            "in training loop, epoch 4, step 822, the loss is 245633.703125\n",
            "in training loop, epoch 4, step 823, the loss is 300334.34375\n",
            "in training loop, epoch 4, step 824, the loss is 184120.75\n",
            "in training loop, epoch 4, step 825, the loss is 230553.921875\n",
            "in training loop, epoch 4, step 826, the loss is 132598.96875\n",
            "in training loop, epoch 4, step 827, the loss is 188160.109375\n",
            "in training loop, epoch 4, step 828, the loss is 120688.0703125\n",
            "in training loop, epoch 4, step 829, the loss is 174327.6875\n",
            "in training loop, epoch 4, step 830, the loss is 250106.84375\n",
            "in training loop, epoch 4, step 831, the loss is 160014.59375\n",
            "in training loop, epoch 4, step 832, the loss is 277431.03125\n",
            "in training loop, epoch 4, step 833, the loss is 184407.640625\n",
            "in training loop, epoch 4, step 834, the loss is 207475.53125\n",
            "in training loop, epoch 4, step 835, the loss is 215148.15625\n",
            "in training loop, epoch 4, step 836, the loss is 147725.78125\n",
            "in training loop, epoch 4, step 837, the loss is 229693.4375\n",
            "in training loop, epoch 4, step 838, the loss is 225972.046875\n",
            "in training loop, epoch 4, step 839, the loss is 201125.34375\n",
            "in training loop, epoch 4, step 840, the loss is 209864.5\n",
            "in training loop, epoch 4, step 841, the loss is 320135.8125\n",
            "in training loop, epoch 4, step 842, the loss is 258710.921875\n",
            "in training loop, epoch 4, step 843, the loss is 168461.078125\n",
            "in training loop, epoch 4, step 844, the loss is 171270.625\n",
            "in training loop, epoch 4, step 845, the loss is 207382.171875\n",
            "in training loop, epoch 4, step 846, the loss is 208458.765625\n",
            "in training loop, epoch 4, step 847, the loss is 246392.75\n",
            "in training loop, epoch 4, step 848, the loss is 147093.59375\n",
            "in training loop, epoch 4, step 849, the loss is 133875.203125\n",
            "in training loop, epoch 4, step 850, the loss is 262544.40625\n",
            "in training loop, epoch 4, step 851, the loss is 156908.515625\n",
            "in training loop, epoch 4, step 852, the loss is 292434.875\n",
            "in training loop, epoch 4, step 853, the loss is 205633.875\n",
            "in training loop, epoch 4, step 854, the loss is 295345.4375\n",
            "in training loop, epoch 4, step 855, the loss is 147815.125\n",
            "in training loop, epoch 4, step 856, the loss is 223297.1875\n",
            "in training loop, epoch 4, step 857, the loss is 407747.03125\n",
            "in training loop, epoch 4, step 858, the loss is 131594.421875\n",
            "in training loop, epoch 4, step 859, the loss is 165991.34375\n",
            "in training loop, epoch 4, step 860, the loss is 284034.1875\n",
            "in training loop, epoch 4, step 861, the loss is 89836.5\n",
            "in training loop, epoch 4, step 862, the loss is 292647.5625\n",
            "in training loop, epoch 4, step 863, the loss is 174352.59375\n",
            "in training loop, epoch 4, step 864, the loss is 187500.859375\n",
            "in training loop, epoch 4, step 865, the loss is 139088.25\n",
            "in training loop, epoch 4, step 866, the loss is 120628.890625\n",
            "in training loop, epoch 4, step 867, the loss is 213591.953125\n",
            "in training loop, epoch 4, step 868, the loss is 224120.125\n",
            "in training loop, epoch 4, step 869, the loss is 199732.703125\n",
            "in training loop, epoch 4, step 870, the loss is 115729.65625\n",
            "in training loop, epoch 4, step 871, the loss is 251198.578125\n",
            "in training loop, epoch 4, step 872, the loss is 178719.1875\n",
            "in training loop, epoch 4, step 873, the loss is 156055.84375\n",
            "in training loop, epoch 4, step 874, the loss is 230405.75\n",
            "in training loop, epoch 4, step 875, the loss is 250499.015625\n",
            "in training loop, epoch 4, step 876, the loss is 261852.78125\n",
            "in training loop, epoch 4, step 877, the loss is 241777.296875\n",
            "in training loop, epoch 4, step 878, the loss is 214244.953125\n",
            "in training loop, epoch 4, step 879, the loss is 210607.859375\n",
            "in training loop, epoch 4, step 880, the loss is 142993.734375\n",
            "in training loop, epoch 4, step 881, the loss is 280675.5625\n",
            "in training loop, epoch 4, step 882, the loss is 205871.8125\n",
            "in training loop, epoch 4, step 883, the loss is 120314.46875\n",
            "in training loop, epoch 4, step 884, the loss is 204835.28125\n",
            "in training loop, epoch 4, step 885, the loss is 157781.0\n",
            "in training loop, epoch 4, step 886, the loss is 171204.28125\n",
            "in training loop, epoch 4, step 887, the loss is 215162.0625\n",
            "in training loop, epoch 4, step 888, the loss is 162271.28125\n",
            "in training loop, epoch 4, step 889, the loss is 177353.984375\n",
            "in training loop, epoch 4, step 890, the loss is 212272.953125\n",
            "in training loop, epoch 4, step 891, the loss is 131998.9375\n",
            "in training loop, epoch 4, step 892, the loss is 169423.9375\n",
            "in training loop, epoch 4, step 893, the loss is 103341.8671875\n",
            "in training loop, epoch 4, step 894, the loss is 222071.890625\n",
            "in training loop, epoch 4, step 895, the loss is 220551.671875\n",
            "in training loop, epoch 4, step 896, the loss is 261105.8125\n",
            "in training loop, epoch 4, step 897, the loss is 121996.9375\n",
            "in training loop, epoch 4, step 898, the loss is 135864.6875\n",
            "in training loop, epoch 4, step 899, the loss is 184664.921875\n",
            "in training loop, epoch 4, step 900, the loss is 322839.0\n",
            "in training loop, epoch 4, step 901, the loss is 293069.875\n",
            "in training loop, epoch 4, step 902, the loss is 172067.0625\n",
            "in training loop, epoch 4, step 903, the loss is 102083.921875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "k-fold 0:: Epoch 4: train loss 207185.98785778484 val loss 295709.99767945544\n",
            "train_loss_mean_list [302594.02656596 261561.08764864 236744.10011062 207185.98785778]\n",
            "test_loss_mean_list [320749.60852413 299039.18456064 313402.79045483 295709.99767946]\n",
            "val_loss_mean_list [316156.98932395 301966.77217692 313130.63626743 290709.44902422]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "FOLD 1\n",
            "--------------------------------\n",
            "length of of train_loader is 904 & length of traindataset is 2008\n",
            "length of of test_loader is 101\n",
            "length of of val_loader is 1004\n",
            "in training loop, epoch 1, step 0, the loss is 105958.5390625\n",
            "in training loop, epoch 1, step 1, the loss is 259161.671875\n",
            "in training loop, epoch 1, step 2, the loss is 181870.125\n",
            "in training loop, epoch 1, step 3, the loss is 107698.546875\n",
            "in training loop, epoch 1, step 4, the loss is 202598.265625\n",
            "in training loop, epoch 1, step 5, the loss is 313395.625\n",
            "in training loop, epoch 1, step 6, the loss is 182223.3125\n",
            "in training loop, epoch 1, step 7, the loss is 318859.3125\n",
            "in training loop, epoch 1, step 8, the loss is 280609.90625\n",
            "in training loop, epoch 1, step 9, the loss is 152860.125\n",
            "in training loop, epoch 1, step 10, the loss is 315656.90625\n",
            "in training loop, epoch 1, step 11, the loss is 177050.609375\n",
            "in training loop, epoch 1, step 12, the loss is 158285.421875\n",
            "in training loop, epoch 1, step 13, the loss is 211777.75\n",
            "in training loop, epoch 1, step 14, the loss is 217936.09375\n",
            "in training loop, epoch 1, step 15, the loss is 191867.0625\n",
            "in training loop, epoch 1, step 16, the loss is 190011.5\n",
            "in training loop, epoch 1, step 17, the loss is 181342.0625\n",
            "in training loop, epoch 1, step 18, the loss is 164273.546875\n",
            "in training loop, epoch 1, step 19, the loss is 153097.890625\n",
            "in training loop, epoch 1, step 20, the loss is 211231.34375\n",
            "in training loop, epoch 1, step 21, the loss is 292637.0625\n",
            "in training loop, epoch 1, step 22, the loss is 219681.796875\n",
            "in training loop, epoch 1, step 23, the loss is 143297.828125\n",
            "in training loop, epoch 1, step 24, the loss is 273042.78125\n",
            "in training loop, epoch 1, step 25, the loss is 178169.515625\n",
            "in training loop, epoch 1, step 26, the loss is 197231.0\n",
            "in training loop, epoch 1, step 27, the loss is 184341.25\n",
            "in training loop, epoch 1, step 28, the loss is 137024.140625\n",
            "in training loop, epoch 1, step 29, the loss is 113307.3671875\n",
            "in training loop, epoch 1, step 30, the loss is 119014.7421875\n",
            "in training loop, epoch 1, step 31, the loss is 198882.953125\n",
            "in training loop, epoch 1, step 32, the loss is 170116.421875\n",
            "in training loop, epoch 1, step 33, the loss is 220713.015625\n",
            "in training loop, epoch 1, step 34, the loss is 157846.53125\n",
            "in training loop, epoch 1, step 35, the loss is 233049.28125\n",
            "in training loop, epoch 1, step 36, the loss is 184838.078125\n",
            "in training loop, epoch 1, step 37, the loss is 176977.265625\n",
            "in training loop, epoch 1, step 38, the loss is 116010.625\n",
            "in training loop, epoch 1, step 39, the loss is 123374.4375\n",
            "in training loop, epoch 1, step 40, the loss is 163942.03125\n",
            "in training loop, epoch 1, step 41, the loss is 186695.03125\n",
            "in training loop, epoch 1, step 42, the loss is 129041.5625\n",
            "in training loop, epoch 1, step 43, the loss is 171116.734375\n",
            "in training loop, epoch 1, step 44, the loss is 157374.828125\n",
            "in training loop, epoch 1, step 45, the loss is 156059.453125\n",
            "in training loop, epoch 1, step 46, the loss is 112062.96875\n",
            "in training loop, epoch 1, step 47, the loss is 170800.1875\n",
            "in training loop, epoch 1, step 48, the loss is 150619.109375\n",
            "in training loop, epoch 1, step 49, the loss is 136079.046875\n",
            "in training loop, epoch 1, step 50, the loss is 206805.421875\n",
            "in training loop, epoch 1, step 51, the loss is 213660.96875\n",
            "in training loop, epoch 1, step 52, the loss is 204077.734375\n",
            "in training loop, epoch 1, step 53, the loss is 170214.90625\n",
            "in training loop, epoch 1, step 54, the loss is 339290.78125\n",
            "in training loop, epoch 1, step 55, the loss is 186413.828125\n",
            "in training loop, epoch 1, step 56, the loss is 153460.40625\n",
            "in training loop, epoch 1, step 57, the loss is 181165.625\n",
            "in training loop, epoch 1, step 58, the loss is 148268.046875\n",
            "in training loop, epoch 1, step 59, the loss is 183737.234375\n",
            "in training loop, epoch 1, step 60, the loss is 268549.59375\n",
            "in training loop, epoch 1, step 61, the loss is 222342.03125\n",
            "in training loop, epoch 1, step 62, the loss is 135957.5\n",
            "in training loop, epoch 1, step 63, the loss is 151268.9375\n",
            "in training loop, epoch 1, step 64, the loss is 522269.03125\n",
            "in training loop, epoch 1, step 65, the loss is 173757.40625\n",
            "in training loop, epoch 1, step 66, the loss is 184103.75\n",
            "in training loop, epoch 1, step 67, the loss is 161160.875\n",
            "in training loop, epoch 1, step 68, the loss is 123740.5078125\n",
            "in training loop, epoch 1, step 69, the loss is 153369.015625\n",
            "in training loop, epoch 1, step 70, the loss is 260165.734375\n",
            "in training loop, epoch 1, step 71, the loss is 170376.359375\n",
            "in training loop, epoch 1, step 72, the loss is 247124.984375\n",
            "in training loop, epoch 1, step 73, the loss is 164212.21875\n",
            "in training loop, epoch 1, step 74, the loss is 212215.78125\n",
            "in training loop, epoch 1, step 75, the loss is 169403.46875\n",
            "in training loop, epoch 1, step 76, the loss is 145781.03125\n",
            "in training loop, epoch 1, step 77, the loss is 182554.78125\n",
            "in training loop, epoch 1, step 78, the loss is 174534.625\n",
            "in training loop, epoch 1, step 79, the loss is 168011.8125\n",
            "in training loop, epoch 1, step 80, the loss is 180145.953125\n",
            "in training loop, epoch 1, step 81, the loss is 135748.296875\n",
            "in training loop, epoch 1, step 82, the loss is 345765.28125\n",
            "in training loop, epoch 1, step 83, the loss is 161062.359375\n",
            "in training loop, epoch 1, step 84, the loss is 147532.40625\n",
            "in training loop, epoch 1, step 85, the loss is 168441.65625\n",
            "in training loop, epoch 1, step 86, the loss is 235332.671875\n",
            "in training loop, epoch 1, step 87, the loss is 122298.84375\n",
            "in training loop, epoch 1, step 88, the loss is 279703.09375\n",
            "in training loop, epoch 1, step 89, the loss is 200505.4375\n",
            "in training loop, epoch 1, step 90, the loss is 137694.71875\n",
            "in training loop, epoch 1, step 91, the loss is 239401.953125\n",
            "in training loop, epoch 1, step 92, the loss is 107304.90625\n",
            "in training loop, epoch 1, step 93, the loss is 123203.7421875\n",
            "in training loop, epoch 1, step 94, the loss is 184350.84375\n",
            "in training loop, epoch 1, step 95, the loss is 177740.3125\n",
            "in training loop, epoch 1, step 96, the loss is 248875.703125\n",
            "in training loop, epoch 1, step 97, the loss is 137056.59375\n",
            "in training loop, epoch 1, step 98, the loss is 201266.46875\n",
            "in training loop, epoch 1, step 99, the loss is 222209.234375\n",
            "in training loop, epoch 1, step 100, the loss is 196888.8125\n",
            "in training loop, epoch 1, step 101, the loss is 210386.15625\n",
            "in training loop, epoch 1, step 102, the loss is 162058.171875\n",
            "in training loop, epoch 1, step 103, the loss is 244702.078125\n",
            "in training loop, epoch 1, step 104, the loss is 141933.5\n",
            "in training loop, epoch 1, step 105, the loss is 316657.0\n",
            "in training loop, epoch 1, step 106, the loss is 279824.125\n",
            "in training loop, epoch 1, step 107, the loss is 163336.765625\n",
            "in training loop, epoch 1, step 108, the loss is 310418.78125\n",
            "in training loop, epoch 1, step 109, the loss is 269910.21875\n",
            "in training loop, epoch 1, step 110, the loss is 179999.25\n",
            "in training loop, epoch 1, step 111, the loss is 170883.921875\n",
            "in training loop, epoch 1, step 112, the loss is 177760.78125\n",
            "in training loop, epoch 1, step 113, the loss is 131066.0546875\n",
            "in training loop, epoch 1, step 114, the loss is 175553.65625\n",
            "in training loop, epoch 1, step 115, the loss is 244056.5\n",
            "in training loop, epoch 1, step 116, the loss is 198500.296875\n",
            "in training loop, epoch 1, step 117, the loss is 190702.671875\n",
            "in training loop, epoch 1, step 118, the loss is 160046.8125\n",
            "in training loop, epoch 1, step 119, the loss is 266846.125\n",
            "in training loop, epoch 1, step 120, the loss is 199195.703125\n",
            "in training loop, epoch 1, step 121, the loss is 214721.921875\n",
            "in training loop, epoch 1, step 122, the loss is 246599.046875\n",
            "in training loop, epoch 1, step 123, the loss is 310231.625\n",
            "in training loop, epoch 1, step 124, the loss is 201160.96875\n",
            "in training loop, epoch 1, step 125, the loss is 229593.25\n",
            "in training loop, epoch 1, step 126, the loss is 211956.0625\n",
            "in training loop, epoch 1, step 127, the loss is 118021.34375\n",
            "in training loop, epoch 1, step 128, the loss is 267592.65625\n",
            "in training loop, epoch 1, step 129, the loss is 191977.984375\n",
            "in training loop, epoch 1, step 130, the loss is 187621.796875\n",
            "in training loop, epoch 1, step 131, the loss is 303583.96875\n",
            "in training loop, epoch 1, step 132, the loss is 164597.15625\n",
            "in training loop, epoch 1, step 133, the loss is 238485.953125\n",
            "in training loop, epoch 1, step 134, the loss is 162542.078125\n",
            "in training loop, epoch 1, step 135, the loss is 151547.234375\n",
            "in training loop, epoch 1, step 136, the loss is 275571.1875\n",
            "in training loop, epoch 1, step 137, the loss is 197254.09375\n",
            "in training loop, epoch 1, step 138, the loss is 261074.625\n",
            "in training loop, epoch 1, step 139, the loss is 267443.9375\n",
            "in training loop, epoch 1, step 140, the loss is 200466.953125\n",
            "in training loop, epoch 1, step 141, the loss is 190856.5625\n",
            "in training loop, epoch 1, step 142, the loss is 256680.15625\n",
            "in training loop, epoch 1, step 143, the loss is 197129.53125\n",
            "in training loop, epoch 1, step 144, the loss is 672767.625\n",
            "in training loop, epoch 1, step 145, the loss is 116026.9609375\n",
            "in training loop, epoch 1, step 146, the loss is 156132.640625\n",
            "in training loop, epoch 1, step 147, the loss is 207869.3125\n",
            "in training loop, epoch 1, step 148, the loss is 202775.59375\n",
            "in training loop, epoch 1, step 149, the loss is 150470.5625\n",
            "in training loop, epoch 1, step 150, the loss is 198669.140625\n",
            "in training loop, epoch 1, step 151, the loss is 229800.8125\n",
            "in training loop, epoch 1, step 152, the loss is 312836.6875\n",
            "in training loop, epoch 1, step 153, the loss is 161244.546875\n",
            "in training loop, epoch 1, step 154, the loss is 259371.0625\n",
            "in training loop, epoch 1, step 155, the loss is 177173.171875\n",
            "in training loop, epoch 1, step 156, the loss is 125699.296875\n",
            "in training loop, epoch 1, step 157, the loss is 266016.96875\n",
            "in training loop, epoch 1, step 158, the loss is 133483.40625\n",
            "in training loop, epoch 1, step 159, the loss is 189629.453125\n",
            "in training loop, epoch 1, step 160, the loss is 77624.3359375\n",
            "in training loop, epoch 1, step 161, the loss is 197081.828125\n",
            "in training loop, epoch 1, step 162, the loss is 197536.671875\n",
            "in training loop, epoch 1, step 163, the loss is 254215.390625\n",
            "in training loop, epoch 1, step 164, the loss is 184078.015625\n",
            "in training loop, epoch 1, step 165, the loss is 153424.5625\n",
            "in training loop, epoch 1, step 166, the loss is 162881.90625\n",
            "in training loop, epoch 1, step 167, the loss is 138631.390625\n",
            "in training loop, epoch 1, step 168, the loss is 232036.03125\n",
            "in training loop, epoch 1, step 169, the loss is 173007.203125\n",
            "in training loop, epoch 1, step 170, the loss is 81265.734375\n",
            "in training loop, epoch 1, step 171, the loss is 144247.875\n",
            "in training loop, epoch 1, step 172, the loss is 164875.796875\n",
            "in training loop, epoch 1, step 173, the loss is 279848.8125\n",
            "in training loop, epoch 1, step 174, the loss is 100650.234375\n",
            "in training loop, epoch 1, step 175, the loss is 176296.203125\n",
            "in training loop, epoch 1, step 176, the loss is 207346.75\n",
            "in training loop, epoch 1, step 177, the loss is 219189.078125\n",
            "in training loop, epoch 1, step 178, the loss is 104370.0\n",
            "in training loop, epoch 1, step 179, the loss is 205008.703125\n",
            "in training loop, epoch 1, step 180, the loss is 197801.34375\n",
            "in training loop, epoch 1, step 181, the loss is 185486.78125\n",
            "in training loop, epoch 1, step 182, the loss is 151328.328125\n",
            "in training loop, epoch 1, step 183, the loss is 181680.875\n",
            "in training loop, epoch 1, step 184, the loss is 115196.1171875\n",
            "in training loop, epoch 1, step 185, the loss is 125410.2578125\n",
            "in training loop, epoch 1, step 186, the loss is 172504.375\n",
            "in training loop, epoch 1, step 187, the loss is 162178.078125\n",
            "in training loop, epoch 1, step 188, the loss is 136653.46875\n",
            "in training loop, epoch 1, step 189, the loss is 218056.875\n",
            "in training loop, epoch 1, step 190, the loss is 192139.15625\n",
            "in training loop, epoch 1, step 191, the loss is 176159.4375\n",
            "in training loop, epoch 1, step 192, the loss is 138535.359375\n",
            "in training loop, epoch 1, step 193, the loss is 116342.4765625\n",
            "in training loop, epoch 1, step 194, the loss is 172006.09375\n",
            "in training loop, epoch 1, step 195, the loss is 174969.921875\n",
            "in training loop, epoch 1, step 196, the loss is 183900.03125\n",
            "in training loop, epoch 1, step 197, the loss is 269158.59375\n",
            "in training loop, epoch 1, step 198, the loss is 153513.890625\n",
            "in training loop, epoch 1, step 199, the loss is 188749.125\n",
            "in training loop, epoch 1, step 200, the loss is 134108.3125\n",
            "in training loop, epoch 1, step 201, the loss is 187920.390625\n",
            "in training loop, epoch 1, step 202, the loss is 280209.8125\n",
            "in training loop, epoch 1, step 203, the loss is 133118.0625\n",
            "in training loop, epoch 1, step 204, the loss is 92125.71875\n",
            "in training loop, epoch 1, step 205, the loss is 108156.8515625\n",
            "in training loop, epoch 1, step 206, the loss is 119866.34375\n",
            "in training loop, epoch 1, step 207, the loss is 138455.5625\n",
            "in training loop, epoch 1, step 208, the loss is 208676.796875\n",
            "in training loop, epoch 1, step 209, the loss is 154807.53125\n",
            "in training loop, epoch 1, step 210, the loss is 150009.59375\n",
            "in training loop, epoch 1, step 211, the loss is 156273.25\n",
            "in training loop, epoch 1, step 212, the loss is 173787.46875\n",
            "in training loop, epoch 1, step 213, the loss is 186487.609375\n",
            "in training loop, epoch 1, step 214, the loss is 227282.328125\n",
            "in training loop, epoch 1, step 215, the loss is 186293.171875\n",
            "in training loop, epoch 1, step 216, the loss is 157996.09375\n",
            "in training loop, epoch 1, step 217, the loss is 55829.3203125\n",
            "in training loop, epoch 1, step 218, the loss is 144121.890625\n",
            "in training loop, epoch 1, step 219, the loss is 157898.125\n",
            "in training loop, epoch 1, step 220, the loss is 194537.84375\n",
            "in training loop, epoch 1, step 221, the loss is 331411.375\n",
            "in training loop, epoch 1, step 222, the loss is 158580.5625\n",
            "in training loop, epoch 1, step 223, the loss is 162731.421875\n",
            "in training loop, epoch 1, step 224, the loss is 218234.46875\n",
            "in training loop, epoch 1, step 225, the loss is 260376.28125\n",
            "in training loop, epoch 1, step 226, the loss is 134166.609375\n",
            "in training loop, epoch 1, step 227, the loss is 159605.84375\n",
            "in training loop, epoch 1, step 228, the loss is 422515.96875\n",
            "in training loop, epoch 1, step 229, the loss is 192281.5\n",
            "in training loop, epoch 1, step 230, the loss is 153978.21875\n",
            "in training loop, epoch 1, step 231, the loss is 261342.609375\n",
            "in training loop, epoch 1, step 232, the loss is 97179.390625\n",
            "in training loop, epoch 1, step 233, the loss is 209476.125\n",
            "in training loop, epoch 1, step 234, the loss is 231609.109375\n",
            "in training loop, epoch 1, step 235, the loss is 136899.6875\n",
            "in training loop, epoch 1, step 236, the loss is 224765.984375\n",
            "in training loop, epoch 1, step 237, the loss is 121696.484375\n",
            "in training loop, epoch 1, step 238, the loss is 144391.078125\n",
            "in training loop, epoch 1, step 239, the loss is 314344.625\n",
            "in training loop, epoch 1, step 240, the loss is 198015.28125\n",
            "in training loop, epoch 1, step 241, the loss is 174415.609375\n",
            "in training loop, epoch 1, step 242, the loss is 178392.625\n",
            "in training loop, epoch 1, step 243, the loss is 230126.453125\n",
            "in training loop, epoch 1, step 244, the loss is 107056.09375\n",
            "in training loop, epoch 1, step 245, the loss is 250589.875\n",
            "in training loop, epoch 1, step 246, the loss is 168711.84375\n",
            "in training loop, epoch 1, step 247, the loss is 147093.65625\n",
            "in training loop, epoch 1, step 248, the loss is 409831.46875\n",
            "in training loop, epoch 1, step 249, the loss is 192732.734375\n",
            "in training loop, epoch 1, step 250, the loss is 249146.28125\n",
            "in training loop, epoch 1, step 251, the loss is 232177.625\n",
            "in training loop, epoch 1, step 252, the loss is 212875.40625\n",
            "in training loop, epoch 1, step 253, the loss is 102486.65625\n",
            "in training loop, epoch 1, step 254, the loss is 212870.265625\n",
            "in training loop, epoch 1, step 255, the loss is 141064.5\n",
            "in training loop, epoch 1, step 256, the loss is 189909.21875\n",
            "in training loop, epoch 1, step 257, the loss is 137685.109375\n",
            "in training loop, epoch 1, step 258, the loss is 143012.96875\n",
            "in training loop, epoch 1, step 259, the loss is 361315.9375\n",
            "in training loop, epoch 1, step 260, the loss is 206501.625\n",
            "in training loop, epoch 1, step 261, the loss is 147975.421875\n",
            "in training loop, epoch 1, step 262, the loss is 226467.65625\n",
            "in training loop, epoch 1, step 263, the loss is 428303.875\n",
            "in training loop, epoch 1, step 264, the loss is 144187.84375\n",
            "in training loop, epoch 1, step 265, the loss is 157957.609375\n",
            "in training loop, epoch 1, step 266, the loss is 76264.7109375\n",
            "in training loop, epoch 1, step 267, the loss is 160633.6875\n",
            "in training loop, epoch 1, step 268, the loss is 196941.4375\n",
            "in training loop, epoch 1, step 269, the loss is 320086.5625\n",
            "in training loop, epoch 1, step 270, the loss is 140364.5\n",
            "in training loop, epoch 1, step 271, the loss is 139404.53125\n",
            "in training loop, epoch 1, step 272, the loss is 270177.5\n",
            "in training loop, epoch 1, step 273, the loss is 264792.4375\n",
            "in training loop, epoch 1, step 274, the loss is 376728.5\n",
            "in training loop, epoch 1, step 275, the loss is 341047.96875\n",
            "in training loop, epoch 1, step 276, the loss is 168467.640625\n",
            "in training loop, epoch 1, step 277, the loss is 288461.25\n",
            "in training loop, epoch 1, step 278, the loss is 221977.5625\n",
            "in training loop, epoch 1, step 279, the loss is 198805.84375\n",
            "in training loop, epoch 1, step 280, the loss is 231768.71875\n",
            "in training loop, epoch 1, step 281, the loss is 213479.671875\n",
            "in training loop, epoch 1, step 282, the loss is 231738.015625\n",
            "in training loop, epoch 1, step 283, the loss is 326660.21875\n",
            "in training loop, epoch 1, step 284, the loss is 238553.203125\n",
            "in training loop, epoch 1, step 285, the loss is 202944.6875\n",
            "in training loop, epoch 1, step 286, the loss is 138911.46875\n",
            "in training loop, epoch 1, step 287, the loss is 156846.375\n",
            "in training loop, epoch 1, step 288, the loss is 142090.046875\n",
            "in training loop, epoch 1, step 289, the loss is 161704.84375\n",
            "in training loop, epoch 1, step 290, the loss is 248761.1875\n",
            "in training loop, epoch 1, step 291, the loss is 222386.359375\n",
            "in training loop, epoch 1, step 292, the loss is 186144.125\n",
            "in training loop, epoch 1, step 293, the loss is 219831.421875\n",
            "in training loop, epoch 1, step 294, the loss is 174925.46875\n",
            "in training loop, epoch 1, step 295, the loss is 201899.9375\n",
            "in training loop, epoch 1, step 296, the loss is 214981.96875\n",
            "in training loop, epoch 1, step 297, the loss is 174975.828125\n",
            "in training loop, epoch 1, step 298, the loss is 209339.25\n",
            "in training loop, epoch 1, step 299, the loss is 399696.34375\n",
            "in training loop, epoch 1, step 300, the loss is 254829.875\n",
            "in training loop, epoch 1, step 301, the loss is 166981.046875\n",
            "in training loop, epoch 1, step 302, the loss is 174444.40625\n",
            "in training loop, epoch 1, step 303, the loss is 226783.15625\n",
            "in training loop, epoch 1, step 304, the loss is 185162.84375\n",
            "in training loop, epoch 1, step 305, the loss is 257436.28125\n",
            "in training loop, epoch 1, step 306, the loss is 270231.96875\n",
            "in training loop, epoch 1, step 307, the loss is 199031.375\n",
            "in training loop, epoch 1, step 308, the loss is 219603.375\n",
            "in training loop, epoch 1, step 309, the loss is 177694.984375\n",
            "in training loop, epoch 1, step 310, the loss is 186037.0\n",
            "in training loop, epoch 1, step 311, the loss is 179252.78125\n",
            "in training loop, epoch 1, step 312, the loss is 116223.265625\n",
            "in training loop, epoch 1, step 313, the loss is 118168.0625\n",
            "in training loop, epoch 1, step 314, the loss is 214853.109375\n",
            "in training loop, epoch 1, step 315, the loss is 182406.171875\n",
            "in training loop, epoch 1, step 316, the loss is 147856.03125\n",
            "in training loop, epoch 1, step 317, the loss is 151952.125\n",
            "in training loop, epoch 1, step 318, the loss is 158484.625\n",
            "in training loop, epoch 1, step 319, the loss is 227650.53125\n",
            "in training loop, epoch 1, step 320, the loss is 222413.546875\n",
            "in training loop, epoch 1, step 321, the loss is 187347.84375\n",
            "in training loop, epoch 1, step 322, the loss is 153647.0\n",
            "in training loop, epoch 1, step 323, the loss is 127362.9453125\n",
            "in training loop, epoch 1, step 324, the loss is 145688.90625\n",
            "in training loop, epoch 1, step 325, the loss is 154766.4375\n",
            "in training loop, epoch 1, step 326, the loss is 217054.859375\n",
            "in training loop, epoch 1, step 327, the loss is 175300.578125\n",
            "in training loop, epoch 1, step 328, the loss is 304137.9375\n",
            "in training loop, epoch 1, step 329, the loss is 151744.625\n",
            "in training loop, epoch 1, step 330, the loss is 210577.890625\n",
            "in training loop, epoch 1, step 331, the loss is 116763.0\n",
            "in training loop, epoch 1, step 332, the loss is 166846.6875\n",
            "in training loop, epoch 1, step 333, the loss is 183465.15625\n",
            "in training loop, epoch 1, step 334, the loss is 181958.640625\n",
            "in training loop, epoch 1, step 335, the loss is 182561.921875\n",
            "in training loop, epoch 1, step 336, the loss is 116795.40625\n",
            "in training loop, epoch 1, step 337, the loss is 123508.5625\n",
            "in training loop, epoch 1, step 338, the loss is 255040.671875\n",
            "in training loop, epoch 1, step 339, the loss is 191825.296875\n",
            "in training loop, epoch 1, step 340, the loss is 152266.53125\n",
            "in training loop, epoch 1, step 341, the loss is 162980.203125\n",
            "in training loop, epoch 1, step 342, the loss is 187639.578125\n",
            "in training loop, epoch 1, step 343, the loss is 162259.734375\n",
            "in training loop, epoch 1, step 344, the loss is 174717.53125\n",
            "in training loop, epoch 1, step 345, the loss is 170562.03125\n",
            "in training loop, epoch 1, step 346, the loss is 166773.984375\n",
            "in training loop, epoch 1, step 347, the loss is 123150.8828125\n",
            "in training loop, epoch 1, step 348, the loss is 182917.53125\n",
            "in training loop, epoch 1, step 349, the loss is 148482.109375\n",
            "in training loop, epoch 1, step 350, the loss is 149752.78125\n",
            "in training loop, epoch 1, step 351, the loss is 249505.34375\n",
            "in training loop, epoch 1, step 352, the loss is 152841.546875\n",
            "in training loop, epoch 1, step 353, the loss is 205274.59375\n",
            "in training loop, epoch 1, step 354, the loss is 154620.578125\n",
            "in training loop, epoch 1, step 355, the loss is 187934.21875\n",
            "in training loop, epoch 1, step 356, the loss is 97459.578125\n",
            "in training loop, epoch 1, step 357, the loss is 140087.890625\n",
            "in training loop, epoch 1, step 358, the loss is 152678.8125\n",
            "in training loop, epoch 1, step 359, the loss is 128626.3671875\n",
            "in training loop, epoch 1, step 360, the loss is 178105.265625\n",
            "in training loop, epoch 1, step 361, the loss is 222944.8125\n",
            "in training loop, epoch 1, step 362, the loss is 148696.59375\n",
            "in training loop, epoch 1, step 363, the loss is 112989.6875\n",
            "in training loop, epoch 1, step 364, the loss is 247473.90625\n",
            "in training loop, epoch 1, step 365, the loss is 256904.8125\n",
            "in training loop, epoch 1, step 366, the loss is 139655.40625\n",
            "in training loop, epoch 1, step 367, the loss is 164441.46875\n",
            "in training loop, epoch 1, step 368, the loss is 143472.46875\n",
            "in training loop, epoch 1, step 369, the loss is 142674.484375\n",
            "in training loop, epoch 1, step 370, the loss is 239248.8125\n",
            "in training loop, epoch 1, step 371, the loss is 148611.609375\n",
            "in training loop, epoch 1, step 372, the loss is 93049.4296875\n",
            "in training loop, epoch 1, step 373, the loss is 111171.9296875\n",
            "in training loop, epoch 1, step 374, the loss is 166497.890625\n",
            "in training loop, epoch 1, step 375, the loss is 123335.8828125\n",
            "in training loop, epoch 1, step 376, the loss is 164716.859375\n",
            "in training loop, epoch 1, step 377, the loss is 169287.734375\n",
            "in training loop, epoch 1, step 378, the loss is 177788.921875\n",
            "in training loop, epoch 1, step 379, the loss is 144992.796875\n",
            "in training loop, epoch 1, step 380, the loss is 184996.796875\n",
            "in training loop, epoch 1, step 381, the loss is 164264.703125\n",
            "in training loop, epoch 1, step 382, the loss is 364293.71875\n",
            "in training loop, epoch 1, step 383, the loss is 216400.953125\n",
            "in training loop, epoch 1, step 384, the loss is 212032.21875\n",
            "in training loop, epoch 1, step 385, the loss is 279059.6875\n",
            "in training loop, epoch 1, step 386, the loss is 171084.84375\n",
            "in training loop, epoch 1, step 387, the loss is 131924.3125\n",
            "in training loop, epoch 1, step 388, the loss is 124446.4453125\n",
            "in training loop, epoch 1, step 389, the loss is 317944.9375\n",
            "in training loop, epoch 1, step 390, the loss is 181488.34375\n",
            "in training loop, epoch 1, step 391, the loss is 150454.21875\n",
            "in training loop, epoch 1, step 392, the loss is 374658.4375\n",
            "in training loop, epoch 1, step 393, the loss is 178927.28125\n",
            "in training loop, epoch 1, step 394, the loss is 190331.5\n",
            "in training loop, epoch 1, step 395, the loss is 199253.40625\n",
            "in training loop, epoch 1, step 396, the loss is 193696.09375\n",
            "in training loop, epoch 1, step 397, the loss is 144742.765625\n",
            "in training loop, epoch 1, step 398, the loss is 324882.71875\n",
            "in training loop, epoch 1, step 399, the loss is 400120.75\n",
            "in training loop, epoch 1, step 400, the loss is 105859.859375\n",
            "in training loop, epoch 1, step 401, the loss is 119721.0078125\n",
            "in training loop, epoch 1, step 402, the loss is 127530.703125\n",
            "in training loop, epoch 1, step 403, the loss is 264671.53125\n",
            "in training loop, epoch 1, step 404, the loss is 122621.0078125\n",
            "in training loop, epoch 1, step 405, the loss is 187871.015625\n",
            "in training loop, epoch 1, step 406, the loss is 197042.75\n",
            "in training loop, epoch 1, step 407, the loss is 164714.953125\n",
            "in training loop, epoch 1, step 408, the loss is 166221.359375\n",
            "in training loop, epoch 1, step 409, the loss is 190794.71875\n",
            "in training loop, epoch 1, step 410, the loss is 153567.421875\n",
            "in training loop, epoch 1, step 411, the loss is 242410.03125\n",
            "in training loop, epoch 1, step 412, the loss is 268486.53125\n",
            "in training loop, epoch 1, step 413, the loss is 187235.84375\n",
            "in training loop, epoch 1, step 414, the loss is 126851.8125\n",
            "in training loop, epoch 1, step 415, the loss is 211530.65625\n",
            "in training loop, epoch 1, step 416, the loss is 150444.890625\n",
            "in training loop, epoch 1, step 417, the loss is 110783.3671875\n",
            "in training loop, epoch 1, step 418, the loss is 181024.28125\n",
            "in training loop, epoch 1, step 419, the loss is 198950.796875\n",
            "in training loop, epoch 1, step 420, the loss is 179192.75\n",
            "in training loop, epoch 1, step 421, the loss is 162616.546875\n",
            "in training loop, epoch 1, step 422, the loss is 80417.234375\n",
            "in training loop, epoch 1, step 423, the loss is 104698.140625\n",
            "in training loop, epoch 1, step 424, the loss is 215824.921875\n",
            "in training loop, epoch 1, step 425, the loss is 151111.859375\n",
            "in training loop, epoch 1, step 426, the loss is 192931.09375\n",
            "in training loop, epoch 1, step 427, the loss is 192187.6875\n",
            "in training loop, epoch 1, step 428, the loss is 151944.515625\n",
            "in training loop, epoch 1, step 429, the loss is 178981.9375\n",
            "in training loop, epoch 1, step 430, the loss is 188010.078125\n",
            "in training loop, epoch 1, step 431, the loss is 108102.53125\n",
            "in training loop, epoch 1, step 432, the loss is 157951.9375\n",
            "in training loop, epoch 1, step 433, the loss is 180877.734375\n",
            "in training loop, epoch 1, step 434, the loss is 144266.734375\n",
            "in training loop, epoch 1, step 435, the loss is 119618.375\n",
            "in training loop, epoch 1, step 436, the loss is 173973.125\n",
            "in training loop, epoch 1, step 437, the loss is 166754.96875\n",
            "in training loop, epoch 1, step 438, the loss is 213707.46875\n",
            "in training loop, epoch 1, step 439, the loss is 239263.328125\n",
            "in training loop, epoch 1, step 440, the loss is 164759.953125\n",
            "in training loop, epoch 1, step 441, the loss is 234389.125\n",
            "in training loop, epoch 1, step 442, the loss is 122081.5390625\n",
            "in training loop, epoch 1, step 443, the loss is 134348.34375\n",
            "in training loop, epoch 1, step 444, the loss is 193459.625\n",
            "in training loop, epoch 1, step 445, the loss is 223133.375\n",
            "in training loop, epoch 1, step 446, the loss is 129361.0546875\n",
            "in training loop, epoch 1, step 447, the loss is 237628.125\n",
            "in training loop, epoch 1, step 448, the loss is 201195.984375\n",
            "in training loop, epoch 1, step 449, the loss is 168908.5\n",
            "in training loop, epoch 1, step 450, the loss is 222848.0625\n",
            "in training loop, epoch 1, step 451, the loss is 201080.109375\n",
            "in training loop, epoch 1, step 452, the loss is 167731.96875\n",
            "in training loop, epoch 1, step 453, the loss is 445338.40625\n",
            "in training loop, epoch 1, step 454, the loss is 195860.78125\n",
            "in training loop, epoch 1, step 455, the loss is 136380.609375\n",
            "in training loop, epoch 1, step 456, the loss is 174081.40625\n",
            "in training loop, epoch 1, step 457, the loss is 138558.90625\n",
            "in training loop, epoch 1, step 458, the loss is 63218.203125\n",
            "in training loop, epoch 1, step 459, the loss is 270974.5\n",
            "in training loop, epoch 1, step 460, the loss is 161319.859375\n",
            "in training loop, epoch 1, step 461, the loss is 166916.609375\n",
            "in training loop, epoch 1, step 462, the loss is 201802.734375\n",
            "in training loop, epoch 1, step 463, the loss is 198755.21875\n",
            "in training loop, epoch 1, step 464, the loss is 148275.390625\n",
            "in training loop, epoch 1, step 465, the loss is 170540.78125\n",
            "in training loop, epoch 1, step 466, the loss is 178601.703125\n",
            "in training loop, epoch 1, step 467, the loss is 205145.1875\n",
            "in training loop, epoch 1, step 468, the loss is 234691.8125\n",
            "in training loop, epoch 1, step 469, the loss is 166804.75\n",
            "in training loop, epoch 1, step 470, the loss is 340705.3125\n",
            "in training loop, epoch 1, step 471, the loss is 334129.96875\n",
            "in training loop, epoch 1, step 472, the loss is 205897.375\n",
            "in training loop, epoch 1, step 473, the loss is 242785.546875\n",
            "in training loop, epoch 1, step 474, the loss is 254706.8125\n",
            "in training loop, epoch 1, step 475, the loss is 213743.5\n",
            "in training loop, epoch 1, step 476, the loss is 270839.8125\n",
            "in training loop, epoch 1, step 477, the loss is 218283.28125\n",
            "in training loop, epoch 1, step 478, the loss is 238490.5\n",
            "in training loop, epoch 1, step 479, the loss is 267665.09375\n",
            "in training loop, epoch 1, step 480, the loss is 305428.4375\n",
            "in training loop, epoch 1, step 481, the loss is 286765.09375\n",
            "in training loop, epoch 1, step 482, the loss is 242883.671875\n",
            "in training loop, epoch 1, step 483, the loss is 206232.1875\n",
            "in training loop, epoch 1, step 484, the loss is 242983.484375\n",
            "in training loop, epoch 1, step 485, the loss is 222475.984375\n",
            "in training loop, epoch 1, step 486, the loss is 220232.921875\n",
            "in training loop, epoch 1, step 487, the loss is 470308.15625\n",
            "in training loop, epoch 1, step 488, the loss is 153473.984375\n",
            "in training loop, epoch 1, step 489, the loss is 272452.34375\n",
            "in training loop, epoch 1, step 490, the loss is 332451.3125\n",
            "in training loop, epoch 1, step 491, the loss is 268762.84375\n",
            "in training loop, epoch 1, step 492, the loss is 169217.6875\n",
            "in training loop, epoch 1, step 493, the loss is 209562.28125\n",
            "in training loop, epoch 1, step 494, the loss is 187570.96875\n",
            "in training loop, epoch 1, step 495, the loss is 505875.25\n",
            "in training loop, epoch 1, step 496, the loss is 177928.984375\n",
            "in training loop, epoch 1, step 497, the loss is 173094.765625\n",
            "in training loop, epoch 1, step 498, the loss is 328225.09375\n",
            "in training loop, epoch 1, step 499, the loss is 325597.625\n",
            "in training loop, epoch 1, step 500, the loss is 177100.109375\n",
            "in training loop, epoch 1, step 501, the loss is 264963.59375\n",
            "in training loop, epoch 1, step 502, the loss is 182778.859375\n",
            "in training loop, epoch 1, step 503, the loss is 195953.5\n",
            "in training loop, epoch 1, step 504, the loss is 189945.859375\n",
            "in training loop, epoch 1, step 505, the loss is 190949.390625\n",
            "in training loop, epoch 1, step 506, the loss is 176630.921875\n",
            "in training loop, epoch 1, step 507, the loss is 214712.0625\n",
            "in training loop, epoch 1, step 508, the loss is 180637.5\n",
            "in training loop, epoch 1, step 509, the loss is 173563.84375\n",
            "in training loop, epoch 1, step 510, the loss is 226792.59375\n",
            "in training loop, epoch 1, step 511, the loss is 137786.21875\n",
            "in training loop, epoch 1, step 512, the loss is 170282.1875\n",
            "in training loop, epoch 1, step 513, the loss is 266806.46875\n",
            "in training loop, epoch 1, step 514, the loss is 201193.375\n",
            "in training loop, epoch 1, step 515, the loss is 188366.59375\n",
            "in training loop, epoch 1, step 516, the loss is 91295.0703125\n",
            "in training loop, epoch 1, step 517, the loss is 224195.6875\n",
            "in training loop, epoch 1, step 518, the loss is 173292.953125\n",
            "in training loop, epoch 1, step 519, the loss is 203384.734375\n",
            "in training loop, epoch 1, step 520, the loss is 188735.125\n",
            "in training loop, epoch 1, step 521, the loss is 311808.8125\n",
            "in training loop, epoch 1, step 522, the loss is 250479.109375\n",
            "in training loop, epoch 1, step 523, the loss is 163795.03125\n",
            "in training loop, epoch 1, step 524, the loss is 183712.1875\n",
            "in training loop, epoch 1, step 525, the loss is 139480.859375\n",
            "in training loop, epoch 1, step 526, the loss is 169870.53125\n",
            "in training loop, epoch 1, step 527, the loss is 216303.46875\n",
            "in training loop, epoch 1, step 528, the loss is 284026.5625\n",
            "in training loop, epoch 1, step 529, the loss is 168757.78125\n",
            "in training loop, epoch 1, step 530, the loss is 237646.5\n",
            "in training loop, epoch 1, step 531, the loss is 197347.1875\n",
            "in training loop, epoch 1, step 532, the loss is 260057.09375\n",
            "in training loop, epoch 1, step 533, the loss is 202050.078125\n",
            "in training loop, epoch 1, step 534, the loss is 167048.828125\n",
            "in training loop, epoch 1, step 535, the loss is 277121.125\n",
            "in training loop, epoch 1, step 536, the loss is 194590.875\n",
            "in training loop, epoch 1, step 537, the loss is 181398.015625\n",
            "in training loop, epoch 1, step 538, the loss is 208068.4375\n",
            "in training loop, epoch 1, step 539, the loss is 178910.546875\n",
            "in training loop, epoch 1, step 540, the loss is 321116.15625\n",
            "in training loop, epoch 1, step 541, the loss is 223042.9375\n",
            "in training loop, epoch 1, step 542, the loss is 211136.0\n",
            "in training loop, epoch 1, step 543, the loss is 139427.265625\n",
            "in training loop, epoch 1, step 544, the loss is 220320.796875\n",
            "in training loop, epoch 1, step 545, the loss is 194268.921875\n",
            "in training loop, epoch 1, step 546, the loss is 154797.234375\n",
            "in training loop, epoch 1, step 547, the loss is 230064.0\n",
            "in training loop, epoch 1, step 548, the loss is 167588.328125\n",
            "in training loop, epoch 1, step 549, the loss is 416018.6875\n",
            "in training loop, epoch 1, step 550, the loss is 200589.34375\n",
            "in training loop, epoch 1, step 551, the loss is 175417.109375\n",
            "in training loop, epoch 1, step 552, the loss is 219105.8125\n",
            "in training loop, epoch 1, step 553, the loss is 171859.09375\n",
            "in training loop, epoch 1, step 554, the loss is 256798.40625\n",
            "in training loop, epoch 1, step 555, the loss is 245237.640625\n",
            "in training loop, epoch 1, step 556, the loss is 209956.671875\n",
            "in training loop, epoch 1, step 557, the loss is 264642.5625\n",
            "in training loop, epoch 1, step 558, the loss is 287732.0625\n",
            "in training loop, epoch 1, step 559, the loss is 752491.9375\n",
            "in training loop, epoch 1, step 560, the loss is 150053.5\n",
            "in training loop, epoch 1, step 561, the loss is 131816.890625\n",
            "in training loop, epoch 1, step 562, the loss is 205891.96875\n",
            "in training loop, epoch 1, step 563, the loss is 217891.03125\n",
            "in training loop, epoch 1, step 564, the loss is 187193.78125\n",
            "in training loop, epoch 1, step 565, the loss is 195104.59375\n",
            "in training loop, epoch 1, step 566, the loss is 241797.84375\n",
            "in training loop, epoch 1, step 567, the loss is 132897.125\n",
            "in training loop, epoch 1, step 568, the loss is 211120.921875\n",
            "in training loop, epoch 1, step 569, the loss is 128495.8828125\n",
            "in training loop, epoch 1, step 570, the loss is 181517.28125\n",
            "in training loop, epoch 1, step 571, the loss is 265907.375\n",
            "in training loop, epoch 1, step 572, the loss is 229845.90625\n",
            "in training loop, epoch 1, step 573, the loss is 131068.4296875\n",
            "in training loop, epoch 1, step 574, the loss is 187709.828125\n",
            "in training loop, epoch 1, step 575, the loss is 144432.6875\n",
            "in training loop, epoch 1, step 576, the loss is 119864.9296875\n",
            "in training loop, epoch 1, step 577, the loss is 222924.03125\n",
            "in training loop, epoch 1, step 578, the loss is 146483.15625\n",
            "in training loop, epoch 1, step 579, the loss is 161815.765625\n",
            "in training loop, epoch 1, step 580, the loss is 215110.0625\n",
            "in training loop, epoch 1, step 581, the loss is 158224.34375\n",
            "in training loop, epoch 1, step 582, the loss is 136287.984375\n",
            "in training loop, epoch 1, step 583, the loss is 224813.09375\n",
            "in training loop, epoch 1, step 584, the loss is 218104.234375\n",
            "in training loop, epoch 1, step 585, the loss is 164092.859375\n",
            "in training loop, epoch 1, step 586, the loss is 192491.890625\n",
            "in training loop, epoch 1, step 587, the loss is 208161.265625\n",
            "in training loop, epoch 1, step 588, the loss is 184825.828125\n",
            "in training loop, epoch 1, step 589, the loss is 168196.59375\n",
            "in training loop, epoch 1, step 590, the loss is 236542.109375\n",
            "in training loop, epoch 1, step 591, the loss is 185191.5\n",
            "in training loop, epoch 1, step 592, the loss is 190310.3125\n",
            "in training loop, epoch 1, step 593, the loss is 200118.984375\n",
            "in training loop, epoch 1, step 594, the loss is 267864.0625\n",
            "in training loop, epoch 1, step 595, the loss is 180780.59375\n",
            "in training loop, epoch 1, step 596, the loss is 258310.53125\n",
            "in training loop, epoch 1, step 597, the loss is 465575.8125\n",
            "in training loop, epoch 1, step 598, the loss is 126828.484375\n",
            "in training loop, epoch 1, step 599, the loss is 203815.9375\n",
            "in training loop, epoch 1, step 600, the loss is 279206.59375\n",
            "in training loop, epoch 1, step 601, the loss is 222801.75\n",
            "in training loop, epoch 1, step 602, the loss is 242145.859375\n",
            "in training loop, epoch 1, step 603, the loss is 272119.09375\n",
            "in training loop, epoch 1, step 604, the loss is 243736.59375\n",
            "in training loop, epoch 1, step 605, the loss is 257473.40625\n",
            "in training loop, epoch 1, step 606, the loss is 189126.484375\n",
            "in training loop, epoch 1, step 607, the loss is 192176.75\n",
            "in training loop, epoch 1, step 608, the loss is 158903.03125\n",
            "in training loop, epoch 1, step 609, the loss is 225457.09375\n",
            "in training loop, epoch 1, step 610, the loss is 233235.953125\n",
            "in training loop, epoch 1, step 611, the loss is 171192.578125\n",
            "in training loop, epoch 1, step 612, the loss is 175102.09375\n",
            "in training loop, epoch 1, step 613, the loss is 131393.109375\n",
            "in training loop, epoch 1, step 614, the loss is 249398.25\n",
            "in training loop, epoch 1, step 615, the loss is 133981.875\n",
            "in training loop, epoch 1, step 616, the loss is 188962.40625\n",
            "in training loop, epoch 1, step 617, the loss is 228470.53125\n",
            "in training loop, epoch 1, step 618, the loss is 127326.328125\n",
            "in training loop, epoch 1, step 619, the loss is 170042.953125\n",
            "in training loop, epoch 1, step 620, the loss is 358695.875\n",
            "in training loop, epoch 1, step 621, the loss is 189433.875\n",
            "in training loop, epoch 1, step 622, the loss is 228775.671875\n",
            "in training loop, epoch 1, step 623, the loss is 283685.71875\n",
            "in training loop, epoch 1, step 624, the loss is 131326.28125\n",
            "in training loop, epoch 1, step 625, the loss is 193823.625\n",
            "in training loop, epoch 1, step 626, the loss is 212681.921875\n",
            "in training loop, epoch 1, step 627, the loss is 182439.609375\n",
            "in training loop, epoch 1, step 628, the loss is 152717.546875\n",
            "in training loop, epoch 1, step 629, the loss is 204517.28125\n",
            "in training loop, epoch 1, step 630, the loss is 236007.515625\n",
            "in training loop, epoch 1, step 631, the loss is 115747.6640625\n",
            "in training loop, epoch 1, step 632, the loss is 192094.59375\n",
            "in training loop, epoch 1, step 633, the loss is 228846.140625\n",
            "in training loop, epoch 1, step 634, the loss is 152528.015625\n",
            "in training loop, epoch 1, step 635, the loss is 225904.390625\n",
            "in training loop, epoch 1, step 636, the loss is 202244.375\n",
            "in training loop, epoch 1, step 637, the loss is 178389.140625\n",
            "in training loop, epoch 1, step 638, the loss is 218882.765625\n",
            "in training loop, epoch 1, step 639, the loss is 169542.90625\n",
            "in training loop, epoch 1, step 640, the loss is 118688.1953125\n",
            "in training loop, epoch 1, step 641, the loss is 130012.8359375\n",
            "in training loop, epoch 1, step 642, the loss is 153199.546875\n",
            "in training loop, epoch 1, step 643, the loss is 285699.84375\n",
            "in training loop, epoch 1, step 644, the loss is 252525.96875\n",
            "in training loop, epoch 1, step 645, the loss is 205228.671875\n",
            "in training loop, epoch 1, step 646, the loss is 222592.078125\n",
            "in training loop, epoch 1, step 647, the loss is 127538.8515625\n",
            "in training loop, epoch 1, step 648, the loss is 252758.703125\n",
            "in training loop, epoch 1, step 649, the loss is 242489.4375\n",
            "in training loop, epoch 1, step 650, the loss is 176979.46875\n",
            "in training loop, epoch 1, step 651, the loss is 200907.625\n",
            "in training loop, epoch 1, step 652, the loss is 147301.46875\n",
            "in training loop, epoch 1, step 653, the loss is 121039.7578125\n",
            "in training loop, epoch 1, step 654, the loss is 190566.90625\n",
            "in training loop, epoch 1, step 655, the loss is 239969.75\n",
            "in training loop, epoch 1, step 656, the loss is 128925.5703125\n",
            "in training loop, epoch 1, step 657, the loss is 181426.46875\n",
            "in training loop, epoch 1, step 658, the loss is 216692.3125\n",
            "in training loop, epoch 1, step 659, the loss is 189936.5625\n",
            "in training loop, epoch 1, step 660, the loss is 178470.140625\n",
            "in training loop, epoch 1, step 661, the loss is 192826.40625\n",
            "in training loop, epoch 1, step 662, the loss is 140499.15625\n",
            "in training loop, epoch 1, step 663, the loss is 237380.328125\n",
            "in training loop, epoch 1, step 664, the loss is 143271.75\n",
            "in training loop, epoch 1, step 665, the loss is 125734.3359375\n",
            "in training loop, epoch 1, step 666, the loss is 143350.84375\n",
            "in training loop, epoch 1, step 667, the loss is 138708.75\n",
            "in training loop, epoch 1, step 668, the loss is 201476.484375\n",
            "in training loop, epoch 1, step 669, the loss is 112384.015625\n",
            "in training loop, epoch 1, step 670, the loss is 183350.4375\n",
            "in training loop, epoch 1, step 671, the loss is 150658.765625\n",
            "in training loop, epoch 1, step 672, the loss is 166451.734375\n",
            "in training loop, epoch 1, step 673, the loss is 127024.734375\n",
            "in training loop, epoch 1, step 674, the loss is 200987.9375\n",
            "in training loop, epoch 1, step 675, the loss is 117689.4140625\n",
            "in training loop, epoch 1, step 676, the loss is 155384.3125\n",
            "in training loop, epoch 1, step 677, the loss is 334563.5\n",
            "in training loop, epoch 1, step 678, the loss is 157403.15625\n",
            "in training loop, epoch 1, step 679, the loss is 199623.9375\n",
            "in training loop, epoch 1, step 680, the loss is 214643.015625\n",
            "in training loop, epoch 1, step 681, the loss is 432037.90625\n",
            "in training loop, epoch 1, step 682, the loss is 134668.0625\n",
            "in training loop, epoch 1, step 683, the loss is 147627.640625\n",
            "in training loop, epoch 1, step 684, the loss is 254425.140625\n",
            "in training loop, epoch 1, step 685, the loss is 745616.5625\n",
            "in training loop, epoch 1, step 686, the loss is 308859.28125\n",
            "in training loop, epoch 1, step 687, the loss is 210259.859375\n",
            "in training loop, epoch 1, step 688, the loss is 175472.78125\n",
            "in training loop, epoch 1, step 689, the loss is 225482.546875\n",
            "in training loop, epoch 1, step 690, the loss is 409454.71875\n",
            "in training loop, epoch 1, step 691, the loss is 161865.90625\n",
            "in training loop, epoch 1, step 692, the loss is 228994.640625\n",
            "in training loop, epoch 1, step 693, the loss is 175829.90625\n",
            "in training loop, epoch 1, step 694, the loss is 187491.6875\n",
            "in training loop, epoch 1, step 695, the loss is 258722.296875\n",
            "in training loop, epoch 1, step 696, the loss is 230693.78125\n",
            "in training loop, epoch 1, step 697, the loss is 145374.125\n",
            "in training loop, epoch 1, step 698, the loss is 120133.0390625\n",
            "in training loop, epoch 1, step 699, the loss is 207493.96875\n",
            "in training loop, epoch 1, step 700, the loss is 209610.75\n",
            "in training loop, epoch 1, step 701, the loss is 149982.78125\n",
            "in training loop, epoch 1, step 702, the loss is 210634.65625\n",
            "in training loop, epoch 1, step 703, the loss is 474858.90625\n",
            "in training loop, epoch 1, step 704, the loss is 433107.125\n",
            "in training loop, epoch 1, step 705, the loss is 167843.625\n",
            "in training loop, epoch 1, step 706, the loss is 168273.1875\n",
            "in training loop, epoch 1, step 707, the loss is 248173.640625\n",
            "in training loop, epoch 1, step 708, the loss is 183683.421875\n",
            "in training loop, epoch 1, step 709, the loss is 218374.28125\n",
            "in training loop, epoch 1, step 710, the loss is 182066.296875\n",
            "in training loop, epoch 1, step 711, the loss is 267719.875\n",
            "in training loop, epoch 1, step 712, the loss is 368652.34375\n",
            "in training loop, epoch 1, step 713, the loss is 213820.484375\n",
            "in training loop, epoch 1, step 714, the loss is 561258.9375\n",
            "in training loop, epoch 1, step 715, the loss is 183436.453125\n",
            "in training loop, epoch 1, step 716, the loss is 253292.640625\n",
            "in training loop, epoch 1, step 717, the loss is 272429.15625\n",
            "in training loop, epoch 1, step 718, the loss is 381224.3125\n",
            "in training loop, epoch 1, step 719, the loss is 256425.53125\n",
            "in training loop, epoch 1, step 720, the loss is 253159.859375\n",
            "in training loop, epoch 1, step 721, the loss is 313411.8125\n",
            "in training loop, epoch 1, step 722, the loss is 212313.671875\n",
            "in training loop, epoch 1, step 723, the loss is 222264.65625\n",
            "in training loop, epoch 1, step 724, the loss is 247836.5625\n",
            "in training loop, epoch 1, step 725, the loss is 315969.375\n",
            "in training loop, epoch 1, step 726, the loss is 258290.09375\n",
            "in training loop, epoch 1, step 727, the loss is 177440.46875\n",
            "in training loop, epoch 1, step 728, the loss is 261769.140625\n",
            "in training loop, epoch 1, step 729, the loss is 229428.65625\n",
            "in training loop, epoch 1, step 730, the loss is 349710.03125\n",
            "in training loop, epoch 1, step 731, the loss is 294764.6875\n",
            "in training loop, epoch 1, step 732, the loss is 156329.625\n",
            "in training loop, epoch 1, step 733, the loss is 232266.9375\n",
            "in training loop, epoch 1, step 734, the loss is 235184.75\n",
            "in training loop, epoch 1, step 735, the loss is 176201.75\n",
            "in training loop, epoch 1, step 736, the loss is 147929.484375\n",
            "in training loop, epoch 1, step 737, the loss is 202910.671875\n",
            "in training loop, epoch 1, step 738, the loss is 249864.796875\n",
            "in training loop, epoch 1, step 739, the loss is 168941.78125\n",
            "in training loop, epoch 1, step 740, the loss is 250938.0\n",
            "in training loop, epoch 1, step 741, the loss is 144347.59375\n",
            "in training loop, epoch 1, step 742, the loss is 179797.328125\n",
            "in training loop, epoch 1, step 743, the loss is 205621.84375\n",
            "in training loop, epoch 1, step 744, the loss is 212379.828125\n",
            "in training loop, epoch 1, step 745, the loss is 249098.0\n",
            "in training loop, epoch 1, step 746, the loss is 252550.25\n",
            "in training loop, epoch 1, step 747, the loss is 197042.859375\n",
            "in training loop, epoch 1, step 748, the loss is 140916.421875\n",
            "in training loop, epoch 1, step 749, the loss is 194894.6875\n",
            "in training loop, epoch 1, step 750, the loss is 200017.8125\n",
            "in training loop, epoch 1, step 751, the loss is 66573.125\n",
            "in training loop, epoch 1, step 752, the loss is 179257.6875\n",
            "in training loop, epoch 1, step 753, the loss is 271688.84375\n",
            "in training loop, epoch 1, step 754, the loss is 147630.140625\n",
            "in training loop, epoch 1, step 755, the loss is 199906.65625\n",
            "in training loop, epoch 1, step 756, the loss is 228156.03125\n",
            "in training loop, epoch 1, step 757, the loss is 216376.96875\n",
            "in training loop, epoch 1, step 758, the loss is 135925.359375\n",
            "in training loop, epoch 1, step 759, the loss is 223627.25\n",
            "in training loop, epoch 1, step 760, the loss is 206555.765625\n",
            "in training loop, epoch 1, step 761, the loss is 224042.25\n",
            "in training loop, epoch 1, step 762, the loss is 271679.59375\n",
            "in training loop, epoch 1, step 763, the loss is 187650.28125\n",
            "in training loop, epoch 1, step 764, the loss is 93226.09375\n",
            "in training loop, epoch 1, step 765, the loss is 241050.125\n",
            "in training loop, epoch 1, step 766, the loss is 182962.59375\n",
            "in training loop, epoch 1, step 767, the loss is 174009.828125\n",
            "in training loop, epoch 1, step 768, the loss is 158732.71875\n",
            "in training loop, epoch 1, step 769, the loss is 159786.25\n",
            "in training loop, epoch 1, step 770, the loss is 252004.0625\n",
            "in training loop, epoch 1, step 771, the loss is 118032.1328125\n",
            "in training loop, epoch 1, step 772, the loss is 188829.15625\n",
            "in training loop, epoch 1, step 773, the loss is 267753.1875\n",
            "in training loop, epoch 1, step 774, the loss is 190872.90625\n",
            "in training loop, epoch 1, step 775, the loss is 229328.140625\n",
            "in training loop, epoch 1, step 776, the loss is 210541.53125\n",
            "in training loop, epoch 1, step 777, the loss is 150179.484375\n",
            "in training loop, epoch 1, step 778, the loss is 190457.3125\n",
            "in training loop, epoch 1, step 779, the loss is 117474.171875\n",
            "in training loop, epoch 1, step 780, the loss is 247994.84375\n",
            "in training loop, epoch 1, step 781, the loss is 284455.375\n",
            "in training loop, epoch 1, step 782, the loss is 129844.0859375\n",
            "in training loop, epoch 1, step 783, the loss is 101581.15625\n",
            "in training loop, epoch 1, step 784, the loss is 208955.8125\n",
            "in training loop, epoch 1, step 785, the loss is 159580.703125\n",
            "in training loop, epoch 1, step 786, the loss is 118299.0625\n",
            "in training loop, epoch 1, step 787, the loss is 183973.0625\n",
            "in training loop, epoch 1, step 788, the loss is 216901.140625\n",
            "in training loop, epoch 1, step 789, the loss is 170682.25\n",
            "in training loop, epoch 1, step 790, the loss is 185035.953125\n",
            "in training loop, epoch 1, step 791, the loss is 223344.703125\n",
            "in training loop, epoch 1, step 792, the loss is 261979.859375\n",
            "in training loop, epoch 1, step 793, the loss is 227050.921875\n",
            "in training loop, epoch 1, step 794, the loss is 113472.1015625\n",
            "in training loop, epoch 1, step 795, the loss is 119417.203125\n",
            "in training loop, epoch 1, step 796, the loss is 122067.109375\n",
            "in training loop, epoch 1, step 797, the loss is 340064.90625\n",
            "in training loop, epoch 1, step 798, the loss is 124197.96875\n",
            "in training loop, epoch 1, step 799, the loss is 151684.40625\n",
            "in training loop, epoch 1, step 800, the loss is 158819.0625\n",
            "in training loop, epoch 1, step 801, the loss is 199683.203125\n",
            "in training loop, epoch 1, step 802, the loss is 144015.171875\n",
            "in training loop, epoch 1, step 803, the loss is 326146.34375\n",
            "in training loop, epoch 1, step 804, the loss is 226364.375\n",
            "in training loop, epoch 1, step 805, the loss is 161283.5\n",
            "in training loop, epoch 1, step 806, the loss is 119684.5078125\n",
            "in training loop, epoch 1, step 807, the loss is 188234.625\n",
            "in training loop, epoch 1, step 808, the loss is 140431.984375\n",
            "in training loop, epoch 1, step 809, the loss is 259878.40625\n",
            "in training loop, epoch 1, step 810, the loss is 211618.59375\n",
            "in training loop, epoch 1, step 811, the loss is 202924.734375\n",
            "in training loop, epoch 1, step 812, the loss is 169887.953125\n",
            "in training loop, epoch 1, step 813, the loss is 140901.9375\n",
            "in training loop, epoch 1, step 814, the loss is 188267.78125\n",
            "in training loop, epoch 1, step 815, the loss is 195423.96875\n",
            "in training loop, epoch 1, step 816, the loss is 184642.90625\n",
            "in training loop, epoch 1, step 817, the loss is 224358.703125\n",
            "in training loop, epoch 1, step 818, the loss is 189643.078125\n",
            "in training loop, epoch 1, step 819, the loss is 259547.53125\n",
            "in training loop, epoch 1, step 820, the loss is 176955.15625\n",
            "in training loop, epoch 1, step 821, the loss is 182722.34375\n",
            "in training loop, epoch 1, step 822, the loss is 207559.265625\n",
            "in training loop, epoch 1, step 823, the loss is 156625.25\n",
            "in training loop, epoch 1, step 824, the loss is 117629.71875\n",
            "in training loop, epoch 1, step 825, the loss is 142680.8125\n",
            "in training loop, epoch 1, step 826, the loss is 215196.640625\n",
            "in training loop, epoch 1, step 827, the loss is 171196.953125\n",
            "in training loop, epoch 1, step 828, the loss is 185014.890625\n",
            "in training loop, epoch 1, step 829, the loss is 190070.28125\n",
            "in training loop, epoch 1, step 830, the loss is 192818.1875\n",
            "in training loop, epoch 1, step 831, the loss is 165634.875\n",
            "in training loop, epoch 1, step 832, the loss is 217377.828125\n",
            "in training loop, epoch 1, step 833, the loss is 192299.453125\n",
            "in training loop, epoch 1, step 834, the loss is 197506.90625\n",
            "in training loop, epoch 1, step 835, the loss is 130133.703125\n",
            "in training loop, epoch 1, step 836, the loss is 208823.359375\n",
            "in training loop, epoch 1, step 837, the loss is 212560.859375\n",
            "in training loop, epoch 1, step 838, the loss is 149390.5\n",
            "in training loop, epoch 1, step 839, the loss is 127188.4453125\n",
            "in training loop, epoch 1, step 840, the loss is 149084.59375\n",
            "in training loop, epoch 1, step 841, the loss is 163171.703125\n",
            "in training loop, epoch 1, step 842, the loss is 339248.6875\n",
            "in training loop, epoch 1, step 843, the loss is 368349.15625\n",
            "in training loop, epoch 1, step 844, the loss is 184756.078125\n",
            "in training loop, epoch 1, step 845, the loss is 123533.640625\n",
            "in training loop, epoch 1, step 846, the loss is 170143.375\n",
            "in training loop, epoch 1, step 847, the loss is 150169.46875\n",
            "in training loop, epoch 1, step 848, the loss is 128260.890625\n",
            "in training loop, epoch 1, step 849, the loss is 253410.34375\n",
            "in training loop, epoch 1, step 850, the loss is 156876.0625\n",
            "in training loop, epoch 1, step 851, the loss is 208515.875\n",
            "in training loop, epoch 1, step 852, the loss is 129576.546875\n",
            "in training loop, epoch 1, step 853, the loss is 206514.140625\n",
            "in training loop, epoch 1, step 854, the loss is 313457.375\n",
            "in training loop, epoch 1, step 855, the loss is 96666.171875\n",
            "in training loop, epoch 1, step 856, the loss is 303331.25\n",
            "in training loop, epoch 1, step 857, the loss is 209619.21875\n",
            "in training loop, epoch 1, step 858, the loss is 294341.0\n",
            "in training loop, epoch 1, step 859, the loss is 214635.53125\n",
            "in training loop, epoch 1, step 860, the loss is 174041.765625\n",
            "in training loop, epoch 1, step 861, the loss is 236356.59375\n",
            "in training loop, epoch 1, step 862, the loss is 267624.1875\n",
            "in training loop, epoch 1, step 863, the loss is 162932.5\n",
            "in training loop, epoch 1, step 864, the loss is 195201.1875\n",
            "in training loop, epoch 1, step 865, the loss is 337699.25\n",
            "in training loop, epoch 1, step 866, the loss is 328697.375\n",
            "in training loop, epoch 1, step 867, the loss is 137860.234375\n",
            "in training loop, epoch 1, step 868, the loss is 179784.84375\n",
            "in training loop, epoch 1, step 869, the loss is 177176.28125\n",
            "in training loop, epoch 1, step 870, the loss is 109214.7578125\n",
            "in training loop, epoch 1, step 871, the loss is 272267.28125\n",
            "in training loop, epoch 1, step 872, the loss is 201173.59375\n",
            "in training loop, epoch 1, step 873, the loss is 193334.53125\n",
            "in training loop, epoch 1, step 874, the loss is 215518.5625\n",
            "in training loop, epoch 1, step 875, the loss is 152995.328125\n",
            "in training loop, epoch 1, step 876, the loss is 205532.109375\n",
            "in training loop, epoch 1, step 877, the loss is 200010.359375\n",
            "in training loop, epoch 1, step 878, the loss is 441726.3125\n",
            "in training loop, epoch 1, step 879, the loss is 199381.53125\n",
            "in training loop, epoch 1, step 880, the loss is 198157.53125\n",
            "in training loop, epoch 1, step 881, the loss is 164930.40625\n",
            "in training loop, epoch 1, step 882, the loss is 184772.25\n",
            "in training loop, epoch 1, step 883, the loss is 201952.875\n",
            "in training loop, epoch 1, step 884, the loss is 167472.21875\n",
            "in training loop, epoch 1, step 885, the loss is 289825.34375\n",
            "in training loop, epoch 1, step 886, the loss is 255752.53125\n",
            "in training loop, epoch 1, step 887, the loss is 193926.640625\n",
            "in training loop, epoch 1, step 888, the loss is 259714.546875\n",
            "in training loop, epoch 1, step 889, the loss is 265239.03125\n",
            "in training loop, epoch 1, step 890, the loss is 180115.21875\n",
            "in training loop, epoch 1, step 891, the loss is 188699.375\n",
            "in training loop, epoch 1, step 892, the loss is 194432.625\n",
            "in training loop, epoch 1, step 893, the loss is 217147.09375\n",
            "in training loop, epoch 1, step 894, the loss is 211395.046875\n",
            "in training loop, epoch 1, step 895, the loss is 191550.109375\n",
            "in training loop, epoch 1, step 896, the loss is 199931.984375\n",
            "in training loop, epoch 1, step 897, the loss is 253336.0625\n",
            "in training loop, epoch 1, step 898, the loss is 171263.609375\n",
            "in training loop, epoch 1, step 899, the loss is 212121.40625\n",
            "in training loop, epoch 1, step 900, the loss is 217863.875\n",
            "in training loop, epoch 1, step 901, the loss is 160180.640625\n",
            "in training loop, epoch 1, step 902, the loss is 214462.84375\n",
            "in training loop, epoch 1, step 903, the loss is 94750.7109375\n",
            "k-fold 1:: Epoch 1: train loss 200512.19038647678 val loss 177777.48762376237\n",
            "in training loop, epoch 2, step 0, the loss is 113682.84375\n",
            "in training loop, epoch 2, step 1, the loss is 206827.65625\n",
            "in training loop, epoch 2, step 2, the loss is 127839.390625\n",
            "in training loop, epoch 2, step 3, the loss is 180026.71875\n",
            "in training loop, epoch 2, step 4, the loss is 246069.25\n",
            "in training loop, epoch 2, step 5, the loss is 144251.9375\n",
            "in training loop, epoch 2, step 6, the loss is 103033.625\n",
            "in training loop, epoch 2, step 7, the loss is 180388.953125\n",
            "in training loop, epoch 2, step 8, the loss is 166313.09375\n",
            "in training loop, epoch 2, step 9, the loss is 107705.515625\n",
            "in training loop, epoch 2, step 10, the loss is 180891.6875\n",
            "in training loop, epoch 2, step 11, the loss is 147116.328125\n",
            "in training loop, epoch 2, step 12, the loss is 155312.0\n",
            "in training loop, epoch 2, step 13, the loss is 122094.6796875\n",
            "in training loop, epoch 2, step 14, the loss is 149178.875\n",
            "in training loop, epoch 2, step 15, the loss is 161155.40625\n",
            "in training loop, epoch 2, step 16, the loss is 129812.6875\n",
            "in training loop, epoch 2, step 17, the loss is 180732.734375\n",
            "in training loop, epoch 2, step 18, the loss is 123523.3203125\n",
            "in training loop, epoch 2, step 19, the loss is 221147.78125\n",
            "in training loop, epoch 2, step 20, the loss is 98065.96875\n",
            "in training loop, epoch 2, step 21, the loss is 148702.21875\n",
            "in training loop, epoch 2, step 22, the loss is 126842.46875\n",
            "in training loop, epoch 2, step 23, the loss is 148838.5625\n",
            "in training loop, epoch 2, step 24, the loss is 154165.640625\n",
            "in training loop, epoch 2, step 25, the loss is 173431.71875\n",
            "in training loop, epoch 2, step 26, the loss is 94930.609375\n",
            "in training loop, epoch 2, step 27, the loss is 200876.15625\n",
            "in training loop, epoch 2, step 28, the loss is 158776.8125\n",
            "in training loop, epoch 2, step 29, the loss is 154207.984375\n",
            "in training loop, epoch 2, step 30, the loss is 156407.25\n",
            "in training loop, epoch 2, step 31, the loss is 154371.625\n",
            "in training loop, epoch 2, step 32, the loss is 155738.0625\n",
            "in training loop, epoch 2, step 33, the loss is 124191.3203125\n",
            "in training loop, epoch 2, step 34, the loss is 124113.1328125\n",
            "in training loop, epoch 2, step 35, the loss is 191494.0\n",
            "in training loop, epoch 2, step 36, the loss is 141383.171875\n",
            "in training loop, epoch 2, step 37, the loss is 123071.578125\n",
            "in training loop, epoch 2, step 38, the loss is 129305.9921875\n",
            "in training loop, epoch 2, step 39, the loss is 129114.0078125\n",
            "in training loop, epoch 2, step 40, the loss is 145217.453125\n",
            "in training loop, epoch 2, step 41, the loss is 109016.0234375\n",
            "in training loop, epoch 2, step 42, the loss is 133775.75\n",
            "in training loop, epoch 2, step 43, the loss is 118084.265625\n",
            "in training loop, epoch 2, step 44, the loss is 208817.734375\n",
            "in training loop, epoch 2, step 45, the loss is 144373.4375\n",
            "in training loop, epoch 2, step 46, the loss is 141265.65625\n",
            "in training loop, epoch 2, step 47, the loss is 128520.515625\n",
            "in training loop, epoch 2, step 48, the loss is 199619.28125\n",
            "in training loop, epoch 2, step 49, the loss is 167048.953125\n",
            "in training loop, epoch 2, step 50, the loss is 139583.5\n",
            "in training loop, epoch 2, step 51, the loss is 124832.15625\n",
            "in training loop, epoch 2, step 52, the loss is 118783.625\n",
            "in training loop, epoch 2, step 53, the loss is 205050.921875\n",
            "in training loop, epoch 2, step 54, the loss is 161202.171875\n",
            "in training loop, epoch 2, step 55, the loss is 140937.875\n",
            "in training loop, epoch 2, step 56, the loss is 124314.6640625\n",
            "in training loop, epoch 2, step 57, the loss is 131129.0\n",
            "in training loop, epoch 2, step 58, the loss is 189010.515625\n",
            "in training loop, epoch 2, step 59, the loss is 193324.359375\n",
            "in training loop, epoch 2, step 60, the loss is 155428.828125\n",
            "in training loop, epoch 2, step 61, the loss is 156672.796875\n",
            "in training loop, epoch 2, step 62, the loss is 100405.125\n",
            "in training loop, epoch 2, step 63, the loss is 94796.9375\n",
            "in training loop, epoch 2, step 64, the loss is 182764.625\n",
            "in training loop, epoch 2, step 65, the loss is 136206.359375\n",
            "in training loop, epoch 2, step 66, the loss is 205454.28125\n",
            "in training loop, epoch 2, step 67, the loss is 169255.796875\n",
            "in training loop, epoch 2, step 68, the loss is 250422.421875\n",
            "in training loop, epoch 2, step 69, the loss is 114150.8515625\n",
            "in training loop, epoch 2, step 70, the loss is 192203.1875\n",
            "in training loop, epoch 2, step 71, the loss is 123053.875\n",
            "in training loop, epoch 2, step 72, the loss is 139891.609375\n",
            "in training loop, epoch 2, step 73, the loss is 132011.78125\n",
            "in training loop, epoch 2, step 74, the loss is 98845.75\n",
            "in training loop, epoch 2, step 75, the loss is 142287.671875\n",
            "in training loop, epoch 2, step 76, the loss is 157065.296875\n",
            "in training loop, epoch 2, step 77, the loss is 152817.125\n",
            "in training loop, epoch 2, step 78, the loss is 107146.6796875\n",
            "in training loop, epoch 2, step 79, the loss is 274356.3125\n",
            "in training loop, epoch 2, step 80, the loss is 138550.5\n",
            "in training loop, epoch 2, step 81, the loss is 220969.703125\n",
            "in training loop, epoch 2, step 82, the loss is 149626.046875\n",
            "in training loop, epoch 2, step 83, the loss is 121846.625\n",
            "in training loop, epoch 2, step 84, the loss is 97280.0078125\n",
            "in training loop, epoch 2, step 85, the loss is 149478.6875\n",
            "in training loop, epoch 2, step 86, the loss is 150816.546875\n",
            "in training loop, epoch 2, step 87, the loss is 137910.515625\n",
            "in training loop, epoch 2, step 88, the loss is 151557.109375\n",
            "in training loop, epoch 2, step 89, the loss is 97137.609375\n",
            "in training loop, epoch 2, step 90, the loss is 185966.21875\n",
            "in training loop, epoch 2, step 91, the loss is 164157.046875\n",
            "in training loop, epoch 2, step 92, the loss is 91270.65625\n",
            "in training loop, epoch 2, step 93, the loss is 180215.09375\n",
            "in training loop, epoch 2, step 94, the loss is 138718.90625\n",
            "in training loop, epoch 2, step 95, the loss is 131421.078125\n",
            "in training loop, epoch 2, step 96, the loss is 205168.40625\n",
            "in training loop, epoch 2, step 97, the loss is 135971.6875\n",
            "in training loop, epoch 2, step 98, the loss is 151594.734375\n",
            "in training loop, epoch 2, step 99, the loss is 151387.515625\n",
            "in training loop, epoch 2, step 100, the loss is 162414.640625\n",
            "in training loop, epoch 2, step 101, the loss is 152092.6875\n",
            "in training loop, epoch 2, step 102, the loss is 124622.71875\n",
            "in training loop, epoch 2, step 103, the loss is 198213.046875\n",
            "in training loop, epoch 2, step 104, the loss is 246249.3125\n",
            "in training loop, epoch 2, step 105, the loss is 171287.953125\n",
            "in training loop, epoch 2, step 106, the loss is 114991.90625\n",
            "in training loop, epoch 2, step 107, the loss is 122298.875\n",
            "in training loop, epoch 2, step 108, the loss is 143801.40625\n",
            "in training loop, epoch 2, step 109, the loss is 136658.984375\n",
            "in training loop, epoch 2, step 110, the loss is 102995.2421875\n",
            "in training loop, epoch 2, step 111, the loss is 126003.859375\n",
            "in training loop, epoch 2, step 112, the loss is 150041.328125\n",
            "in training loop, epoch 2, step 113, the loss is 149662.609375\n",
            "in training loop, epoch 2, step 114, the loss is 141453.421875\n",
            "in training loop, epoch 2, step 115, the loss is 131018.828125\n",
            "in training loop, epoch 2, step 116, the loss is 185486.40625\n",
            "in training loop, epoch 2, step 117, the loss is 301147.0\n",
            "in training loop, epoch 2, step 118, the loss is 136586.03125\n",
            "in training loop, epoch 2, step 119, the loss is 124336.03125\n",
            "in training loop, epoch 2, step 120, the loss is 100459.28125\n",
            "in training loop, epoch 2, step 121, the loss is 120107.703125\n",
            "in training loop, epoch 2, step 122, the loss is 164255.921875\n",
            "in training loop, epoch 2, step 123, the loss is 113289.78125\n",
            "in training loop, epoch 2, step 124, the loss is 213473.625\n",
            "in training loop, epoch 2, step 125, the loss is 103264.046875\n",
            "in training loop, epoch 2, step 126, the loss is 115288.0078125\n",
            "in training loop, epoch 2, step 127, the loss is 200043.65625\n",
            "in training loop, epoch 2, step 128, the loss is 131381.84375\n",
            "in training loop, epoch 2, step 129, the loss is 247369.1875\n",
            "in training loop, epoch 2, step 130, the loss is 248130.359375\n",
            "in training loop, epoch 2, step 131, the loss is 148652.328125\n",
            "in training loop, epoch 2, step 132, the loss is 116862.1328125\n",
            "in training loop, epoch 2, step 133, the loss is 155891.609375\n",
            "in training loop, epoch 2, step 134, the loss is 169925.921875\n",
            "in training loop, epoch 2, step 135, the loss is 201555.5625\n",
            "in training loop, epoch 2, step 136, the loss is 135453.328125\n",
            "in training loop, epoch 2, step 137, the loss is 150191.671875\n",
            "in training loop, epoch 2, step 138, the loss is 124911.265625\n",
            "in training loop, epoch 2, step 139, the loss is 128181.1796875\n",
            "in training loop, epoch 2, step 140, the loss is 106480.3984375\n",
            "in training loop, epoch 2, step 141, the loss is 256006.4375\n",
            "in training loop, epoch 2, step 142, the loss is 237572.703125\n",
            "in training loop, epoch 2, step 143, the loss is 145985.515625\n",
            "in training loop, epoch 2, step 144, the loss is 153399.140625\n",
            "in training loop, epoch 2, step 145, the loss is 181856.71875\n",
            "in training loop, epoch 2, step 146, the loss is 129555.9375\n",
            "in training loop, epoch 2, step 147, the loss is 167639.375\n",
            "in training loop, epoch 2, step 148, the loss is 135653.15625\n",
            "in training loop, epoch 2, step 149, the loss is 140448.71875\n",
            "in training loop, epoch 2, step 150, the loss is 121731.5390625\n",
            "in training loop, epoch 2, step 151, the loss is 174496.640625\n",
            "in training loop, epoch 2, step 152, the loss is 132486.609375\n",
            "in training loop, epoch 2, step 153, the loss is 223891.5625\n",
            "in training loop, epoch 2, step 154, the loss is 149865.609375\n",
            "in training loop, epoch 2, step 155, the loss is 166925.765625\n",
            "in training loop, epoch 2, step 156, the loss is 96310.6875\n",
            "in training loop, epoch 2, step 157, the loss is 209035.53125\n",
            "in training loop, epoch 2, step 158, the loss is 189202.828125\n",
            "in training loop, epoch 2, step 159, the loss is 113414.7421875\n",
            "in training loop, epoch 2, step 160, the loss is 364847.375\n",
            "in training loop, epoch 2, step 161, the loss is 178936.015625\n",
            "in training loop, epoch 2, step 162, the loss is 195419.265625\n",
            "in training loop, epoch 2, step 163, the loss is 99535.4765625\n",
            "in training loop, epoch 2, step 164, the loss is 137648.15625\n",
            "in training loop, epoch 2, step 165, the loss is 180360.671875\n",
            "in training loop, epoch 2, step 166, the loss is 144453.65625\n",
            "in training loop, epoch 2, step 167, the loss is 147711.28125\n",
            "in training loop, epoch 2, step 168, the loss is 133795.09375\n",
            "in training loop, epoch 2, step 169, the loss is 129535.84375\n",
            "in training loop, epoch 2, step 170, the loss is 131779.296875\n",
            "in training loop, epoch 2, step 171, the loss is 301610.25\n",
            "in training loop, epoch 2, step 172, the loss is 186450.34375\n",
            "in training loop, epoch 2, step 173, the loss is 210461.984375\n",
            "in training loop, epoch 2, step 174, the loss is 81002.4921875\n",
            "in training loop, epoch 2, step 175, the loss is 173555.28125\n",
            "in training loop, epoch 2, step 176, the loss is 140236.0625\n",
            "in training loop, epoch 2, step 177, the loss is 143112.265625\n",
            "in training loop, epoch 2, step 178, the loss is 131137.71875\n",
            "in training loop, epoch 2, step 179, the loss is 146303.0\n",
            "in training loop, epoch 2, step 180, the loss is 94515.3984375\n",
            "in training loop, epoch 2, step 181, the loss is 183151.234375\n",
            "in training loop, epoch 2, step 182, the loss is 149160.109375\n",
            "in training loop, epoch 2, step 183, the loss is 111496.171875\n",
            "in training loop, epoch 2, step 184, the loss is 122198.3828125\n",
            "in training loop, epoch 2, step 185, the loss is 182568.375\n",
            "in training loop, epoch 2, step 186, the loss is 240606.78125\n",
            "in training loop, epoch 2, step 187, the loss is 111495.21875\n",
            "in training loop, epoch 2, step 188, the loss is 159166.59375\n",
            "in training loop, epoch 2, step 189, the loss is 194001.40625\n",
            "in training loop, epoch 2, step 190, the loss is 143800.4375\n",
            "in training loop, epoch 2, step 191, the loss is 159003.828125\n",
            "in training loop, epoch 2, step 192, the loss is 147795.453125\n",
            "in training loop, epoch 2, step 193, the loss is 113610.0859375\n",
            "in training loop, epoch 2, step 194, the loss is 93055.984375\n",
            "in training loop, epoch 2, step 195, the loss is 227007.65625\n",
            "in training loop, epoch 2, step 196, the loss is 118473.34375\n",
            "in training loop, epoch 2, step 197, the loss is 151593.484375\n",
            "in training loop, epoch 2, step 198, the loss is 161753.5625\n",
            "in training loop, epoch 2, step 199, the loss is 162768.46875\n",
            "in training loop, epoch 2, step 200, the loss is 158819.625\n",
            "in training loop, epoch 2, step 201, the loss is 178542.0625\n",
            "in training loop, epoch 2, step 202, the loss is 163158.765625\n",
            "in training loop, epoch 2, step 203, the loss is 172790.9375\n",
            "in training loop, epoch 2, step 204, the loss is 108007.4296875\n",
            "in training loop, epoch 2, step 205, the loss is 138722.328125\n",
            "in training loop, epoch 2, step 206, the loss is 160875.71875\n",
            "in training loop, epoch 2, step 207, the loss is 107543.2109375\n",
            "in training loop, epoch 2, step 208, the loss is 176992.734375\n",
            "in training loop, epoch 2, step 209, the loss is 95375.265625\n",
            "in training loop, epoch 2, step 210, the loss is 114798.84375\n",
            "in training loop, epoch 2, step 211, the loss is 145041.421875\n",
            "in training loop, epoch 2, step 212, the loss is 147864.5625\n",
            "in training loop, epoch 2, step 213, the loss is 143133.6875\n",
            "in training loop, epoch 2, step 214, the loss is 117268.546875\n",
            "in training loop, epoch 2, step 215, the loss is 125402.0625\n",
            "in training loop, epoch 2, step 216, the loss is 125752.328125\n",
            "in training loop, epoch 2, step 217, the loss is 155230.609375\n",
            "in training loop, epoch 2, step 218, the loss is 142681.015625\n",
            "in training loop, epoch 2, step 219, the loss is 105268.0625\n",
            "in training loop, epoch 2, step 220, the loss is 127197.6484375\n",
            "in training loop, epoch 2, step 221, the loss is 164387.296875\n",
            "in training loop, epoch 2, step 222, the loss is 153728.453125\n",
            "in training loop, epoch 2, step 223, the loss is 147047.015625\n",
            "in training loop, epoch 2, step 224, the loss is 108567.1875\n",
            "in training loop, epoch 2, step 225, the loss is 132185.5\n",
            "in training loop, epoch 2, step 226, the loss is 139357.25\n",
            "in training loop, epoch 2, step 227, the loss is 150612.90625\n",
            "in training loop, epoch 2, step 228, the loss is 204947.53125\n",
            "in training loop, epoch 2, step 229, the loss is 163963.953125\n",
            "in training loop, epoch 2, step 230, the loss is 153773.578125\n",
            "in training loop, epoch 2, step 231, the loss is 137144.609375\n",
            "in training loop, epoch 2, step 232, the loss is 176379.203125\n",
            "in training loop, epoch 2, step 233, the loss is 148324.921875\n",
            "in training loop, epoch 2, step 234, the loss is 175123.9375\n",
            "in training loop, epoch 2, step 235, the loss is 93340.0234375\n",
            "in training loop, epoch 2, step 236, the loss is 179605.34375\n",
            "in training loop, epoch 2, step 237, the loss is 109635.875\n",
            "in training loop, epoch 2, step 238, the loss is 154000.734375\n",
            "in training loop, epoch 2, step 239, the loss is 144205.3125\n",
            "in training loop, epoch 2, step 240, the loss is 130648.4375\n",
            "in training loop, epoch 2, step 241, the loss is 108395.9296875\n",
            "in training loop, epoch 2, step 242, the loss is 137864.78125\n",
            "in training loop, epoch 2, step 243, the loss is 131637.1875\n",
            "in training loop, epoch 2, step 244, the loss is 191833.0625\n",
            "in training loop, epoch 2, step 245, the loss is 183074.75\n",
            "in training loop, epoch 2, step 246, the loss is 113946.453125\n",
            "in training loop, epoch 2, step 247, the loss is 154937.109375\n",
            "in training loop, epoch 2, step 248, the loss is 155129.15625\n",
            "in training loop, epoch 2, step 249, the loss is 201465.75\n",
            "in training loop, epoch 2, step 250, the loss is 197837.890625\n",
            "in training loop, epoch 2, step 251, the loss is 167404.265625\n",
            "in training loop, epoch 2, step 252, the loss is 170678.859375\n",
            "in training loop, epoch 2, step 253, the loss is 140644.625\n",
            "in training loop, epoch 2, step 254, the loss is 197823.40625\n",
            "in training loop, epoch 2, step 255, the loss is 92860.9140625\n",
            "in training loop, epoch 2, step 256, the loss is 171006.65625\n",
            "in training loop, epoch 2, step 257, the loss is 121429.328125\n",
            "in training loop, epoch 2, step 258, the loss is 73777.1796875\n",
            "in training loop, epoch 2, step 259, the loss is 191481.78125\n",
            "in training loop, epoch 2, step 260, the loss is 113332.21875\n",
            "in training loop, epoch 2, step 261, the loss is 129638.7421875\n",
            "in training loop, epoch 2, step 262, the loss is 237601.8125\n",
            "in training loop, epoch 2, step 263, the loss is 180356.890625\n",
            "in training loop, epoch 2, step 264, the loss is 140123.421875\n",
            "in training loop, epoch 2, step 265, the loss is 106886.078125\n",
            "in training loop, epoch 2, step 266, the loss is 100240.8671875\n",
            "in training loop, epoch 2, step 267, the loss is 193089.796875\n",
            "in training loop, epoch 2, step 268, the loss is 137855.15625\n",
            "in training loop, epoch 2, step 269, the loss is 205856.234375\n",
            "in training loop, epoch 2, step 270, the loss is 159583.796875\n",
            "in training loop, epoch 2, step 271, the loss is 134829.46875\n",
            "in training loop, epoch 2, step 272, the loss is 101872.46875\n",
            "in training loop, epoch 2, step 273, the loss is 147764.78125\n",
            "in training loop, epoch 2, step 274, the loss is 126113.4765625\n",
            "in training loop, epoch 2, step 275, the loss is 139493.0625\n",
            "in training loop, epoch 2, step 276, the loss is 126563.6796875\n",
            "in training loop, epoch 2, step 277, the loss is 153986.6875\n",
            "in training loop, epoch 2, step 278, the loss is 144710.25\n",
            "in training loop, epoch 2, step 279, the loss is 159079.34375\n",
            "in training loop, epoch 2, step 280, the loss is 144179.265625\n",
            "in training loop, epoch 2, step 281, the loss is 121350.25\n",
            "in training loop, epoch 2, step 282, the loss is 176865.4375\n",
            "in training loop, epoch 2, step 283, the loss is 156832.890625\n",
            "in training loop, epoch 2, step 284, the loss is 156261.9375\n",
            "in training loop, epoch 2, step 285, the loss is 202548.3125\n",
            "in training loop, epoch 2, step 286, the loss is 145953.046875\n",
            "in training loop, epoch 2, step 287, the loss is 120495.390625\n",
            "in training loop, epoch 2, step 288, the loss is 146163.25\n",
            "in training loop, epoch 2, step 289, the loss is 152389.1875\n",
            "in training loop, epoch 2, step 290, the loss is 141515.765625\n",
            "in training loop, epoch 2, step 291, the loss is 166044.671875\n",
            "in training loop, epoch 2, step 292, the loss is 121617.9609375\n",
            "in training loop, epoch 2, step 293, the loss is 146911.0\n",
            "in training loop, epoch 2, step 294, the loss is 129422.875\n",
            "in training loop, epoch 2, step 295, the loss is 159566.796875\n",
            "in training loop, epoch 2, step 296, the loss is 199938.59375\n",
            "in training loop, epoch 2, step 297, the loss is 158015.265625\n",
            "in training loop, epoch 2, step 298, the loss is 240130.78125\n",
            "in training loop, epoch 2, step 299, the loss is 147601.625\n",
            "in training loop, epoch 2, step 300, the loss is 133104.203125\n",
            "in training loop, epoch 2, step 301, the loss is 118936.078125\n",
            "in training loop, epoch 2, step 302, the loss is 163422.984375\n",
            "in training loop, epoch 2, step 303, the loss is 116662.5\n",
            "in training loop, epoch 2, step 304, the loss is 182361.578125\n",
            "in training loop, epoch 2, step 305, the loss is 107627.234375\n",
            "in training loop, epoch 2, step 306, the loss is 211441.203125\n",
            "in training loop, epoch 2, step 307, the loss is 110189.2578125\n",
            "in training loop, epoch 2, step 308, the loss is 200040.5625\n",
            "in training loop, epoch 2, step 309, the loss is 128233.1640625\n",
            "in training loop, epoch 2, step 310, the loss is 167724.53125\n",
            "in training loop, epoch 2, step 311, the loss is 134423.421875\n",
            "in training loop, epoch 2, step 312, the loss is 159765.3125\n",
            "in training loop, epoch 2, step 313, the loss is 186607.4375\n",
            "in training loop, epoch 2, step 314, the loss is 206192.4375\n",
            "in training loop, epoch 2, step 315, the loss is 216666.375\n",
            "in training loop, epoch 2, step 316, the loss is 162308.96875\n",
            "in training loop, epoch 2, step 317, the loss is 116384.921875\n",
            "in training loop, epoch 2, step 318, the loss is 156997.1875\n",
            "in training loop, epoch 2, step 319, the loss is 176389.6875\n",
            "in training loop, epoch 2, step 320, the loss is 169537.140625\n",
            "in training loop, epoch 2, step 321, the loss is 230559.796875\n",
            "in training loop, epoch 2, step 322, the loss is 172595.78125\n",
            "in training loop, epoch 2, step 323, the loss is 152710.125\n",
            "in training loop, epoch 2, step 324, the loss is 226665.78125\n",
            "in training loop, epoch 2, step 325, the loss is 204854.09375\n",
            "in training loop, epoch 2, step 326, the loss is 152019.5\n",
            "in training loop, epoch 2, step 327, the loss is 203276.46875\n",
            "in training loop, epoch 2, step 328, the loss is 127486.5234375\n",
            "in training loop, epoch 2, step 329, the loss is 203797.25\n",
            "in training loop, epoch 2, step 330, the loss is 157922.03125\n",
            "in training loop, epoch 2, step 331, the loss is 158446.953125\n",
            "in training loop, epoch 2, step 332, the loss is 195107.125\n",
            "in training loop, epoch 2, step 333, the loss is 88696.09375\n",
            "in training loop, epoch 2, step 334, the loss is 142346.828125\n",
            "in training loop, epoch 2, step 335, the loss is 136121.625\n",
            "in training loop, epoch 2, step 336, the loss is 149175.8125\n",
            "in training loop, epoch 2, step 337, the loss is 132208.71875\n",
            "in training loop, epoch 2, step 338, the loss is 151418.28125\n",
            "in training loop, epoch 2, step 339, the loss is 147453.609375\n",
            "in training loop, epoch 2, step 340, the loss is 184804.1875\n",
            "in training loop, epoch 2, step 341, the loss is 140249.03125\n",
            "in training loop, epoch 2, step 342, the loss is 139017.203125\n",
            "in training loop, epoch 2, step 343, the loss is 177801.96875\n",
            "in training loop, epoch 2, step 344, the loss is 168037.125\n",
            "in training loop, epoch 2, step 345, the loss is 157420.6875\n",
            "in training loop, epoch 2, step 346, the loss is 163903.171875\n",
            "in training loop, epoch 2, step 347, the loss is 180110.953125\n",
            "in training loop, epoch 2, step 348, the loss is 94058.875\n",
            "in training loop, epoch 2, step 349, the loss is 116391.90625\n",
            "in training loop, epoch 2, step 350, the loss is 130852.640625\n",
            "in training loop, epoch 2, step 351, the loss is 182783.421875\n",
            "in training loop, epoch 2, step 352, the loss is 134153.375\n",
            "in training loop, epoch 2, step 353, the loss is 138569.0625\n",
            "in training loop, epoch 2, step 354, the loss is 152852.25\n",
            "in training loop, epoch 2, step 355, the loss is 92716.828125\n",
            "in training loop, epoch 2, step 356, the loss is 179227.5\n",
            "in training loop, epoch 2, step 357, the loss is 166325.828125\n",
            "in training loop, epoch 2, step 358, the loss is 159927.765625\n",
            "in training loop, epoch 2, step 359, the loss is 107227.0703125\n",
            "in training loop, epoch 2, step 360, the loss is 156607.640625\n",
            "in training loop, epoch 2, step 361, the loss is 99433.859375\n",
            "in training loop, epoch 2, step 362, the loss is 130356.8671875\n",
            "in training loop, epoch 2, step 363, the loss is 153661.15625\n",
            "in training loop, epoch 2, step 364, the loss is 129687.265625\n",
            "in training loop, epoch 2, step 365, the loss is 193340.71875\n",
            "in training loop, epoch 2, step 366, the loss is 132006.34375\n",
            "in training loop, epoch 2, step 367, the loss is 170482.515625\n",
            "in training loop, epoch 2, step 368, the loss is 194096.46875\n",
            "in training loop, epoch 2, step 369, the loss is 185228.84375\n",
            "in training loop, epoch 2, step 370, the loss is 177211.609375\n",
            "in training loop, epoch 2, step 371, the loss is 164090.140625\n",
            "in training loop, epoch 2, step 372, the loss is 173471.9375\n",
            "in training loop, epoch 2, step 373, the loss is 185665.71875\n",
            "in training loop, epoch 2, step 374, the loss is 160476.9375\n",
            "in training loop, epoch 2, step 375, the loss is 162473.71875\n",
            "in training loop, epoch 2, step 376, the loss is 141977.703125\n",
            "in training loop, epoch 2, step 377, the loss is 190145.984375\n",
            "in training loop, epoch 2, step 378, the loss is 163461.8125\n",
            "in training loop, epoch 2, step 379, the loss is 118878.8515625\n",
            "in training loop, epoch 2, step 380, the loss is 130431.203125\n",
            "in training loop, epoch 2, step 381, the loss is 155155.453125\n",
            "in training loop, epoch 2, step 382, the loss is 213789.03125\n",
            "in training loop, epoch 2, step 383, the loss is 146049.25\n",
            "in training loop, epoch 2, step 384, the loss is 96102.6875\n",
            "in training loop, epoch 2, step 385, the loss is 94825.6640625\n",
            "in training loop, epoch 2, step 386, the loss is 184181.328125\n",
            "in training loop, epoch 2, step 387, the loss is 189838.21875\n",
            "in training loop, epoch 2, step 388, the loss is 128038.453125\n",
            "in training loop, epoch 2, step 389, the loss is 102480.78125\n",
            "in training loop, epoch 2, step 390, the loss is 176879.28125\n",
            "in training loop, epoch 2, step 391, the loss is 118451.40625\n",
            "in training loop, epoch 2, step 392, the loss is 153798.15625\n",
            "in training loop, epoch 2, step 393, the loss is 176108.09375\n",
            "in training loop, epoch 2, step 394, the loss is 148476.625\n",
            "in training loop, epoch 2, step 395, the loss is 124564.671875\n",
            "in training loop, epoch 2, step 396, the loss is 169238.8125\n",
            "in training loop, epoch 2, step 397, the loss is 207307.78125\n",
            "in training loop, epoch 2, step 398, the loss is 138169.671875\n",
            "in training loop, epoch 2, step 399, the loss is 112660.4375\n",
            "in training loop, epoch 2, step 400, the loss is 119014.0625\n",
            "in training loop, epoch 2, step 401, the loss is 174648.078125\n",
            "in training loop, epoch 2, step 402, the loss is 145032.421875\n",
            "in training loop, epoch 2, step 403, the loss is 191112.828125\n",
            "in training loop, epoch 2, step 404, the loss is 156093.75\n",
            "in training loop, epoch 2, step 405, the loss is 113120.03125\n",
            "in training loop, epoch 2, step 406, the loss is 202755.96875\n",
            "in training loop, epoch 2, step 407, the loss is 154804.265625\n",
            "in training loop, epoch 2, step 408, the loss is 140625.6875\n",
            "in training loop, epoch 2, step 409, the loss is 149701.765625\n",
            "in training loop, epoch 2, step 410, the loss is 265045.75\n",
            "in training loop, epoch 2, step 411, the loss is 146575.40625\n",
            "in training loop, epoch 2, step 412, the loss is 73329.0234375\n",
            "in training loop, epoch 2, step 413, the loss is 136097.09375\n",
            "in training loop, epoch 2, step 414, the loss is 174833.28125\n",
            "in training loop, epoch 2, step 415, the loss is 188954.9375\n",
            "in training loop, epoch 2, step 416, the loss is 213169.0\n",
            "in training loop, epoch 2, step 417, the loss is 216067.484375\n",
            "in training loop, epoch 2, step 418, the loss is 166041.796875\n",
            "in training loop, epoch 2, step 419, the loss is 151712.59375\n",
            "in training loop, epoch 2, step 420, the loss is 130187.1875\n",
            "in training loop, epoch 2, step 421, the loss is 167425.21875\n",
            "in training loop, epoch 2, step 422, the loss is 157617.578125\n",
            "in training loop, epoch 2, step 423, the loss is 119152.46875\n",
            "in training loop, epoch 2, step 424, the loss is 183167.640625\n",
            "in training loop, epoch 2, step 425, the loss is 259553.40625\n",
            "in training loop, epoch 2, step 426, the loss is 223296.078125\n",
            "in training loop, epoch 2, step 427, the loss is 173434.53125\n",
            "in training loop, epoch 2, step 428, the loss is 135874.515625\n",
            "in training loop, epoch 2, step 429, the loss is 155249.78125\n",
            "in training loop, epoch 2, step 430, the loss is 178972.078125\n",
            "in training loop, epoch 2, step 431, the loss is 195534.515625\n",
            "in training loop, epoch 2, step 432, the loss is 193833.8125\n",
            "in training loop, epoch 2, step 433, the loss is 207400.578125\n",
            "in training loop, epoch 2, step 434, the loss is 170701.828125\n",
            "in training loop, epoch 2, step 435, the loss is 162396.75\n",
            "in training loop, epoch 2, step 436, the loss is 150004.234375\n",
            "in training loop, epoch 2, step 437, the loss is 200406.65625\n",
            "in training loop, epoch 2, step 438, the loss is 178879.25\n",
            "in training loop, epoch 2, step 439, the loss is 146038.34375\n",
            "in training loop, epoch 2, step 440, the loss is 132180.953125\n",
            "in training loop, epoch 2, step 441, the loss is 167231.109375\n",
            "in training loop, epoch 2, step 442, the loss is 112279.4453125\n",
            "in training loop, epoch 2, step 443, the loss is 139150.5625\n",
            "in training loop, epoch 2, step 444, the loss is 170074.25\n",
            "in training loop, epoch 2, step 445, the loss is 148195.96875\n",
            "in training loop, epoch 2, step 446, the loss is 188443.875\n",
            "in training loop, epoch 2, step 447, the loss is 155483.21875\n",
            "in training loop, epoch 2, step 448, the loss is 148359.78125\n",
            "in training loop, epoch 2, step 449, the loss is 139957.3125\n",
            "in training loop, epoch 2, step 450, the loss is 171176.078125\n",
            "in training loop, epoch 2, step 451, the loss is 117553.671875\n",
            "in training loop, epoch 2, step 452, the loss is 125957.125\n",
            "in training loop, epoch 2, step 453, the loss is 164397.9375\n",
            "in training loop, epoch 2, step 454, the loss is 128505.640625\n",
            "in training loop, epoch 2, step 455, the loss is 161343.609375\n",
            "in training loop, epoch 2, step 456, the loss is 152871.8125\n",
            "in training loop, epoch 2, step 457, the loss is 190561.640625\n",
            "in training loop, epoch 2, step 458, the loss is 202066.90625\n",
            "in training loop, epoch 2, step 459, the loss is 120335.90625\n",
            "in training loop, epoch 2, step 460, the loss is 115669.9296875\n",
            "in training loop, epoch 2, step 461, the loss is 111193.765625\n",
            "in training loop, epoch 2, step 462, the loss is 216078.625\n",
            "in training loop, epoch 2, step 463, the loss is 160348.484375\n",
            "in training loop, epoch 2, step 464, the loss is 142928.90625\n",
            "in training loop, epoch 2, step 465, the loss is 135333.984375\n",
            "in training loop, epoch 2, step 466, the loss is 106576.5\n",
            "in training loop, epoch 2, step 467, the loss is 125569.859375\n",
            "in training loop, epoch 2, step 468, the loss is 122876.7109375\n",
            "in training loop, epoch 2, step 469, the loss is 126920.328125\n",
            "in training loop, epoch 2, step 470, the loss is 145490.59375\n",
            "in training loop, epoch 2, step 471, the loss is 153420.28125\n",
            "in training loop, epoch 2, step 472, the loss is 181957.078125\n",
            "in training loop, epoch 2, step 473, the loss is 148972.46875\n",
            "in training loop, epoch 2, step 474, the loss is 86912.5234375\n",
            "in training loop, epoch 2, step 475, the loss is 95856.328125\n",
            "in training loop, epoch 2, step 476, the loss is 125003.5078125\n",
            "in training loop, epoch 2, step 477, the loss is 150488.703125\n",
            "in training loop, epoch 2, step 478, the loss is 148631.96875\n",
            "in training loop, epoch 2, step 479, the loss is 128498.4765625\n",
            "in training loop, epoch 2, step 480, the loss is 152621.078125\n",
            "in training loop, epoch 2, step 481, the loss is 147030.84375\n",
            "in training loop, epoch 2, step 482, the loss is 160441.6875\n",
            "in training loop, epoch 2, step 483, the loss is 180607.265625\n",
            "in training loop, epoch 2, step 484, the loss is 175377.25\n",
            "in training loop, epoch 2, step 485, the loss is 132981.6875\n",
            "in training loop, epoch 2, step 486, the loss is 127684.6875\n",
            "in training loop, epoch 2, step 487, the loss is 167783.71875\n",
            "in training loop, epoch 2, step 488, the loss is 207823.140625\n",
            "in training loop, epoch 2, step 489, the loss is 136364.21875\n",
            "in training loop, epoch 2, step 490, the loss is 165420.359375\n",
            "in training loop, epoch 2, step 491, the loss is 186355.390625\n",
            "in training loop, epoch 2, step 492, the loss is 117552.21875\n",
            "in training loop, epoch 2, step 493, the loss is 123439.03125\n",
            "in training loop, epoch 2, step 494, the loss is 154123.90625\n",
            "in training loop, epoch 2, step 495, the loss is 157119.546875\n",
            "in training loop, epoch 2, step 496, the loss is 125747.9296875\n",
            "in training loop, epoch 2, step 497, the loss is 143365.328125\n",
            "in training loop, epoch 2, step 498, the loss is 189379.3125\n",
            "in training loop, epoch 2, step 499, the loss is 118918.1953125\n",
            "in training loop, epoch 2, step 500, the loss is 179920.96875\n",
            "in training loop, epoch 2, step 501, the loss is 128835.4453125\n",
            "in training loop, epoch 2, step 502, the loss is 188625.40625\n",
            "in training loop, epoch 2, step 503, the loss is 171168.71875\n",
            "in training loop, epoch 2, step 504, the loss is 82422.9921875\n",
            "in training loop, epoch 2, step 505, the loss is 161214.4375\n",
            "in training loop, epoch 2, step 506, the loss is 113633.734375\n",
            "in training loop, epoch 2, step 507, the loss is 126948.5546875\n",
            "in training loop, epoch 2, step 508, the loss is 219403.3125\n",
            "in training loop, epoch 2, step 509, the loss is 163518.34375\n",
            "in training loop, epoch 2, step 510, the loss is 102572.4140625\n",
            "in training loop, epoch 2, step 511, the loss is 145821.90625\n",
            "in training loop, epoch 2, step 512, the loss is 171211.828125\n",
            "in training loop, epoch 2, step 513, the loss is 152987.46875\n",
            "in training loop, epoch 2, step 514, the loss is 120757.1328125\n",
            "in training loop, epoch 2, step 515, the loss is 139851.65625\n",
            "in training loop, epoch 2, step 516, the loss is 250165.5625\n",
            "in training loop, epoch 2, step 517, the loss is 151693.4375\n",
            "in training loop, epoch 2, step 518, the loss is 64165.12109375\n",
            "in training loop, epoch 2, step 519, the loss is 166387.109375\n",
            "in training loop, epoch 2, step 520, the loss is 163776.4375\n",
            "in training loop, epoch 2, step 521, the loss is 185907.859375\n",
            "in training loop, epoch 2, step 522, the loss is 157794.0\n",
            "in training loop, epoch 2, step 523, the loss is 128456.546875\n",
            "in training loop, epoch 2, step 524, the loss is 140000.296875\n",
            "in training loop, epoch 2, step 525, the loss is 195621.71875\n",
            "in training loop, epoch 2, step 526, the loss is 182910.25\n",
            "in training loop, epoch 2, step 527, the loss is 218981.796875\n",
            "in training loop, epoch 2, step 528, the loss is 182802.640625\n",
            "in training loop, epoch 2, step 529, the loss is 194279.203125\n",
            "in training loop, epoch 2, step 530, the loss is 235902.484375\n",
            "in training loop, epoch 2, step 531, the loss is 178287.3125\n",
            "in training loop, epoch 2, step 532, the loss is 138286.53125\n",
            "in training loop, epoch 2, step 533, the loss is 76222.4453125\n",
            "in training loop, epoch 2, step 534, the loss is 172793.0\n",
            "in training loop, epoch 2, step 535, the loss is 153901.828125\n",
            "in training loop, epoch 2, step 536, the loss is 121216.1796875\n",
            "in training loop, epoch 2, step 537, the loss is 140263.8125\n",
            "in training loop, epoch 2, step 538, the loss is 158330.8125\n",
            "in training loop, epoch 2, step 539, the loss is 132054.546875\n",
            "in training loop, epoch 2, step 540, the loss is 172576.8125\n",
            "in training loop, epoch 2, step 541, the loss is 121000.59375\n",
            "in training loop, epoch 2, step 542, the loss is 137346.6875\n",
            "in training loop, epoch 2, step 543, the loss is 142260.015625\n",
            "in training loop, epoch 2, step 544, the loss is 143533.5\n",
            "in training loop, epoch 2, step 545, the loss is 242364.125\n",
            "in training loop, epoch 2, step 546, the loss is 118836.0390625\n",
            "in training loop, epoch 2, step 547, the loss is 84090.4765625\n",
            "in training loop, epoch 2, step 548, the loss is 159836.75\n",
            "in training loop, epoch 2, step 549, the loss is 187411.671875\n",
            "in training loop, epoch 2, step 550, the loss is 163644.234375\n",
            "in training loop, epoch 2, step 551, the loss is 148427.609375\n",
            "in training loop, epoch 2, step 552, the loss is 229769.8125\n",
            "in training loop, epoch 2, step 553, the loss is 172276.4375\n",
            "in training loop, epoch 2, step 554, the loss is 252858.8125\n",
            "in training loop, epoch 2, step 555, the loss is 164821.09375\n",
            "in training loop, epoch 2, step 556, the loss is 179011.8125\n",
            "in training loop, epoch 2, step 557, the loss is 145472.484375\n",
            "in training loop, epoch 2, step 558, the loss is 243931.828125\n",
            "in training loop, epoch 2, step 559, the loss is 177468.171875\n",
            "in training loop, epoch 2, step 560, the loss is 133351.09375\n",
            "in training loop, epoch 2, step 561, the loss is 183470.234375\n",
            "in training loop, epoch 2, step 562, the loss is 106195.765625\n",
            "in training loop, epoch 2, step 563, the loss is 194365.671875\n",
            "in training loop, epoch 2, step 564, the loss is 140356.03125\n",
            "in training loop, epoch 2, step 565, the loss is 192564.828125\n",
            "in training loop, epoch 2, step 566, the loss is 187071.671875\n",
            "in training loop, epoch 2, step 567, the loss is 251976.484375\n",
            "in training loop, epoch 2, step 568, the loss is 295220.09375\n",
            "in training loop, epoch 2, step 569, the loss is 106760.03125\n",
            "in training loop, epoch 2, step 570, the loss is 127920.7734375\n",
            "in training loop, epoch 2, step 571, the loss is 168469.421875\n",
            "in training loop, epoch 2, step 572, the loss is 146059.171875\n",
            "in training loop, epoch 2, step 573, the loss is 174088.1875\n",
            "in training loop, epoch 2, step 574, the loss is 232575.5625\n",
            "in training loop, epoch 2, step 575, the loss is 164974.390625\n",
            "in training loop, epoch 2, step 576, the loss is 216965.0\n",
            "in training loop, epoch 2, step 577, the loss is 132261.0\n",
            "in training loop, epoch 2, step 578, the loss is 222342.359375\n",
            "in training loop, epoch 2, step 579, the loss is 107396.2109375\n",
            "in training loop, epoch 2, step 580, the loss is 182133.765625\n",
            "in training loop, epoch 2, step 581, the loss is 177098.5\n",
            "in training loop, epoch 2, step 582, the loss is 203615.96875\n",
            "in training loop, epoch 2, step 583, the loss is 117996.8828125\n",
            "in training loop, epoch 2, step 584, the loss is 139582.296875\n",
            "in training loop, epoch 2, step 585, the loss is 150199.234375\n",
            "in training loop, epoch 2, step 586, the loss is 78902.4375\n",
            "in training loop, epoch 2, step 587, the loss is 138119.8125\n",
            "in training loop, epoch 2, step 588, the loss is 162964.390625\n",
            "in training loop, epoch 2, step 589, the loss is 150721.703125\n",
            "in training loop, epoch 2, step 590, the loss is 195418.71875\n",
            "in training loop, epoch 2, step 591, the loss is 134372.25\n",
            "in training loop, epoch 2, step 592, the loss is 236069.265625\n",
            "in training loop, epoch 2, step 593, the loss is 102440.6015625\n",
            "in training loop, epoch 2, step 594, the loss is 187308.078125\n",
            "in training loop, epoch 2, step 595, the loss is 233053.3125\n",
            "in training loop, epoch 2, step 596, the loss is 152738.140625\n",
            "in training loop, epoch 2, step 597, the loss is 165715.90625\n",
            "in training loop, epoch 2, step 598, the loss is 263364.5\n",
            "in training loop, epoch 2, step 599, the loss is 159075.09375\n",
            "in training loop, epoch 2, step 600, the loss is 208182.046875\n",
            "in training loop, epoch 2, step 601, the loss is 122052.296875\n",
            "in training loop, epoch 2, step 602, the loss is 138118.609375\n",
            "in training loop, epoch 2, step 603, the loss is 163294.40625\n",
            "in training loop, epoch 2, step 604, the loss is 234932.859375\n",
            "in training loop, epoch 2, step 605, the loss is 200353.90625\n",
            "in training loop, epoch 2, step 606, the loss is 259765.65625\n",
            "in training loop, epoch 2, step 607, the loss is 187593.515625\n",
            "in training loop, epoch 2, step 608, the loss is 155130.703125\n",
            "in training loop, epoch 2, step 609, the loss is 178620.640625\n",
            "in training loop, epoch 2, step 610, the loss is 131980.59375\n",
            "in training loop, epoch 2, step 611, the loss is 169053.3125\n",
            "in training loop, epoch 2, step 612, the loss is 229055.28125\n",
            "in training loop, epoch 2, step 613, the loss is 135371.59375\n",
            "in training loop, epoch 2, step 614, the loss is 152817.796875\n",
            "in training loop, epoch 2, step 615, the loss is 227273.84375\n",
            "in training loop, epoch 2, step 616, the loss is 160392.796875\n",
            "in training loop, epoch 2, step 617, the loss is 127962.4453125\n",
            "in training loop, epoch 2, step 618, the loss is 197242.328125\n",
            "in training loop, epoch 2, step 619, the loss is 217314.375\n",
            "in training loop, epoch 2, step 620, the loss is 169126.828125\n",
            "in training loop, epoch 2, step 621, the loss is 110019.3828125\n",
            "in training loop, epoch 2, step 622, the loss is 167763.40625\n",
            "in training loop, epoch 2, step 623, the loss is 157733.921875\n",
            "in training loop, epoch 2, step 624, the loss is 129955.6953125\n",
            "in training loop, epoch 2, step 625, the loss is 155648.625\n",
            "in training loop, epoch 2, step 626, the loss is 122570.671875\n",
            "in training loop, epoch 2, step 627, the loss is 128709.25\n",
            "in training loop, epoch 2, step 628, the loss is 204892.0625\n",
            "in training loop, epoch 2, step 629, the loss is 207945.53125\n",
            "in training loop, epoch 2, step 630, the loss is 212938.03125\n",
            "in training loop, epoch 2, step 631, the loss is 156631.59375\n",
            "in training loop, epoch 2, step 632, the loss is 150397.859375\n",
            "in training loop, epoch 2, step 633, the loss is 127812.0234375\n",
            "in training loop, epoch 2, step 634, the loss is 167172.4375\n",
            "in training loop, epoch 2, step 635, the loss is 116424.5\n",
            "in training loop, epoch 2, step 636, the loss is 148464.0625\n",
            "in training loop, epoch 2, step 637, the loss is 192375.859375\n",
            "in training loop, epoch 2, step 638, the loss is 152712.8125\n",
            "in training loop, epoch 2, step 639, the loss is 196050.203125\n",
            "in training loop, epoch 2, step 640, the loss is 156640.0625\n",
            "in training loop, epoch 2, step 641, the loss is 180868.5\n",
            "in training loop, epoch 2, step 642, the loss is 204616.4375\n",
            "in training loop, epoch 2, step 643, the loss is 119881.515625\n",
            "in training loop, epoch 2, step 644, the loss is 125222.2578125\n",
            "in training loop, epoch 2, step 645, the loss is 145653.6875\n",
            "in training loop, epoch 2, step 646, the loss is 102757.6953125\n",
            "in training loop, epoch 2, step 647, the loss is 143241.546875\n",
            "in training loop, epoch 2, step 648, the loss is 137262.515625\n",
            "in training loop, epoch 2, step 649, the loss is 110357.7890625\n",
            "in training loop, epoch 2, step 650, the loss is 191395.328125\n",
            "in training loop, epoch 2, step 651, the loss is 152900.921875\n",
            "in training loop, epoch 2, step 652, the loss is 154045.28125\n",
            "in training loop, epoch 2, step 653, the loss is 122880.328125\n",
            "in training loop, epoch 2, step 654, the loss is 179843.015625\n",
            "in training loop, epoch 2, step 655, the loss is 133219.859375\n",
            "in training loop, epoch 2, step 656, the loss is 168973.3125\n",
            "in training loop, epoch 2, step 657, the loss is 183072.328125\n",
            "in training loop, epoch 2, step 658, the loss is 202623.15625\n",
            "in training loop, epoch 2, step 659, the loss is 82164.265625\n",
            "in training loop, epoch 2, step 660, the loss is 124595.125\n",
            "in training loop, epoch 2, step 661, the loss is 140702.234375\n",
            "in training loop, epoch 2, step 662, the loss is 105866.40625\n",
            "in training loop, epoch 2, step 663, the loss is 186831.109375\n",
            "in training loop, epoch 2, step 664, the loss is 172233.625\n",
            "in training loop, epoch 2, step 665, the loss is 152413.578125\n",
            "in training loop, epoch 2, step 666, the loss is 163212.28125\n",
            "in training loop, epoch 2, step 667, the loss is 129987.3046875\n",
            "in training loop, epoch 2, step 668, the loss is 143146.1875\n",
            "in training loop, epoch 2, step 669, the loss is 116951.4765625\n",
            "in training loop, epoch 2, step 670, the loss is 91447.8359375\n",
            "in training loop, epoch 2, step 671, the loss is 173233.8125\n",
            "in training loop, epoch 2, step 672, the loss is 229486.9375\n",
            "in training loop, epoch 2, step 673, the loss is 153219.5\n",
            "in training loop, epoch 2, step 674, the loss is 213371.5\n",
            "in training loop, epoch 2, step 675, the loss is 241486.546875\n",
            "in training loop, epoch 2, step 676, the loss is 191905.25\n",
            "in training loop, epoch 2, step 677, the loss is 85361.7421875\n",
            "in training loop, epoch 2, step 678, the loss is 142621.75\n",
            "in training loop, epoch 2, step 679, the loss is 156190.1875\n",
            "in training loop, epoch 2, step 680, the loss is 253350.34375\n",
            "in training loop, epoch 2, step 681, the loss is 81818.1015625\n",
            "in training loop, epoch 2, step 682, the loss is 162595.0\n",
            "in training loop, epoch 2, step 683, the loss is 151550.4375\n",
            "in training loop, epoch 2, step 684, the loss is 186913.078125\n",
            "in training loop, epoch 2, step 685, the loss is 145509.28125\n",
            "in training loop, epoch 2, step 686, the loss is 154893.96875\n",
            "in training loop, epoch 2, step 687, the loss is 136435.96875\n",
            "in training loop, epoch 2, step 688, the loss is 168303.25\n",
            "in training loop, epoch 2, step 689, the loss is 153073.71875\n",
            "in training loop, epoch 2, step 690, the loss is 123083.1953125\n",
            "in training loop, epoch 2, step 691, the loss is 236546.8125\n",
            "in training loop, epoch 2, step 692, the loss is 151629.734375\n",
            "in training loop, epoch 2, step 693, the loss is 213737.8125\n",
            "in training loop, epoch 2, step 694, the loss is 143362.296875\n",
            "in training loop, epoch 2, step 695, the loss is 149363.53125\n",
            "in training loop, epoch 2, step 696, the loss is 143117.125\n",
            "in training loop, epoch 2, step 697, the loss is 279473.65625\n",
            "in training loop, epoch 2, step 698, the loss is 135918.40625\n",
            "in training loop, epoch 2, step 699, the loss is 147340.15625\n",
            "in training loop, epoch 2, step 700, the loss is 134664.40625\n",
            "in training loop, epoch 2, step 701, the loss is 135678.140625\n",
            "in training loop, epoch 2, step 702, the loss is 158099.15625\n",
            "in training loop, epoch 2, step 703, the loss is 169250.484375\n",
            "in training loop, epoch 2, step 704, the loss is 169995.375\n",
            "in training loop, epoch 2, step 705, the loss is 139851.6875\n",
            "in training loop, epoch 2, step 706, the loss is 132023.5625\n",
            "in training loop, epoch 2, step 707, the loss is 160794.921875\n",
            "in training loop, epoch 2, step 708, the loss is 172877.03125\n",
            "in training loop, epoch 2, step 709, the loss is 152509.78125\n",
            "in training loop, epoch 2, step 710, the loss is 184750.125\n",
            "in training loop, epoch 2, step 711, the loss is 186866.984375\n",
            "in training loop, epoch 2, step 712, the loss is 124725.265625\n",
            "in training loop, epoch 2, step 713, the loss is 134656.953125\n",
            "in training loop, epoch 2, step 714, the loss is 147163.125\n",
            "in training loop, epoch 2, step 715, the loss is 151629.546875\n",
            "in training loop, epoch 2, step 716, the loss is 206678.28125\n",
            "in training loop, epoch 2, step 717, the loss is 186867.328125\n",
            "in training loop, epoch 2, step 718, the loss is 144764.890625\n",
            "in training loop, epoch 2, step 719, the loss is 293412.8125\n",
            "in training loop, epoch 2, step 720, the loss is 148274.09375\n",
            "in training loop, epoch 2, step 721, the loss is 190315.171875\n",
            "in training loop, epoch 2, step 722, the loss is 192726.515625\n",
            "in training loop, epoch 2, step 723, the loss is 102399.609375\n",
            "in training loop, epoch 2, step 724, the loss is 167595.890625\n",
            "in training loop, epoch 2, step 725, the loss is 151220.796875\n",
            "in training loop, epoch 2, step 726, the loss is 184245.265625\n",
            "in training loop, epoch 2, step 727, the loss is 227595.5625\n",
            "in training loop, epoch 2, step 728, the loss is 156137.5625\n",
            "in training loop, epoch 2, step 729, the loss is 136552.640625\n",
            "in training loop, epoch 2, step 730, the loss is 110310.484375\n",
            "in training loop, epoch 2, step 731, the loss is 186492.625\n",
            "in training loop, epoch 2, step 732, the loss is 109624.703125\n",
            "in training loop, epoch 2, step 733, the loss is 241014.296875\n",
            "in training loop, epoch 2, step 734, the loss is 139108.03125\n",
            "in training loop, epoch 2, step 735, the loss is 131676.1875\n",
            "in training loop, epoch 2, step 736, the loss is 140256.625\n",
            "in training loop, epoch 2, step 737, the loss is 177506.1875\n",
            "in training loop, epoch 2, step 738, the loss is 224583.734375\n",
            "in training loop, epoch 2, step 739, the loss is 134074.265625\n",
            "in training loop, epoch 2, step 740, the loss is 167571.3125\n",
            "in training loop, epoch 2, step 741, the loss is 181468.546875\n",
            "in training loop, epoch 2, step 742, the loss is 144512.734375\n",
            "in training loop, epoch 2, step 743, the loss is 194014.484375\n",
            "in training loop, epoch 2, step 744, the loss is 130338.8671875\n",
            "in training loop, epoch 2, step 745, the loss is 98325.546875\n",
            "in training loop, epoch 2, step 746, the loss is 174652.328125\n",
            "in training loop, epoch 2, step 747, the loss is 186330.515625\n",
            "in training loop, epoch 2, step 748, the loss is 140447.15625\n",
            "in training loop, epoch 2, step 749, the loss is 154233.25\n",
            "in training loop, epoch 2, step 750, the loss is 168610.140625\n",
            "in training loop, epoch 2, step 751, the loss is 131972.546875\n",
            "in training loop, epoch 2, step 752, the loss is 174833.546875\n",
            "in training loop, epoch 2, step 753, the loss is 143725.4375\n",
            "in training loop, epoch 2, step 754, the loss is 134128.328125\n",
            "in training loop, epoch 2, step 755, the loss is 163028.5\n",
            "in training loop, epoch 2, step 756, the loss is 161850.71875\n",
            "in training loop, epoch 2, step 757, the loss is 132790.03125\n",
            "in training loop, epoch 2, step 758, the loss is 218684.4375\n",
            "in training loop, epoch 2, step 759, the loss is 184626.25\n",
            "in training loop, epoch 2, step 760, the loss is 153264.875\n",
            "in training loop, epoch 2, step 761, the loss is 163572.8125\n",
            "in training loop, epoch 2, step 762, the loss is 153863.21875\n",
            "in training loop, epoch 2, step 763, the loss is 198778.953125\n",
            "in training loop, epoch 2, step 764, the loss is 111451.53125\n",
            "in training loop, epoch 2, step 765, the loss is 96928.1328125\n",
            "in training loop, epoch 2, step 766, the loss is 223216.0\n",
            "in training loop, epoch 2, step 767, the loss is 192767.046875\n",
            "in training loop, epoch 2, step 768, the loss is 197612.359375\n",
            "in training loop, epoch 2, step 769, the loss is 110582.96875\n",
            "in training loop, epoch 2, step 770, the loss is 165993.421875\n",
            "in training loop, epoch 2, step 771, the loss is 165271.875\n",
            "in training loop, epoch 2, step 772, the loss is 214075.0\n",
            "in training loop, epoch 2, step 773, the loss is 170638.046875\n",
            "in training loop, epoch 2, step 774, the loss is 144546.265625\n",
            "in training loop, epoch 2, step 775, the loss is 162689.1875\n",
            "in training loop, epoch 2, step 776, the loss is 179632.453125\n",
            "in training loop, epoch 2, step 777, the loss is 143044.0625\n",
            "in training loop, epoch 2, step 778, the loss is 219901.265625\n",
            "in training loop, epoch 2, step 779, the loss is 139066.8125\n",
            "in training loop, epoch 2, step 780, the loss is 118376.4375\n",
            "in training loop, epoch 2, step 781, the loss is 212389.25\n",
            "in training loop, epoch 2, step 782, the loss is 190884.921875\n",
            "in training loop, epoch 2, step 783, the loss is 232563.203125\n",
            "in training loop, epoch 2, step 784, the loss is 104434.3125\n",
            "in training loop, epoch 2, step 785, the loss is 136313.421875\n",
            "in training loop, epoch 2, step 786, the loss is 122125.421875\n",
            "in training loop, epoch 2, step 787, the loss is 88572.390625\n",
            "in training loop, epoch 2, step 788, the loss is 202361.6875\n",
            "in training loop, epoch 2, step 789, the loss is 132484.984375\n",
            "in training loop, epoch 2, step 790, the loss is 134279.046875\n",
            "in training loop, epoch 2, step 791, the loss is 140670.34375\n",
            "in training loop, epoch 2, step 792, the loss is 111785.3203125\n",
            "in training loop, epoch 2, step 793, the loss is 224306.21875\n",
            "in training loop, epoch 2, step 794, the loss is 105166.796875\n",
            "in training loop, epoch 2, step 795, the loss is 113068.1484375\n",
            "in training loop, epoch 2, step 796, the loss is 265661.125\n",
            "in training loop, epoch 2, step 797, the loss is 116369.546875\n",
            "in training loop, epoch 2, step 798, the loss is 196884.15625\n",
            "in training loop, epoch 2, step 799, the loss is 165678.78125\n",
            "in training loop, epoch 2, step 800, the loss is 136566.203125\n",
            "in training loop, epoch 2, step 801, the loss is 206713.3125\n",
            "in training loop, epoch 2, step 802, the loss is 195621.765625\n",
            "in training loop, epoch 2, step 803, the loss is 173458.9375\n",
            "in training loop, epoch 2, step 804, the loss is 190822.84375\n",
            "in training loop, epoch 2, step 805, the loss is 137226.75\n",
            "in training loop, epoch 2, step 806, the loss is 166425.1875\n",
            "in training loop, epoch 2, step 807, the loss is 124212.578125\n",
            "in training loop, epoch 2, step 808, the loss is 165537.078125\n",
            "in training loop, epoch 2, step 809, the loss is 155834.0\n",
            "in training loop, epoch 2, step 810, the loss is 174019.625\n",
            "in training loop, epoch 2, step 811, the loss is 217698.53125\n",
            "in training loop, epoch 2, step 812, the loss is 181486.40625\n",
            "in training loop, epoch 2, step 813, the loss is 254111.953125\n",
            "in training loop, epoch 2, step 814, the loss is 94843.859375\n",
            "in training loop, epoch 2, step 815, the loss is 236701.96875\n",
            "in training loop, epoch 2, step 816, the loss is 276233.40625\n",
            "in training loop, epoch 2, step 817, the loss is 134731.5625\n",
            "in training loop, epoch 2, step 818, the loss is 197230.796875\n",
            "in training loop, epoch 2, step 819, the loss is 184760.875\n",
            "in training loop, epoch 2, step 820, the loss is 160730.421875\n",
            "in training loop, epoch 2, step 821, the loss is 228687.0\n",
            "in training loop, epoch 2, step 822, the loss is 253404.09375\n",
            "in training loop, epoch 2, step 823, the loss is 131643.046875\n",
            "in training loop, epoch 2, step 824, the loss is 185623.09375\n",
            "in training loop, epoch 2, step 825, the loss is 114375.765625\n",
            "in training loop, epoch 2, step 826, the loss is 159330.328125\n",
            "in training loop, epoch 2, step 827, the loss is 289962.90625\n",
            "in training loop, epoch 2, step 828, the loss is 163860.84375\n",
            "in training loop, epoch 2, step 829, the loss is 91950.703125\n",
            "in training loop, epoch 2, step 830, the loss is 169933.25\n",
            "in training loop, epoch 2, step 831, the loss is 163332.640625\n",
            "in training loop, epoch 2, step 832, the loss is 156909.890625\n",
            "in training loop, epoch 2, step 833, the loss is 78969.8828125\n",
            "in training loop, epoch 2, step 834, the loss is 145147.34375\n",
            "in training loop, epoch 2, step 835, the loss is 129587.359375\n",
            "in training loop, epoch 2, step 836, the loss is 179405.0\n",
            "in training loop, epoch 2, step 837, the loss is 153055.203125\n",
            "in training loop, epoch 2, step 838, the loss is 94569.1953125\n",
            "in training loop, epoch 2, step 839, the loss is 187859.03125\n",
            "in training loop, epoch 2, step 840, the loss is 143686.578125\n",
            "in training loop, epoch 2, step 841, the loss is 132085.96875\n",
            "in training loop, epoch 2, step 842, the loss is 180930.03125\n",
            "in training loop, epoch 2, step 843, the loss is 173935.015625\n",
            "in training loop, epoch 2, step 844, the loss is 104212.3515625\n",
            "in training loop, epoch 2, step 845, the loss is 169403.671875\n",
            "in training loop, epoch 2, step 846, the loss is 149527.0625\n",
            "in training loop, epoch 2, step 847, the loss is 115946.953125\n",
            "in training loop, epoch 2, step 848, the loss is 208898.0\n",
            "in training loop, epoch 2, step 849, the loss is 116317.2265625\n",
            "in training loop, epoch 2, step 850, the loss is 180340.59375\n",
            "in training loop, epoch 2, step 851, the loss is 85472.6640625\n",
            "in training loop, epoch 2, step 852, the loss is 345008.78125\n",
            "in training loop, epoch 2, step 853, the loss is 106962.7265625\n",
            "in training loop, epoch 2, step 854, the loss is 133695.453125\n",
            "in training loop, epoch 2, step 855, the loss is 179688.1875\n",
            "in training loop, epoch 2, step 856, the loss is 330936.9375\n",
            "in training loop, epoch 2, step 857, the loss is 126834.546875\n",
            "in training loop, epoch 2, step 858, the loss is 151241.734375\n",
            "in training loop, epoch 2, step 859, the loss is 121240.6875\n",
            "in training loop, epoch 2, step 860, the loss is 273654.75\n",
            "in training loop, epoch 2, step 861, the loss is 364320.375\n",
            "in training loop, epoch 2, step 862, the loss is 210688.6875\n",
            "in training loop, epoch 2, step 863, the loss is 220950.703125\n",
            "in training loop, epoch 2, step 864, the loss is 208462.59375\n",
            "in training loop, epoch 2, step 865, the loss is 264266.0625\n",
            "in training loop, epoch 2, step 866, the loss is 260006.234375\n",
            "in training loop, epoch 2, step 867, the loss is 226470.71875\n",
            "in training loop, epoch 2, step 868, the loss is 194111.625\n",
            "in training loop, epoch 2, step 869, the loss is 191352.953125\n",
            "in training loop, epoch 2, step 870, the loss is 128245.3203125\n",
            "in training loop, epoch 2, step 871, the loss is 188649.015625\n",
            "in training loop, epoch 2, step 872, the loss is 156122.96875\n",
            "in training loop, epoch 2, step 873, the loss is 224801.9375\n",
            "in training loop, epoch 2, step 874, the loss is 164789.71875\n",
            "in training loop, epoch 2, step 875, the loss is 223453.0625\n",
            "in training loop, epoch 2, step 876, the loss is 200668.03125\n",
            "in training loop, epoch 2, step 877, the loss is 201833.609375\n",
            "in training loop, epoch 2, step 878, the loss is 209562.40625\n",
            "in training loop, epoch 2, step 879, the loss is 142618.0\n",
            "in training loop, epoch 2, step 880, the loss is 209065.6875\n",
            "in training loop, epoch 2, step 881, the loss is 149012.078125\n",
            "in training loop, epoch 2, step 882, the loss is 173245.96875\n",
            "in training loop, epoch 2, step 883, the loss is 128292.328125\n",
            "in training loop, epoch 2, step 884, the loss is 149916.65625\n",
            "in training loop, epoch 2, step 885, the loss is 204900.78125\n",
            "in training loop, epoch 2, step 886, the loss is 180925.703125\n",
            "in training loop, epoch 2, step 887, the loss is 216358.8125\n",
            "in training loop, epoch 2, step 888, the loss is 156545.625\n",
            "in training loop, epoch 2, step 889, the loss is 159904.859375\n",
            "in training loop, epoch 2, step 890, the loss is 149270.859375\n",
            "in training loop, epoch 2, step 891, the loss is 187560.15625\n",
            "in training loop, epoch 2, step 892, the loss is 184015.546875\n",
            "in training loop, epoch 2, step 893, the loss is 158366.390625\n",
            "in training loop, epoch 2, step 894, the loss is 149887.4375\n",
            "in training loop, epoch 2, step 895, the loss is 275932.03125\n",
            "in training loop, epoch 2, step 896, the loss is 185289.03125\n",
            "in training loop, epoch 2, step 897, the loss is 137851.234375\n",
            "in training loop, epoch 2, step 898, the loss is 128699.390625\n",
            "in training loop, epoch 2, step 899, the loss is 168518.15625\n",
            "in training loop, epoch 2, step 900, the loss is 133314.5\n",
            "in training loop, epoch 2, step 901, the loss is 422365.9375\n",
            "in training loop, epoch 2, step 902, the loss is 164253.21875\n",
            "in training loop, epoch 2, step 903, the loss is 152927.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "k-fold 1:: Epoch 2: train loss 159781.2044688537 val loss 323072.56025680696\n",
            "in training loop, epoch 3, step 0, the loss is 220335.0625\n",
            "in training loop, epoch 3, step 1, the loss is 141907.5\n",
            "in training loop, epoch 3, step 2, the loss is 268811.5625\n",
            "in training loop, epoch 3, step 3, the loss is 116072.7265625\n",
            "in training loop, epoch 3, step 4, the loss is 246253.9375\n",
            "in training loop, epoch 3, step 5, the loss is 280782.84375\n",
            "in training loop, epoch 3, step 6, the loss is 245720.375\n",
            "in training loop, epoch 3, step 7, the loss is 121473.984375\n",
            "in training loop, epoch 3, step 8, the loss is 129293.2421875\n",
            "in training loop, epoch 3, step 9, the loss is 171236.21875\n",
            "in training loop, epoch 3, step 10, the loss is 159473.921875\n",
            "in training loop, epoch 3, step 11, the loss is 156054.625\n",
            "in training loop, epoch 3, step 12, the loss is 159638.09375\n",
            "in training loop, epoch 3, step 13, the loss is 133313.140625\n",
            "in training loop, epoch 3, step 14, the loss is 114563.890625\n",
            "in training loop, epoch 3, step 15, the loss is 250185.046875\n",
            "in training loop, epoch 3, step 16, the loss is 144274.5625\n",
            "in training loop, epoch 3, step 17, the loss is 194216.0\n",
            "in training loop, epoch 3, step 18, the loss is 127938.859375\n",
            "in training loop, epoch 3, step 19, the loss is 114907.21875\n",
            "in training loop, epoch 3, step 20, the loss is 92821.625\n",
            "in training loop, epoch 3, step 21, the loss is 133242.71875\n",
            "in training loop, epoch 3, step 22, the loss is 159520.421875\n",
            "in training loop, epoch 3, step 23, the loss is 109828.6796875\n",
            "in training loop, epoch 3, step 24, the loss is 163699.375\n",
            "in training loop, epoch 3, step 25, the loss is 144810.25\n",
            "in training loop, epoch 3, step 26, the loss is 147409.234375\n",
            "in training loop, epoch 3, step 27, the loss is 115354.578125\n",
            "in training loop, epoch 3, step 28, the loss is 144028.96875\n",
            "in training loop, epoch 3, step 29, the loss is 156509.03125\n",
            "in training loop, epoch 3, step 30, the loss is 117577.4765625\n",
            "in training loop, epoch 3, step 31, the loss is 155780.890625\n",
            "in training loop, epoch 3, step 32, the loss is 215899.640625\n",
            "in training loop, epoch 3, step 33, the loss is 157268.28125\n",
            "in training loop, epoch 3, step 34, the loss is 118730.2109375\n",
            "in training loop, epoch 3, step 35, the loss is 141625.3125\n",
            "in training loop, epoch 3, step 36, the loss is 147905.625\n",
            "in training loop, epoch 3, step 37, the loss is 155127.9375\n",
            "in training loop, epoch 3, step 38, the loss is 99564.140625\n",
            "in training loop, epoch 3, step 39, the loss is 120191.6171875\n",
            "in training loop, epoch 3, step 40, the loss is 214828.71875\n",
            "in training loop, epoch 3, step 41, the loss is 95617.1796875\n",
            "in training loop, epoch 3, step 42, the loss is 179118.984375\n",
            "in training loop, epoch 3, step 43, the loss is 137114.609375\n",
            "in training loop, epoch 3, step 44, the loss is 130100.625\n",
            "in training loop, epoch 3, step 45, the loss is 110254.640625\n",
            "in training loop, epoch 3, step 46, the loss is 116465.65625\n",
            "in training loop, epoch 3, step 47, the loss is 135909.5\n",
            "in training loop, epoch 3, step 48, the loss is 114324.9375\n",
            "in training loop, epoch 3, step 49, the loss is 122420.40625\n",
            "in training loop, epoch 3, step 50, the loss is 178339.34375\n",
            "in training loop, epoch 3, step 51, the loss is 141807.9375\n",
            "in training loop, epoch 3, step 52, the loss is 101457.203125\n",
            "in training loop, epoch 3, step 53, the loss is 221772.1875\n",
            "in training loop, epoch 3, step 54, the loss is 118724.640625\n",
            "in training loop, epoch 3, step 55, the loss is 142983.609375\n",
            "in training loop, epoch 3, step 56, the loss is 186960.96875\n",
            "in training loop, epoch 3, step 57, the loss is 97923.6953125\n",
            "in training loop, epoch 3, step 58, the loss is 164079.65625\n",
            "in training loop, epoch 3, step 59, the loss is 178975.328125\n",
            "in training loop, epoch 3, step 60, the loss is 116967.5078125\n",
            "in training loop, epoch 3, step 61, the loss is 127857.3046875\n",
            "in training loop, epoch 3, step 62, the loss is 115962.4765625\n",
            "in training loop, epoch 3, step 63, the loss is 201166.21875\n",
            "in training loop, epoch 3, step 64, the loss is 143699.828125\n",
            "in training loop, epoch 3, step 65, the loss is 171986.296875\n",
            "in training loop, epoch 3, step 66, the loss is 143901.390625\n",
            "in training loop, epoch 3, step 67, the loss is 137787.3125\n",
            "in training loop, epoch 3, step 68, the loss is 95006.8125\n",
            "in training loop, epoch 3, step 69, the loss is 161826.84375\n",
            "in training loop, epoch 3, step 70, the loss is 455546.625\n",
            "in training loop, epoch 3, step 71, the loss is 112711.8828125\n",
            "in training loop, epoch 3, step 72, the loss is 169669.46875\n",
            "in training loop, epoch 3, step 73, the loss is 243253.234375\n",
            "in training loop, epoch 3, step 74, the loss is 127402.015625\n",
            "in training loop, epoch 3, step 75, the loss is 147875.71875\n",
            "in training loop, epoch 3, step 76, the loss is 209921.875\n",
            "in training loop, epoch 3, step 77, the loss is 137466.4375\n",
            "in training loop, epoch 3, step 78, the loss is 168625.890625\n",
            "in training loop, epoch 3, step 79, the loss is 136605.40625\n",
            "in training loop, epoch 3, step 80, the loss is 246069.5625\n",
            "in training loop, epoch 3, step 81, the loss is 170063.109375\n",
            "in training loop, epoch 3, step 82, the loss is 192857.390625\n",
            "in training loop, epoch 3, step 83, the loss is 225459.984375\n",
            "in training loop, epoch 3, step 84, the loss is 232896.34375\n",
            "in training loop, epoch 3, step 85, the loss is 182796.875\n",
            "in training loop, epoch 3, step 86, the loss is 92727.15625\n",
            "in training loop, epoch 3, step 87, the loss is 132587.96875\n",
            "in training loop, epoch 3, step 88, the loss is 152795.90625\n",
            "in training loop, epoch 3, step 89, the loss is 100713.90625\n",
            "in training loop, epoch 3, step 90, the loss is 160528.546875\n",
            "in training loop, epoch 3, step 91, the loss is 121674.1953125\n",
            "in training loop, epoch 3, step 92, the loss is 192577.625\n",
            "in training loop, epoch 3, step 93, the loss is 120449.3359375\n",
            "in training loop, epoch 3, step 94, the loss is 172182.390625\n",
            "in training loop, epoch 3, step 95, the loss is 130343.1875\n",
            "in training loop, epoch 3, step 96, the loss is 179305.984375\n",
            "in training loop, epoch 3, step 97, the loss is 151324.796875\n",
            "in training loop, epoch 3, step 98, the loss is 185385.8125\n",
            "in training loop, epoch 3, step 99, the loss is 179372.78125\n",
            "in training loop, epoch 3, step 100, the loss is 140607.875\n",
            "in training loop, epoch 3, step 101, the loss is 162743.90625\n",
            "in training loop, epoch 3, step 102, the loss is 151629.515625\n",
            "in training loop, epoch 3, step 103, the loss is 108145.4140625\n",
            "in training loop, epoch 3, step 104, the loss is 193474.71875\n",
            "in training loop, epoch 3, step 105, the loss is 131606.0625\n",
            "in training loop, epoch 3, step 106, the loss is 123590.109375\n",
            "in training loop, epoch 3, step 107, the loss is 182521.0\n",
            "in training loop, epoch 3, step 108, the loss is 133960.265625\n",
            "in training loop, epoch 3, step 109, the loss is 109872.234375\n",
            "in training loop, epoch 3, step 110, the loss is 109730.859375\n",
            "in training loop, epoch 3, step 111, the loss is 172364.078125\n",
            "in training loop, epoch 3, step 112, the loss is 162500.765625\n",
            "in training loop, epoch 3, step 113, the loss is 107481.1640625\n",
            "in training loop, epoch 3, step 114, the loss is 156149.5\n",
            "in training loop, epoch 3, step 115, the loss is 142567.0\n",
            "in training loop, epoch 3, step 116, the loss is 143333.125\n",
            "in training loop, epoch 3, step 117, the loss is 96870.84375\n",
            "in training loop, epoch 3, step 118, the loss is 133574.125\n",
            "in training loop, epoch 3, step 119, the loss is 232203.03125\n",
            "in training loop, epoch 3, step 120, the loss is 106040.84375\n",
            "in training loop, epoch 3, step 121, the loss is 190753.59375\n",
            "in training loop, epoch 3, step 122, the loss is 131476.28125\n",
            "in training loop, epoch 3, step 123, the loss is 144509.875\n",
            "in training loop, epoch 3, step 124, the loss is 114794.015625\n",
            "in training loop, epoch 3, step 125, the loss is 132650.359375\n",
            "in training loop, epoch 3, step 126, the loss is 248915.265625\n",
            "in training loop, epoch 3, step 127, the loss is 129005.171875\n",
            "in training loop, epoch 3, step 128, the loss is 162389.75\n",
            "in training loop, epoch 3, step 129, the loss is 161912.09375\n",
            "in training loop, epoch 3, step 130, the loss is 151575.90625\n",
            "in training loop, epoch 3, step 131, the loss is 124034.59375\n",
            "in training loop, epoch 3, step 132, the loss is 119772.25\n",
            "in training loop, epoch 3, step 133, the loss is 101941.28125\n",
            "in training loop, epoch 3, step 134, the loss is 141858.75\n",
            "in training loop, epoch 3, step 135, the loss is 162045.03125\n",
            "in training loop, epoch 3, step 136, the loss is 171975.625\n",
            "in training loop, epoch 3, step 137, the loss is 139959.875\n",
            "in training loop, epoch 3, step 138, the loss is 153299.15625\n",
            "in training loop, epoch 3, step 139, the loss is 138595.15625\n",
            "in training loop, epoch 3, step 140, the loss is 118628.9765625\n",
            "in training loop, epoch 3, step 141, the loss is 100752.8828125\n",
            "in training loop, epoch 3, step 142, the loss is 163870.84375\n",
            "in training loop, epoch 3, step 143, the loss is 124773.7890625\n",
            "in training loop, epoch 3, step 144, the loss is 125791.0703125\n",
            "in training loop, epoch 3, step 145, the loss is 147024.890625\n",
            "in training loop, epoch 3, step 146, the loss is 135545.578125\n",
            "in training loop, epoch 3, step 147, the loss is 115697.3828125\n",
            "in training loop, epoch 3, step 148, the loss is 145218.484375\n",
            "in training loop, epoch 3, step 149, the loss is 163274.765625\n",
            "in training loop, epoch 3, step 150, the loss is 122050.1015625\n",
            "in training loop, epoch 3, step 151, the loss is 130684.546875\n",
            "in training loop, epoch 3, step 152, the loss is 131091.0625\n",
            "in training loop, epoch 3, step 153, the loss is 146985.390625\n",
            "in training loop, epoch 3, step 154, the loss is 111773.953125\n",
            "in training loop, epoch 3, step 155, the loss is 142232.921875\n",
            "in training loop, epoch 3, step 156, the loss is 94579.203125\n",
            "in training loop, epoch 3, step 157, the loss is 101270.1484375\n",
            "in training loop, epoch 3, step 158, the loss is 132991.390625\n",
            "in training loop, epoch 3, step 159, the loss is 109526.8828125\n",
            "in training loop, epoch 3, step 160, the loss is 129106.015625\n",
            "in training loop, epoch 3, step 161, the loss is 155542.203125\n",
            "in training loop, epoch 3, step 162, the loss is 100191.703125\n",
            "in training loop, epoch 3, step 163, the loss is 124126.15625\n",
            "in training loop, epoch 3, step 164, the loss is 116506.0234375\n",
            "in training loop, epoch 3, step 165, the loss is 202841.75\n",
            "in training loop, epoch 3, step 166, the loss is 123611.734375\n",
            "in training loop, epoch 3, step 167, the loss is 165978.046875\n",
            "in training loop, epoch 3, step 168, the loss is 122129.046875\n",
            "in training loop, epoch 3, step 169, the loss is 120499.4296875\n",
            "in training loop, epoch 3, step 170, the loss is 216871.40625\n",
            "in training loop, epoch 3, step 171, the loss is 99239.375\n",
            "in training loop, epoch 3, step 172, the loss is 123927.1953125\n",
            "in training loop, epoch 3, step 173, the loss is 174882.6875\n",
            "in training loop, epoch 3, step 174, the loss is 109979.90625\n",
            "in training loop, epoch 3, step 175, the loss is 172856.875\n",
            "in training loop, epoch 3, step 176, the loss is 123451.78125\n",
            "in training loop, epoch 3, step 177, the loss is 149744.859375\n",
            "in training loop, epoch 3, step 178, the loss is 140221.96875\n",
            "in training loop, epoch 3, step 179, the loss is 97779.9921875\n",
            "in training loop, epoch 3, step 180, the loss is 144971.390625\n",
            "in training loop, epoch 3, step 181, the loss is 104349.8671875\n",
            "in training loop, epoch 3, step 182, the loss is 132684.4375\n",
            "in training loop, epoch 3, step 183, the loss is 122700.296875\n",
            "in training loop, epoch 3, step 184, the loss is 89464.9140625\n",
            "in training loop, epoch 3, step 185, the loss is 126230.2734375\n",
            "in training loop, epoch 3, step 186, the loss is 175376.78125\n",
            "in training loop, epoch 3, step 187, the loss is 138284.296875\n",
            "in training loop, epoch 3, step 188, the loss is 126705.65625\n",
            "in training loop, epoch 3, step 189, the loss is 77703.0859375\n",
            "in training loop, epoch 3, step 190, the loss is 165138.234375\n",
            "in training loop, epoch 3, step 191, the loss is 120189.0078125\n",
            "in training loop, epoch 3, step 192, the loss is 140143.546875\n",
            "in training loop, epoch 3, step 193, the loss is 125155.75\n",
            "in training loop, epoch 3, step 194, the loss is 135777.109375\n",
            "in training loop, epoch 3, step 195, the loss is 138125.515625\n",
            "in training loop, epoch 3, step 196, the loss is 91270.125\n",
            "in training loop, epoch 3, step 197, the loss is 138929.21875\n",
            "in training loop, epoch 3, step 198, the loss is 128296.671875\n",
            "in training loop, epoch 3, step 199, the loss is 195690.78125\n",
            "in training loop, epoch 3, step 200, the loss is 164387.59375\n",
            "in training loop, epoch 3, step 201, the loss is 151687.703125\n",
            "in training loop, epoch 3, step 202, the loss is 129558.0390625\n",
            "in training loop, epoch 3, step 203, the loss is 119358.5625\n",
            "in training loop, epoch 3, step 204, the loss is 102764.1953125\n",
            "in training loop, epoch 3, step 205, the loss is 135299.625\n",
            "in training loop, epoch 3, step 206, the loss is 111202.5\n",
            "in training loop, epoch 3, step 207, the loss is 90031.6328125\n",
            "in training loop, epoch 3, step 208, the loss is 123244.34375\n",
            "in training loop, epoch 3, step 209, the loss is 140323.8125\n",
            "in training loop, epoch 3, step 210, the loss is 119723.8359375\n",
            "in training loop, epoch 3, step 211, the loss is 134113.109375\n",
            "in training loop, epoch 3, step 212, the loss is 181515.546875\n",
            "in training loop, epoch 3, step 213, the loss is 128001.6328125\n",
            "in training loop, epoch 3, step 214, the loss is 122684.4296875\n",
            "in training loop, epoch 3, step 215, the loss is 189116.609375\n",
            "in training loop, epoch 3, step 216, the loss is 120866.359375\n",
            "in training loop, epoch 3, step 217, the loss is 128823.3828125\n",
            "in training loop, epoch 3, step 218, the loss is 106755.140625\n",
            "in training loop, epoch 3, step 219, the loss is 132012.953125\n",
            "in training loop, epoch 3, step 220, the loss is 176455.015625\n",
            "in training loop, epoch 3, step 221, the loss is 92241.140625\n",
            "in training loop, epoch 3, step 222, the loss is 150265.5625\n",
            "in training loop, epoch 3, step 223, the loss is 129395.984375\n",
            "in training loop, epoch 3, step 224, the loss is 107040.28125\n",
            "in training loop, epoch 3, step 225, the loss is 114473.7578125\n",
            "in training loop, epoch 3, step 226, the loss is 135412.1875\n",
            "in training loop, epoch 3, step 227, the loss is 121802.6484375\n",
            "in training loop, epoch 3, step 228, the loss is 86551.6328125\n",
            "in training loop, epoch 3, step 229, the loss is 129054.34375\n",
            "in training loop, epoch 3, step 230, the loss is 137901.8125\n",
            "in training loop, epoch 3, step 231, the loss is 134524.8125\n",
            "in training loop, epoch 3, step 232, the loss is 111271.71875\n",
            "in training loop, epoch 3, step 233, the loss is 167929.78125\n",
            "in training loop, epoch 3, step 234, the loss is 87239.4765625\n",
            "in training loop, epoch 3, step 235, the loss is 125267.0\n",
            "in training loop, epoch 3, step 236, the loss is 99411.0546875\n",
            "in training loop, epoch 3, step 237, the loss is 147270.28125\n",
            "in training loop, epoch 3, step 238, the loss is 122936.7734375\n",
            "in training loop, epoch 3, step 239, the loss is 134744.921875\n",
            "in training loop, epoch 3, step 240, the loss is 134452.3125\n",
            "in training loop, epoch 3, step 241, the loss is 115708.0078125\n",
            "in training loop, epoch 3, step 242, the loss is 118742.96875\n",
            "in training loop, epoch 3, step 243, the loss is 136987.296875\n",
            "in training loop, epoch 3, step 244, the loss is 119624.5234375\n",
            "in training loop, epoch 3, step 245, the loss is 185688.140625\n",
            "in training loop, epoch 3, step 246, the loss is 132780.859375\n",
            "in training loop, epoch 3, step 247, the loss is 132333.875\n",
            "in training loop, epoch 3, step 248, the loss is 184328.890625\n",
            "in training loop, epoch 3, step 249, the loss is 157892.328125\n",
            "in training loop, epoch 3, step 250, the loss is 146029.84375\n",
            "in training loop, epoch 3, step 251, the loss is 110826.0390625\n",
            "in training loop, epoch 3, step 252, the loss is 192327.59375\n",
            "in training loop, epoch 3, step 253, the loss is 199462.8125\n",
            "in training loop, epoch 3, step 254, the loss is 112209.859375\n",
            "in training loop, epoch 3, step 255, the loss is 109171.46875\n",
            "in training loop, epoch 3, step 256, the loss is 146523.890625\n",
            "in training loop, epoch 3, step 257, the loss is 128736.28125\n",
            "in training loop, epoch 3, step 258, the loss is 95836.21875\n",
            "in training loop, epoch 3, step 259, the loss is 164640.0\n",
            "in training loop, epoch 3, step 260, the loss is 105067.671875\n",
            "in training loop, epoch 3, step 261, the loss is 133815.21875\n",
            "in training loop, epoch 3, step 262, the loss is 208160.75\n",
            "in training loop, epoch 3, step 263, the loss is 106202.1953125\n",
            "in training loop, epoch 3, step 264, the loss is 120406.140625\n",
            "in training loop, epoch 3, step 265, the loss is 108061.4921875\n",
            "in training loop, epoch 3, step 266, the loss is 135014.625\n",
            "in training loop, epoch 3, step 267, the loss is 144126.84375\n",
            "in training loop, epoch 3, step 268, the loss is 112111.234375\n",
            "in training loop, epoch 3, step 269, the loss is 124072.2265625\n",
            "in training loop, epoch 3, step 270, the loss is 107475.03125\n",
            "in training loop, epoch 3, step 271, the loss is 139155.90625\n",
            "in training loop, epoch 3, step 272, the loss is 146898.0625\n",
            "in training loop, epoch 3, step 273, the loss is 125417.5625\n",
            "in training loop, epoch 3, step 274, the loss is 131487.859375\n",
            "in training loop, epoch 3, step 275, the loss is 121817.5078125\n",
            "in training loop, epoch 3, step 276, the loss is 203670.75\n",
            "in training loop, epoch 3, step 277, the loss is 173105.09375\n",
            "in training loop, epoch 3, step 278, the loss is 143081.6875\n",
            "in training loop, epoch 3, step 279, the loss is 138608.546875\n",
            "in training loop, epoch 3, step 280, the loss is 127920.390625\n",
            "in training loop, epoch 3, step 281, the loss is 140279.859375\n",
            "in training loop, epoch 3, step 282, the loss is 179021.671875\n",
            "in training loop, epoch 3, step 283, the loss is 142908.671875\n",
            "in training loop, epoch 3, step 284, the loss is 115627.84375\n",
            "in training loop, epoch 3, step 285, the loss is 215314.765625\n",
            "in training loop, epoch 3, step 286, the loss is 123096.2734375\n",
            "in training loop, epoch 3, step 287, the loss is 153707.328125\n",
            "in training loop, epoch 3, step 288, the loss is 154346.3125\n",
            "in training loop, epoch 3, step 289, the loss is 187519.21875\n",
            "in training loop, epoch 3, step 290, the loss is 151780.5\n",
            "in training loop, epoch 3, step 291, the loss is 186752.5\n",
            "in training loop, epoch 3, step 292, the loss is 134081.609375\n",
            "in training loop, epoch 3, step 293, the loss is 171558.171875\n",
            "in training loop, epoch 3, step 294, the loss is 91730.796875\n",
            "in training loop, epoch 3, step 295, the loss is 189718.828125\n",
            "in training loop, epoch 3, step 296, the loss is 133804.1875\n",
            "in training loop, epoch 3, step 297, the loss is 120545.8046875\n",
            "in training loop, epoch 3, step 298, the loss is 106259.34375\n",
            "in training loop, epoch 3, step 299, the loss is 114784.1484375\n",
            "in training loop, epoch 3, step 300, the loss is 95566.3671875\n",
            "in training loop, epoch 3, step 301, the loss is 109140.265625\n",
            "in training loop, epoch 3, step 302, the loss is 178469.921875\n",
            "in training loop, epoch 3, step 303, the loss is 144173.78125\n",
            "in training loop, epoch 3, step 304, the loss is 153805.21875\n",
            "in training loop, epoch 3, step 305, the loss is 134822.375\n",
            "in training loop, epoch 3, step 306, the loss is 174669.0625\n",
            "in training loop, epoch 3, step 307, the loss is 93250.0703125\n",
            "in training loop, epoch 3, step 308, the loss is 169838.859375\n",
            "in training loop, epoch 3, step 309, the loss is 180401.53125\n",
            "in training loop, epoch 3, step 310, the loss is 134970.890625\n",
            "in training loop, epoch 3, step 311, the loss is 173573.703125\n",
            "in training loop, epoch 3, step 312, the loss is 151822.609375\n",
            "in training loop, epoch 3, step 313, the loss is 118846.5859375\n",
            "in training loop, epoch 3, step 314, the loss is 153549.71875\n",
            "in training loop, epoch 3, step 315, the loss is 75912.5\n",
            "in training loop, epoch 3, step 316, the loss is 213011.609375\n",
            "in training loop, epoch 3, step 317, the loss is 197264.171875\n",
            "in training loop, epoch 3, step 318, the loss is 129500.6875\n",
            "in training loop, epoch 3, step 319, the loss is 113489.1015625\n",
            "in training loop, epoch 3, step 320, the loss is 140732.0625\n",
            "in training loop, epoch 3, step 321, the loss is 195335.046875\n",
            "in training loop, epoch 3, step 322, the loss is 194813.734375\n",
            "in training loop, epoch 3, step 323, the loss is 161846.359375\n",
            "in training loop, epoch 3, step 324, the loss is 198550.75\n",
            "in training loop, epoch 3, step 325, the loss is 165005.828125\n",
            "in training loop, epoch 3, step 326, the loss is 182923.328125\n",
            "in training loop, epoch 3, step 327, the loss is 168968.5\n",
            "in training loop, epoch 3, step 328, the loss is 217767.609375\n",
            "in training loop, epoch 3, step 329, the loss is 157182.359375\n",
            "in training loop, epoch 3, step 330, the loss is 102506.328125\n",
            "in training loop, epoch 3, step 331, the loss is 144282.171875\n",
            "in training loop, epoch 3, step 332, the loss is 164908.9375\n",
            "in training loop, epoch 3, step 333, the loss is 115937.0390625\n",
            "in training loop, epoch 3, step 334, the loss is 145005.84375\n",
            "in training loop, epoch 3, step 335, the loss is 130258.1171875\n",
            "in training loop, epoch 3, step 336, the loss is 139471.75\n",
            "in training loop, epoch 3, step 337, the loss is 159271.359375\n",
            "in training loop, epoch 3, step 338, the loss is 173182.78125\n",
            "in training loop, epoch 3, step 339, the loss is 183165.0625\n",
            "in training loop, epoch 3, step 340, the loss is 168478.34375\n",
            "in training loop, epoch 3, step 341, the loss is 150404.203125\n",
            "in training loop, epoch 3, step 342, the loss is 94320.25\n",
            "in training loop, epoch 3, step 343, the loss is 141486.3125\n",
            "in training loop, epoch 3, step 344, the loss is 123667.4765625\n",
            "in training loop, epoch 3, step 345, the loss is 145656.984375\n",
            "in training loop, epoch 3, step 346, the loss is 202678.484375\n",
            "in training loop, epoch 3, step 347, the loss is 125461.328125\n",
            "in training loop, epoch 3, step 348, the loss is 157281.078125\n",
            "in training loop, epoch 3, step 349, the loss is 126943.078125\n",
            "in training loop, epoch 3, step 350, the loss is 196090.578125\n",
            "in training loop, epoch 3, step 351, the loss is 106848.15625\n",
            "in training loop, epoch 3, step 352, the loss is 133985.390625\n",
            "in training loop, epoch 3, step 353, the loss is 129413.9609375\n",
            "in training loop, epoch 3, step 354, the loss is 158508.859375\n",
            "in training loop, epoch 3, step 355, the loss is 199626.421875\n",
            "in training loop, epoch 3, step 356, the loss is 177108.359375\n",
            "in training loop, epoch 3, step 357, the loss is 186292.9375\n",
            "in training loop, epoch 3, step 358, the loss is 116678.4765625\n",
            "in training loop, epoch 3, step 359, the loss is 131545.0625\n",
            "in training loop, epoch 3, step 360, the loss is 156021.234375\n",
            "in training loop, epoch 3, step 361, the loss is 154009.5625\n",
            "in training loop, epoch 3, step 362, the loss is 138348.625\n",
            "in training loop, epoch 3, step 363, the loss is 107396.4375\n",
            "in training loop, epoch 3, step 364, the loss is 181409.4375\n",
            "in training loop, epoch 3, step 365, the loss is 219032.1875\n",
            "in training loop, epoch 3, step 366, the loss is 149109.0625\n",
            "in training loop, epoch 3, step 367, the loss is 70293.53125\n",
            "in training loop, epoch 3, step 368, the loss is 157967.34375\n",
            "in training loop, epoch 3, step 369, the loss is 200846.921875\n",
            "in training loop, epoch 3, step 370, the loss is 90662.171875\n",
            "in training loop, epoch 3, step 371, the loss is 222691.171875\n",
            "in training loop, epoch 3, step 372, the loss is 179061.96875\n",
            "in training loop, epoch 3, step 373, the loss is 145535.46875\n",
            "in training loop, epoch 3, step 374, the loss is 109093.0703125\n",
            "in training loop, epoch 3, step 375, the loss is 223088.1875\n",
            "in training loop, epoch 3, step 376, the loss is 189379.21875\n",
            "in training loop, epoch 3, step 377, the loss is 159994.734375\n",
            "in training loop, epoch 3, step 378, the loss is 108337.6953125\n",
            "in training loop, epoch 3, step 379, the loss is 87661.7890625\n",
            "in training loop, epoch 3, step 380, the loss is 142686.53125\n",
            "in training loop, epoch 3, step 381, the loss is 140869.3125\n",
            "in training loop, epoch 3, step 382, the loss is 166960.046875\n",
            "in training loop, epoch 3, step 383, the loss is 131671.921875\n",
            "in training loop, epoch 3, step 384, the loss is 100890.5859375\n",
            "in training loop, epoch 3, step 385, the loss is 139385.421875\n",
            "in training loop, epoch 3, step 386, the loss is 116335.5703125\n",
            "in training loop, epoch 3, step 387, the loss is 136925.34375\n",
            "in training loop, epoch 3, step 388, the loss is 134036.140625\n",
            "in training loop, epoch 3, step 389, the loss is 89802.5546875\n",
            "in training loop, epoch 3, step 390, the loss is 120266.9140625\n",
            "in training loop, epoch 3, step 391, the loss is 117869.109375\n",
            "in training loop, epoch 3, step 392, the loss is 138031.359375\n",
            "in training loop, epoch 3, step 393, the loss is 96746.0625\n",
            "in training loop, epoch 3, step 394, the loss is 176665.640625\n",
            "in training loop, epoch 3, step 395, the loss is 118947.40625\n",
            "in training loop, epoch 3, step 396, the loss is 115789.609375\n",
            "in training loop, epoch 3, step 397, the loss is 143053.546875\n",
            "in training loop, epoch 3, step 398, the loss is 83837.125\n",
            "in training loop, epoch 3, step 399, the loss is 136755.09375\n",
            "in training loop, epoch 3, step 400, the loss is 108768.4375\n",
            "in training loop, epoch 3, step 401, the loss is 147136.40625\n",
            "in training loop, epoch 3, step 402, the loss is 220843.078125\n",
            "in training loop, epoch 3, step 403, the loss is 150054.21875\n",
            "in training loop, epoch 3, step 404, the loss is 151826.0625\n",
            "in training loop, epoch 3, step 405, the loss is 133434.875\n",
            "in training loop, epoch 3, step 406, the loss is 199260.71875\n",
            "in training loop, epoch 3, step 407, the loss is 130111.5\n",
            "in training loop, epoch 3, step 408, the loss is 236169.15625\n",
            "in training loop, epoch 3, step 409, the loss is 166043.375\n",
            "in training loop, epoch 3, step 410, the loss is 159159.0\n",
            "in training loop, epoch 3, step 411, the loss is 146441.734375\n",
            "in training loop, epoch 3, step 412, the loss is 146726.84375\n",
            "in training loop, epoch 3, step 413, the loss is 131287.859375\n",
            "in training loop, epoch 3, step 414, the loss is 122662.2578125\n",
            "in training loop, epoch 3, step 415, the loss is 102096.5\n",
            "in training loop, epoch 3, step 416, the loss is 171011.0625\n",
            "in training loop, epoch 3, step 417, the loss is 155745.046875\n",
            "in training loop, epoch 3, step 418, the loss is 139578.40625\n",
            "in training loop, epoch 3, step 419, the loss is 109130.7734375\n",
            "in training loop, epoch 3, step 420, the loss is 137642.484375\n",
            "in training loop, epoch 3, step 421, the loss is 155101.84375\n",
            "in training loop, epoch 3, step 422, the loss is 116651.53125\n",
            "in training loop, epoch 3, step 423, the loss is 139499.953125\n",
            "in training loop, epoch 3, step 424, the loss is 154144.65625\n",
            "in training loop, epoch 3, step 425, the loss is 162529.5\n",
            "in training loop, epoch 3, step 426, the loss is 112485.34375\n",
            "in training loop, epoch 3, step 427, the loss is 120749.390625\n",
            "in training loop, epoch 3, step 428, the loss is 131144.171875\n",
            "in training loop, epoch 3, step 429, the loss is 107822.0625\n",
            "in training loop, epoch 3, step 430, the loss is 104047.25\n",
            "in training loop, epoch 3, step 431, the loss is 143949.59375\n",
            "in training loop, epoch 3, step 432, the loss is 163221.859375\n",
            "in training loop, epoch 3, step 433, the loss is 113637.328125\n",
            "in training loop, epoch 3, step 434, the loss is 127705.4140625\n",
            "in training loop, epoch 3, step 435, the loss is 114700.5234375\n",
            "in training loop, epoch 3, step 436, the loss is 152545.984375\n",
            "in training loop, epoch 3, step 437, the loss is 90371.921875\n",
            "in training loop, epoch 3, step 438, the loss is 123784.953125\n",
            "in training loop, epoch 3, step 439, the loss is 117703.8125\n",
            "in training loop, epoch 3, step 440, the loss is 97823.2421875\n",
            "in training loop, epoch 3, step 441, the loss is 87024.671875\n",
            "in training loop, epoch 3, step 442, the loss is 145719.84375\n",
            "in training loop, epoch 3, step 443, the loss is 161651.40625\n",
            "in training loop, epoch 3, step 444, the loss is 155790.171875\n",
            "in training loop, epoch 3, step 445, the loss is 120681.625\n",
            "in training loop, epoch 3, step 446, the loss is 115577.65625\n",
            "in training loop, epoch 3, step 447, the loss is 136613.96875\n",
            "in training loop, epoch 3, step 448, the loss is 127200.3359375\n",
            "in training loop, epoch 3, step 449, the loss is 176829.546875\n",
            "in training loop, epoch 3, step 450, the loss is 144024.828125\n",
            "in training loop, epoch 3, step 451, the loss is 150087.703125\n",
            "in training loop, epoch 3, step 452, the loss is 173654.65625\n",
            "in training loop, epoch 3, step 453, the loss is 131604.546875\n",
            "in training loop, epoch 3, step 454, the loss is 101406.7265625\n",
            "in training loop, epoch 3, step 455, the loss is 110343.6875\n",
            "in training loop, epoch 3, step 456, the loss is 150655.46875\n",
            "in training loop, epoch 3, step 457, the loss is 131703.1875\n",
            "in training loop, epoch 3, step 458, the loss is 164161.921875\n",
            "in training loop, epoch 3, step 459, the loss is 143203.046875\n",
            "in training loop, epoch 3, step 460, the loss is 125167.140625\n",
            "in training loop, epoch 3, step 461, the loss is 168644.765625\n",
            "in training loop, epoch 3, step 462, the loss is 135319.109375\n",
            "in training loop, epoch 3, step 463, the loss is 174119.3125\n",
            "in training loop, epoch 3, step 464, the loss is 145672.171875\n",
            "in training loop, epoch 3, step 465, the loss is 166839.46875\n",
            "in training loop, epoch 3, step 466, the loss is 126274.59375\n",
            "in training loop, epoch 3, step 467, the loss is 136827.421875\n",
            "in training loop, epoch 3, step 468, the loss is 130524.734375\n",
            "in training loop, epoch 3, step 469, the loss is 135354.421875\n",
            "in training loop, epoch 3, step 470, the loss is 129688.703125\n",
            "in training loop, epoch 3, step 471, the loss is 168465.53125\n",
            "in training loop, epoch 3, step 472, the loss is 107820.4921875\n",
            "in training loop, epoch 3, step 473, the loss is 151243.625\n",
            "in training loop, epoch 3, step 474, the loss is 129970.8125\n",
            "in training loop, epoch 3, step 475, the loss is 128439.71875\n",
            "in training loop, epoch 3, step 476, the loss is 138610.5625\n",
            "in training loop, epoch 3, step 477, the loss is 184231.359375\n",
            "in training loop, epoch 3, step 478, the loss is 164077.515625\n",
            "in training loop, epoch 3, step 479, the loss is 151867.140625\n",
            "in training loop, epoch 3, step 480, the loss is 142264.109375\n",
            "in training loop, epoch 3, step 481, the loss is 194344.765625\n",
            "in training loop, epoch 3, step 482, the loss is 156295.21875\n",
            "in training loop, epoch 3, step 483, the loss is 193778.734375\n",
            "in training loop, epoch 3, step 484, the loss is 137946.90625\n",
            "in training loop, epoch 3, step 485, the loss is 169355.8125\n",
            "in training loop, epoch 3, step 486, the loss is 147876.03125\n",
            "in training loop, epoch 3, step 487, the loss is 145505.015625\n",
            "in training loop, epoch 3, step 488, the loss is 74949.34375\n",
            "in training loop, epoch 3, step 489, the loss is 195280.515625\n",
            "in training loop, epoch 3, step 490, the loss is 138207.6875\n",
            "in training loop, epoch 3, step 491, the loss is 184592.015625\n",
            "in training loop, epoch 3, step 492, the loss is 193542.046875\n",
            "in training loop, epoch 3, step 493, the loss is 180521.59375\n",
            "in training loop, epoch 3, step 494, the loss is 138223.765625\n",
            "in training loop, epoch 3, step 495, the loss is 126993.546875\n",
            "in training loop, epoch 3, step 496, the loss is 160311.8125\n",
            "in training loop, epoch 3, step 497, the loss is 141422.34375\n",
            "in training loop, epoch 3, step 498, the loss is 177342.671875\n",
            "in training loop, epoch 3, step 499, the loss is 89547.890625\n",
            "in training loop, epoch 3, step 500, the loss is 127139.59375\n",
            "in training loop, epoch 3, step 501, the loss is 148278.125\n",
            "in training loop, epoch 3, step 502, the loss is 141891.34375\n",
            "in training loop, epoch 3, step 503, the loss is 128979.90625\n",
            "in training loop, epoch 3, step 504, the loss is 130058.359375\n",
            "in training loop, epoch 3, step 505, the loss is 119853.5625\n",
            "in training loop, epoch 3, step 506, the loss is 94429.4609375\n",
            "in training loop, epoch 3, step 507, the loss is 146868.9375\n",
            "in training loop, epoch 3, step 508, the loss is 122101.4140625\n",
            "in training loop, epoch 3, step 509, the loss is 131590.75\n",
            "in training loop, epoch 3, step 510, the loss is 95401.375\n",
            "in training loop, epoch 3, step 511, the loss is 152269.890625\n",
            "in training loop, epoch 3, step 512, the loss is 117530.25\n",
            "in training loop, epoch 3, step 513, the loss is 152512.625\n",
            "in training loop, epoch 3, step 514, the loss is 115498.625\n",
            "in training loop, epoch 3, step 515, the loss is 118498.5546875\n",
            "in training loop, epoch 3, step 516, the loss is 315719.75\n",
            "in training loop, epoch 3, step 517, the loss is 103874.328125\n",
            "in training loop, epoch 3, step 518, the loss is 164548.578125\n",
            "in training loop, epoch 3, step 519, the loss is 138089.046875\n",
            "in training loop, epoch 3, step 520, the loss is 177621.125\n",
            "in training loop, epoch 3, step 521, the loss is 155671.5625\n",
            "in training loop, epoch 3, step 522, the loss is 173211.3125\n",
            "in training loop, epoch 3, step 523, the loss is 316070.65625\n",
            "in training loop, epoch 3, step 524, the loss is 181065.125\n",
            "in training loop, epoch 3, step 525, the loss is 152230.1875\n",
            "in training loop, epoch 3, step 526, the loss is 137559.234375\n",
            "in training loop, epoch 3, step 527, the loss is 154805.53125\n",
            "in training loop, epoch 3, step 528, the loss is 222704.953125\n",
            "in training loop, epoch 3, step 529, the loss is 140884.078125\n",
            "in training loop, epoch 3, step 530, the loss is 240405.984375\n",
            "in training loop, epoch 3, step 531, the loss is 190186.34375\n",
            "in training loop, epoch 3, step 532, the loss is 147018.09375\n",
            "in training loop, epoch 3, step 533, the loss is 110327.234375\n",
            "in training loop, epoch 3, step 534, the loss is 174844.0625\n",
            "in training loop, epoch 3, step 535, the loss is 163262.3125\n",
            "in training loop, epoch 3, step 536, the loss is 206799.46875\n",
            "in training loop, epoch 3, step 537, the loss is 176639.125\n",
            "in training loop, epoch 3, step 538, the loss is 195788.921875\n",
            "in training loop, epoch 3, step 539, the loss is 134743.375\n",
            "in training loop, epoch 3, step 540, the loss is 95689.1640625\n",
            "in training loop, epoch 3, step 541, the loss is 117210.9140625\n",
            "in training loop, epoch 3, step 542, the loss is 175227.03125\n",
            "in training loop, epoch 3, step 543, the loss is 139674.125\n",
            "in training loop, epoch 3, step 544, the loss is 136449.84375\n",
            "in training loop, epoch 3, step 545, the loss is 132408.84375\n",
            "in training loop, epoch 3, step 546, the loss is 433597.53125\n",
            "in training loop, epoch 3, step 547, the loss is 192253.0\n",
            "in training loop, epoch 3, step 548, the loss is 141580.8125\n",
            "in training loop, epoch 3, step 549, the loss is 177773.96875\n",
            "in training loop, epoch 3, step 550, the loss is 257351.65625\n",
            "in training loop, epoch 3, step 551, the loss is 205224.390625\n",
            "in training loop, epoch 3, step 552, the loss is 113983.796875\n",
            "in training loop, epoch 3, step 553, the loss is 160941.78125\n",
            "in training loop, epoch 3, step 554, the loss is 177290.953125\n",
            "in training loop, epoch 3, step 555, the loss is 176497.890625\n",
            "in training loop, epoch 3, step 556, the loss is 115748.765625\n",
            "in training loop, epoch 3, step 557, the loss is 163019.265625\n",
            "in training loop, epoch 3, step 558, the loss is 133094.15625\n",
            "in training loop, epoch 3, step 559, the loss is 159838.015625\n",
            "in training loop, epoch 3, step 560, the loss is 163120.0\n",
            "in training loop, epoch 3, step 561, the loss is 148314.21875\n",
            "in training loop, epoch 3, step 562, the loss is 136374.140625\n",
            "in training loop, epoch 3, step 563, the loss is 175769.625\n",
            "in training loop, epoch 3, step 564, the loss is 120616.0390625\n",
            "in training loop, epoch 3, step 565, the loss is 132386.9375\n",
            "in training loop, epoch 3, step 566, the loss is 131657.65625\n",
            "in training loop, epoch 3, step 567, the loss is 126964.8359375\n",
            "in training loop, epoch 3, step 568, the loss is 143398.515625\n",
            "in training loop, epoch 3, step 569, the loss is 135231.359375\n",
            "in training loop, epoch 3, step 570, the loss is 133345.5\n",
            "in training loop, epoch 3, step 571, the loss is 114933.7265625\n",
            "in training loop, epoch 3, step 572, the loss is 154961.359375\n",
            "in training loop, epoch 3, step 573, the loss is 136596.046875\n",
            "in training loop, epoch 3, step 574, the loss is 189133.953125\n",
            "in training loop, epoch 3, step 575, the loss is 139292.921875\n",
            "in training loop, epoch 3, step 576, the loss is 169561.703125\n",
            "in training loop, epoch 3, step 577, the loss is 146042.78125\n",
            "in training loop, epoch 3, step 578, the loss is 139855.484375\n",
            "in training loop, epoch 3, step 579, the loss is 107587.375\n",
            "in training loop, epoch 3, step 580, the loss is 180065.890625\n",
            "in training loop, epoch 3, step 581, the loss is 84865.625\n",
            "in training loop, epoch 3, step 582, the loss is 127913.15625\n",
            "in training loop, epoch 3, step 583, the loss is 174626.984375\n",
            "in training loop, epoch 3, step 584, the loss is 120092.0234375\n",
            "in training loop, epoch 3, step 585, the loss is 102923.4140625\n",
            "in training loop, epoch 3, step 586, the loss is 111160.4453125\n",
            "in training loop, epoch 3, step 587, the loss is 105583.90625\n",
            "in training loop, epoch 3, step 588, the loss is 169538.78125\n",
            "in training loop, epoch 3, step 589, the loss is 170474.59375\n",
            "in training loop, epoch 3, step 590, the loss is 124304.328125\n",
            "in training loop, epoch 3, step 591, the loss is 105431.7265625\n",
            "in training loop, epoch 3, step 592, the loss is 296522.0\n",
            "in training loop, epoch 3, step 593, the loss is 114819.890625\n",
            "in training loop, epoch 3, step 594, the loss is 145397.046875\n",
            "in training loop, epoch 3, step 595, the loss is 130154.8046875\n",
            "in training loop, epoch 3, step 596, the loss is 104636.515625\n",
            "in training loop, epoch 3, step 597, the loss is 110113.9765625\n",
            "in training loop, epoch 3, step 598, the loss is 136284.1875\n",
            "in training loop, epoch 3, step 599, the loss is 123957.015625\n",
            "in training loop, epoch 3, step 600, the loss is 136209.640625\n",
            "in training loop, epoch 3, step 601, the loss is 132859.640625\n",
            "in training loop, epoch 3, step 602, the loss is 111538.90625\n",
            "in training loop, epoch 3, step 603, the loss is 98825.640625\n",
            "in training loop, epoch 3, step 604, the loss is 85732.046875\n",
            "in training loop, epoch 3, step 605, the loss is 197664.28125\n",
            "in training loop, epoch 3, step 606, the loss is 177935.28125\n",
            "in training loop, epoch 3, step 607, the loss is 115669.234375\n",
            "in training loop, epoch 3, step 608, the loss is 119111.203125\n",
            "in training loop, epoch 3, step 609, the loss is 153289.0\n",
            "in training loop, epoch 3, step 610, the loss is 170232.96875\n",
            "in training loop, epoch 3, step 611, the loss is 174974.84375\n",
            "in training loop, epoch 3, step 612, the loss is 157812.703125\n",
            "in training loop, epoch 3, step 613, the loss is 150952.78125\n",
            "in training loop, epoch 3, step 614, the loss is 135493.46875\n",
            "in training loop, epoch 3, step 615, the loss is 162286.125\n",
            "in training loop, epoch 3, step 616, the loss is 139143.9375\n",
            "in training loop, epoch 3, step 617, the loss is 131486.328125\n",
            "in training loop, epoch 3, step 618, the loss is 131665.84375\n",
            "in training loop, epoch 3, step 619, the loss is 114249.6484375\n",
            "in training loop, epoch 3, step 620, the loss is 161006.953125\n",
            "in training loop, epoch 3, step 621, the loss is 130755.6171875\n",
            "in training loop, epoch 3, step 622, the loss is 101369.046875\n",
            "in training loop, epoch 3, step 623, the loss is 114249.9765625\n",
            "in training loop, epoch 3, step 624, the loss is 135463.328125\n",
            "in training loop, epoch 3, step 625, the loss is 146805.875\n",
            "in training loop, epoch 3, step 626, the loss is 119180.171875\n",
            "in training loop, epoch 3, step 627, the loss is 196606.234375\n",
            "in training loop, epoch 3, step 628, the loss is 128677.59375\n",
            "in training loop, epoch 3, step 629, the loss is 142076.703125\n",
            "in training loop, epoch 3, step 630, the loss is 174133.0625\n",
            "in training loop, epoch 3, step 631, the loss is 271095.25\n",
            "in training loop, epoch 3, step 632, the loss is 183197.234375\n",
            "in training loop, epoch 3, step 633, the loss is 155780.171875\n",
            "in training loop, epoch 3, step 634, the loss is 121625.484375\n",
            "in training loop, epoch 3, step 635, the loss is 161967.59375\n",
            "in training loop, epoch 3, step 636, the loss is 244282.03125\n",
            "in training loop, epoch 3, step 637, the loss is 177390.5\n",
            "in training loop, epoch 3, step 638, the loss is 106977.875\n",
            "in training loop, epoch 3, step 639, the loss is 142824.265625\n",
            "in training loop, epoch 3, step 640, the loss is 119617.8828125\n",
            "in training loop, epoch 3, step 641, the loss is 82034.84375\n",
            "in training loop, epoch 3, step 642, the loss is 113508.90625\n",
            "in training loop, epoch 3, step 643, the loss is 251387.875\n",
            "in training loop, epoch 3, step 644, the loss is 134270.328125\n",
            "in training loop, epoch 3, step 645, the loss is 110901.3203125\n",
            "in training loop, epoch 3, step 646, the loss is 229041.25\n",
            "in training loop, epoch 3, step 647, the loss is 155773.34375\n",
            "in training loop, epoch 3, step 648, the loss is 121387.0\n",
            "in training loop, epoch 3, step 649, the loss is 244194.109375\n",
            "in training loop, epoch 3, step 650, the loss is 58387.06640625\n",
            "in training loop, epoch 3, step 651, the loss is 171840.703125\n",
            "in training loop, epoch 3, step 652, the loss is 113112.328125\n",
            "in training loop, epoch 3, step 653, the loss is 129060.65625\n",
            "in training loop, epoch 3, step 654, the loss is 235540.921875\n",
            "in training loop, epoch 3, step 655, the loss is 197696.125\n",
            "in training loop, epoch 3, step 656, the loss is 149313.40625\n",
            "in training loop, epoch 3, step 657, the loss is 150694.0\n",
            "in training loop, epoch 3, step 658, the loss is 181533.890625\n",
            "in training loop, epoch 3, step 659, the loss is 176127.5625\n",
            "in training loop, epoch 3, step 660, the loss is 133849.765625\n",
            "in training loop, epoch 3, step 661, the loss is 139404.5625\n",
            "in training loop, epoch 3, step 662, the loss is 183883.34375\n",
            "in training loop, epoch 3, step 663, the loss is 210440.5\n",
            "in training loop, epoch 3, step 664, the loss is 158022.046875\n",
            "in training loop, epoch 3, step 665, the loss is 163175.40625\n",
            "in training loop, epoch 3, step 666, the loss is 167969.9375\n",
            "in training loop, epoch 3, step 667, the loss is 150387.578125\n",
            "in training loop, epoch 3, step 668, the loss is 113777.3671875\n",
            "in training loop, epoch 3, step 669, the loss is 142462.484375\n",
            "in training loop, epoch 3, step 670, the loss is 179978.46875\n",
            "in training loop, epoch 3, step 671, the loss is 106886.46875\n",
            "in training loop, epoch 3, step 672, the loss is 116035.1953125\n",
            "in training loop, epoch 3, step 673, the loss is 190031.34375\n",
            "in training loop, epoch 3, step 674, the loss is 101934.984375\n",
            "in training loop, epoch 3, step 675, the loss is 190408.53125\n",
            "in training loop, epoch 3, step 676, the loss is 136230.0\n",
            "in training loop, epoch 3, step 677, the loss is 137344.078125\n",
            "in training loop, epoch 3, step 678, the loss is 142008.359375\n",
            "in training loop, epoch 3, step 679, the loss is 306944.21875\n",
            "in training loop, epoch 3, step 680, the loss is 128543.5\n",
            "in training loop, epoch 3, step 681, the loss is 114296.25\n",
            "in training loop, epoch 3, step 682, the loss is 175008.46875\n",
            "in training loop, epoch 3, step 683, the loss is 153262.9375\n",
            "in training loop, epoch 3, step 684, the loss is 128733.0078125\n",
            "in training loop, epoch 3, step 685, the loss is 173996.109375\n",
            "in training loop, epoch 3, step 686, the loss is 136292.75\n",
            "in training loop, epoch 3, step 687, the loss is 113424.578125\n",
            "in training loop, epoch 3, step 688, the loss is 168387.703125\n",
            "in training loop, epoch 3, step 689, the loss is 104006.3125\n",
            "in training loop, epoch 3, step 690, the loss is 165221.125\n",
            "in training loop, epoch 3, step 691, the loss is 145059.109375\n",
            "in training loop, epoch 3, step 692, the loss is 100052.7421875\n",
            "in training loop, epoch 3, step 693, the loss is 149275.109375\n",
            "in training loop, epoch 3, step 694, the loss is 192672.171875\n",
            "in training loop, epoch 3, step 695, the loss is 86293.3828125\n",
            "in training loop, epoch 3, step 696, the loss is 97282.125\n",
            "in training loop, epoch 3, step 697, the loss is 114380.6875\n",
            "in training loop, epoch 3, step 698, the loss is 129108.515625\n",
            "in training loop, epoch 3, step 699, the loss is 150830.453125\n",
            "in training loop, epoch 3, step 700, the loss is 235159.640625\n",
            "in training loop, epoch 3, step 701, the loss is 85031.09375\n",
            "in training loop, epoch 3, step 702, the loss is 99979.078125\n",
            "in training loop, epoch 3, step 703, the loss is 145324.5\n",
            "in training loop, epoch 3, step 704, the loss is 163019.703125\n",
            "in training loop, epoch 3, step 705, the loss is 159498.171875\n",
            "in training loop, epoch 3, step 706, the loss is 117015.046875\n",
            "in training loop, epoch 3, step 707, the loss is 115687.4296875\n",
            "in training loop, epoch 3, step 708, the loss is 147818.640625\n",
            "in training loop, epoch 3, step 709, the loss is 157644.53125\n",
            "in training loop, epoch 3, step 710, the loss is 128511.453125\n",
            "in training loop, epoch 3, step 711, the loss is 163797.109375\n",
            "in training loop, epoch 3, step 712, the loss is 173551.578125\n",
            "in training loop, epoch 3, step 713, the loss is 140528.078125\n",
            "in training loop, epoch 3, step 714, the loss is 119433.5\n",
            "in training loop, epoch 3, step 715, the loss is 173983.78125\n",
            "in training loop, epoch 3, step 716, the loss is 118777.640625\n",
            "in training loop, epoch 3, step 717, the loss is 125570.703125\n",
            "in training loop, epoch 3, step 718, the loss is 77949.109375\n",
            "in training loop, epoch 3, step 719, the loss is 171546.546875\n",
            "in training loop, epoch 3, step 720, the loss is 173402.6875\n",
            "in training loop, epoch 3, step 721, the loss is 211103.0625\n",
            "in training loop, epoch 3, step 722, the loss is 119649.921875\n",
            "in training loop, epoch 3, step 723, the loss is 161185.890625\n",
            "in training loop, epoch 3, step 724, the loss is 114592.109375\n",
            "in training loop, epoch 3, step 725, the loss is 167892.0625\n",
            "in training loop, epoch 3, step 726, the loss is 124020.890625\n",
            "in training loop, epoch 3, step 727, the loss is 223278.8125\n",
            "in training loop, epoch 3, step 728, the loss is 121418.203125\n",
            "in training loop, epoch 3, step 729, the loss is 150065.125\n",
            "in training loop, epoch 3, step 730, the loss is 217255.265625\n",
            "in training loop, epoch 3, step 731, the loss is 112062.5546875\n",
            "in training loop, epoch 3, step 732, the loss is 121916.359375\n",
            "in training loop, epoch 3, step 733, the loss is 101643.2265625\n",
            "in training loop, epoch 3, step 734, the loss is 125253.984375\n",
            "in training loop, epoch 3, step 735, the loss is 122096.125\n",
            "in training loop, epoch 3, step 736, the loss is 121409.328125\n",
            "in training loop, epoch 3, step 737, the loss is 163689.234375\n",
            "in training loop, epoch 3, step 738, the loss is 199786.515625\n",
            "in training loop, epoch 3, step 739, the loss is 112990.5390625\n",
            "in training loop, epoch 3, step 740, the loss is 146887.25\n",
            "in training loop, epoch 3, step 741, the loss is 125824.7421875\n",
            "in training loop, epoch 3, step 742, the loss is 144816.75\n",
            "in training loop, epoch 3, step 743, the loss is 166976.875\n",
            "in training loop, epoch 3, step 744, the loss is 153573.578125\n",
            "in training loop, epoch 3, step 745, the loss is 136285.453125\n",
            "in training loop, epoch 3, step 746, the loss is 107980.9296875\n",
            "in training loop, epoch 3, step 747, the loss is 121380.15625\n",
            "in training loop, epoch 3, step 748, the loss is 128120.1875\n",
            "in training loop, epoch 3, step 749, the loss is 167793.234375\n",
            "in training loop, epoch 3, step 750, the loss is 125331.3828125\n",
            "in training loop, epoch 3, step 751, the loss is 120258.640625\n",
            "in training loop, epoch 3, step 752, the loss is 170826.90625\n",
            "in training loop, epoch 3, step 753, the loss is 149441.40625\n",
            "in training loop, epoch 3, step 754, the loss is 91801.03125\n",
            "in training loop, epoch 3, step 755, the loss is 122211.3125\n",
            "in training loop, epoch 3, step 756, the loss is 118547.453125\n",
            "in training loop, epoch 3, step 757, the loss is 135784.90625\n",
            "in training loop, epoch 3, step 758, the loss is 151525.546875\n",
            "in training loop, epoch 3, step 759, the loss is 229126.671875\n",
            "in training loop, epoch 3, step 760, the loss is 110453.15625\n",
            "in training loop, epoch 3, step 761, the loss is 74732.546875\n",
            "in training loop, epoch 3, step 762, the loss is 257872.234375\n",
            "in training loop, epoch 3, step 763, the loss is 121159.953125\n",
            "in training loop, epoch 3, step 764, the loss is 100527.96875\n",
            "in training loop, epoch 3, step 765, the loss is 108473.328125\n",
            "in training loop, epoch 3, step 766, the loss is 143608.546875\n",
            "in training loop, epoch 3, step 767, the loss is 127810.0625\n",
            "in training loop, epoch 3, step 768, the loss is 136308.6875\n",
            "in training loop, epoch 3, step 769, the loss is 180169.1875\n",
            "in training loop, epoch 3, step 770, the loss is 99059.640625\n",
            "in training loop, epoch 3, step 771, the loss is 112262.6171875\n",
            "in training loop, epoch 3, step 772, the loss is 206728.90625\n",
            "in training loop, epoch 3, step 773, the loss is 113183.09375\n",
            "in training loop, epoch 3, step 774, the loss is 141663.609375\n",
            "in training loop, epoch 3, step 775, the loss is 140046.765625\n",
            "in training loop, epoch 3, step 776, the loss is 82827.1640625\n",
            "in training loop, epoch 3, step 777, the loss is 77313.609375\n",
            "in training loop, epoch 3, step 778, the loss is 200664.34375\n",
            "in training loop, epoch 3, step 779, the loss is 163416.9375\n",
            "in training loop, epoch 3, step 780, the loss is 153716.109375\n",
            "in training loop, epoch 3, step 781, the loss is 112295.015625\n",
            "in training loop, epoch 3, step 782, the loss is 139444.984375\n",
            "in training loop, epoch 3, step 783, the loss is 171378.328125\n",
            "in training loop, epoch 3, step 784, the loss is 163672.21875\n",
            "in training loop, epoch 3, step 785, the loss is 154909.6875\n",
            "in training loop, epoch 3, step 786, the loss is 177684.375\n",
            "in training loop, epoch 3, step 787, the loss is 138494.671875\n",
            "in training loop, epoch 3, step 788, the loss is 146061.28125\n",
            "in training loop, epoch 3, step 789, the loss is 132549.390625\n",
            "in training loop, epoch 3, step 790, the loss is 140804.171875\n",
            "in training loop, epoch 3, step 791, the loss is 260390.25\n",
            "in training loop, epoch 3, step 792, the loss is 125990.7890625\n",
            "in training loop, epoch 3, step 793, the loss is 123186.609375\n",
            "in training loop, epoch 3, step 794, the loss is 128801.90625\n",
            "in training loop, epoch 3, step 795, the loss is 139007.53125\n",
            "in training loop, epoch 3, step 796, the loss is 84102.15625\n",
            "in training loop, epoch 3, step 797, the loss is 114839.0625\n",
            "in training loop, epoch 3, step 798, the loss is 161365.359375\n",
            "in training loop, epoch 3, step 799, the loss is 143678.25\n",
            "in training loop, epoch 3, step 800, the loss is 137466.109375\n",
            "in training loop, epoch 3, step 801, the loss is 153629.90625\n",
            "in training loop, epoch 3, step 802, the loss is 110489.2421875\n",
            "in training loop, epoch 3, step 803, the loss is 124778.375\n",
            "in training loop, epoch 3, step 804, the loss is 164551.9375\n",
            "in training loop, epoch 3, step 805, the loss is 173681.71875\n",
            "in training loop, epoch 3, step 806, the loss is 207498.703125\n",
            "in training loop, epoch 3, step 807, the loss is 161916.09375\n",
            "in training loop, epoch 3, step 808, the loss is 114713.78125\n",
            "in training loop, epoch 3, step 809, the loss is 164869.046875\n",
            "in training loop, epoch 3, step 810, the loss is 140548.15625\n",
            "in training loop, epoch 3, step 811, the loss is 131087.234375\n",
            "in training loop, epoch 3, step 812, the loss is 119967.40625\n",
            "in training loop, epoch 3, step 813, the loss is 196643.09375\n",
            "in training loop, epoch 3, step 814, the loss is 160305.53125\n",
            "in training loop, epoch 3, step 815, the loss is 117707.046875\n",
            "in training loop, epoch 3, step 816, the loss is 118510.7890625\n",
            "in training loop, epoch 3, step 817, the loss is 105816.9296875\n",
            "in training loop, epoch 3, step 818, the loss is 100729.6015625\n",
            "in training loop, epoch 3, step 819, the loss is 150400.640625\n",
            "in training loop, epoch 3, step 820, the loss is 118453.1953125\n",
            "in training loop, epoch 3, step 821, the loss is 130239.171875\n",
            "in training loop, epoch 3, step 822, the loss is 206055.15625\n",
            "in training loop, epoch 3, step 823, the loss is 162815.0625\n",
            "in training loop, epoch 3, step 824, the loss is 176468.0\n",
            "in training loop, epoch 3, step 825, the loss is 138269.515625\n",
            "in training loop, epoch 3, step 826, the loss is 169472.875\n",
            "in training loop, epoch 3, step 827, the loss is 162816.828125\n",
            "in training loop, epoch 3, step 828, the loss is 262764.125\n",
            "in training loop, epoch 3, step 829, the loss is 93851.3046875\n",
            "in training loop, epoch 3, step 830, the loss is 113403.5\n",
            "in training loop, epoch 3, step 831, the loss is 145182.796875\n",
            "in training loop, epoch 3, step 832, the loss is 154622.328125\n",
            "in training loop, epoch 3, step 833, the loss is 134662.03125\n",
            "in training loop, epoch 3, step 834, the loss is 132664.71875\n",
            "in training loop, epoch 3, step 835, the loss is 149914.15625\n",
            "in training loop, epoch 3, step 836, the loss is 162094.625\n",
            "in training loop, epoch 3, step 837, the loss is 87959.984375\n",
            "in training loop, epoch 3, step 838, the loss is 193760.859375\n",
            "in training loop, epoch 3, step 839, the loss is 181807.203125\n",
            "in training loop, epoch 3, step 840, the loss is 123021.3203125\n",
            "in training loop, epoch 3, step 841, the loss is 154516.859375\n",
            "in training loop, epoch 3, step 842, the loss is 138190.625\n",
            "in training loop, epoch 3, step 843, the loss is 179902.703125\n",
            "in training loop, epoch 3, step 844, the loss is 131742.390625\n",
            "in training loop, epoch 3, step 845, the loss is 138575.84375\n",
            "in training loop, epoch 3, step 846, the loss is 159330.109375\n",
            "in training loop, epoch 3, step 847, the loss is 135968.53125\n",
            "in training loop, epoch 3, step 848, the loss is 129678.03125\n",
            "in training loop, epoch 3, step 849, the loss is 155024.953125\n",
            "in training loop, epoch 3, step 850, the loss is 194801.484375\n",
            "in training loop, epoch 3, step 851, the loss is 123879.171875\n",
            "in training loop, epoch 3, step 852, the loss is 145792.75\n",
            "in training loop, epoch 3, step 853, the loss is 115286.3828125\n",
            "in training loop, epoch 3, step 854, the loss is 188961.953125\n",
            "in training loop, epoch 3, step 855, the loss is 143571.90625\n",
            "in training loop, epoch 3, step 856, the loss is 123990.3359375\n",
            "in training loop, epoch 3, step 857, the loss is 166157.21875\n",
            "in training loop, epoch 3, step 858, the loss is 137889.265625\n",
            "in training loop, epoch 3, step 859, the loss is 143462.171875\n",
            "in training loop, epoch 3, step 860, the loss is 164929.3125\n",
            "in training loop, epoch 3, step 861, the loss is 177569.421875\n",
            "in training loop, epoch 3, step 862, the loss is 187105.84375\n",
            "in training loop, epoch 3, step 863, the loss is 115397.390625\n",
            "in training loop, epoch 3, step 864, the loss is 169809.25\n",
            "in training loop, epoch 3, step 865, the loss is 133502.390625\n",
            "in training loop, epoch 3, step 866, the loss is 179375.453125\n",
            "in training loop, epoch 3, step 867, the loss is 60969.7109375\n",
            "in training loop, epoch 3, step 868, the loss is 109103.8515625\n",
            "in training loop, epoch 3, step 869, the loss is 105016.6640625\n",
            "in training loop, epoch 3, step 870, the loss is 144279.890625\n",
            "in training loop, epoch 3, step 871, the loss is 212518.265625\n",
            "in training loop, epoch 3, step 872, the loss is 211006.90625\n",
            "in training loop, epoch 3, step 873, the loss is 99656.3203125\n",
            "in training loop, epoch 3, step 874, the loss is 167075.0625\n",
            "in training loop, epoch 3, step 875, the loss is 128491.8203125\n",
            "in training loop, epoch 3, step 876, the loss is 143516.75\n",
            "in training loop, epoch 3, step 877, the loss is 165386.25\n",
            "in training loop, epoch 3, step 878, the loss is 125520.125\n",
            "in training loop, epoch 3, step 879, the loss is 131517.828125\n",
            "in training loop, epoch 3, step 880, the loss is 220404.5\n",
            "in training loop, epoch 3, step 881, the loss is 135236.421875\n",
            "in training loop, epoch 3, step 882, the loss is 52323.59765625\n",
            "in training loop, epoch 3, step 883, the loss is 101590.1484375\n",
            "in training loop, epoch 3, step 884, the loss is 166365.078125\n",
            "in training loop, epoch 3, step 885, the loss is 117953.1015625\n",
            "in training loop, epoch 3, step 886, the loss is 140598.484375\n",
            "in training loop, epoch 3, step 887, the loss is 197509.546875\n",
            "in training loop, epoch 3, step 888, the loss is 105163.65625\n",
            "in training loop, epoch 3, step 889, the loss is 147625.984375\n",
            "in training loop, epoch 3, step 890, the loss is 136023.4375\n",
            "in training loop, epoch 3, step 891, the loss is 153915.53125\n",
            "in training loop, epoch 3, step 892, the loss is 145372.078125\n",
            "in training loop, epoch 3, step 893, the loss is 119045.8125\n",
            "in training loop, epoch 3, step 894, the loss is 156096.71875\n",
            "in training loop, epoch 3, step 895, the loss is 105686.75\n",
            "in training loop, epoch 3, step 896, the loss is 155025.484375\n",
            "in training loop, epoch 3, step 897, the loss is 139416.359375\n",
            "in training loop, epoch 3, step 898, the loss is 131647.125\n",
            "in training loop, epoch 3, step 899, the loss is 104959.546875\n",
            "in training loop, epoch 3, step 900, the loss is 142834.0625\n",
            "in training loop, epoch 3, step 901, the loss is 109485.7578125\n",
            "in training loop, epoch 3, step 902, the loss is 139295.28125\n",
            "in training loop, epoch 3, step 903, the loss is 24363.642578125\n",
            "k-fold 1:: Epoch 3: train loss 145650.86525381982 val loss 183731.2081141708\n",
            "in training loop, epoch 4, step 0, the loss is 86155.6484375\n",
            "in training loop, epoch 4, step 1, the loss is 124860.9609375\n",
            "in training loop, epoch 4, step 2, the loss is 93772.8828125\n",
            "in training loop, epoch 4, step 3, the loss is 120066.609375\n",
            "in training loop, epoch 4, step 4, the loss is 107186.375\n",
            "in training loop, epoch 4, step 5, the loss is 81553.765625\n",
            "in training loop, epoch 4, step 6, the loss is 94913.109375\n",
            "in training loop, epoch 4, step 7, the loss is 79100.03125\n",
            "in training loop, epoch 4, step 8, the loss is 129872.125\n",
            "in training loop, epoch 4, step 9, the loss is 118868.078125\n",
            "in training loop, epoch 4, step 10, the loss is 124343.7578125\n",
            "in training loop, epoch 4, step 11, the loss is 129080.2421875\n",
            "in training loop, epoch 4, step 12, the loss is 98428.296875\n",
            "in training loop, epoch 4, step 13, the loss is 129234.359375\n",
            "in training loop, epoch 4, step 14, the loss is 128874.59375\n",
            "in training loop, epoch 4, step 15, the loss is 89598.28125\n",
            "in training loop, epoch 4, step 16, the loss is 81022.796875\n",
            "in training loop, epoch 4, step 17, the loss is 87175.46875\n",
            "in training loop, epoch 4, step 18, the loss is 88240.09375\n",
            "in training loop, epoch 4, step 19, the loss is 77025.4453125\n",
            "in training loop, epoch 4, step 20, the loss is 101535.3515625\n",
            "in training loop, epoch 4, step 21, the loss is 87263.796875\n",
            "in training loop, epoch 4, step 22, the loss is 84392.0390625\n",
            "in training loop, epoch 4, step 23, the loss is 89136.5703125\n",
            "in training loop, epoch 4, step 24, the loss is 85723.890625\n",
            "in training loop, epoch 4, step 25, the loss is 63093.15234375\n",
            "in training loop, epoch 4, step 26, the loss is 74011.234375\n",
            "in training loop, epoch 4, step 27, the loss is 126897.390625\n",
            "in training loop, epoch 4, step 28, the loss is 93433.4609375\n",
            "in training loop, epoch 4, step 29, the loss is 129908.828125\n",
            "in training loop, epoch 4, step 30, the loss is 118975.171875\n",
            "in training loop, epoch 4, step 31, the loss is 105168.515625\n",
            "in training loop, epoch 4, step 32, the loss is 86532.71875\n",
            "in training loop, epoch 4, step 33, the loss is 82415.5859375\n",
            "in training loop, epoch 4, step 34, the loss is 70196.5\n",
            "in training loop, epoch 4, step 35, the loss is 126215.03125\n",
            "in training loop, epoch 4, step 36, the loss is 100914.828125\n",
            "in training loop, epoch 4, step 37, the loss is 91444.484375\n",
            "in training loop, epoch 4, step 38, the loss is 135094.703125\n",
            "in training loop, epoch 4, step 39, the loss is 84403.375\n",
            "in training loop, epoch 4, step 40, the loss is 130210.46875\n",
            "in training loop, epoch 4, step 41, the loss is 108477.8671875\n",
            "in training loop, epoch 4, step 42, the loss is 97326.5546875\n",
            "in training loop, epoch 4, step 43, the loss is 77664.7734375\n",
            "in training loop, epoch 4, step 44, the loss is 87318.6484375\n",
            "in training loop, epoch 4, step 45, the loss is 116724.8359375\n",
            "in training loop, epoch 4, step 46, the loss is 124511.296875\n",
            "in training loop, epoch 4, step 47, the loss is 99374.875\n",
            "in training loop, epoch 4, step 48, the loss is 128601.609375\n",
            "in training loop, epoch 4, step 49, the loss is 91295.703125\n",
            "in training loop, epoch 4, step 50, the loss is 93063.625\n",
            "in training loop, epoch 4, step 51, the loss is 104367.484375\n",
            "in training loop, epoch 4, step 52, the loss is 110642.0546875\n",
            "in training loop, epoch 4, step 53, the loss is 121043.328125\n",
            "in training loop, epoch 4, step 54, the loss is 142580.953125\n",
            "in training loop, epoch 4, step 55, the loss is 84384.5546875\n",
            "in training loop, epoch 4, step 56, the loss is 118662.5625\n",
            "in training loop, epoch 4, step 57, the loss is 108807.1484375\n",
            "in training loop, epoch 4, step 58, the loss is 85085.8125\n",
            "in training loop, epoch 4, step 59, the loss is 93238.3515625\n",
            "in training loop, epoch 4, step 60, the loss is 137064.640625\n",
            "in training loop, epoch 4, step 61, the loss is 101609.46875\n",
            "in training loop, epoch 4, step 62, the loss is 103767.7421875\n",
            "in training loop, epoch 4, step 63, the loss is 96836.859375\n",
            "in training loop, epoch 4, step 64, the loss is 91474.015625\n",
            "in training loop, epoch 4, step 65, the loss is 89933.6328125\n",
            "in training loop, epoch 4, step 66, the loss is 145193.546875\n",
            "in training loop, epoch 4, step 67, the loss is 124907.84375\n",
            "in training loop, epoch 4, step 68, the loss is 66860.6328125\n",
            "in training loop, epoch 4, step 69, the loss is 133858.4375\n",
            "in training loop, epoch 4, step 70, the loss is 158230.203125\n",
            "in training loop, epoch 4, step 71, the loss is 113729.203125\n",
            "in training loop, epoch 4, step 72, the loss is 97346.359375\n",
            "in training loop, epoch 4, step 73, the loss is 104397.796875\n",
            "in training loop, epoch 4, step 74, the loss is 83107.4609375\n",
            "in training loop, epoch 4, step 75, the loss is 101350.515625\n",
            "in training loop, epoch 4, step 76, the loss is 98094.015625\n",
            "in training loop, epoch 4, step 77, the loss is 151169.390625\n",
            "in training loop, epoch 4, step 78, the loss is 119434.578125\n",
            "in training loop, epoch 4, step 79, the loss is 134623.78125\n",
            "in training loop, epoch 4, step 80, the loss is 130983.125\n",
            "in training loop, epoch 4, step 81, the loss is 113816.015625\n",
            "in training loop, epoch 4, step 82, the loss is 98189.546875\n",
            "in training loop, epoch 4, step 83, the loss is 112866.21875\n",
            "in training loop, epoch 4, step 84, the loss is 110880.7265625\n",
            "in training loop, epoch 4, step 85, the loss is 102887.9609375\n",
            "in training loop, epoch 4, step 86, the loss is 113175.9609375\n",
            "in training loop, epoch 4, step 87, the loss is 127082.109375\n",
            "in training loop, epoch 4, step 88, the loss is 125343.609375\n",
            "in training loop, epoch 4, step 89, the loss is 109882.1875\n",
            "in training loop, epoch 4, step 90, the loss is 85728.5\n",
            "in training loop, epoch 4, step 91, the loss is 106625.8359375\n",
            "in training loop, epoch 4, step 92, the loss is 104050.65625\n",
            "in training loop, epoch 4, step 93, the loss is 96244.828125\n",
            "in training loop, epoch 4, step 94, the loss is 113512.1171875\n",
            "in training loop, epoch 4, step 95, the loss is 119301.078125\n",
            "in training loop, epoch 4, step 96, the loss is 59525.359375\n",
            "in training loop, epoch 4, step 97, the loss is 85678.421875\n",
            "in training loop, epoch 4, step 98, the loss is 83109.0625\n",
            "in training loop, epoch 4, step 99, the loss is 87666.96875\n",
            "in training loop, epoch 4, step 100, the loss is 104867.109375\n",
            "in training loop, epoch 4, step 101, the loss is 77645.3828125\n",
            "in training loop, epoch 4, step 102, the loss is 70758.65625\n",
            "in training loop, epoch 4, step 103, the loss is 149858.515625\n",
            "in training loop, epoch 4, step 104, the loss is 106511.6875\n",
            "in training loop, epoch 4, step 105, the loss is 96685.8203125\n",
            "in training loop, epoch 4, step 106, the loss is 92793.1640625\n",
            "in training loop, epoch 4, step 107, the loss is 155655.03125\n",
            "in training loop, epoch 4, step 108, the loss is 91981.34375\n",
            "in training loop, epoch 4, step 109, the loss is 115465.953125\n",
            "in training loop, epoch 4, step 110, the loss is 145240.1875\n",
            "in training loop, epoch 4, step 111, the loss is 101861.9296875\n",
            "in training loop, epoch 4, step 112, the loss is 122445.453125\n",
            "in training loop, epoch 4, step 113, the loss is 105292.46875\n",
            "in training loop, epoch 4, step 114, the loss is 116003.109375\n",
            "in training loop, epoch 4, step 115, the loss is 127581.921875\n",
            "in training loop, epoch 4, step 116, the loss is 94412.125\n",
            "in training loop, epoch 4, step 117, the loss is 95136.09375\n",
            "in training loop, epoch 4, step 118, the loss is 115658.140625\n",
            "in training loop, epoch 4, step 119, the loss is 73976.125\n",
            "in training loop, epoch 4, step 120, the loss is 115394.9765625\n",
            "in training loop, epoch 4, step 121, the loss is 123556.4609375\n",
            "in training loop, epoch 4, step 122, the loss is 109099.203125\n",
            "in training loop, epoch 4, step 123, the loss is 90584.4921875\n",
            "in training loop, epoch 4, step 124, the loss is 116538.046875\n",
            "in training loop, epoch 4, step 125, the loss is 99401.3046875\n",
            "in training loop, epoch 4, step 126, the loss is 91267.65625\n",
            "in training loop, epoch 4, step 127, the loss is 62382.359375\n",
            "in training loop, epoch 4, step 128, the loss is 104811.8125\n",
            "in training loop, epoch 4, step 129, the loss is 147950.265625\n",
            "in training loop, epoch 4, step 130, the loss is 114148.71875\n",
            "in training loop, epoch 4, step 131, the loss is 93226.40625\n",
            "in training loop, epoch 4, step 132, the loss is 76702.0234375\n",
            "in training loop, epoch 4, step 133, the loss is 103844.2109375\n",
            "in training loop, epoch 4, step 134, the loss is 135855.71875\n",
            "in training loop, epoch 4, step 135, the loss is 116492.71875\n",
            "in training loop, epoch 4, step 136, the loss is 94226.53125\n",
            "in training loop, epoch 4, step 137, the loss is 93173.859375\n",
            "in training loop, epoch 4, step 138, the loss is 102770.421875\n",
            "in training loop, epoch 4, step 139, the loss is 122155.1953125\n",
            "in training loop, epoch 4, step 140, the loss is 121612.8671875\n",
            "in training loop, epoch 4, step 141, the loss is 157174.875\n",
            "in training loop, epoch 4, step 142, the loss is 97946.453125\n",
            "in training loop, epoch 4, step 143, the loss is 119098.1953125\n",
            "in training loop, epoch 4, step 144, the loss is 100955.046875\n",
            "in training loop, epoch 4, step 145, the loss is 97968.265625\n",
            "in training loop, epoch 4, step 146, the loss is 95348.9453125\n",
            "in training loop, epoch 4, step 147, the loss is 98057.3671875\n",
            "in training loop, epoch 4, step 148, the loss is 95705.1875\n",
            "in training loop, epoch 4, step 149, the loss is 102157.1171875\n",
            "in training loop, epoch 4, step 150, the loss is 124256.515625\n",
            "in training loop, epoch 4, step 151, the loss is 74083.15625\n",
            "in training loop, epoch 4, step 152, the loss is 98587.46875\n",
            "in training loop, epoch 4, step 153, the loss is 103519.90625\n",
            "in training loop, epoch 4, step 154, the loss is 101643.5859375\n",
            "in training loop, epoch 4, step 155, the loss is 134097.84375\n",
            "in training loop, epoch 4, step 156, the loss is 86161.421875\n",
            "in training loop, epoch 4, step 157, the loss is 94555.5625\n",
            "in training loop, epoch 4, step 158, the loss is 110730.0546875\n",
            "in training loop, epoch 4, step 159, the loss is 97326.21875\n",
            "in training loop, epoch 4, step 160, the loss is 81442.703125\n",
            "in training loop, epoch 4, step 161, the loss is 88317.5\n",
            "in training loop, epoch 4, step 162, the loss is 106838.6875\n",
            "in training loop, epoch 4, step 163, the loss is 99009.7890625\n",
            "in training loop, epoch 4, step 164, the loss is 116216.8671875\n",
            "in training loop, epoch 4, step 165, the loss is 94475.34375\n",
            "in training loop, epoch 4, step 166, the loss is 77304.828125\n",
            "in training loop, epoch 4, step 167, the loss is 117309.03125\n",
            "in training loop, epoch 4, step 168, the loss is 108142.6015625\n",
            "in training loop, epoch 4, step 169, the loss is 115549.015625\n",
            "in training loop, epoch 4, step 170, the loss is 145070.28125\n",
            "in training loop, epoch 4, step 171, the loss is 121770.953125\n",
            "in training loop, epoch 4, step 172, the loss is 108740.78125\n",
            "in training loop, epoch 4, step 173, the loss is 95296.5859375\n",
            "in training loop, epoch 4, step 174, the loss is 88543.0703125\n",
            "in training loop, epoch 4, step 175, the loss is 96746.7421875\n",
            "in training loop, epoch 4, step 176, the loss is 117988.1796875\n",
            "in training loop, epoch 4, step 177, the loss is 160049.34375\n",
            "in training loop, epoch 4, step 178, the loss is 95808.9921875\n",
            "in training loop, epoch 4, step 179, the loss is 101357.171875\n",
            "in training loop, epoch 4, step 180, the loss is 87231.4375\n",
            "in training loop, epoch 4, step 181, the loss is 107212.40625\n",
            "in training loop, epoch 4, step 182, the loss is 133173.03125\n",
            "in training loop, epoch 4, step 183, the loss is 60828.4140625\n",
            "in training loop, epoch 4, step 184, the loss is 63721.8828125\n",
            "in training loop, epoch 4, step 185, the loss is 101876.7734375\n",
            "in training loop, epoch 4, step 186, the loss is 97340.34375\n",
            "in training loop, epoch 4, step 187, the loss is 126608.8203125\n",
            "in training loop, epoch 4, step 188, the loss is 87308.0078125\n",
            "in training loop, epoch 4, step 189, the loss is 94685.6875\n",
            "in training loop, epoch 4, step 190, the loss is 136443.890625\n",
            "in training loop, epoch 4, step 191, the loss is 103325.125\n",
            "in training loop, epoch 4, step 192, the loss is 109120.3359375\n",
            "in training loop, epoch 4, step 193, the loss is 86744.9453125\n",
            "in training loop, epoch 4, step 194, the loss is 100205.03125\n",
            "in training loop, epoch 4, step 195, the loss is 92724.4609375\n",
            "in training loop, epoch 4, step 196, the loss is 86748.3125\n",
            "in training loop, epoch 4, step 197, the loss is 114214.46875\n",
            "in training loop, epoch 4, step 198, the loss is 103726.046875\n",
            "in training loop, epoch 4, step 199, the loss is 80497.328125\n",
            "in training loop, epoch 4, step 200, the loss is 108292.3046875\n",
            "in training loop, epoch 4, step 201, the loss is 113198.515625\n",
            "in training loop, epoch 4, step 202, the loss is 146682.5\n",
            "in training loop, epoch 4, step 203, the loss is 120573.0234375\n",
            "in training loop, epoch 4, step 204, the loss is 89545.421875\n",
            "in training loop, epoch 4, step 205, the loss is 136897.25\n",
            "in training loop, epoch 4, step 206, the loss is 107243.0625\n",
            "in training loop, epoch 4, step 207, the loss is 227535.3125\n",
            "in training loop, epoch 4, step 208, the loss is 117244.7109375\n",
            "in training loop, epoch 4, step 209, the loss is 130045.28125\n",
            "in training loop, epoch 4, step 210, the loss is 91987.796875\n",
            "in training loop, epoch 4, step 211, the loss is 121083.6015625\n",
            "in training loop, epoch 4, step 212, the loss is 182451.703125\n",
            "in training loop, epoch 4, step 213, the loss is 136178.890625\n",
            "in training loop, epoch 4, step 214, the loss is 106393.5\n",
            "in training loop, epoch 4, step 215, the loss is 124419.34375\n",
            "in training loop, epoch 4, step 216, the loss is 247988.671875\n",
            "in training loop, epoch 4, step 217, the loss is 235889.96875\n",
            "in training loop, epoch 4, step 218, the loss is 162946.640625\n",
            "in training loop, epoch 4, step 219, the loss is 103006.5390625\n",
            "in training loop, epoch 4, step 220, the loss is 204832.140625\n",
            "in training loop, epoch 4, step 221, the loss is 245257.734375\n",
            "in training loop, epoch 4, step 222, the loss is 176640.984375\n",
            "in training loop, epoch 4, step 223, the loss is 150644.9375\n",
            "in training loop, epoch 4, step 224, the loss is 149371.953125\n",
            "in training loop, epoch 4, step 225, the loss is 219901.90625\n",
            "in training loop, epoch 4, step 226, the loss is 167675.125\n",
            "in training loop, epoch 4, step 227, the loss is 223189.875\n",
            "in training loop, epoch 4, step 228, the loss is 162338.421875\n",
            "in training loop, epoch 4, step 229, the loss is 152616.765625\n",
            "in training loop, epoch 4, step 230, the loss is 138383.875\n",
            "in training loop, epoch 4, step 231, the loss is 119323.0390625\n",
            "in training loop, epoch 4, step 232, the loss is 147135.53125\n",
            "in training loop, epoch 4, step 233, the loss is 162705.03125\n",
            "in training loop, epoch 4, step 234, the loss is 144152.75\n",
            "in training loop, epoch 4, step 235, the loss is 157681.5625\n",
            "in training loop, epoch 4, step 236, the loss is 188164.96875\n",
            "in training loop, epoch 4, step 237, the loss is 192513.515625\n",
            "in training loop, epoch 4, step 238, the loss is 336757.625\n",
            "in training loop, epoch 4, step 239, the loss is 140364.078125\n",
            "in training loop, epoch 4, step 240, the loss is 135837.984375\n",
            "in training loop, epoch 4, step 241, the loss is 163670.4375\n",
            "in training loop, epoch 4, step 242, the loss is 170543.828125\n",
            "in training loop, epoch 4, step 243, the loss is 128277.84375\n",
            "in training loop, epoch 4, step 244, the loss is 103936.15625\n",
            "in training loop, epoch 4, step 245, the loss is 106282.53125\n",
            "in training loop, epoch 4, step 246, the loss is 584047.375\n",
            "in training loop, epoch 4, step 247, the loss is 138314.390625\n",
            "in training loop, epoch 4, step 248, the loss is 131386.546875\n",
            "in training loop, epoch 4, step 249, the loss is 163704.1875\n",
            "in training loop, epoch 4, step 250, the loss is 222100.359375\n",
            "in training loop, epoch 4, step 251, the loss is 165452.25\n",
            "in training loop, epoch 4, step 252, the loss is 123753.296875\n",
            "in training loop, epoch 4, step 253, the loss is 141894.109375\n",
            "in training loop, epoch 4, step 254, the loss is 178070.890625\n",
            "in training loop, epoch 4, step 255, the loss is 152626.4375\n",
            "in training loop, epoch 4, step 256, the loss is 142910.0\n",
            "in training loop, epoch 4, step 257, the loss is 200238.125\n",
            "in training loop, epoch 4, step 258, the loss is 173602.609375\n",
            "in training loop, epoch 4, step 259, the loss is 93246.4140625\n",
            "in training loop, epoch 4, step 260, the loss is 108416.9296875\n",
            "in training loop, epoch 4, step 261, the loss is 231817.890625\n",
            "in training loop, epoch 4, step 262, the loss is 212773.03125\n",
            "in training loop, epoch 4, step 263, the loss is 156518.34375\n",
            "in training loop, epoch 4, step 264, the loss is 183540.0625\n",
            "in training loop, epoch 4, step 265, the loss is 106323.4921875\n",
            "in training loop, epoch 4, step 266, the loss is 253072.875\n",
            "in training loop, epoch 4, step 267, the loss is 108962.1796875\n",
            "in training loop, epoch 4, step 268, the loss is 98551.390625\n",
            "in training loop, epoch 4, step 269, the loss is 156552.84375\n",
            "in training loop, epoch 4, step 270, the loss is 233985.9375\n",
            "in training loop, epoch 4, step 271, the loss is 132026.640625\n",
            "in training loop, epoch 4, step 272, the loss is 94916.5625\n",
            "in training loop, epoch 4, step 273, the loss is 196217.28125\n",
            "in training loop, epoch 4, step 274, the loss is 116006.109375\n",
            "in training loop, epoch 4, step 275, the loss is 109391.53125\n",
            "in training loop, epoch 4, step 276, the loss is 146690.21875\n",
            "in training loop, epoch 4, step 277, the loss is 110393.5\n",
            "in training loop, epoch 4, step 278, the loss is 128027.78125\n",
            "in training loop, epoch 4, step 279, the loss is 113736.96875\n",
            "in training loop, epoch 4, step 280, the loss is 90141.2421875\n",
            "in training loop, epoch 4, step 281, the loss is 94774.5703125\n",
            "in training loop, epoch 4, step 282, the loss is 200871.09375\n",
            "in training loop, epoch 4, step 283, the loss is 201437.25\n",
            "in training loop, epoch 4, step 284, the loss is 97237.515625\n",
            "in training loop, epoch 4, step 285, the loss is 118369.7890625\n",
            "in training loop, epoch 4, step 286, the loss is 161059.1875\n",
            "in training loop, epoch 4, step 287, the loss is 133766.84375\n",
            "in training loop, epoch 4, step 288, the loss is 116882.234375\n",
            "in training loop, epoch 4, step 289, the loss is 122773.265625\n",
            "in training loop, epoch 4, step 290, the loss is 115035.0625\n",
            "in training loop, epoch 4, step 291, the loss is 103523.828125\n",
            "in training loop, epoch 4, step 292, the loss is 119320.765625\n",
            "in training loop, epoch 4, step 293, the loss is 128106.0703125\n",
            "in training loop, epoch 4, step 294, the loss is 115047.453125\n",
            "in training loop, epoch 4, step 295, the loss is 130838.0859375\n",
            "in training loop, epoch 4, step 296, the loss is 162063.9375\n",
            "in training loop, epoch 4, step 297, the loss is 92713.3828125\n",
            "in training loop, epoch 4, step 298, the loss is 119827.8125\n",
            "in training loop, epoch 4, step 299, the loss is 117962.765625\n",
            "in training loop, epoch 4, step 300, the loss is 114234.640625\n",
            "in training loop, epoch 4, step 301, the loss is 162539.484375\n",
            "in training loop, epoch 4, step 302, the loss is 119176.953125\n",
            "in training loop, epoch 4, step 303, the loss is 103051.9375\n",
            "in training loop, epoch 4, step 304, the loss is 123409.2578125\n",
            "in training loop, epoch 4, step 305, the loss is 80904.21875\n",
            "in training loop, epoch 4, step 306, the loss is 99654.984375\n",
            "in training loop, epoch 4, step 307, the loss is 104820.7265625\n",
            "in training loop, epoch 4, step 308, the loss is 116650.5\n",
            "in training loop, epoch 4, step 309, the loss is 105266.3125\n",
            "in training loop, epoch 4, step 310, the loss is 104624.9921875\n",
            "in training loop, epoch 4, step 311, the loss is 92828.078125\n",
            "in training loop, epoch 4, step 312, the loss is 136359.859375\n",
            "in training loop, epoch 4, step 313, the loss is 128810.515625\n",
            "in training loop, epoch 4, step 314, the loss is 83496.234375\n",
            "in training loop, epoch 4, step 315, the loss is 104112.2890625\n",
            "in training loop, epoch 4, step 316, the loss is 174537.234375\n",
            "in training loop, epoch 4, step 317, the loss is 102317.5234375\n",
            "in training loop, epoch 4, step 318, the loss is 127017.140625\n",
            "in training loop, epoch 4, step 319, the loss is 153812.5625\n",
            "in training loop, epoch 4, step 320, the loss is 98046.7109375\n",
            "in training loop, epoch 4, step 321, the loss is 107870.8828125\n",
            "in training loop, epoch 4, step 322, the loss is 156286.90625\n",
            "in training loop, epoch 4, step 323, the loss is 119824.7890625\n",
            "in training loop, epoch 4, step 324, the loss is 94630.25\n",
            "in training loop, epoch 4, step 325, the loss is 89757.359375\n",
            "in training loop, epoch 4, step 326, the loss is 98815.7421875\n",
            "in training loop, epoch 4, step 327, the loss is 95496.859375\n",
            "in training loop, epoch 4, step 328, the loss is 148516.3125\n",
            "in training loop, epoch 4, step 329, the loss is 104685.3046875\n",
            "in training loop, epoch 4, step 330, the loss is 128790.21875\n",
            "in training loop, epoch 4, step 331, the loss is 101318.3046875\n",
            "in training loop, epoch 4, step 332, the loss is 132622.1875\n",
            "in training loop, epoch 4, step 333, the loss is 106659.2109375\n",
            "in training loop, epoch 4, step 334, the loss is 149194.75\n",
            "in training loop, epoch 4, step 335, the loss is 62845.84765625\n",
            "in training loop, epoch 4, step 336, the loss is 118825.5625\n",
            "in training loop, epoch 4, step 337, the loss is 79444.9375\n",
            "in training loop, epoch 4, step 338, the loss is 110605.59375\n",
            "in training loop, epoch 4, step 339, the loss is 83708.515625\n",
            "in training loop, epoch 4, step 340, the loss is 123600.5703125\n",
            "in training loop, epoch 4, step 341, the loss is 90447.6796875\n",
            "in training loop, epoch 4, step 342, the loss is 93937.609375\n",
            "in training loop, epoch 4, step 343, the loss is 87533.125\n",
            "in training loop, epoch 4, step 344, the loss is 96446.6640625\n",
            "in training loop, epoch 4, step 345, the loss is 92170.8984375\n",
            "in training loop, epoch 4, step 346, the loss is 165769.21875\n",
            "in training loop, epoch 4, step 347, the loss is 183136.703125\n",
            "in training loop, epoch 4, step 348, the loss is 99925.5625\n",
            "in training loop, epoch 4, step 349, the loss is 101194.9140625\n",
            "in training loop, epoch 4, step 350, the loss is 147547.078125\n",
            "in training loop, epoch 4, step 351, the loss is 92680.453125\n",
            "in training loop, epoch 4, step 352, the loss is 127013.640625\n",
            "in training loop, epoch 4, step 353, the loss is 119111.953125\n",
            "in training loop, epoch 4, step 354, the loss is 87462.2578125\n",
            "in training loop, epoch 4, step 355, the loss is 127012.0\n",
            "in training loop, epoch 4, step 356, the loss is 105278.5859375\n",
            "in training loop, epoch 4, step 357, the loss is 183983.34375\n",
            "in training loop, epoch 4, step 358, the loss is 142356.65625\n",
            "in training loop, epoch 4, step 359, the loss is 117391.1640625\n",
            "in training loop, epoch 4, step 360, the loss is 138928.5625\n",
            "in training loop, epoch 4, step 361, the loss is 83670.296875\n",
            "in training loop, epoch 4, step 362, the loss is 130479.8359375\n",
            "in training loop, epoch 4, step 363, the loss is 131083.484375\n",
            "in training loop, epoch 4, step 364, the loss is 84986.1875\n",
            "in training loop, epoch 4, step 365, the loss is 87050.7734375\n",
            "in training loop, epoch 4, step 366, the loss is 94531.59375\n",
            "in training loop, epoch 4, step 367, the loss is 167768.0625\n",
            "in training loop, epoch 4, step 368, the loss is 122020.328125\n",
            "in training loop, epoch 4, step 369, the loss is 109131.4609375\n",
            "in training loop, epoch 4, step 370, the loss is 142616.71875\n",
            "in training loop, epoch 4, step 371, the loss is 121194.515625\n",
            "in training loop, epoch 4, step 372, the loss is 129199.7734375\n",
            "in training loop, epoch 4, step 373, the loss is 106887.0859375\n",
            "in training loop, epoch 4, step 374, the loss is 160086.015625\n",
            "in training loop, epoch 4, step 375, the loss is 110144.03125\n",
            "in training loop, epoch 4, step 376, the loss is 111248.2578125\n",
            "in training loop, epoch 4, step 377, the loss is 112865.953125\n",
            "in training loop, epoch 4, step 378, the loss is 151038.109375\n",
            "in training loop, epoch 4, step 379, the loss is 90019.578125\n",
            "in training loop, epoch 4, step 380, the loss is 86058.703125\n",
            "in training loop, epoch 4, step 381, the loss is 121699.9140625\n",
            "in training loop, epoch 4, step 382, the loss is 133195.71875\n",
            "in training loop, epoch 4, step 383, the loss is 139120.40625\n",
            "in training loop, epoch 4, step 384, the loss is 127269.1328125\n",
            "in training loop, epoch 4, step 385, the loss is 100507.1640625\n",
            "in training loop, epoch 4, step 386, the loss is 120561.109375\n",
            "in training loop, epoch 4, step 387, the loss is 104622.484375\n",
            "in training loop, epoch 4, step 388, the loss is 101849.9375\n",
            "in training loop, epoch 4, step 389, the loss is 109790.09375\n",
            "in training loop, epoch 4, step 390, the loss is 109176.8203125\n",
            "in training loop, epoch 4, step 391, the loss is 127394.7734375\n",
            "in training loop, epoch 4, step 392, the loss is 138029.46875\n",
            "in training loop, epoch 4, step 393, the loss is 119890.9609375\n",
            "in training loop, epoch 4, step 394, the loss is 152306.8125\n",
            "in training loop, epoch 4, step 395, the loss is 117878.75\n",
            "in training loop, epoch 4, step 396, the loss is 126666.0\n",
            "in training loop, epoch 4, step 397, the loss is 118343.0859375\n",
            "in training loop, epoch 4, step 398, the loss is 116352.4140625\n",
            "in training loop, epoch 4, step 399, the loss is 146632.96875\n",
            "in training loop, epoch 4, step 400, the loss is 109766.671875\n",
            "in training loop, epoch 4, step 401, the loss is 97934.8828125\n",
            "in training loop, epoch 4, step 402, the loss is 120384.5\n",
            "in training loop, epoch 4, step 403, the loss is 212251.375\n",
            "in training loop, epoch 4, step 404, the loss is 94991.3515625\n",
            "in training loop, epoch 4, step 405, the loss is 118188.421875\n",
            "in training loop, epoch 4, step 406, the loss is 95585.8828125\n",
            "in training loop, epoch 4, step 407, the loss is 121002.265625\n",
            "in training loop, epoch 4, step 408, the loss is 66200.1953125\n",
            "in training loop, epoch 4, step 409, the loss is 107671.8125\n",
            "in training loop, epoch 4, step 410, the loss is 122811.359375\n",
            "in training loop, epoch 4, step 411, the loss is 102587.3984375\n",
            "in training loop, epoch 4, step 412, the loss is 183877.078125\n",
            "in training loop, epoch 4, step 413, the loss is 99424.296875\n",
            "in training loop, epoch 4, step 414, the loss is 130237.40625\n",
            "in training loop, epoch 4, step 415, the loss is 219478.453125\n",
            "in training loop, epoch 4, step 416, the loss is 130986.28125\n",
            "in training loop, epoch 4, step 417, the loss is 100962.0078125\n",
            "in training loop, epoch 4, step 418, the loss is 120709.6328125\n",
            "in training loop, epoch 4, step 419, the loss is 95964.1953125\n",
            "in training loop, epoch 4, step 420, the loss is 174555.203125\n",
            "in training loop, epoch 4, step 421, the loss is 103355.4296875\n",
            "in training loop, epoch 4, step 422, the loss is 107492.4296875\n",
            "in training loop, epoch 4, step 423, the loss is 183350.96875\n",
            "in training loop, epoch 4, step 424, the loss is 72872.328125\n",
            "in training loop, epoch 4, step 425, the loss is 151301.171875\n",
            "in training loop, epoch 4, step 426, the loss is 128054.1796875\n",
            "in training loop, epoch 4, step 427, the loss is 175371.328125\n",
            "in training loop, epoch 4, step 428, the loss is 131299.8125\n",
            "in training loop, epoch 4, step 429, the loss is 99829.2578125\n",
            "in training loop, epoch 4, step 430, the loss is 257724.1875\n",
            "in training loop, epoch 4, step 431, the loss is 164732.296875\n",
            "in training loop, epoch 4, step 432, the loss is 112394.3203125\n",
            "in training loop, epoch 4, step 433, the loss is 99747.046875\n",
            "in training loop, epoch 4, step 434, the loss is 157523.21875\n",
            "in training loop, epoch 4, step 435, the loss is 164963.0\n",
            "in training loop, epoch 4, step 436, the loss is 225284.0\n",
            "in training loop, epoch 4, step 437, the loss is 234159.734375\n",
            "in training loop, epoch 4, step 438, the loss is 142086.453125\n",
            "in training loop, epoch 4, step 439, the loss is 233093.296875\n",
            "in training loop, epoch 4, step 440, the loss is 307167.875\n",
            "in training loop, epoch 4, step 441, the loss is 264761.875\n",
            "in training loop, epoch 4, step 442, the loss is 309949.25\n",
            "in training loop, epoch 4, step 443, the loss is 204614.46875\n",
            "in training loop, epoch 4, step 444, the loss is 130036.8125\n",
            "in training loop, epoch 4, step 445, the loss is 178564.078125\n",
            "in training loop, epoch 4, step 446, the loss is 302031.6875\n",
            "in training loop, epoch 4, step 447, the loss is 254401.25\n",
            "in training loop, epoch 4, step 448, the loss is 228459.703125\n",
            "in training loop, epoch 4, step 449, the loss is 134817.90625\n",
            "in training loop, epoch 4, step 450, the loss is 159647.3125\n",
            "in training loop, epoch 4, step 451, the loss is 231095.890625\n",
            "in training loop, epoch 4, step 452, the loss is 189954.171875\n",
            "in training loop, epoch 4, step 453, the loss is 277020.40625\n",
            "in training loop, epoch 4, step 454, the loss is 119916.125\n",
            "in training loop, epoch 4, step 455, the loss is 123556.890625\n",
            "in training loop, epoch 4, step 456, the loss is 173103.0\n",
            "in training loop, epoch 4, step 457, the loss is 160468.203125\n",
            "in training loop, epoch 4, step 458, the loss is 213670.5\n",
            "in training loop, epoch 4, step 459, the loss is 166888.359375\n",
            "in training loop, epoch 4, step 460, the loss is 181460.375\n",
            "in training loop, epoch 4, step 461, the loss is 168274.671875\n",
            "in training loop, epoch 4, step 462, the loss is 163814.34375\n",
            "in training loop, epoch 4, step 463, the loss is 152710.0625\n",
            "in training loop, epoch 4, step 464, the loss is 108214.203125\n",
            "in training loop, epoch 4, step 465, the loss is 171094.75\n",
            "in training loop, epoch 4, step 466, the loss is 129336.109375\n",
            "in training loop, epoch 4, step 467, the loss is 133479.625\n",
            "in training loop, epoch 4, step 468, the loss is 140199.640625\n",
            "in training loop, epoch 4, step 469, the loss is 131539.78125\n",
            "in training loop, epoch 4, step 470, the loss is 154516.375\n",
            "in training loop, epoch 4, step 471, the loss is 138352.796875\n",
            "in training loop, epoch 4, step 472, the loss is 107513.734375\n",
            "in training loop, epoch 4, step 473, the loss is 102088.5\n",
            "in training loop, epoch 4, step 474, the loss is 124245.1328125\n",
            "in training loop, epoch 4, step 475, the loss is 153694.96875\n",
            "in training loop, epoch 4, step 476, the loss is 138224.875\n",
            "in training loop, epoch 4, step 477, the loss is 114266.4375\n",
            "in training loop, epoch 4, step 478, the loss is 94759.28125\n",
            "in training loop, epoch 4, step 479, the loss is 124536.0390625\n",
            "in training loop, epoch 4, step 480, the loss is 262278.5625\n",
            "in training loop, epoch 4, step 481, the loss is 67973.0\n",
            "in training loop, epoch 4, step 482, the loss is 146903.703125\n",
            "in training loop, epoch 4, step 483, the loss is 112580.9453125\n",
            "in training loop, epoch 4, step 484, the loss is 123992.859375\n",
            "in training loop, epoch 4, step 485, the loss is 94181.0234375\n",
            "in training loop, epoch 4, step 486, the loss is 160201.6875\n",
            "in training loop, epoch 4, step 487, the loss is 126194.9140625\n",
            "in training loop, epoch 4, step 488, the loss is 130770.8984375\n",
            "in training loop, epoch 4, step 489, the loss is 174656.171875\n",
            "in training loop, epoch 4, step 490, the loss is 111804.0\n",
            "in training loop, epoch 4, step 491, the loss is 95035.6640625\n",
            "in training loop, epoch 4, step 492, the loss is 127890.734375\n",
            "in training loop, epoch 4, step 493, the loss is 142578.328125\n",
            "in training loop, epoch 4, step 494, the loss is 137935.625\n",
            "in training loop, epoch 4, step 495, the loss is 85511.9765625\n",
            "in training loop, epoch 4, step 496, the loss is 157914.0\n",
            "in training loop, epoch 4, step 497, the loss is 146643.1875\n",
            "in training loop, epoch 4, step 498, the loss is 140021.671875\n",
            "in training loop, epoch 4, step 499, the loss is 118744.375\n",
            "in training loop, epoch 4, step 500, the loss is 142001.6875\n",
            "in training loop, epoch 4, step 501, the loss is 158866.515625\n",
            "in training loop, epoch 4, step 502, the loss is 251989.28125\n",
            "in training loop, epoch 4, step 503, the loss is 109369.6640625\n",
            "in training loop, epoch 4, step 504, the loss is 121139.65625\n",
            "in training loop, epoch 4, step 505, the loss is 171411.484375\n",
            "in training loop, epoch 4, step 506, the loss is 141526.890625\n",
            "in training loop, epoch 4, step 507, the loss is 152141.734375\n",
            "in training loop, epoch 4, step 508, the loss is 154222.40625\n",
            "in training loop, epoch 4, step 509, the loss is 120845.6953125\n",
            "in training loop, epoch 4, step 510, the loss is 105902.8828125\n",
            "in training loop, epoch 4, step 511, the loss is 98110.21875\n",
            "in training loop, epoch 4, step 512, the loss is 159366.921875\n",
            "in training loop, epoch 4, step 513, the loss is 164074.359375\n",
            "in training loop, epoch 4, step 514, the loss is 169132.1875\n",
            "in training loop, epoch 4, step 515, the loss is 121075.3125\n",
            "in training loop, epoch 4, step 516, the loss is 114605.1328125\n",
            "in training loop, epoch 4, step 517, the loss is 135225.84375\n",
            "in training loop, epoch 4, step 518, the loss is 97498.0078125\n",
            "in training loop, epoch 4, step 519, the loss is 176753.65625\n",
            "in training loop, epoch 4, step 520, the loss is 108902.984375\n",
            "in training loop, epoch 4, step 521, the loss is 137996.6875\n",
            "in training loop, epoch 4, step 522, the loss is 127609.265625\n",
            "in training loop, epoch 4, step 523, the loss is 172896.53125\n",
            "in training loop, epoch 4, step 524, the loss is 192379.71875\n",
            "in training loop, epoch 4, step 525, the loss is 128910.6796875\n",
            "in training loop, epoch 4, step 526, the loss is 130746.40625\n",
            "in training loop, epoch 4, step 527, the loss is 126257.296875\n",
            "in training loop, epoch 4, step 528, the loss is 167843.6875\n",
            "in training loop, epoch 4, step 529, the loss is 154269.5625\n",
            "in training loop, epoch 4, step 530, the loss is 101247.8046875\n",
            "in training loop, epoch 4, step 531, the loss is 144501.59375\n",
            "in training loop, epoch 4, step 532, the loss is 101655.7421875\n",
            "in training loop, epoch 4, step 533, the loss is 109274.1015625\n",
            "in training loop, epoch 4, step 534, the loss is 122384.828125\n",
            "in training loop, epoch 4, step 535, the loss is 119960.0546875\n",
            "in training loop, epoch 4, step 536, the loss is 137829.03125\n",
            "in training loop, epoch 4, step 537, the loss is 215871.96875\n",
            "in training loop, epoch 4, step 538, the loss is 144830.46875\n",
            "in training loop, epoch 4, step 539, the loss is 177201.5\n",
            "in training loop, epoch 4, step 540, the loss is 131123.0625\n",
            "in training loop, epoch 4, step 541, the loss is 150418.765625\n",
            "in training loop, epoch 4, step 542, the loss is 120430.8828125\n",
            "in training loop, epoch 4, step 543, the loss is 117316.6796875\n",
            "in training loop, epoch 4, step 544, the loss is 136785.546875\n",
            "in training loop, epoch 4, step 545, the loss is 104571.5078125\n",
            "in training loop, epoch 4, step 546, the loss is 116065.1796875\n",
            "in training loop, epoch 4, step 547, the loss is 188445.375\n",
            "in training loop, epoch 4, step 548, the loss is 137605.875\n",
            "in training loop, epoch 4, step 549, the loss is 143543.09375\n",
            "in training loop, epoch 4, step 550, the loss is 134043.96875\n",
            "in training loop, epoch 4, step 551, the loss is 145423.984375\n",
            "in training loop, epoch 4, step 552, the loss is 149006.359375\n",
            "in training loop, epoch 4, step 553, the loss is 145504.1875\n",
            "in training loop, epoch 4, step 554, the loss is 159555.78125\n",
            "in training loop, epoch 4, step 555, the loss is 99409.1953125\n",
            "in training loop, epoch 4, step 556, the loss is 119148.3359375\n",
            "in training loop, epoch 4, step 557, the loss is 88184.6015625\n",
            "in training loop, epoch 4, step 558, the loss is 301837.0\n",
            "in training loop, epoch 4, step 559, the loss is 130694.9765625\n",
            "in training loop, epoch 4, step 560, the loss is 182589.78125\n",
            "in training loop, epoch 4, step 561, the loss is 132798.046875\n",
            "in training loop, epoch 4, step 562, the loss is 154293.8125\n",
            "in training loop, epoch 4, step 563, the loss is 118582.390625\n",
            "in training loop, epoch 4, step 564, the loss is 108141.7734375\n",
            "in training loop, epoch 4, step 565, the loss is 106827.7421875\n",
            "in training loop, epoch 4, step 566, the loss is 116535.4375\n",
            "in training loop, epoch 4, step 567, the loss is 120852.984375\n",
            "in training loop, epoch 4, step 568, the loss is 94852.671875\n",
            "in training loop, epoch 4, step 569, the loss is 117669.1015625\n",
            "in training loop, epoch 4, step 570, the loss is 100074.125\n",
            "in training loop, epoch 4, step 571, the loss is 121761.328125\n",
            "in training loop, epoch 4, step 572, the loss is 196054.53125\n",
            "in training loop, epoch 4, step 573, the loss is 93096.9765625\n",
            "in training loop, epoch 4, step 574, the loss is 118847.671875\n",
            "in training loop, epoch 4, step 575, the loss is 124191.578125\n",
            "in training loop, epoch 4, step 576, the loss is 109300.6640625\n",
            "in training loop, epoch 4, step 577, the loss is 168064.578125\n",
            "in training loop, epoch 4, step 578, the loss is 128711.84375\n",
            "in training loop, epoch 4, step 579, the loss is 116145.2265625\n",
            "in training loop, epoch 4, step 580, the loss is 117447.265625\n",
            "in training loop, epoch 4, step 581, the loss is 109525.9453125\n",
            "in training loop, epoch 4, step 582, the loss is 101505.359375\n",
            "in training loop, epoch 4, step 583, the loss is 122711.25\n",
            "in training loop, epoch 4, step 584, the loss is 127421.671875\n",
            "in training loop, epoch 4, step 585, the loss is 98794.46875\n",
            "in training loop, epoch 4, step 586, the loss is 141913.109375\n",
            "in training loop, epoch 4, step 587, the loss is 102381.4453125\n",
            "in training loop, epoch 4, step 588, the loss is 127458.0078125\n",
            "in training loop, epoch 4, step 589, the loss is 154492.5625\n",
            "in training loop, epoch 4, step 590, the loss is 110535.3828125\n",
            "in training loop, epoch 4, step 591, the loss is 93278.609375\n",
            "in training loop, epoch 4, step 592, the loss is 152945.6875\n",
            "in training loop, epoch 4, step 593, the loss is 81541.5234375\n",
            "in training loop, epoch 4, step 594, the loss is 167730.125\n",
            "in training loop, epoch 4, step 595, the loss is 92916.5546875\n",
            "in training loop, epoch 4, step 596, the loss is 160791.21875\n",
            "in training loop, epoch 4, step 597, the loss is 129340.53125\n",
            "in training loop, epoch 4, step 598, the loss is 202967.9375\n",
            "in training loop, epoch 4, step 599, the loss is 158263.359375\n",
            "in training loop, epoch 4, step 600, the loss is 149307.46875\n",
            "in training loop, epoch 4, step 601, the loss is 129684.6796875\n",
            "in training loop, epoch 4, step 602, the loss is 80658.96875\n",
            "in training loop, epoch 4, step 603, the loss is 132103.90625\n",
            "in training loop, epoch 4, step 604, the loss is 116596.890625\n",
            "in training loop, epoch 4, step 605, the loss is 123962.671875\n",
            "in training loop, epoch 4, step 606, the loss is 117469.0625\n",
            "in training loop, epoch 4, step 607, the loss is 136001.15625\n",
            "in training loop, epoch 4, step 608, the loss is 139140.375\n",
            "in training loop, epoch 4, step 609, the loss is 104579.6015625\n",
            "in training loop, epoch 4, step 610, the loss is 90681.3203125\n",
            "in training loop, epoch 4, step 611, the loss is 117965.890625\n",
            "in training loop, epoch 4, step 612, the loss is 172047.484375\n",
            "in training loop, epoch 4, step 613, the loss is 127859.125\n",
            "in training loop, epoch 4, step 614, the loss is 116774.203125\n",
            "in training loop, epoch 4, step 615, the loss is 98951.234375\n",
            "in training loop, epoch 4, step 616, the loss is 159756.71875\n",
            "in training loop, epoch 4, step 617, the loss is 146497.125\n",
            "in training loop, epoch 4, step 618, the loss is 172326.28125\n",
            "in training loop, epoch 4, step 619, the loss is 124317.171875\n",
            "in training loop, epoch 4, step 620, the loss is 107395.3046875\n",
            "in training loop, epoch 4, step 621, the loss is 135272.15625\n",
            "in training loop, epoch 4, step 622, the loss is 115136.7265625\n",
            "in training loop, epoch 4, step 623, the loss is 143746.546875\n",
            "in training loop, epoch 4, step 624, the loss is 187581.4375\n",
            "in training loop, epoch 4, step 625, the loss is 137847.546875\n",
            "in training loop, epoch 4, step 626, the loss is 83433.203125\n",
            "in training loop, epoch 4, step 627, the loss is 144300.546875\n",
            "in training loop, epoch 4, step 628, the loss is 142207.09375\n",
            "in training loop, epoch 4, step 629, the loss is 118532.71875\n",
            "in training loop, epoch 4, step 630, the loss is 89544.2890625\n",
            "in training loop, epoch 4, step 631, the loss is 266770.3125\n",
            "in training loop, epoch 4, step 632, the loss is 127555.7265625\n",
            "in training loop, epoch 4, step 633, the loss is 147486.421875\n",
            "in training loop, epoch 4, step 634, the loss is 161675.234375\n",
            "in training loop, epoch 4, step 635, the loss is 228208.25\n",
            "in training loop, epoch 4, step 636, the loss is 177894.90625\n",
            "in training loop, epoch 4, step 637, the loss is 118110.390625\n",
            "in training loop, epoch 4, step 638, the loss is 109816.625\n",
            "in training loop, epoch 4, step 639, the loss is 144426.046875\n",
            "in training loop, epoch 4, step 640, the loss is 171446.0625\n",
            "in training loop, epoch 4, step 641, the loss is 106914.7109375\n",
            "in training loop, epoch 4, step 642, the loss is 120285.6640625\n",
            "in training loop, epoch 4, step 643, the loss is 105892.875\n",
            "in training loop, epoch 4, step 644, the loss is 151183.28125\n",
            "in training loop, epoch 4, step 645, the loss is 148128.015625\n",
            "in training loop, epoch 4, step 646, the loss is 147627.6875\n",
            "in training loop, epoch 4, step 647, the loss is 153907.4375\n",
            "in training loop, epoch 4, step 648, the loss is 128183.875\n",
            "in training loop, epoch 4, step 649, the loss is 201243.65625\n",
            "in training loop, epoch 4, step 650, the loss is 139569.84375\n",
            "in training loop, epoch 4, step 651, the loss is 120126.1953125\n",
            "in training loop, epoch 4, step 652, the loss is 135256.25\n",
            "in training loop, epoch 4, step 653, the loss is 198567.484375\n",
            "in training loop, epoch 4, step 654, the loss is 122838.40625\n",
            "in training loop, epoch 4, step 655, the loss is 318612.375\n",
            "in training loop, epoch 4, step 656, the loss is 75737.0859375\n",
            "in training loop, epoch 4, step 657, the loss is 149638.046875\n",
            "in training loop, epoch 4, step 658, the loss is 140130.15625\n",
            "in training loop, epoch 4, step 659, the loss is 111308.46875\n",
            "in training loop, epoch 4, step 660, the loss is 179858.96875\n",
            "in training loop, epoch 4, step 661, the loss is 192083.65625\n",
            "in training loop, epoch 4, step 662, the loss is 145016.1875\n",
            "in training loop, epoch 4, step 663, the loss is 136919.8125\n",
            "in training loop, epoch 4, step 664, the loss is 121988.59375\n",
            "in training loop, epoch 4, step 665, the loss is 117314.5546875\n",
            "in training loop, epoch 4, step 666, the loss is 144374.8125\n",
            "in training loop, epoch 4, step 667, the loss is 166712.625\n",
            "in training loop, epoch 4, step 668, the loss is 159305.8125\n",
            "in training loop, epoch 4, step 669, the loss is 81696.796875\n",
            "in training loop, epoch 4, step 670, the loss is 161825.703125\n",
            "in training loop, epoch 4, step 671, the loss is 133770.703125\n",
            "in training loop, epoch 4, step 672, the loss is 114287.5078125\n",
            "in training loop, epoch 4, step 673, the loss is 99798.328125\n",
            "in training loop, epoch 4, step 674, the loss is 115104.296875\n",
            "in training loop, epoch 4, step 675, the loss is 164438.46875\n",
            "in training loop, epoch 4, step 676, the loss is 143294.078125\n",
            "in training loop, epoch 4, step 677, the loss is 147539.859375\n",
            "in training loop, epoch 4, step 678, the loss is 103529.78125\n",
            "in training loop, epoch 4, step 679, the loss is 152086.3125\n",
            "in training loop, epoch 4, step 680, the loss is 130760.53125\n",
            "in training loop, epoch 4, step 681, the loss is 144847.53125\n",
            "in training loop, epoch 4, step 682, the loss is 107360.9609375\n",
            "in training loop, epoch 4, step 683, the loss is 126135.5546875\n",
            "in training loop, epoch 4, step 684, the loss is 184725.21875\n",
            "in training loop, epoch 4, step 685, the loss is 121776.640625\n",
            "in training loop, epoch 4, step 686, the loss is 121021.53125\n",
            "in training loop, epoch 4, step 687, the loss is 137196.796875\n",
            "in training loop, epoch 4, step 688, the loss is 125406.1875\n",
            "in training loop, epoch 4, step 689, the loss is 132795.6875\n",
            "in training loop, epoch 4, step 690, the loss is 104178.34375\n",
            "in training loop, epoch 4, step 691, the loss is 130103.640625\n",
            "in training loop, epoch 4, step 692, the loss is 203185.421875\n",
            "in training loop, epoch 4, step 693, the loss is 130488.6015625\n",
            "in training loop, epoch 4, step 694, the loss is 153629.6875\n",
            "in training loop, epoch 4, step 695, the loss is 130823.09375\n",
            "in training loop, epoch 4, step 696, the loss is 111712.59375\n",
            "in training loop, epoch 4, step 697, the loss is 213838.171875\n",
            "in training loop, epoch 4, step 698, the loss is 97085.3984375\n",
            "in training loop, epoch 4, step 699, the loss is 140857.40625\n",
            "in training loop, epoch 4, step 700, the loss is 94670.234375\n",
            "in training loop, epoch 4, step 701, the loss is 102975.0078125\n",
            "in training loop, epoch 4, step 702, the loss is 103519.65625\n",
            "in training loop, epoch 4, step 703, the loss is 131204.1875\n",
            "in training loop, epoch 4, step 704, the loss is 94569.5\n",
            "in training loop, epoch 4, step 705, the loss is 101941.4375\n",
            "in training loop, epoch 4, step 706, the loss is 111453.65625\n",
            "in training loop, epoch 4, step 707, the loss is 127901.78125\n",
            "in training loop, epoch 4, step 708, the loss is 148419.875\n",
            "in training loop, epoch 4, step 709, the loss is 112594.1953125\n",
            "in training loop, epoch 4, step 710, the loss is 144073.015625\n",
            "in training loop, epoch 4, step 711, the loss is 123362.96875\n",
            "in training loop, epoch 4, step 712, the loss is 183401.3125\n",
            "in training loop, epoch 4, step 713, the loss is 121404.28125\n",
            "in training loop, epoch 4, step 714, the loss is 211133.75\n",
            "in training loop, epoch 4, step 715, the loss is 100859.484375\n",
            "in training loop, epoch 4, step 716, the loss is 106398.2890625\n",
            "in training loop, epoch 4, step 717, the loss is 100053.8828125\n",
            "in training loop, epoch 4, step 718, the loss is 117441.3515625\n",
            "in training loop, epoch 4, step 719, the loss is 151662.1875\n",
            "in training loop, epoch 4, step 720, the loss is 103907.03125\n",
            "in training loop, epoch 4, step 721, the loss is 119673.4375\n",
            "in training loop, epoch 4, step 722, the loss is 113647.953125\n",
            "in training loop, epoch 4, step 723, the loss is 174272.390625\n",
            "in training loop, epoch 4, step 724, the loss is 99038.6640625\n",
            "in training loop, epoch 4, step 725, the loss is 113256.140625\n",
            "in training loop, epoch 4, step 726, the loss is 117480.1015625\n",
            "in training loop, epoch 4, step 727, the loss is 107316.15625\n",
            "in training loop, epoch 4, step 728, the loss is 147108.46875\n",
            "in training loop, epoch 4, step 729, the loss is 90185.8046875\n",
            "in training loop, epoch 4, step 730, the loss is 99358.8828125\n",
            "in training loop, epoch 4, step 731, the loss is 132572.984375\n",
            "in training loop, epoch 4, step 732, the loss is 139061.234375\n",
            "in training loop, epoch 4, step 733, the loss is 92889.7890625\n",
            "in training loop, epoch 4, step 734, the loss is 154883.390625\n",
            "in training loop, epoch 4, step 735, the loss is 149127.234375\n",
            "in training loop, epoch 4, step 736, the loss is 110035.578125\n",
            "in training loop, epoch 4, step 737, the loss is 95638.90625\n",
            "in training loop, epoch 4, step 738, the loss is 106941.3515625\n",
            "in training loop, epoch 4, step 739, the loss is 132777.546875\n",
            "in training loop, epoch 4, step 740, the loss is 161169.15625\n",
            "in training loop, epoch 4, step 741, the loss is 98745.8359375\n",
            "in training loop, epoch 4, step 742, the loss is 85027.640625\n",
            "in training loop, epoch 4, step 743, the loss is 149837.28125\n",
            "in training loop, epoch 4, step 744, the loss is 155216.625\n",
            "in training loop, epoch 4, step 745, the loss is 100451.3671875\n",
            "in training loop, epoch 4, step 746, the loss is 140505.421875\n",
            "in training loop, epoch 4, step 747, the loss is 131500.96875\n",
            "in training loop, epoch 4, step 748, the loss is 111863.515625\n",
            "in training loop, epoch 4, step 749, the loss is 111099.953125\n",
            "in training loop, epoch 4, step 750, the loss is 97292.265625\n",
            "in training loop, epoch 4, step 751, the loss is 165000.0625\n",
            "in training loop, epoch 4, step 752, the loss is 160953.140625\n",
            "in training loop, epoch 4, step 753, the loss is 139269.640625\n",
            "in training loop, epoch 4, step 754, the loss is 96582.734375\n",
            "in training loop, epoch 4, step 755, the loss is 69643.9453125\n",
            "in training loop, epoch 4, step 756, the loss is 138135.40625\n",
            "in training loop, epoch 4, step 757, the loss is 152470.6875\n",
            "in training loop, epoch 4, step 758, the loss is 131313.46875\n",
            "in training loop, epoch 4, step 759, the loss is 147362.359375\n",
            "in training loop, epoch 4, step 760, the loss is 123204.515625\n",
            "in training loop, epoch 4, step 761, the loss is 124694.796875\n",
            "in training loop, epoch 4, step 762, the loss is 112354.7890625\n",
            "in training loop, epoch 4, step 763, the loss is 133410.34375\n",
            "in training loop, epoch 4, step 764, the loss is 123350.3359375\n",
            "in training loop, epoch 4, step 765, the loss is 116055.4609375\n",
            "in training loop, epoch 4, step 766, the loss is 116980.0078125\n",
            "in training loop, epoch 4, step 767, the loss is 124677.8203125\n",
            "in training loop, epoch 4, step 768, the loss is 115786.046875\n",
            "in training loop, epoch 4, step 769, the loss is 137617.59375\n",
            "in training loop, epoch 4, step 770, the loss is 167657.859375\n",
            "in training loop, epoch 4, step 771, the loss is 132715.578125\n",
            "in training loop, epoch 4, step 772, the loss is 118719.1640625\n",
            "in training loop, epoch 4, step 773, the loss is 96593.09375\n",
            "in training loop, epoch 4, step 774, the loss is 190996.0\n",
            "in training loop, epoch 4, step 775, the loss is 133065.890625\n",
            "in training loop, epoch 4, step 776, the loss is 127738.3828125\n",
            "in training loop, epoch 4, step 777, the loss is 145597.203125\n",
            "in training loop, epoch 4, step 778, the loss is 157551.453125\n",
            "in training loop, epoch 4, step 779, the loss is 84258.0703125\n",
            "in training loop, epoch 4, step 780, the loss is 140600.609375\n",
            "in training loop, epoch 4, step 781, the loss is 112985.578125\n",
            "in training loop, epoch 4, step 782, the loss is 133627.5625\n",
            "in training loop, epoch 4, step 783, the loss is 134777.640625\n",
            "in training loop, epoch 4, step 784, the loss is 137355.734375\n",
            "in training loop, epoch 4, step 785, the loss is 256909.40625\n",
            "in training loop, epoch 4, step 786, the loss is 85886.921875\n",
            "in training loop, epoch 4, step 787, the loss is 110918.640625\n",
            "in training loop, epoch 4, step 788, the loss is 163396.53125\n",
            "in training loop, epoch 4, step 789, the loss is 146787.890625\n",
            "in training loop, epoch 4, step 790, the loss is 122411.125\n",
            "in training loop, epoch 4, step 791, the loss is 122276.84375\n",
            "in training loop, epoch 4, step 792, the loss is 110475.9921875\n",
            "in training loop, epoch 4, step 793, the loss is 117136.3125\n",
            "in training loop, epoch 4, step 794, the loss is 112457.8046875\n",
            "in training loop, epoch 4, step 795, the loss is 113778.4375\n",
            "in training loop, epoch 4, step 796, the loss is 116795.125\n",
            "in training loop, epoch 4, step 797, the loss is 172464.171875\n",
            "in training loop, epoch 4, step 798, the loss is 153503.859375\n",
            "in training loop, epoch 4, step 799, the loss is 97268.734375\n",
            "in training loop, epoch 4, step 800, the loss is 122102.296875\n",
            "in training loop, epoch 4, step 801, the loss is 132826.96875\n",
            "in training loop, epoch 4, step 802, the loss is 118187.9453125\n",
            "in training loop, epoch 4, step 803, the loss is 139520.96875\n",
            "in training loop, epoch 4, step 804, the loss is 113470.3359375\n",
            "in training loop, epoch 4, step 805, the loss is 109400.34375\n",
            "in training loop, epoch 4, step 806, the loss is 170311.859375\n",
            "in training loop, epoch 4, step 807, the loss is 172141.21875\n",
            "in training loop, epoch 4, step 808, the loss is 118427.3828125\n",
            "in training loop, epoch 4, step 809, the loss is 117127.078125\n",
            "in training loop, epoch 4, step 810, the loss is 100884.1171875\n",
            "in training loop, epoch 4, step 811, the loss is 131014.71875\n",
            "in training loop, epoch 4, step 812, the loss is 122288.9296875\n",
            "in training loop, epoch 4, step 813, the loss is 123517.171875\n",
            "in training loop, epoch 4, step 814, the loss is 102898.875\n",
            "in training loop, epoch 4, step 815, the loss is 149805.375\n",
            "in training loop, epoch 4, step 816, the loss is 139812.640625\n",
            "in training loop, epoch 4, step 817, the loss is 103469.734375\n",
            "in training loop, epoch 4, step 818, the loss is 175789.171875\n",
            "in training loop, epoch 4, step 819, the loss is 156056.421875\n",
            "in training loop, epoch 4, step 820, the loss is 154135.59375\n",
            "in training loop, epoch 4, step 821, the loss is 78879.5390625\n",
            "in training loop, epoch 4, step 822, the loss is 134012.953125\n",
            "in training loop, epoch 4, step 823, the loss is 130836.15625\n",
            "in training loop, epoch 4, step 824, the loss is 147903.546875\n",
            "in training loop, epoch 4, step 825, the loss is 211791.265625\n",
            "in training loop, epoch 4, step 826, the loss is 166812.53125\n",
            "in training loop, epoch 4, step 827, the loss is 137735.484375\n",
            "in training loop, epoch 4, step 828, the loss is 82609.046875\n",
            "in training loop, epoch 4, step 829, the loss is 123609.375\n",
            "in training loop, epoch 4, step 830, the loss is 164375.25\n",
            "in training loop, epoch 4, step 831, the loss is 231933.0625\n",
            "in training loop, epoch 4, step 832, the loss is 151727.859375\n",
            "in training loop, epoch 4, step 833, the loss is 147946.25\n",
            "in training loop, epoch 4, step 834, the loss is 88125.75\n",
            "in training loop, epoch 4, step 835, the loss is 121266.7421875\n",
            "in training loop, epoch 4, step 836, the loss is 135774.96875\n",
            "in training loop, epoch 4, step 837, the loss is 151721.53125\n",
            "in training loop, epoch 4, step 838, the loss is 118991.765625\n",
            "in training loop, epoch 4, step 839, the loss is 126929.4375\n",
            "in training loop, epoch 4, step 840, the loss is 112849.46875\n",
            "in training loop, epoch 4, step 841, the loss is 137996.328125\n",
            "in training loop, epoch 4, step 842, the loss is 166116.78125\n",
            "in training loop, epoch 4, step 843, the loss is 139883.578125\n",
            "in training loop, epoch 4, step 844, the loss is 171267.265625\n",
            "in training loop, epoch 4, step 845, the loss is 135106.015625\n",
            "in training loop, epoch 4, step 846, the loss is 124593.2421875\n",
            "in training loop, epoch 4, step 847, the loss is 209396.21875\n",
            "in training loop, epoch 4, step 848, the loss is 120636.2421875\n",
            "in training loop, epoch 4, step 849, the loss is 165263.78125\n",
            "in training loop, epoch 4, step 850, the loss is 153141.75\n",
            "in training loop, epoch 4, step 851, the loss is 143900.25\n",
            "in training loop, epoch 4, step 852, the loss is 146511.484375\n",
            "in training loop, epoch 4, step 853, the loss is 212462.609375\n",
            "in training loop, epoch 4, step 854, the loss is 141172.421875\n",
            "in training loop, epoch 4, step 855, the loss is 131918.828125\n",
            "in training loop, epoch 4, step 856, the loss is 113675.765625\n",
            "in training loop, epoch 4, step 857, the loss is 121468.734375\n",
            "in training loop, epoch 4, step 858, the loss is 145953.15625\n",
            "in training loop, epoch 4, step 859, the loss is 139595.703125\n",
            "in training loop, epoch 4, step 860, the loss is 138056.78125\n",
            "in training loop, epoch 4, step 861, the loss is 113639.078125\n",
            "in training loop, epoch 4, step 862, the loss is 155714.0\n",
            "in training loop, epoch 4, step 863, the loss is 101989.6328125\n",
            "in training loop, epoch 4, step 864, the loss is 119594.296875\n",
            "in training loop, epoch 4, step 865, the loss is 121082.796875\n",
            "in training loop, epoch 4, step 866, the loss is 144038.875\n",
            "in training loop, epoch 4, step 867, the loss is 106256.0546875\n",
            "in training loop, epoch 4, step 868, the loss is 98939.09375\n",
            "in training loop, epoch 4, step 869, the loss is 97340.8828125\n",
            "in training loop, epoch 4, step 870, the loss is 133701.046875\n",
            "in training loop, epoch 4, step 871, the loss is 128878.53125\n",
            "in training loop, epoch 4, step 872, the loss is 185892.65625\n",
            "in training loop, epoch 4, step 873, the loss is 94328.0390625\n",
            "in training loop, epoch 4, step 874, the loss is 116476.6875\n",
            "in training loop, epoch 4, step 875, the loss is 98677.6640625\n",
            "in training loop, epoch 4, step 876, the loss is 118640.9296875\n",
            "in training loop, epoch 4, step 877, the loss is 130584.640625\n",
            "in training loop, epoch 4, step 878, the loss is 125657.90625\n",
            "in training loop, epoch 4, step 879, the loss is 158125.28125\n",
            "in training loop, epoch 4, step 880, the loss is 100104.53125\n",
            "in training loop, epoch 4, step 881, the loss is 124073.3984375\n",
            "in training loop, epoch 4, step 882, the loss is 122288.28125\n",
            "in training loop, epoch 4, step 883, the loss is 115631.859375\n",
            "in training loop, epoch 4, step 884, the loss is 75853.328125\n",
            "in training loop, epoch 4, step 885, the loss is 140159.453125\n",
            "in training loop, epoch 4, step 886, the loss is 108166.828125\n",
            "in training loop, epoch 4, step 887, the loss is 107902.6640625\n",
            "in training loop, epoch 4, step 888, the loss is 136237.875\n",
            "in training loop, epoch 4, step 889, the loss is 88826.15625\n",
            "in training loop, epoch 4, step 890, the loss is 131906.875\n",
            "in training loop, epoch 4, step 891, the loss is 127788.0859375\n",
            "in training loop, epoch 4, step 892, the loss is 138168.359375\n",
            "in training loop, epoch 4, step 893, the loss is 118766.375\n",
            "in training loop, epoch 4, step 894, the loss is 95825.1875\n",
            "in training loop, epoch 4, step 895, the loss is 198346.765625\n",
            "in training loop, epoch 4, step 896, the loss is 87954.8984375\n",
            "in training loop, epoch 4, step 897, the loss is 139002.5\n",
            "in training loop, epoch 4, step 898, the loss is 90795.8125\n",
            "in training loop, epoch 4, step 899, the loss is 126620.375\n",
            "in training loop, epoch 4, step 900, the loss is 161436.515625\n",
            "in training loop, epoch 4, step 901, the loss is 97402.625\n",
            "in training loop, epoch 4, step 902, the loss is 128729.7734375\n",
            "in training loop, epoch 4, step 903, the loss is 56842.9453125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "k-fold 1:: Epoch 4: train loss 129393.5206201604 val loss 226541.41267017327\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-43cb49274db6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0mtrainingWrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainingWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcrossval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainingWrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m     \u001b[0mtrain_loss_mean_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m     \u001b[0mtest_loss_mean_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mval_loss_mean_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cross_val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3I63RgNmk73"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}