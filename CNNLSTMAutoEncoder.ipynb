{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNLSTMAutoEncoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOHHJsqDq5xXFvbgvH+uLn8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singr7/MIRAutoencoder/blob/master/CNNLSTMAutoEncoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9JqXiXhqdkN"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORCKRmYPHk-z"
      },
      "source": [
        "#Mount the google drive\n",
        "#Create list of numpy files for western and indian dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPnD6kkyHfx8",
        "outputId": "3862d529-1bee-4805-a8ec-d2d4cdacb324"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "western_files = []\n",
        "western_file_dir = \"/content/drive/My Drive/MusicResearchColabNB/Western_numpy\"\n",
        "for r,d, fileList in os.walk(western_file_dir):\n",
        "  for file in fileList:\n",
        "    western_files.append(os.path.join(r,file))\n",
        "\n",
        "indian_files = []\n",
        "indian_file_dir = \"/content/drive/My Drive/MusicResearchColabNB/IndianDataset/Indian_numpy\"\n",
        "for r,d, fileList in os.walk(indian_file_dir):\n",
        "  for file in fileList:\n",
        "    indian_files.append(os.path.join(r,file))\n",
        "\n",
        "print(len(western_files))\n",
        "print(len(indian_files))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "7592\n",
            "2008\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMQbF-ylLmdK"
      },
      "source": [
        "# Balance the western dataset by taking files equal to Indian dataset files = 2008"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXHKJAOLL-iX",
        "outputId": "69de3acb-83c1-495b-83d8-6191ddfa77a2"
      },
      "source": [
        "import random \n",
        "#randomize the selection. To avoid getting a different random sample with every run, use seed\n",
        "random.seed(23)\n",
        "bal_western_files = random.sample(western_files,2008)\n",
        "len(bal_western_files)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2008"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeBwpwaTX3Jo"
      },
      "source": [
        "#Define configuration class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP4323imT53f"
      },
      "source": [
        "class Configuration:\n",
        "  seq_len = 200  # taking half of the original timesteps extracted \n",
        "  input_dim = 26  #num of mels\n",
        "  embedding_dim = 64\n",
        "  batch_size = 2\n",
        "  base_dir = \"/content/drive/My Drive/MusicResearchColabNB/vajra/westernAE\"   # need to be edited..\n",
        "  loss_function = torch.nn.MSELoss(reduction='sum')\n",
        "  lr=1e-3  # I edited it from 1e-3 to 1e-5\n",
        "  n_epochs = 6\n",
        "  model_file = \"/content/drive/My Drive/MusicResearchColabNB/vajra/westernAE/models/mel.pkl\"  #need need edits\n",
        "  results_dir = os.path.join(base_dir, \"./results\")  # may need edits\n",
        "  checkpoint_model_file = \"/content/drive/My Drive/MusicResearchColabNB/vajra/westernAE/models/mel_checkpoint.pkl\" #may need edits\n",
        "  kernel_size = 3  #why?\n",
        "  k_folds = 10 "
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RALRBXgZZBA"
      },
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "  def __init__(self, seq_len, n_features, embedding_dim=64, kernel_size=3, stride=1):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.seq_len, self.n_features = seq_len, n_features\n",
        "    self.embedding_dim, self.hidden_dim = embedding_dim, 2 * embedding_dim\n",
        "\n",
        "\n",
        "    self.conv = nn.Conv1d(in_channels=seq_len,out_channels=seq_len,kernel_size=kernel_size,stride=stride, groups=seq_len)\n",
        "    conv_op_dim = int(((n_features - kernel_size)/ stride) + 1)\n",
        "\n",
        "    self.rnn1 = nn.LSTM(\n",
        "      input_size=conv_op_dim,\n",
        "      hidden_size=self.hidden_dim,\n",
        "      num_layers=1,\n",
        "      batch_first=True\n",
        "    )\n",
        "    self.rnn2 = nn.LSTM(\n",
        "      input_size=self.hidden_dim,\n",
        "      hidden_size=embedding_dim,\n",
        "      num_layers=1,\n",
        "      batch_first=True\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    x = x.reshape((1, self.seq_len, self.n_features))\n",
        "   # print('In Encoder')\n",
        "   # print(x.shape)\n",
        "    x = self.conv(x)\n",
        "    x, (_, _) = self.rnn1(x)\n",
        "    x, (hidden_n, _) = self.rnn2(x)\n",
        "    return x"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkW-A8TzZdGT"
      },
      "source": [
        "class Decoder(torch.nn.Module):\n",
        "  def __init__(self, seq_len, embedding_dim=64, n_features=26):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.seq_len, self.embedding_dim = seq_len, embedding_dim\n",
        "    self.hidden_dim, self.n_features = 2 * embedding_dim, n_features\n",
        "    self.rnn1 = nn.LSTM(\n",
        "      input_size=embedding_dim,\n",
        "      hidden_size=embedding_dim,\n",
        "      num_layers=1,\n",
        "      batch_first=True\n",
        "    )\n",
        "    self.rnn2 = nn.LSTM(\n",
        "      input_size=embedding_dim,\n",
        "      hidden_size=self.hidden_dim,\n",
        "      num_layers=1,\n",
        "      batch_first=True\n",
        "    )\n",
        "    self.output_layer = nn.Linear(self.hidden_dim * self.seq_len, n_features * self.seq_len)\n",
        "  def forward(self, x):\n",
        "    x, (hidden_n, cell_n) = self.rnn1(x)\n",
        "    x, (hidden_n, cell_n) = self.rnn2(x)\n",
        "    #print(\"in decoder\", x.shape)\n",
        "    x = x.contiguous()\n",
        "    x = x.view(x.shape[0], -1)\n",
        "    x = self.output_layer(x)\n",
        "    return x.reshape(x.shape[0],self.seq_len, self.n_features)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mqrvU5MZfEA"
      },
      "source": [
        "class RecurrentAutoencoder(torch.nn.Module):\n",
        "  def __init__(self, seq_len, n_features, embedding_dim=64, device='cpu'):\n",
        "    super(RecurrentAutoencoder, self).__init__()\n",
        "    self.encoder = Encoder(seq_len, n_features, embedding_dim).to(device)\n",
        "    self.decoder = Decoder(seq_len, embedding_dim, n_features).to(device)\n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)\n",
        "    x = self.decoder(x)\n",
        "    return x"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRQ-t9aNZiUD",
        "outputId": "bf54ab9a-9c18-469a-f084-dae15de045a4"
      },
      "source": [
        "x = torch.randn(5, 26, 400)\n",
        "print(x.shape)\n",
        "x = x.permute(0, 2, 1)\n",
        "print(x.shape)\n",
        "\n",
        "encoder = Encoder(400, 26, embedding_dim=64, kernel_size=3, stride=1)\n",
        "encoded = encoder(x)\n",
        "print(encoded.shape)\n",
        "\n",
        "decoder = Decoder(400, 64, 26)\n",
        "decoded = decoder(encoded)\n",
        "print(decoded.shape)\n",
        "\n",
        "rae = RecurrentAutoencoder(400, 26, 64)\n",
        "output = rae(x)\n",
        "\n",
        "print(output.shape)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 26, 400])\n",
            "torch.Size([5, 400, 26])\n",
            "In Encoder\n",
            "torch.Size([5, 400, 26])\n",
            "torch.Size([5, 400, 64])\n",
            "in decoder torch.Size([5, 400, 128])\n",
            "torch.Size([5, 400, 26])\n",
            "In Encoder\n",
            "torch.Size([5, 400, 26])\n",
            "in decoder torch.Size([5, 400, 128])\n",
            "torch.Size([5, 400, 26])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDpec6ICZnKN"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data.sampler import SequentialSampler\n",
        "\n",
        "class CustomDatasetMel(Dataset):\n",
        "\n",
        "    def __init__(self, dataList):\n",
        "        self.data = dataList\n",
        "        #self.labels = labelList\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        import numpy as np\n",
        "        fileName = self.data[index]\n",
        "        \n",
        "        mel_spect = np.load(fileName)\n",
        "        data = torch.tensor(mel_spect[:,:200], dtype=torch.float)\n",
        "        data = data.permute(1, 0)\n",
        "        #data = torch.unsqueeze(data, dim =0)\n",
        "\n",
        "        #label = torch.tensor(self.labels[index])\n",
        "        return data"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uAv1gLc6Zwwr",
        "outputId": "bb0d748e-6ff5-48d5-ea7f-db58cf0d5ff0"
      },
      "source": [
        "import copy\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "class TrainingWrapper:\n",
        "\n",
        "  def __init__(self, config, training_loader, test_loader, device, val_loader=None):\n",
        "    self.config = config\n",
        "    self.training_loader = training_loader\n",
        "    self.test_loader = test_loader\n",
        "    self.val_loader = val_loader\n",
        "    self.device = device\n",
        "    self.model = RecurrentAutoencoder(self.config.seq_len, self.config.input_dim, self.config.embedding_dim, device=self.device)\n",
        "    self.model = self.model.to(self.device)\n",
        "    self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.lr)\n",
        "    self.criterion = self.config.loss_function.to(self.device)\n",
        "    self.history = dict(train=[], val=[], cross_val=[])\n",
        "    self.best_model_wts = copy.deepcopy(self.model.state_dict())\n",
        "    self.best_loss = 10000.0\n",
        "    #print(self.config.base_dir + self.config.model_file)\n",
        "    torch.save(self.model.state_dict(),  self.config.model_file)\n",
        "    #self.cross = cross\n",
        "    \n",
        "\n",
        "  def combine_images(self, generated_images):\n",
        "    num = generated_images.shape[0]\n",
        "    width = int(math.sqrt(num))\n",
        "    height = int(math.ceil(float(num)/width))\n",
        "    shape = generated_images.shape[1:3]\n",
        "    image = np.zeros((height*shape[0], width*shape[1]),\n",
        "                     dtype=generated_images.dtype)\n",
        "    for index, img in enumerate(generated_images):\n",
        "        i = int(index/width)\n",
        "        j = index % width\n",
        "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
        "            img[:, :, 0]\n",
        "    return image\n",
        "\n",
        "\n",
        "  def show_reconstruction(self, test_loader, n_images):\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    from PIL import Image\n",
        "    import numpy as np\n",
        "\n",
        "    self.model.eval()\n",
        "    for x, _ in self.test_loader:\n",
        "        x = x[:min(n_images, x.size(0))].to(self,device)\n",
        "        _, x_recon = self.model(x)\n",
        "        data = np.concatenate([x.data.cpu(), x_recon.data.cpu()])\n",
        "        img = self.combine_images(np.transpose(data, [0, 2, 3, 1]))\n",
        "        image = img * 255\n",
        "        Image.fromarray(image.astype(np.uint8)).save(self.config.base_dir + \"/real_and_recon.png\")\n",
        "        print()\n",
        "        print('Reconstructed images are saved to %s/real_and_recon.png' % self.config.base_dir)\n",
        "        print('-' * 70)\n",
        "        plt.imshow(plt.imread(self.config.base_dir + \"/real_and_recon.png\", ))\n",
        "        plt.show()\n",
        "        break\n",
        "\n",
        "  def visualizeTraining(self, epoch, trn_losses, tst_losses, val_losses, save_dir, cross):\n",
        "    # visualize the loss as the network trained\n",
        "    import matplotlib.pyplot as plt\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    plt.plot(range(0, len(trn_losses)), trn_losses, label='Training Loss')\n",
        "    if tst_losses:\n",
        "      plt.plot(range(0, len(tst_losses)), tst_losses, label='Validation Loss')\n",
        "    if val_losses:\n",
        "      plt.plot(range(0, len(val_losses)), val_losses, label='Cross Validation Loss')\n",
        "\n",
        "    minposs = tst_losses.index(min(tst_losses))\n",
        "    plt.axvline(minposs, linestyle='--', color='r', label='Early Stopping Checkpoint')\n",
        "\n",
        "\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('loss')\n",
        "    # plt.ylim(0, 0.5)  # consistent scale\n",
        "    # plt.xlim(0, len(trn_losses))  # consistent scale\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    fig.savefig(os.path.join(save_dir , 'loss_plot_{}.png'.format(cross)), bbox_inches='tight')\n",
        "\n",
        "  def train(self):\n",
        "    for epoch in range(1, self.config.n_epochs + 1):\n",
        "      self.model = self.model.train()\n",
        "      train_losses = []\n",
        "\n",
        "      for i, x in enumerate(self.training_loader):\n",
        "        self.optimizer.zero_grad()\n",
        "        x = x.to(self.device)\n",
        "\n",
        "        output = self.model(x)\n",
        "        loss = self.criterion(output, x)\n",
        "\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        train_losses.append(loss.item())\n",
        "        print(\"in training loop, epoch {}, step {}, the loss is {}\".format(epoch, i, loss.item()))\n",
        "\n",
        "      val_losses = []\n",
        "      self.model = self.model.eval()\n",
        "      with torch.no_grad():\n",
        "        for i, x in enumerate(self.test_loader):\n",
        "          x = x.to(device)\n",
        "          output = self.model(x)\n",
        "          loss = self.criterion(output, x)\n",
        "          val_losses.append(loss.item())\n",
        "\n",
        "\n",
        "      cross_val_losses = []\n",
        "      self.model = self.model.eval()\n",
        "      with torch.no_grad():\n",
        "        for i, x in enumerate(self.val_loader):\n",
        "          x = x.to(device)\n",
        "          output = self.model(x)\n",
        "          loss = self.criterion(output, x)\n",
        "          cross_val_losses.append(loss.item())\n",
        "\n",
        "      train_loss = np.mean(train_losses)\n",
        "      val_loss = np.mean(val_losses)\n",
        "      cross_val_loss = np.mean(cross_val_losses)\n",
        "\n",
        "\n",
        "      self.history['train'].append(train_loss)\n",
        "      self.history['val'].append(val_loss)\n",
        "      self.history['cross_val'].append(cross_val_loss)\n",
        "\n",
        "      if val_loss < self.best_loss:\n",
        "        self.best_loss = val_loss\n",
        "        self.best_model_wts = copy.deepcopy(self.model.state_dict())\n",
        "      if epoch % 2 == 0:\n",
        "        self.visualizeTraining(epoch, trn_losses= self.history['train'], tst_losses=self.history['val'], val_losses =self.history['cross_val'], save_dir=self.config.base_dir + \"/results\", cross= self.cross)\n",
        "        torch.save(self.model.state_dict(), self.config.base_dir + self.config.checkpoint_model_file + \"_\" + str(self.cross))\n",
        "      print(f'Epoch {epoch}: train loss {train_loss} val loss {val_loss}')\n",
        "    self.model.load_state_dict(self.best_model_wts)\n",
        "    torch.save(self.model.state_dict(), self.config.base_dir + self.config.model_file)\n",
        "    return self.model.eval(), self.history\n",
        "\n",
        "  \n",
        "class TestingWrapper:\n",
        "  def __init__(self, config, device):\n",
        "    self.config = config\n",
        "    self.device = device\n",
        "    self.model = RecurrentAutoencoder(self.config.seq_len, self.config.input_dim, self.config.embedding_dim, device=self.device)\n",
        "    PATH = self.config.base_dir + self.config.checkpoint_model_file\n",
        "    print(PATH)\n",
        "    self.model.load_state_dict(torch.load(PATH, map_location=self.device))\n",
        "    self.model = self.model.to(self.device)\n",
        "\n",
        "  def combine_images(self, generated_images):\n",
        "    num = generated_images.shape[0]\n",
        "    width = int(math.sqrt(num))\n",
        "    height = int(math.ceil(float(num)/width))\n",
        "    shape = generated_images.shape[1:3]\n",
        "    image = np.zeros((height*shape[0], width*shape[1]),\n",
        "                     dtype=generated_images.dtype)\n",
        "    for index, img in enumerate(generated_images):\n",
        "        i = int(index/width)\n",
        "        j = index % width\n",
        "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
        "            img[:, :, 0]\n",
        "    return image\n",
        "\n",
        "\n",
        "  def show_reconstruction(self, test_loader, n_images):\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    from PIL import Image\n",
        "    import numpy as np\n",
        "\n",
        "    self.model.eval()\n",
        "    for x, _ in test_loader:\n",
        "      x = x[:min(n_images, x.size(0))].to(self.device)\n",
        "      x_recon = self.model(x)\n",
        "      data = np.concatenate([x.data.cpu(), x_recon.data.cpu()])\n",
        "      img = self.combine_images(np.transpose(data, [0, 2, 3, 1]))\n",
        "      image = img * 255\n",
        "      Image.fromarray(image.astype(np.uint8)).save(self.config.base_dir + \"/real_and_recon.png\")\n",
        "      print()\n",
        "      print('Reconstructed images are saved to %s/real_and_recon.png' % self.config.base_dir)\n",
        "      print('-' * 70)\n",
        "      plt.imshow(plt.imread(self.config.base_dir + \"/real_and_recon.png\", ))\n",
        "      plt.show()\n",
        "      break\n",
        "\n",
        "  def save_reconstruction(self, test_loader):\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    from PIL import Image\n",
        "    import numpy as np\n",
        "    import uuid\n",
        "\n",
        "\n",
        "    self.model.eval()\n",
        "    with torch.no_grad(): \n",
        "      fileCount = 0\n",
        "      for x, _ in test_loader:\n",
        "        x = x.to(self.device)\n",
        "        x_recon = self.model(x)\n",
        "        x_recon = x_recon.data.cpu().detach().numpy()\n",
        "        for mel in x_recon:\n",
        "          #print(mel.shape)\n",
        "          unique_filename = str(uuid.uuid4())\n",
        "          filename = self.config.base_dir + \"/reconstruction/\" + unique_filename + \".npy\"\n",
        "          np.save(filename, mel)\n",
        "          fileCount = fileCount + 1\n",
        "          print(\"saving file {} at index {}\".format(filename, fileCount))\n",
        "\n",
        "mode = 'train'\n",
        "data = \"mel\"\n",
        "#data = \"mnist\"\n",
        "config = Configuration()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import random\n",
        "import os\n",
        "\n",
        "def seed_everything(seed=1234):\n",
        "  \n",
        "  random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  np.random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def visualizeTraining(epoch, trn_losses, tst_losses, val_losses, save_dir):\n",
        "    # visualize the loss as the network trained\n",
        "    import matplotlib.pyplot as plt\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    plt.plot(range(0, len(trn_losses)), trn_losses, label='Training Loss')\n",
        "    if tst_losses:\n",
        "      plt.plot(range(0, len(tst_losses)), tst_losses, label='Validation Loss')\n",
        "    if val_losses:\n",
        "      plt.plot(range(0, len(val_losses)), val_losses, label='Cross Validation Loss')\n",
        "\n",
        "    minposs = tst_losses.index(min(tst_losses))\n",
        "    plt.axvline(minposs, linestyle='--', color='r', label='Early Stopping Checkpoint')\n",
        "\n",
        "\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('loss')\n",
        "    # plt.ylim(0, 0.5)  # consistent scale\n",
        "    # plt.xlim(0, len(trn_losses))  # consistent scale\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    fig.savefig(os.path.join(save_dir , 'loss_plot_{}.png'.format(\"MEAN\")), bbox_inches='tight')\n",
        "\n",
        "seed_everything()\n",
        "\n",
        "train_data = bal_western_files\n",
        "#labels = [1] * len(bal_western_files)\n",
        "\n",
        "val_data = indian_files\n",
        "#val_labels = [1] * len(indian_files)\n",
        "\n",
        "train_loss_mean_list = []\n",
        "test_loss_mean_list = []\n",
        "val_loss_mean_list = []\n",
        "\n",
        "# Cross validation runs\n",
        "# use sklearn KFolds\n",
        "from sklearn.model_selection import KFold\n",
        "kfold = KFold(n_splits=config.k_folds , shuffle=True)\n",
        "\n",
        "train_dataset = CustomDatasetMel(train_data)\n",
        "val_dataset = CustomDatasetMel(val_data)\n",
        "#Load the cross val dataset which is Full Indian dataset\n",
        "#It is identical for all K-folds\n",
        "crossval_loader = torch.utils.data.DataLoader(\n",
        "                      val_dataset,\n",
        "                      batch_size=config.batch_size, \n",
        "                      sampler=SequentialSampler(val_dataset), \n",
        "                      drop_last=False)  \n",
        "\n",
        "for fold, (train_ids, test_ids) in enumerate(kfold.split(train_dataset)):\n",
        "    # Print\n",
        "  print(f'FOLD {fold}')\n",
        "  print('--------------------------------')\n",
        "    \n",
        "    # Sample elements randomly from a given list of ids, no replacement.\n",
        "  train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "  test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
        "  \n",
        "    # Define data loaders for training and testing data in this fold\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "                      train_dataset, \n",
        "                      batch_size=config.batch_size,\n",
        "                      sampler=train_subsampler,\n",
        "                      drop_last=False)\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "                      train_dataset,\n",
        "                      batch_size=config.batch_size,\n",
        "                      sampler=test_subsampler,\n",
        "                      drop_last=False)\n",
        "    \n",
        "  print(\"length of of train_loader is {} & length of traindataset is {}\".format(len(train_loader),len(train_dataset)))\n",
        "  print(\"length of of test_loader is {}\".format(len(test_loader)))\n",
        "  print(\"length of of val_loader is {}\".format(len(crossval_loader)))\n",
        "    \n",
        "  if mode==\"train\":\n",
        "    trainingWrapper = TrainingWrapper(config=config, training_loader=train_loader, test_loader=test_loader, device=device, val_loader=crossval_loader)\n",
        "    model, history = trainingWrapper.train()\n",
        "    train_loss_mean_list.append(history['train'])\n",
        "    test_loss_mean_list.append(history['val'])\n",
        "    val_loss_mean_list.append(history['cross_val'])\n",
        "    \n",
        "\n",
        "    if data==\"mnist\":\n",
        "      #trainingWrapper.show_reconstruction(test_loader=test_loader, n_images=50)\n",
        "      pass\n",
        "\n",
        "  elif mode==\"test\":\n",
        "    testWrapper = TestingWrapper(config=config, device=device)\n",
        "    testWrapper.save_reconstruction(test_loader)\n",
        "  \n",
        "  train_loss_mean_list = np.mean(train_loss_mean_list, axis=0)\n",
        "  test_loss_mean_list = np.mean(test_loss_mean_list, axis=0)\n",
        "  val_loss_mean_list = np.mean(val_loss_mean_list, axis=0)\n",
        "  visualizeTraining(0, train_loss_mean_list, test_loss_mean_list, val_loss_mean_list, save_dir = config.base_dir + \"/results\")\n",
        "\n",
        "  "
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOLD 0\n",
            "--------------------------------\n",
            "length of of train_loader is 904 & length of traindataset is 2008\n",
            "length of of test_loader is 101\n",
            "length of of val_loader is 1004\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([200, 26])) that is different to the input size (torch.Size([1, 200, 26])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "in training loop, epoch 1, step 0, the loss is 9853472.0\n",
            "in training loop, epoch 1, step 1, the loss is 4005032.0\n",
            "in training loop, epoch 1, step 2, the loss is 6652195.5\n",
            "in training loop, epoch 1, step 3, the loss is 1634187.25\n",
            "in training loop, epoch 1, step 4, the loss is 3382299.25\n",
            "in training loop, epoch 1, step 5, the loss is 2438443.5\n",
            "in training loop, epoch 1, step 6, the loss is 1457719.0\n",
            "in training loop, epoch 1, step 7, the loss is 556820.5\n",
            "in training loop, epoch 1, step 8, the loss is 1568167.375\n",
            "in training loop, epoch 1, step 9, the loss is 1516203.375\n",
            "in training loop, epoch 1, step 10, the loss is 2243941.75\n",
            "in training loop, epoch 1, step 11, the loss is 1374629.5\n",
            "in training loop, epoch 1, step 12, the loss is 2388887.25\n",
            "in training loop, epoch 1, step 13, the loss is 313735.875\n",
            "in training loop, epoch 1, step 14, the loss is 382394.34375\n",
            "in training loop, epoch 1, step 15, the loss is 680324.125\n",
            "in training loop, epoch 1, step 16, the loss is 909154.5\n",
            "in training loop, epoch 1, step 17, the loss is 461340.8125\n",
            "in training loop, epoch 1, step 18, the loss is 2556593.25\n",
            "in training loop, epoch 1, step 19, the loss is 343232.03125\n",
            "in training loop, epoch 1, step 20, the loss is 358361.0\n",
            "in training loop, epoch 1, step 21, the loss is 537759.0625\n",
            "in training loop, epoch 1, step 22, the loss is 562912.625\n",
            "in training loop, epoch 1, step 23, the loss is 1157288.125\n",
            "in training loop, epoch 1, step 24, the loss is 3605845.5\n",
            "in training loop, epoch 1, step 25, the loss is 777571.4375\n",
            "in training loop, epoch 1, step 26, the loss is 612836.125\n",
            "in training loop, epoch 1, step 27, the loss is 1210294.75\n",
            "in training loop, epoch 1, step 28, the loss is 470730.8125\n",
            "in training loop, epoch 1, step 29, the loss is 521848.8125\n",
            "in training loop, epoch 1, step 30, the loss is 988108.125\n",
            "in training loop, epoch 1, step 31, the loss is 1111136.625\n",
            "in training loop, epoch 1, step 32, the loss is 1517846.125\n",
            "in training loop, epoch 1, step 33, the loss is 266413.21875\n",
            "in training loop, epoch 1, step 34, the loss is 349001.8125\n",
            "in training loop, epoch 1, step 35, the loss is 270534.5\n",
            "in training loop, epoch 1, step 36, the loss is 1368962.0\n",
            "in training loop, epoch 1, step 37, the loss is 1385163.5\n",
            "in training loop, epoch 1, step 38, the loss is 1106914.75\n",
            "in training loop, epoch 1, step 39, the loss is 1218902.125\n",
            "in training loop, epoch 1, step 40, the loss is 790893.3125\n",
            "in training loop, epoch 1, step 41, the loss is 458612.90625\n",
            "in training loop, epoch 1, step 42, the loss is 354027.1875\n",
            "in training loop, epoch 1, step 43, the loss is 288446.25\n",
            "in training loop, epoch 1, step 44, the loss is 704686.375\n",
            "in training loop, epoch 1, step 45, the loss is 1302802.875\n",
            "in training loop, epoch 1, step 46, the loss is 7972392.0\n",
            "in training loop, epoch 1, step 47, the loss is 455836.25\n",
            "in training loop, epoch 1, step 48, the loss is 959365.9375\n",
            "in training loop, epoch 1, step 49, the loss is 918151.6875\n",
            "in training loop, epoch 1, step 50, the loss is 782246.5\n",
            "in training loop, epoch 1, step 51, the loss is 869003.8125\n",
            "in training loop, epoch 1, step 52, the loss is 283206.4375\n",
            "in training loop, epoch 1, step 53, the loss is 249904.953125\n",
            "in training loop, epoch 1, step 54, the loss is 726455.375\n",
            "in training loop, epoch 1, step 55, the loss is 289391.21875\n",
            "in training loop, epoch 1, step 56, the loss is 752262.75\n",
            "in training loop, epoch 1, step 57, the loss is 2910925.5\n",
            "in training loop, epoch 1, step 58, the loss is 272164.46875\n",
            "in training loop, epoch 1, step 59, the loss is 333342.09375\n",
            "in training loop, epoch 1, step 60, the loss is 857146.6875\n",
            "in training loop, epoch 1, step 61, the loss is 320228.90625\n",
            "in training loop, epoch 1, step 62, the loss is 462215.6875\n",
            "in training loop, epoch 1, step 63, the loss is 279977.6875\n",
            "in training loop, epoch 1, step 64, the loss is 266247.1875\n",
            "in training loop, epoch 1, step 65, the loss is 382616.5\n",
            "in training loop, epoch 1, step 66, the loss is 529707.125\n",
            "in training loop, epoch 1, step 67, the loss is 469328.53125\n",
            "in training loop, epoch 1, step 68, the loss is 743508.0\n",
            "in training loop, epoch 1, step 69, the loss is 398341.125\n",
            "in training loop, epoch 1, step 70, the loss is 750553.25\n",
            "in training loop, epoch 1, step 71, the loss is 323774.90625\n",
            "in training loop, epoch 1, step 72, the loss is 231681.484375\n",
            "in training loop, epoch 1, step 73, the loss is 844801.875\n",
            "in training loop, epoch 1, step 74, the loss is 416008.59375\n",
            "in training loop, epoch 1, step 75, the loss is 483927.8125\n",
            "in training loop, epoch 1, step 76, the loss is 616440.0\n",
            "in training loop, epoch 1, step 77, the loss is 348241.375\n",
            "in training loop, epoch 1, step 78, the loss is 95408.6015625\n",
            "in training loop, epoch 1, step 79, the loss is 737539.375\n",
            "in training loop, epoch 1, step 80, the loss is 431867.46875\n",
            "in training loop, epoch 1, step 81, the loss is 139236.171875\n",
            "in training loop, epoch 1, step 82, the loss is 165176.796875\n",
            "in training loop, epoch 1, step 83, the loss is 497842.71875\n",
            "in training loop, epoch 1, step 84, the loss is 917347.5\n",
            "in training loop, epoch 1, step 85, the loss is 242107.703125\n",
            "in training loop, epoch 1, step 86, the loss is 372214.375\n",
            "in training loop, epoch 1, step 87, the loss is 378868.625\n",
            "in training loop, epoch 1, step 88, the loss is 193411.703125\n",
            "in training loop, epoch 1, step 89, the loss is 1044462.125\n",
            "in training loop, epoch 1, step 90, the loss is 422416.375\n",
            "in training loop, epoch 1, step 91, the loss is 408073.34375\n",
            "in training loop, epoch 1, step 92, the loss is 737177.75\n",
            "in training loop, epoch 1, step 93, the loss is 1664177.875\n",
            "in training loop, epoch 1, step 94, the loss is 1434229.5\n",
            "in training loop, epoch 1, step 95, the loss is 226761.875\n",
            "in training loop, epoch 1, step 96, the loss is 835862.3125\n",
            "in training loop, epoch 1, step 97, the loss is 1012819.8125\n",
            "in training loop, epoch 1, step 98, the loss is 671980.875\n",
            "in training loop, epoch 1, step 99, the loss is 315614.96875\n",
            "in training loop, epoch 1, step 100, the loss is 532653.4375\n",
            "in training loop, epoch 1, step 101, the loss is 458908.96875\n",
            "in training loop, epoch 1, step 102, the loss is 251833.375\n",
            "in training loop, epoch 1, step 103, the loss is 109170.1484375\n",
            "in training loop, epoch 1, step 104, the loss is 551781.5\n",
            "in training loop, epoch 1, step 105, the loss is 1335190.875\n",
            "in training loop, epoch 1, step 106, the loss is 410402.125\n",
            "in training loop, epoch 1, step 107, the loss is 359666.21875\n",
            "in training loop, epoch 1, step 108, the loss is 191942.1875\n",
            "in training loop, epoch 1, step 109, the loss is 232549.5625\n",
            "in training loop, epoch 1, step 110, the loss is 351431.96875\n",
            "in training loop, epoch 1, step 111, the loss is 1198655.5\n",
            "in training loop, epoch 1, step 112, the loss is 246402.953125\n",
            "in training loop, epoch 1, step 113, the loss is 761742.125\n",
            "in training loop, epoch 1, step 114, the loss is 325283.3125\n",
            "in training loop, epoch 1, step 115, the loss is 570805.875\n",
            "in training loop, epoch 1, step 116, the loss is 602537.9375\n",
            "in training loop, epoch 1, step 117, the loss is 681215.5625\n",
            "in training loop, epoch 1, step 118, the loss is 630944.9375\n",
            "in training loop, epoch 1, step 119, the loss is 318098.0\n",
            "in training loop, epoch 1, step 120, the loss is 518892.5\n",
            "in training loop, epoch 1, step 121, the loss is 196463.6875\n",
            "in training loop, epoch 1, step 122, the loss is 437612.75\n",
            "in training loop, epoch 1, step 123, the loss is 572153.875\n",
            "in training loop, epoch 1, step 124, the loss is 284186.8125\n",
            "in training loop, epoch 1, step 125, the loss is 581617.875\n",
            "in training loop, epoch 1, step 126, the loss is 532086.625\n",
            "in training loop, epoch 1, step 127, the loss is 736915.5\n",
            "in training loop, epoch 1, step 128, the loss is 639720.125\n",
            "in training loop, epoch 1, step 129, the loss is 1292750.0\n",
            "in training loop, epoch 1, step 130, the loss is 2185776.75\n",
            "in training loop, epoch 1, step 131, the loss is 697034.6875\n",
            "in training loop, epoch 1, step 132, the loss is 454058.03125\n",
            "in training loop, epoch 1, step 133, the loss is 607833.4375\n",
            "in training loop, epoch 1, step 134, the loss is 484554.1875\n",
            "in training loop, epoch 1, step 135, the loss is 876792.75\n",
            "in training loop, epoch 1, step 136, the loss is 753649.6875\n",
            "in training loop, epoch 1, step 137, the loss is 596018.0625\n",
            "in training loop, epoch 1, step 138, the loss is 470592.46875\n",
            "in training loop, epoch 1, step 139, the loss is 425754.625\n",
            "in training loop, epoch 1, step 140, the loss is 1364347.625\n",
            "in training loop, epoch 1, step 141, the loss is 634944.9375\n",
            "in training loop, epoch 1, step 142, the loss is 1655749.375\n",
            "in training loop, epoch 1, step 143, the loss is 191893.84375\n",
            "in training loop, epoch 1, step 144, the loss is 2817423.0\n",
            "in training loop, epoch 1, step 145, the loss is 828891.3125\n",
            "in training loop, epoch 1, step 146, the loss is 360547.15625\n",
            "in training loop, epoch 1, step 147, the loss is 593129.6875\n",
            "in training loop, epoch 1, step 148, the loss is 1432335.375\n",
            "in training loop, epoch 1, step 149, the loss is 804278.125\n",
            "in training loop, epoch 1, step 150, the loss is 526897.75\n",
            "in training loop, epoch 1, step 151, the loss is 458696.8125\n",
            "in training loop, epoch 1, step 152, the loss is 267722.96875\n",
            "in training loop, epoch 1, step 153, the loss is 1308199.125\n",
            "in training loop, epoch 1, step 154, the loss is 291191.375\n",
            "in training loop, epoch 1, step 155, the loss is 1025769.4375\n",
            "in training loop, epoch 1, step 156, the loss is 456723.59375\n",
            "in training loop, epoch 1, step 157, the loss is 419236.3125\n",
            "in training loop, epoch 1, step 158, the loss is 198275.453125\n",
            "in training loop, epoch 1, step 159, the loss is 666969.25\n",
            "in training loop, epoch 1, step 160, the loss is 362332.53125\n",
            "in training loop, epoch 1, step 161, the loss is 748304.5625\n",
            "in training loop, epoch 1, step 162, the loss is 448885.75\n",
            "in training loop, epoch 1, step 163, the loss is 1303457.875\n",
            "in training loop, epoch 1, step 164, the loss is 206602.078125\n",
            "in training loop, epoch 1, step 165, the loss is 376010.875\n",
            "in training loop, epoch 1, step 166, the loss is 357048.625\n",
            "in training loop, epoch 1, step 167, the loss is 164074.28125\n",
            "in training loop, epoch 1, step 168, the loss is 827642.125\n",
            "in training loop, epoch 1, step 169, the loss is 624238.5\n",
            "in training loop, epoch 1, step 170, the loss is 199815.0\n",
            "in training loop, epoch 1, step 171, the loss is 276039.15625\n",
            "in training loop, epoch 1, step 172, the loss is 474553.9375\n",
            "in training loop, epoch 1, step 173, the loss is 719812.0625\n",
            "in training loop, epoch 1, step 174, the loss is 125028.703125\n",
            "in training loop, epoch 1, step 175, the loss is 515800.75\n",
            "in training loop, epoch 1, step 176, the loss is 332542.21875\n",
            "in training loop, epoch 1, step 177, the loss is 1062902.375\n",
            "in training loop, epoch 1, step 178, the loss is 722938.1875\n",
            "in training loop, epoch 1, step 179, the loss is 1002101.125\n",
            "in training loop, epoch 1, step 180, the loss is 557963.0625\n",
            "in training loop, epoch 1, step 181, the loss is 386401.71875\n",
            "in training loop, epoch 1, step 182, the loss is 195107.8125\n",
            "in training loop, epoch 1, step 183, the loss is 436267.90625\n",
            "in training loop, epoch 1, step 184, the loss is 1754334.5\n",
            "in training loop, epoch 1, step 185, the loss is 1505341.75\n",
            "in training loop, epoch 1, step 186, the loss is 319760.09375\n",
            "in training loop, epoch 1, step 187, the loss is 169392.078125\n",
            "in training loop, epoch 1, step 188, the loss is 276024.65625\n",
            "in training loop, epoch 1, step 189, the loss is 703197.5\n",
            "in training loop, epoch 1, step 190, the loss is 491697.0\n",
            "in training loop, epoch 1, step 191, the loss is 616189.9375\n",
            "in training loop, epoch 1, step 192, the loss is 647950.875\n",
            "in training loop, epoch 1, step 193, the loss is 414488.3125\n",
            "in training loop, epoch 1, step 194, the loss is 390672.59375\n",
            "in training loop, epoch 1, step 195, the loss is 312940.59375\n",
            "in training loop, epoch 1, step 196, the loss is 165197.1875\n",
            "in training loop, epoch 1, step 197, the loss is 1241918.375\n",
            "in training loop, epoch 1, step 198, the loss is 300163.1875\n",
            "in training loop, epoch 1, step 199, the loss is 198232.96875\n",
            "in training loop, epoch 1, step 200, the loss is 331202.40625\n",
            "in training loop, epoch 1, step 201, the loss is 186961.1875\n",
            "in training loop, epoch 1, step 202, the loss is 739597.9375\n",
            "in training loop, epoch 1, step 203, the loss is 576958.25\n",
            "in training loop, epoch 1, step 204, the loss is 449795.625\n",
            "in training loop, epoch 1, step 205, the loss is 586000.8125\n",
            "in training loop, epoch 1, step 206, the loss is 426452.28125\n",
            "in training loop, epoch 1, step 207, the loss is 320827.875\n",
            "in training loop, epoch 1, step 208, the loss is 299423.96875\n",
            "in training loop, epoch 1, step 209, the loss is 151427.203125\n",
            "in training loop, epoch 1, step 210, the loss is 206807.25\n",
            "in training loop, epoch 1, step 211, the loss is 728562.5\n",
            "in training loop, epoch 1, step 212, the loss is 1336305.25\n",
            "in training loop, epoch 1, step 213, the loss is 1351512.5\n",
            "in training loop, epoch 1, step 214, the loss is 989104.6875\n",
            "in training loop, epoch 1, step 215, the loss is 252563.890625\n",
            "in training loop, epoch 1, step 216, the loss is 497625.59375\n",
            "in training loop, epoch 1, step 217, the loss is 804875.25\n",
            "in training loop, epoch 1, step 218, the loss is 358515.0625\n",
            "in training loop, epoch 1, step 219, the loss is 383062.1875\n",
            "in training loop, epoch 1, step 220, the loss is 503053.9375\n",
            "in training loop, epoch 1, step 221, the loss is 986665.625\n",
            "in training loop, epoch 1, step 222, the loss is 675924.125\n",
            "in training loop, epoch 1, step 223, the loss is 375204.0625\n",
            "in training loop, epoch 1, step 224, the loss is 471815.34375\n",
            "in training loop, epoch 1, step 225, the loss is 101502.234375\n",
            "in training loop, epoch 1, step 226, the loss is 1280066.75\n",
            "in training loop, epoch 1, step 227, the loss is 815862.75\n",
            "in training loop, epoch 1, step 228, the loss is 242813.5625\n",
            "in training loop, epoch 1, step 229, the loss is 382869.84375\n",
            "in training loop, epoch 1, step 230, the loss is 383683.03125\n",
            "in training loop, epoch 1, step 231, the loss is 683824.125\n",
            "in training loop, epoch 1, step 232, the loss is 49828.7734375\n",
            "in training loop, epoch 1, step 233, the loss is 301089.25\n",
            "in training loop, epoch 1, step 234, the loss is 419163.25\n",
            "in training loop, epoch 1, step 235, the loss is 1296151.75\n",
            "in training loop, epoch 1, step 236, the loss is 331683.21875\n",
            "in training loop, epoch 1, step 237, the loss is 288195.1875\n",
            "in training loop, epoch 1, step 238, the loss is 837967.125\n",
            "in training loop, epoch 1, step 239, the loss is 447271.6875\n",
            "in training loop, epoch 1, step 240, the loss is 209418.25\n",
            "in training loop, epoch 1, step 241, the loss is 318243.375\n",
            "in training loop, epoch 1, step 242, the loss is 304199.53125\n",
            "in training loop, epoch 1, step 243, the loss is 314822.71875\n",
            "in training loop, epoch 1, step 244, the loss is 188775.6875\n",
            "in training loop, epoch 1, step 245, the loss is 414870.65625\n",
            "in training loop, epoch 1, step 246, the loss is 798274.75\n",
            "in training loop, epoch 1, step 247, the loss is 560979.5625\n",
            "in training loop, epoch 1, step 248, the loss is 366402.53125\n",
            "in training loop, epoch 1, step 249, the loss is 785434.0625\n",
            "in training loop, epoch 1, step 250, the loss is 464021.5625\n",
            "in training loop, epoch 1, step 251, the loss is 664677.875\n",
            "in training loop, epoch 1, step 252, the loss is 275118.78125\n",
            "in training loop, epoch 1, step 253, the loss is 228129.625\n",
            "in training loop, epoch 1, step 254, the loss is 332712.15625\n",
            "in training loop, epoch 1, step 255, the loss is 212290.78125\n",
            "in training loop, epoch 1, step 256, the loss is 270068.0625\n",
            "in training loop, epoch 1, step 257, the loss is 151388.578125\n",
            "in training loop, epoch 1, step 258, the loss is 985809.0\n",
            "in training loop, epoch 1, step 259, the loss is 395609.125\n",
            "in training loop, epoch 1, step 260, the loss is 238642.875\n",
            "in training loop, epoch 1, step 261, the loss is 326228.5\n",
            "in training loop, epoch 1, step 262, the loss is 519564.03125\n",
            "in training loop, epoch 1, step 263, the loss is 287032.84375\n",
            "in training loop, epoch 1, step 264, the loss is 475922.3125\n",
            "in training loop, epoch 1, step 265, the loss is 308649.25\n",
            "in training loop, epoch 1, step 266, the loss is 2020537.0\n",
            "in training loop, epoch 1, step 267, the loss is 348967.0625\n",
            "in training loop, epoch 1, step 268, the loss is 297510.46875\n",
            "in training loop, epoch 1, step 269, the loss is 504416.0625\n",
            "in training loop, epoch 1, step 270, the loss is 191238.671875\n",
            "in training loop, epoch 1, step 271, the loss is 705301.75\n",
            "in training loop, epoch 1, step 272, the loss is 241823.21875\n",
            "in training loop, epoch 1, step 273, the loss is 157551.34375\n",
            "in training loop, epoch 1, step 274, the loss is 321795.125\n",
            "in training loop, epoch 1, step 275, the loss is 197925.5625\n",
            "in training loop, epoch 1, step 276, the loss is 224810.9375\n",
            "in training loop, epoch 1, step 277, the loss is 269118.03125\n",
            "in training loop, epoch 1, step 278, the loss is 645932.1875\n",
            "in training loop, epoch 1, step 279, the loss is 117025.1015625\n",
            "in training loop, epoch 1, step 280, the loss is 299039.90625\n",
            "in training loop, epoch 1, step 281, the loss is 154670.03125\n",
            "in training loop, epoch 1, step 282, the loss is 444910.1875\n",
            "in training loop, epoch 1, step 283, the loss is 771754.0\n",
            "in training loop, epoch 1, step 284, the loss is 271374.40625\n",
            "in training loop, epoch 1, step 285, the loss is 332635.21875\n",
            "in training loop, epoch 1, step 286, the loss is 260587.015625\n",
            "in training loop, epoch 1, step 287, the loss is 1259457.5\n",
            "in training loop, epoch 1, step 288, the loss is 204660.84375\n",
            "in training loop, epoch 1, step 289, the loss is 765753.5\n",
            "in training loop, epoch 1, step 290, the loss is 580683.0625\n",
            "in training loop, epoch 1, step 291, the loss is 451495.1875\n",
            "in training loop, epoch 1, step 292, the loss is 84869.5703125\n",
            "in training loop, epoch 1, step 293, the loss is 146312.828125\n",
            "in training loop, epoch 1, step 294, the loss is 338341.5\n",
            "in training loop, epoch 1, step 295, the loss is 563711.75\n",
            "in training loop, epoch 1, step 296, the loss is 736680.75\n",
            "in training loop, epoch 1, step 297, the loss is 264981.25\n",
            "in training loop, epoch 1, step 298, the loss is 340521.84375\n",
            "in training loop, epoch 1, step 299, the loss is 469765.46875\n",
            "in training loop, epoch 1, step 300, the loss is 380260.4375\n",
            "in training loop, epoch 1, step 301, the loss is 327587.5625\n",
            "in training loop, epoch 1, step 302, the loss is 196599.96875\n",
            "in training loop, epoch 1, step 303, the loss is 157110.75\n",
            "in training loop, epoch 1, step 304, the loss is 576774.25\n",
            "in training loop, epoch 1, step 305, the loss is 173911.34375\n",
            "in training loop, epoch 1, step 306, the loss is 629853.0625\n",
            "in training loop, epoch 1, step 307, the loss is 405934.40625\n",
            "in training loop, epoch 1, step 308, the loss is 349307.15625\n",
            "in training loop, epoch 1, step 309, the loss is 323659.65625\n",
            "in training loop, epoch 1, step 310, the loss is 683790.9375\n",
            "in training loop, epoch 1, step 311, the loss is 276615.90625\n",
            "in training loop, epoch 1, step 312, the loss is 477128.875\n",
            "in training loop, epoch 1, step 313, the loss is 262444.625\n",
            "in training loop, epoch 1, step 314, the loss is 282580.46875\n",
            "in training loop, epoch 1, step 315, the loss is 156496.546875\n",
            "in training loop, epoch 1, step 316, the loss is 398850.9375\n",
            "in training loop, epoch 1, step 317, the loss is 714863.9375\n",
            "in training loop, epoch 1, step 318, the loss is 134024.890625\n",
            "in training loop, epoch 1, step 319, the loss is 195245.25\n",
            "in training loop, epoch 1, step 320, the loss is 399980.4375\n",
            "in training loop, epoch 1, step 321, the loss is 833720.0625\n",
            "in training loop, epoch 1, step 322, the loss is 242082.328125\n",
            "in training loop, epoch 1, step 323, the loss is 167712.78125\n",
            "in training loop, epoch 1, step 324, the loss is 183218.640625\n",
            "in training loop, epoch 1, step 325, the loss is 584446.5625\n",
            "in training loop, epoch 1, step 326, the loss is 828106.75\n",
            "in training loop, epoch 1, step 327, the loss is 152249.734375\n",
            "in training loop, epoch 1, step 328, the loss is 657735.875\n",
            "in training loop, epoch 1, step 329, the loss is 252068.734375\n",
            "in training loop, epoch 1, step 330, the loss is 201625.625\n",
            "in training loop, epoch 1, step 331, the loss is 259205.59375\n",
            "in training loop, epoch 1, step 332, the loss is 288331.84375\n",
            "in training loop, epoch 1, step 333, the loss is 323099.875\n",
            "in training loop, epoch 1, step 334, the loss is 295461.15625\n",
            "in training loop, epoch 1, step 335, the loss is 128865.140625\n",
            "in training loop, epoch 1, step 336, the loss is 218668.84375\n",
            "in training loop, epoch 1, step 337, the loss is 308011.75\n",
            "in training loop, epoch 1, step 338, the loss is 568953.875\n",
            "in training loop, epoch 1, step 339, the loss is 507364.375\n",
            "in training loop, epoch 1, step 340, the loss is 367448.9375\n",
            "in training loop, epoch 1, step 341, the loss is 1313426.5\n",
            "in training loop, epoch 1, step 342, the loss is 620953.75\n",
            "in training loop, epoch 1, step 343, the loss is 831058.6875\n",
            "in training loop, epoch 1, step 344, the loss is 200424.828125\n",
            "in training loop, epoch 1, step 345, the loss is 547426.1875\n",
            "in training loop, epoch 1, step 346, the loss is 179807.375\n",
            "in training loop, epoch 1, step 347, the loss is 212314.8125\n",
            "in training loop, epoch 1, step 348, the loss is 295278.21875\n",
            "in training loop, epoch 1, step 349, the loss is 204838.734375\n",
            "in training loop, epoch 1, step 350, the loss is 314780.34375\n",
            "in training loop, epoch 1, step 351, the loss is 311637.375\n",
            "in training loop, epoch 1, step 352, the loss is 599478.0\n",
            "in training loop, epoch 1, step 353, the loss is 264536.125\n",
            "in training loop, epoch 1, step 354, the loss is 107331.6171875\n",
            "in training loop, epoch 1, step 355, the loss is 344736.375\n",
            "in training loop, epoch 1, step 356, the loss is 606873.25\n",
            "in training loop, epoch 1, step 357, the loss is 341367.0625\n",
            "in training loop, epoch 1, step 358, the loss is 526875.125\n",
            "in training loop, epoch 1, step 359, the loss is 1282275.5\n",
            "in training loop, epoch 1, step 360, the loss is 287983.8125\n",
            "in training loop, epoch 1, step 361, the loss is 174837.03125\n",
            "in training loop, epoch 1, step 362, the loss is 804963.9375\n",
            "in training loop, epoch 1, step 363, the loss is 696996.625\n",
            "in training loop, epoch 1, step 364, the loss is 179397.90625\n",
            "in training loop, epoch 1, step 365, the loss is 179587.3125\n",
            "in training loop, epoch 1, step 366, the loss is 350272.09375\n",
            "in training loop, epoch 1, step 367, the loss is 279396.125\n",
            "in training loop, epoch 1, step 368, the loss is 664753.375\n",
            "in training loop, epoch 1, step 369, the loss is 285736.6875\n",
            "in training loop, epoch 1, step 370, the loss is 785260.8125\n",
            "in training loop, epoch 1, step 371, the loss is 490599.46875\n",
            "in training loop, epoch 1, step 372, the loss is 140311.953125\n",
            "in training loop, epoch 1, step 373, the loss is 244017.25\n",
            "in training loop, epoch 1, step 374, the loss is 483445.90625\n",
            "in training loop, epoch 1, step 375, the loss is 323936.71875\n",
            "in training loop, epoch 1, step 376, the loss is 112525.0390625\n",
            "in training loop, epoch 1, step 377, the loss is 462562.1875\n",
            "in training loop, epoch 1, step 378, the loss is 189185.078125\n",
            "in training loop, epoch 1, step 379, the loss is 184408.28125\n",
            "in training loop, epoch 1, step 380, the loss is 440664.6875\n",
            "in training loop, epoch 1, step 381, the loss is 354619.875\n",
            "in training loop, epoch 1, step 382, the loss is 310153.0\n",
            "in training loop, epoch 1, step 383, the loss is 299126.125\n",
            "in training loop, epoch 1, step 384, the loss is 758326.0625\n",
            "in training loop, epoch 1, step 385, the loss is 414621.375\n",
            "in training loop, epoch 1, step 386, the loss is 337980.9375\n",
            "in training loop, epoch 1, step 387, the loss is 1113260.625\n",
            "in training loop, epoch 1, step 388, the loss is 326958.84375\n",
            "in training loop, epoch 1, step 389, the loss is 166526.71875\n",
            "in training loop, epoch 1, step 390, the loss is 862740.3125\n",
            "in training loop, epoch 1, step 391, the loss is 537691.4375\n",
            "in training loop, epoch 1, step 392, the loss is 473473.09375\n",
            "in training loop, epoch 1, step 393, the loss is 118590.203125\n",
            "in training loop, epoch 1, step 394, the loss is 139928.40625\n",
            "in training loop, epoch 1, step 395, the loss is 194731.84375\n",
            "in training loop, epoch 1, step 396, the loss is 176272.75\n",
            "in training loop, epoch 1, step 397, the loss is 802449.5625\n",
            "in training loop, epoch 1, step 398, the loss is 476326.125\n",
            "in training loop, epoch 1, step 399, the loss is 282146.9375\n",
            "in training loop, epoch 1, step 400, the loss is 457616.28125\n",
            "in training loop, epoch 1, step 401, the loss is 822450.25\n",
            "in training loop, epoch 1, step 402, the loss is 328741.75\n",
            "in training loop, epoch 1, step 403, the loss is 719100.5625\n",
            "in training loop, epoch 1, step 404, the loss is 256394.5\n",
            "in training loop, epoch 1, step 405, the loss is 338974.09375\n",
            "in training loop, epoch 1, step 406, the loss is 393459.0\n",
            "in training loop, epoch 1, step 407, the loss is 441683.15625\n",
            "in training loop, epoch 1, step 408, the loss is 236133.125\n",
            "in training loop, epoch 1, step 409, the loss is 365154.71875\n",
            "in training loop, epoch 1, step 410, the loss is 382647.65625\n",
            "in training loop, epoch 1, step 411, the loss is 137794.5\n",
            "in training loop, epoch 1, step 412, the loss is 318245.75\n",
            "in training loop, epoch 1, step 413, the loss is 296583.15625\n",
            "in training loop, epoch 1, step 414, the loss is 424474.40625\n",
            "in training loop, epoch 1, step 415, the loss is 861438.875\n",
            "in training loop, epoch 1, step 416, the loss is 265231.1875\n",
            "in training loop, epoch 1, step 417, the loss is 167587.5625\n",
            "in training loop, epoch 1, step 418, the loss is 302586.0625\n",
            "in training loop, epoch 1, step 419, the loss is 503179.75\n",
            "in training loop, epoch 1, step 420, the loss is 276896.5\n",
            "in training loop, epoch 1, step 421, the loss is 181662.09375\n",
            "in training loop, epoch 1, step 422, the loss is 299012.375\n",
            "in training loop, epoch 1, step 423, the loss is 224823.6875\n",
            "in training loop, epoch 1, step 424, the loss is 39383.7890625\n",
            "in training loop, epoch 1, step 425, the loss is 664242.5\n",
            "in training loop, epoch 1, step 426, the loss is 549329.0\n",
            "in training loop, epoch 1, step 427, the loss is 352954.96875\n",
            "in training loop, epoch 1, step 428, the loss is 296656.71875\n",
            "in training loop, epoch 1, step 429, the loss is 912701.8125\n",
            "in training loop, epoch 1, step 430, the loss is 450701.6875\n",
            "in training loop, epoch 1, step 431, the loss is 159893.609375\n",
            "in training loop, epoch 1, step 432, the loss is 352494.59375\n",
            "in training loop, epoch 1, step 433, the loss is 222548.21875\n",
            "in training loop, epoch 1, step 434, the loss is 219097.96875\n",
            "in training loop, epoch 1, step 435, the loss is 381885.8125\n",
            "in training loop, epoch 1, step 436, the loss is 498918.40625\n",
            "in training loop, epoch 1, step 437, the loss is 132478.390625\n",
            "in training loop, epoch 1, step 438, the loss is 422090.84375\n",
            "in training loop, epoch 1, step 439, the loss is 725799.625\n",
            "in training loop, epoch 1, step 440, the loss is 498158.15625\n",
            "in training loop, epoch 1, step 441, the loss is 525054.4375\n",
            "in training loop, epoch 1, step 442, the loss is 596329.5\n",
            "in training loop, epoch 1, step 443, the loss is 495884.25\n",
            "in training loop, epoch 1, step 444, the loss is 456908.15625\n",
            "in training loop, epoch 1, step 445, the loss is 116736.9140625\n",
            "in training loop, epoch 1, step 446, the loss is 270235.59375\n",
            "in training loop, epoch 1, step 447, the loss is 278274.03125\n",
            "in training loop, epoch 1, step 448, the loss is 497546.875\n",
            "in training loop, epoch 1, step 449, the loss is 278328.125\n",
            "in training loop, epoch 1, step 450, the loss is 589369.625\n",
            "in training loop, epoch 1, step 451, the loss is 565757.6875\n",
            "in training loop, epoch 1, step 452, the loss is 322456.375\n",
            "in training loop, epoch 1, step 453, the loss is 394160.84375\n",
            "in training loop, epoch 1, step 454, the loss is 323921.4375\n",
            "in training loop, epoch 1, step 455, the loss is 345573.5625\n",
            "in training loop, epoch 1, step 456, the loss is 88528.15625\n",
            "in training loop, epoch 1, step 457, the loss is 322291.6875\n",
            "in training loop, epoch 1, step 458, the loss is 327979.15625\n",
            "in training loop, epoch 1, step 459, the loss is 501516.75\n",
            "in training loop, epoch 1, step 460, the loss is 439237.625\n",
            "in training loop, epoch 1, step 461, the loss is 208205.28125\n",
            "in training loop, epoch 1, step 462, the loss is 4442105.5\n",
            "in training loop, epoch 1, step 463, the loss is 401355.65625\n",
            "in training loop, epoch 1, step 464, the loss is 569440.0\n",
            "in training loop, epoch 1, step 465, the loss is 633242.0\n",
            "in training loop, epoch 1, step 466, the loss is 442797.375\n",
            "in training loop, epoch 1, step 467, the loss is 249544.625\n",
            "in training loop, epoch 1, step 468, the loss is 956957.9375\n",
            "in training loop, epoch 1, step 469, the loss is 361283.53125\n",
            "in training loop, epoch 1, step 470, the loss is 1060856.125\n",
            "in training loop, epoch 1, step 471, the loss is 478960.59375\n",
            "in training loop, epoch 1, step 472, the loss is 216244.125\n",
            "in training loop, epoch 1, step 473, the loss is 676381.75\n",
            "in training loop, epoch 1, step 474, the loss is 313104.8125\n",
            "in training loop, epoch 1, step 475, the loss is 1129213.0\n",
            "in training loop, epoch 1, step 476, the loss is 395296.78125\n",
            "in training loop, epoch 1, step 477, the loss is 231681.6875\n",
            "in training loop, epoch 1, step 478, the loss is 1368038.5\n",
            "in training loop, epoch 1, step 479, the loss is 464445.0625\n",
            "in training loop, epoch 1, step 480, the loss is 233324.546875\n",
            "in training loop, epoch 1, step 481, the loss is 569546.3125\n",
            "in training loop, epoch 1, step 482, the loss is 1626176.875\n",
            "in training loop, epoch 1, step 483, the loss is 613229.375\n",
            "in training loop, epoch 1, step 484, the loss is 558419.5\n",
            "in training loop, epoch 1, step 485, the loss is 929343.375\n",
            "in training loop, epoch 1, step 486, the loss is 362900.8125\n",
            "in training loop, epoch 1, step 487, the loss is 745777.9375\n",
            "in training loop, epoch 1, step 488, the loss is 912479.625\n",
            "in training loop, epoch 1, step 489, the loss is 702442.125\n",
            "in training loop, epoch 1, step 490, the loss is 483035.5\n",
            "in training loop, epoch 1, step 491, the loss is 983578.125\n",
            "in training loop, epoch 1, step 492, the loss is 572820.8125\n",
            "in training loop, epoch 1, step 493, the loss is 2144106.25\n",
            "in training loop, epoch 1, step 494, the loss is 223168.4375\n",
            "in training loop, epoch 1, step 495, the loss is 372298.5625\n",
            "in training loop, epoch 1, step 496, the loss is 361072.0625\n",
            "in training loop, epoch 1, step 497, the loss is 175918.4375\n",
            "in training loop, epoch 1, step 498, the loss is 143078.375\n",
            "in training loop, epoch 1, step 499, the loss is 755828.3125\n",
            "in training loop, epoch 1, step 500, the loss is 1004956.5625\n",
            "in training loop, epoch 1, step 501, the loss is 6209322.5\n",
            "in training loop, epoch 1, step 502, the loss is 200993.5\n",
            "in training loop, epoch 1, step 503, the loss is 2068676.25\n",
            "in training loop, epoch 1, step 504, the loss is 483677.125\n",
            "in training loop, epoch 1, step 505, the loss is 1030176.375\n",
            "in training loop, epoch 1, step 506, the loss is 418758.1875\n",
            "in training loop, epoch 1, step 507, the loss is 587673.1875\n",
            "in training loop, epoch 1, step 508, the loss is 1496933.5\n",
            "in training loop, epoch 1, step 509, the loss is 810036.5625\n",
            "in training loop, epoch 1, step 510, the loss is 290578.625\n",
            "in training loop, epoch 1, step 511, the loss is 2085044.0\n",
            "in training loop, epoch 1, step 512, the loss is 457464.3125\n",
            "in training loop, epoch 1, step 513, the loss is 231626.875\n",
            "in training loop, epoch 1, step 514, the loss is 212789.453125\n",
            "in training loop, epoch 1, step 515, the loss is 134164.34375\n",
            "in training loop, epoch 1, step 516, the loss is 193574.3125\n",
            "in training loop, epoch 1, step 517, the loss is 338529.9375\n",
            "in training loop, epoch 1, step 518, the loss is 142568.796875\n",
            "in training loop, epoch 1, step 519, the loss is 274884.78125\n",
            "in training loop, epoch 1, step 520, the loss is 938249.5\n",
            "in training loop, epoch 1, step 521, the loss is 528813.1875\n",
            "in training loop, epoch 1, step 522, the loss is 107456.484375\n",
            "in training loop, epoch 1, step 523, the loss is 176699.4375\n",
            "in training loop, epoch 1, step 524, the loss is 120026.9296875\n",
            "in training loop, epoch 1, step 525, the loss is 1681392.5\n",
            "in training loop, epoch 1, step 526, the loss is 425768.5625\n",
            "in training loop, epoch 1, step 527, the loss is 763308.75\n",
            "in training loop, epoch 1, step 528, the loss is 603053.375\n",
            "in training loop, epoch 1, step 529, the loss is 2562916.0\n",
            "in training loop, epoch 1, step 530, the loss is 519066.71875\n",
            "in training loop, epoch 1, step 531, the loss is 394350.28125\n",
            "in training loop, epoch 1, step 532, the loss is 922075.8125\n",
            "in training loop, epoch 1, step 533, the loss is 557298.8125\n",
            "in training loop, epoch 1, step 534, the loss is 434898.5625\n",
            "in training loop, epoch 1, step 535, the loss is 772691.9375\n",
            "in training loop, epoch 1, step 536, the loss is 1299900.625\n",
            "in training loop, epoch 1, step 537, the loss is 620571.0\n",
            "in training loop, epoch 1, step 538, the loss is 1001064.125\n",
            "in training loop, epoch 1, step 539, the loss is 505950.6875\n",
            "in training loop, epoch 1, step 540, the loss is 286633.28125\n",
            "in training loop, epoch 1, step 541, the loss is 230250.21875\n",
            "in training loop, epoch 1, step 542, the loss is 840610.625\n",
            "in training loop, epoch 1, step 543, the loss is 221419.140625\n",
            "in training loop, epoch 1, step 544, the loss is 452913.125\n",
            "in training loop, epoch 1, step 545, the loss is 173941.234375\n",
            "in training loop, epoch 1, step 546, the loss is 482012.0\n",
            "in training loop, epoch 1, step 547, the loss is 505150.96875\n",
            "in training loop, epoch 1, step 548, the loss is 756412.1875\n",
            "in training loop, epoch 1, step 549, the loss is 2014235.125\n",
            "in training loop, epoch 1, step 550, the loss is 500192.46875\n",
            "in training loop, epoch 1, step 551, the loss is 174331.140625\n",
            "in training loop, epoch 1, step 552, the loss is 274761.28125\n",
            "in training loop, epoch 1, step 553, the loss is 339707.53125\n",
            "in training loop, epoch 1, step 554, the loss is 653719.875\n",
            "in training loop, epoch 1, step 555, the loss is 423330.65625\n",
            "in training loop, epoch 1, step 556, the loss is 466799.0625\n",
            "in training loop, epoch 1, step 557, the loss is 217183.65625\n",
            "in training loop, epoch 1, step 558, the loss is 459172.375\n",
            "in training loop, epoch 1, step 559, the loss is 384352.78125\n",
            "in training loop, epoch 1, step 560, the loss is 213646.375\n",
            "in training loop, epoch 1, step 561, the loss is 693071.9375\n",
            "in training loop, epoch 1, step 562, the loss is 1221242.25\n",
            "in training loop, epoch 1, step 563, the loss is 462523.59375\n",
            "in training loop, epoch 1, step 564, the loss is 422303.0625\n",
            "in training loop, epoch 1, step 565, the loss is 269423.9375\n",
            "in training loop, epoch 1, step 566, the loss is 1503837.25\n",
            "in training loop, epoch 1, step 567, the loss is 229457.4375\n",
            "in training loop, epoch 1, step 568, the loss is 396347.0625\n",
            "in training loop, epoch 1, step 569, the loss is 489896.125\n",
            "in training loop, epoch 1, step 570, the loss is 492654.46875\n",
            "in training loop, epoch 1, step 571, the loss is 1055170.0\n",
            "in training loop, epoch 1, step 572, the loss is 343232.59375\n",
            "in training loop, epoch 1, step 573, the loss is 273393.8125\n",
            "in training loop, epoch 1, step 574, the loss is 209085.1875\n",
            "in training loop, epoch 1, step 575, the loss is 558487.9375\n",
            "in training loop, epoch 1, step 576, the loss is 309737.28125\n",
            "in training loop, epoch 1, step 577, the loss is 293831.6875\n",
            "in training loop, epoch 1, step 578, the loss is 286566.9375\n",
            "in training loop, epoch 1, step 579, the loss is 1455952.875\n",
            "in training loop, epoch 1, step 580, the loss is 575602.8125\n",
            "in training loop, epoch 1, step 581, the loss is 280244.875\n",
            "in training loop, epoch 1, step 582, the loss is 469194.15625\n",
            "in training loop, epoch 1, step 583, the loss is 446402.125\n",
            "in training loop, epoch 1, step 584, the loss is 408711.09375\n",
            "in training loop, epoch 1, step 585, the loss is 496231.0\n",
            "in training loop, epoch 1, step 586, the loss is 571183.125\n",
            "in training loop, epoch 1, step 587, the loss is 402217.6875\n",
            "in training loop, epoch 1, step 588, the loss is 569531.5625\n",
            "in training loop, epoch 1, step 589, the loss is 269792.21875\n",
            "in training loop, epoch 1, step 590, the loss is 559336.125\n",
            "in training loop, epoch 1, step 591, the loss is 202718.578125\n",
            "in training loop, epoch 1, step 592, the loss is 226746.90625\n",
            "in training loop, epoch 1, step 593, the loss is 177027.25\n",
            "in training loop, epoch 1, step 594, the loss is 195245.515625\n",
            "in training loop, epoch 1, step 595, the loss is 155117.75\n",
            "in training loop, epoch 1, step 596, the loss is 649571.4375\n",
            "in training loop, epoch 1, step 597, the loss is 873456.375\n",
            "in training loop, epoch 1, step 598, the loss is 240804.578125\n",
            "in training loop, epoch 1, step 599, the loss is 183206.21875\n",
            "in training loop, epoch 1, step 600, the loss is 371793.5\n",
            "in training loop, epoch 1, step 601, the loss is 130884.7890625\n",
            "in training loop, epoch 1, step 602, the loss is 967150.5625\n",
            "in training loop, epoch 1, step 603, the loss is 331708.53125\n",
            "in training loop, epoch 1, step 604, the loss is 361325.15625\n",
            "in training loop, epoch 1, step 605, the loss is 314217.78125\n",
            "in training loop, epoch 1, step 606, the loss is 197083.296875\n",
            "in training loop, epoch 1, step 607, the loss is 68655.703125\n",
            "in training loop, epoch 1, step 608, the loss is 284703.28125\n",
            "in training loop, epoch 1, step 609, the loss is 398625.125\n",
            "in training loop, epoch 1, step 610, the loss is 200022.125\n",
            "in training loop, epoch 1, step 611, the loss is 234901.4375\n",
            "in training loop, epoch 1, step 612, the loss is 215613.5\n",
            "in training loop, epoch 1, step 613, the loss is 197815.140625\n",
            "in training loop, epoch 1, step 614, the loss is 423583.1875\n",
            "in training loop, epoch 1, step 615, the loss is 187230.09375\n",
            "in training loop, epoch 1, step 616, the loss is 229179.0\n",
            "in training loop, epoch 1, step 617, the loss is 263332.90625\n",
            "in training loop, epoch 1, step 618, the loss is 331223.875\n",
            "in training loop, epoch 1, step 619, the loss is 356414.5625\n",
            "in training loop, epoch 1, step 620, the loss is 739733.625\n",
            "in training loop, epoch 1, step 621, the loss is 303378.0\n",
            "in training loop, epoch 1, step 622, the loss is 250475.40625\n",
            "in training loop, epoch 1, step 623, the loss is 479649.5625\n",
            "in training loop, epoch 1, step 624, the loss is 790327.25\n",
            "in training loop, epoch 1, step 625, the loss is 130536.1328125\n",
            "in training loop, epoch 1, step 626, the loss is 180153.34375\n",
            "in training loop, epoch 1, step 627, the loss is 566550.0625\n",
            "in training loop, epoch 1, step 628, the loss is 238338.15625\n",
            "in training loop, epoch 1, step 629, the loss is 312722.03125\n",
            "in training loop, epoch 1, step 630, the loss is 84422.09375\n",
            "in training loop, epoch 1, step 631, the loss is 581208.25\n",
            "in training loop, epoch 1, step 632, the loss is 399733.875\n",
            "in training loop, epoch 1, step 633, the loss is 728529.875\n",
            "in training loop, epoch 1, step 634, the loss is 1404015.0\n",
            "in training loop, epoch 1, step 635, the loss is 305395.625\n",
            "in training loop, epoch 1, step 636, the loss is 289180.3125\n",
            "in training loop, epoch 1, step 637, the loss is 162673.203125\n",
            "in training loop, epoch 1, step 638, the loss is 244950.71875\n",
            "in training loop, epoch 1, step 639, the loss is 275701.625\n",
            "in training loop, epoch 1, step 640, the loss is 447842.59375\n",
            "in training loop, epoch 1, step 641, the loss is 1270376.0\n",
            "in training loop, epoch 1, step 642, the loss is 259794.015625\n",
            "in training loop, epoch 1, step 643, the loss is 278133.71875\n",
            "in training loop, epoch 1, step 644, the loss is 139715.171875\n",
            "in training loop, epoch 1, step 645, the loss is 402838.5625\n",
            "in training loop, epoch 1, step 646, the loss is 1457837.0\n",
            "in training loop, epoch 1, step 647, the loss is 289477.40625\n",
            "in training loop, epoch 1, step 648, the loss is 599589.125\n",
            "in training loop, epoch 1, step 649, the loss is 805925.125\n",
            "in training loop, epoch 1, step 650, the loss is 340428.90625\n",
            "in training loop, epoch 1, step 651, the loss is 388828.78125\n",
            "in training loop, epoch 1, step 652, the loss is 148574.1875\n",
            "in training loop, epoch 1, step 653, the loss is 149455.375\n",
            "in training loop, epoch 1, step 654, the loss is 1555263.625\n",
            "in training loop, epoch 1, step 655, the loss is 313148.5625\n",
            "in training loop, epoch 1, step 656, the loss is 901615.6875\n",
            "in training loop, epoch 1, step 657, the loss is 243545.703125\n",
            "in training loop, epoch 1, step 658, the loss is 429447.21875\n",
            "in training loop, epoch 1, step 659, the loss is 520472.59375\n",
            "in training loop, epoch 1, step 660, the loss is 1603035.0\n",
            "in training loop, epoch 1, step 661, the loss is 319793.1875\n",
            "in training loop, epoch 1, step 662, the loss is 396514.96875\n",
            "in training loop, epoch 1, step 663, the loss is 679097.0625\n",
            "in training loop, epoch 1, step 664, the loss is 245811.78125\n",
            "in training loop, epoch 1, step 665, the loss is 314994.28125\n",
            "in training loop, epoch 1, step 666, the loss is 111475.2265625\n",
            "in training loop, epoch 1, step 667, the loss is 320690.9375\n",
            "in training loop, epoch 1, step 668, the loss is 685794.8125\n",
            "in training loop, epoch 1, step 669, the loss is 405824.5625\n",
            "in training loop, epoch 1, step 670, the loss is 410328.25\n",
            "in training loop, epoch 1, step 671, the loss is 276020.125\n",
            "in training loop, epoch 1, step 672, the loss is 179244.078125\n",
            "in training loop, epoch 1, step 673, the loss is 351901.0\n",
            "in training loop, epoch 1, step 674, the loss is 155334.453125\n",
            "in training loop, epoch 1, step 675, the loss is 334673.65625\n",
            "in training loop, epoch 1, step 676, the loss is 155292.265625\n",
            "in training loop, epoch 1, step 677, the loss is 417318.875\n",
            "in training loop, epoch 1, step 678, the loss is 570912.5\n",
            "in training loop, epoch 1, step 679, the loss is 254875.296875\n",
            "in training loop, epoch 1, step 680, the loss is 326625.1875\n",
            "in training loop, epoch 1, step 681, the loss is 590239.625\n",
            "in training loop, epoch 1, step 682, the loss is 952576.375\n",
            "in training loop, epoch 1, step 683, the loss is 405407.65625\n",
            "in training loop, epoch 1, step 684, the loss is 445086.75\n",
            "in training loop, epoch 1, step 685, the loss is 177528.765625\n",
            "in training loop, epoch 1, step 686, the loss is 240946.625\n",
            "in training loop, epoch 1, step 687, the loss is 1652727.875\n",
            "in training loop, epoch 1, step 688, the loss is 327364.5\n",
            "in training loop, epoch 1, step 689, the loss is 285701.21875\n",
            "in training loop, epoch 1, step 690, the loss is 94270.5859375\n",
            "in training loop, epoch 1, step 691, the loss is 1251200.5\n",
            "in training loop, epoch 1, step 692, the loss is 1086456.125\n",
            "in training loop, epoch 1, step 693, the loss is 243813.09375\n",
            "in training loop, epoch 1, step 694, the loss is 153887.9375\n",
            "in training loop, epoch 1, step 695, the loss is 242159.125\n",
            "in training loop, epoch 1, step 696, the loss is 189541.75\n",
            "in training loop, epoch 1, step 697, the loss is 163140.53125\n",
            "in training loop, epoch 1, step 698, the loss is 504019.0\n",
            "in training loop, epoch 1, step 699, the loss is 211526.46875\n",
            "in training loop, epoch 1, step 700, the loss is 229973.5\n",
            "in training loop, epoch 1, step 701, the loss is 354203.5\n",
            "in training loop, epoch 1, step 702, the loss is 398840.1875\n",
            "in training loop, epoch 1, step 703, the loss is 256154.125\n",
            "in training loop, epoch 1, step 704, the loss is 84739.3984375\n",
            "in training loop, epoch 1, step 705, the loss is 519164.90625\n",
            "in training loop, epoch 1, step 706, the loss is 702402.0625\n",
            "in training loop, epoch 1, step 707, the loss is 619426.0\n",
            "in training loop, epoch 1, step 708, the loss is 308783.5\n",
            "in training loop, epoch 1, step 709, the loss is 765695.375\n",
            "in training loop, epoch 1, step 710, the loss is 1649206.5\n",
            "in training loop, epoch 1, step 711, the loss is 690091.375\n",
            "in training loop, epoch 1, step 712, the loss is 194582.765625\n",
            "in training loop, epoch 1, step 713, the loss is 707725.1875\n",
            "in training loop, epoch 1, step 714, the loss is 87698.703125\n",
            "in training loop, epoch 1, step 715, the loss is 625154.5\n",
            "in training loop, epoch 1, step 716, the loss is 1123427.125\n",
            "in training loop, epoch 1, step 717, the loss is 317622.75\n",
            "in training loop, epoch 1, step 718, the loss is 232126.140625\n",
            "in training loop, epoch 1, step 719, the loss is 467376.3125\n",
            "in training loop, epoch 1, step 720, the loss is 750824.375\n",
            "in training loop, epoch 1, step 721, the loss is 164034.953125\n",
            "in training loop, epoch 1, step 722, the loss is 1502714.0\n",
            "in training loop, epoch 1, step 723, the loss is 1196102.75\n",
            "in training loop, epoch 1, step 724, the loss is 153327.90625\n",
            "in training loop, epoch 1, step 725, the loss is 872435.75\n",
            "in training loop, epoch 1, step 726, the loss is 87301.0\n",
            "in training loop, epoch 1, step 727, the loss is 586879.5\n",
            "in training loop, epoch 1, step 728, the loss is 271574.65625\n",
            "in training loop, epoch 1, step 729, the loss is 86578.875\n",
            "in training loop, epoch 1, step 730, the loss is 172056.859375\n",
            "in training loop, epoch 1, step 731, the loss is 289748.53125\n",
            "in training loop, epoch 1, step 732, the loss is 1065857.125\n",
            "in training loop, epoch 1, step 733, the loss is 360516.71875\n",
            "in training loop, epoch 1, step 734, the loss is 189510.8125\n",
            "in training loop, epoch 1, step 735, the loss is 223267.296875\n",
            "in training loop, epoch 1, step 736, the loss is 140503.828125\n",
            "in training loop, epoch 1, step 737, the loss is 498599.28125\n",
            "in training loop, epoch 1, step 738, the loss is 533130.125\n",
            "in training loop, epoch 1, step 739, the loss is 328491.71875\n",
            "in training loop, epoch 1, step 740, the loss is 631856.0625\n",
            "in training loop, epoch 1, step 741, the loss is 455177.875\n",
            "in training loop, epoch 1, step 742, the loss is 155527.71875\n",
            "in training loop, epoch 1, step 743, the loss is 758607.875\n",
            "in training loop, epoch 1, step 744, the loss is 196691.609375\n",
            "in training loop, epoch 1, step 745, the loss is 491093.875\n",
            "in training loop, epoch 1, step 746, the loss is 443455.34375\n",
            "in training loop, epoch 1, step 747, the loss is 311075.3125\n",
            "in training loop, epoch 1, step 748, the loss is 891325.375\n",
            "in training loop, epoch 1, step 749, the loss is 512576.75\n",
            "in training loop, epoch 1, step 750, the loss is 409891.875\n",
            "in training loop, epoch 1, step 751, the loss is 225252.1875\n",
            "in training loop, epoch 1, step 752, the loss is 208757.796875\n",
            "in training loop, epoch 1, step 753, the loss is 516842.59375\n",
            "in training loop, epoch 1, step 754, the loss is 217252.734375\n",
            "in training loop, epoch 1, step 755, the loss is 856895.1875\n",
            "in training loop, epoch 1, step 756, the loss is 415054.59375\n",
            "in training loop, epoch 1, step 757, the loss is 589155.5625\n",
            "in training loop, epoch 1, step 758, the loss is 302287.875\n",
            "in training loop, epoch 1, step 759, the loss is 483292.75\n",
            "in training loop, epoch 1, step 760, the loss is 171636.984375\n",
            "in training loop, epoch 1, step 761, the loss is 199161.109375\n",
            "in training loop, epoch 1, step 762, the loss is 172378.25\n",
            "in training loop, epoch 1, step 763, the loss is 552257.5625\n",
            "in training loop, epoch 1, step 764, the loss is 243976.421875\n",
            "in training loop, epoch 1, step 765, the loss is 248224.40625\n",
            "in training loop, epoch 1, step 766, the loss is 329099.375\n",
            "in training loop, epoch 1, step 767, the loss is 199112.796875\n",
            "in training loop, epoch 1, step 768, the loss is 162279.28125\n",
            "in training loop, epoch 1, step 769, the loss is 849788.1875\n",
            "in training loop, epoch 1, step 770, the loss is 504414.96875\n",
            "in training loop, epoch 1, step 771, the loss is 516154.90625\n",
            "in training loop, epoch 1, step 772, the loss is 594244.8125\n",
            "in training loop, epoch 1, step 773, the loss is 503377.4375\n",
            "in training loop, epoch 1, step 774, the loss is 231771.671875\n",
            "in training loop, epoch 1, step 775, the loss is 185002.546875\n",
            "in training loop, epoch 1, step 776, the loss is 148159.484375\n",
            "in training loop, epoch 1, step 777, the loss is 632719.375\n",
            "in training loop, epoch 1, step 778, the loss is 228262.640625\n",
            "in training loop, epoch 1, step 779, the loss is 506553.40625\n",
            "in training loop, epoch 1, step 780, the loss is 472655.4375\n",
            "in training loop, epoch 1, step 781, the loss is 282250.28125\n",
            "in training loop, epoch 1, step 782, the loss is 1568811.625\n",
            "in training loop, epoch 1, step 783, the loss is 847712.8125\n",
            "in training loop, epoch 1, step 784, the loss is 262932.1875\n",
            "in training loop, epoch 1, step 785, the loss is 416010.3125\n",
            "in training loop, epoch 1, step 786, the loss is 430532.6875\n",
            "in training loop, epoch 1, step 787, the loss is 768838.75\n",
            "in training loop, epoch 1, step 788, the loss is 1165896.0\n",
            "in training loop, epoch 1, step 789, the loss is 103656.25\n",
            "in training loop, epoch 1, step 790, the loss is 380129.46875\n",
            "in training loop, epoch 1, step 791, the loss is 261277.71875\n",
            "in training loop, epoch 1, step 792, the loss is 311061.75\n",
            "in training loop, epoch 1, step 793, the loss is 282077.375\n",
            "in training loop, epoch 1, step 794, the loss is 391083.8125\n",
            "in training loop, epoch 1, step 795, the loss is 223668.875\n",
            "in training loop, epoch 1, step 796, the loss is 315624.3125\n",
            "in training loop, epoch 1, step 797, the loss is 87343.484375\n",
            "in training loop, epoch 1, step 798, the loss is 623560.0625\n",
            "in training loop, epoch 1, step 799, the loss is 182217.65625\n",
            "in training loop, epoch 1, step 800, the loss is 266601.09375\n",
            "in training loop, epoch 1, step 801, the loss is 259812.984375\n",
            "in training loop, epoch 1, step 802, the loss is 882097.875\n",
            "in training loop, epoch 1, step 803, the loss is 361215.53125\n",
            "in training loop, epoch 1, step 804, the loss is 1168276.25\n",
            "in training loop, epoch 1, step 805, the loss is 262769.1875\n",
            "in training loop, epoch 1, step 806, the loss is 1086334.125\n",
            "in training loop, epoch 1, step 807, the loss is 740085.625\n",
            "in training loop, epoch 1, step 808, the loss is 254054.859375\n",
            "in training loop, epoch 1, step 809, the loss is 126458.4296875\n",
            "in training loop, epoch 1, step 810, the loss is 949420.375\n",
            "in training loop, epoch 1, step 811, the loss is 396401.40625\n",
            "in training loop, epoch 1, step 812, the loss is 221152.328125\n",
            "in training loop, epoch 1, step 813, the loss is 863873.375\n",
            "in training loop, epoch 1, step 814, the loss is 729242.5\n",
            "in training loop, epoch 1, step 815, the loss is 716216.75\n",
            "in training loop, epoch 1, step 816, the loss is 579559.5625\n",
            "in training loop, epoch 1, step 817, the loss is 128779.3046875\n",
            "in training loop, epoch 1, step 818, the loss is 313864.6875\n",
            "in training loop, epoch 1, step 819, the loss is 482427.75\n",
            "in training loop, epoch 1, step 820, the loss is 166135.109375\n",
            "in training loop, epoch 1, step 821, the loss is 386652.125\n",
            "in training loop, epoch 1, step 822, the loss is 333259.6875\n",
            "in training loop, epoch 1, step 823, the loss is 1316684.875\n",
            "in training loop, epoch 1, step 824, the loss is 1601819.75\n",
            "in training loop, epoch 1, step 825, the loss is 210596.171875\n",
            "in training loop, epoch 1, step 826, the loss is 277543.40625\n",
            "in training loop, epoch 1, step 827, the loss is 739793.125\n",
            "in training loop, epoch 1, step 828, the loss is 378555.40625\n",
            "in training loop, epoch 1, step 829, the loss is 220699.6875\n",
            "in training loop, epoch 1, step 830, the loss is 394483.0\n",
            "in training loop, epoch 1, step 831, the loss is 232693.625\n",
            "in training loop, epoch 1, step 832, the loss is 483261.6875\n",
            "in training loop, epoch 1, step 833, the loss is 348563.90625\n",
            "in training loop, epoch 1, step 834, the loss is 121439.453125\n",
            "in training loop, epoch 1, step 835, the loss is 222015.09375\n",
            "in training loop, epoch 1, step 836, the loss is 470372.09375\n",
            "in training loop, epoch 1, step 837, the loss is 245568.515625\n",
            "in training loop, epoch 1, step 838, the loss is 1009345.8125\n",
            "in training loop, epoch 1, step 839, the loss is 318909.59375\n",
            "in training loop, epoch 1, step 840, the loss is 305455.5625\n",
            "in training loop, epoch 1, step 841, the loss is 123657.46875\n",
            "in training loop, epoch 1, step 842, the loss is 495030.5\n",
            "in training loop, epoch 1, step 843, the loss is 266565.34375\n",
            "in training loop, epoch 1, step 844, the loss is 668303.5\n",
            "in training loop, epoch 1, step 845, the loss is 552016.4375\n",
            "in training loop, epoch 1, step 846, the loss is 333742.0625\n",
            "in training loop, epoch 1, step 847, the loss is 147164.328125\n",
            "in training loop, epoch 1, step 848, the loss is 116750.4375\n",
            "in training loop, epoch 1, step 849, the loss is 310220.5625\n",
            "in training loop, epoch 1, step 850, the loss is 74649.6015625\n",
            "in training loop, epoch 1, step 851, the loss is 188602.375\n",
            "in training loop, epoch 1, step 852, the loss is 158862.890625\n",
            "in training loop, epoch 1, step 853, the loss is 262547.375\n",
            "in training loop, epoch 1, step 854, the loss is 108076.3125\n",
            "in training loop, epoch 1, step 855, the loss is 164773.96875\n",
            "in training loop, epoch 1, step 856, the loss is 374506.25\n",
            "in training loop, epoch 1, step 857, the loss is 368817.21875\n",
            "in training loop, epoch 1, step 858, the loss is 294091.3125\n",
            "in training loop, epoch 1, step 859, the loss is 233808.65625\n",
            "in training loop, epoch 1, step 860, the loss is 190948.296875\n",
            "in training loop, epoch 1, step 861, the loss is 237668.46875\n",
            "in training loop, epoch 1, step 862, the loss is 107881.6796875\n",
            "in training loop, epoch 1, step 863, the loss is 1375811.75\n",
            "in training loop, epoch 1, step 864, the loss is 90456.140625\n",
            "in training loop, epoch 1, step 865, the loss is 259174.75\n",
            "in training loop, epoch 1, step 866, the loss is 318696.125\n",
            "in training loop, epoch 1, step 867, the loss is 310701.9375\n",
            "in training loop, epoch 1, step 868, the loss is 140037.53125\n",
            "in training loop, epoch 1, step 869, the loss is 56745.5859375\n",
            "in training loop, epoch 1, step 870, the loss is 449604.15625\n",
            "in training loop, epoch 1, step 871, the loss is 211830.34375\n",
            "in training loop, epoch 1, step 872, the loss is 446659.375\n",
            "in training loop, epoch 1, step 873, the loss is 164005.6875\n",
            "in training loop, epoch 1, step 874, the loss is 142684.140625\n",
            "in training loop, epoch 1, step 875, the loss is 434488.65625\n",
            "in training loop, epoch 1, step 876, the loss is 362702.03125\n",
            "in training loop, epoch 1, step 877, the loss is 215365.5625\n",
            "in training loop, epoch 1, step 878, the loss is 243804.4375\n",
            "in training loop, epoch 1, step 879, the loss is 483719.84375\n",
            "in training loop, epoch 1, step 880, the loss is 381353.03125\n",
            "in training loop, epoch 1, step 881, the loss is 478409.625\n",
            "in training loop, epoch 1, step 882, the loss is 1890444.75\n",
            "in training loop, epoch 1, step 883, the loss is 364112.9375\n",
            "in training loop, epoch 1, step 884, the loss is 649508.75\n",
            "in training loop, epoch 1, step 885, the loss is 369796.375\n",
            "in training loop, epoch 1, step 886, the loss is 481604.9375\n",
            "in training loop, epoch 1, step 887, the loss is 306276.0625\n",
            "in training loop, epoch 1, step 888, the loss is 203193.421875\n",
            "in training loop, epoch 1, step 889, the loss is 255063.375\n",
            "in training loop, epoch 1, step 890, the loss is 758046.0\n",
            "in training loop, epoch 1, step 891, the loss is 230764.875\n",
            "in training loop, epoch 1, step 892, the loss is 167150.6875\n",
            "in training loop, epoch 1, step 893, the loss is 574373.75\n",
            "in training loop, epoch 1, step 894, the loss is 280933.96875\n",
            "in training loop, epoch 1, step 895, the loss is 302075.625\n",
            "in training loop, epoch 1, step 896, the loss is 298450.78125\n",
            "in training loop, epoch 1, step 897, the loss is 148935.609375\n",
            "in training loop, epoch 1, step 898, the loss is 188009.640625\n",
            "in training loop, epoch 1, step 899, the loss is 194198.65625\n",
            "in training loop, epoch 1, step 900, the loss is 254136.71875\n",
            "in training loop, epoch 1, step 901, the loss is 530148.0\n",
            "in training loop, epoch 1, step 902, the loss is 348338.8125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-a6a982a285b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    308\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0mtrainingWrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainingWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcrossval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainingWrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m     \u001b[0mtrain_loss_mean_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0mtest_loss_mean_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-a6a982a285b4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m       \u001b[0mtrain_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3I63RgNmk73"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}