{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNLSTMAutoEncoder_with48bins.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOwc1MTprgEQ+KUbCx6sh1z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singr7/MIRAutoencoder/blob/master/CNNLSTMAutoEncoder_with48bins.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9JqXiXhqdkN"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORCKRmYPHk-z"
      },
      "source": [
        "#Mount the google drive\n",
        "#Create list of numpy files for western and indian dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPnD6kkyHfx8",
        "outputId": "622ef1b3-4280-4851-c03a-9e8def483502"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "western_files = []\n",
        "western_file_dir = \"/content/drive/My Drive/MusicResearchColabNB/MelFeatures/Western_mel_numpy\"\n",
        "for r,d, fileList in os.walk(western_file_dir):\n",
        "  for file in fileList:\n",
        "    western_files.append(os.path.join(r,file))\n",
        "\n",
        "indian_files = []\n",
        "indian_file_dir = \"/content/drive/My Drive/MusicResearchColabNB/MelFeatures/Indian_mel_numpy\"\n",
        "for r,d, fileList in os.walk(indian_file_dir):\n",
        "  for file in fileList:\n",
        "    indian_files.append(os.path.join(r,file))\n",
        "\n",
        "print(len(western_files))\n",
        "print(len(indian_files))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "7894\n",
            "2008\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMQbF-ylLmdK"
      },
      "source": [
        "# Balance the western dataset by taking files equal to Indian dataset files = 2008"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXHKJAOLL-iX",
        "outputId": "1d85df79-fbb6-4044-d9b9-f03869a049e5"
      },
      "source": [
        "import random \n",
        "#randomize the selection. To avoid getting a different random sample with every run, use seed\n",
        "random.seed(234)\n",
        "bal_western_files = random.sample(western_files,2008)\n",
        "len(bal_western_files)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2008"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeBwpwaTX3Jo"
      },
      "source": [
        "#Define configuration class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP4323imT53f"
      },
      "source": [
        "class Configuration:\n",
        "  seq_len = 200  # taking half of the original timesteps extracted \n",
        "  input_dim = 48  #num of mels\n",
        "  embedding_dim = 64\n",
        "  batch_size = 2\n",
        "  base_dir = \"/content/drive/My Drive/MusicResearchColabNB/vajra/westernAE_48bins\"   # need to be edited..\n",
        "  loss_function = torch.nn.MSELoss(reduction='sum')\n",
        "  lr=1e-3  # I edited it from 1e-3 to 1e-5\n",
        "  n_epochs = 4\n",
        "  model_file = \"/content/drive/My Drive/MusicResearchColabNB/vajra/westernAE_48bins/models/mel.pkl\"  #need need edits\n",
        "  results_dir = os.path.join(base_dir, \"./results\")  # may need edits\n",
        "  checkpoint_model_file = \"/content/drive/My Drive/MusicResearchColabNB/vajra/westernAE_48bins/models/mel_checkpoint.pkl\" #may need edits\n",
        "  kernel_size = 3  #why?\n",
        "  k_folds = 10 "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RALRBXgZZBA"
      },
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "  def __init__(self, seq_len, n_features, embedding_dim=64, kernel_size=3, stride=1):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.seq_len, self.n_features = seq_len, n_features\n",
        "    self.embedding_dim, self.hidden_dim = embedding_dim, 2 * embedding_dim\n",
        "\n",
        "\n",
        "    self.conv = nn.Conv1d(in_channels=seq_len,out_channels=seq_len,kernel_size=kernel_size,stride=stride, groups=seq_len)\n",
        "    conv_op_dim = int(((n_features - kernel_size)/ stride) + 1)\n",
        "\n",
        "    self.rnn1 = nn.LSTM(\n",
        "      input_size=conv_op_dim,\n",
        "      hidden_size=self.hidden_dim,\n",
        "      num_layers=1,\n",
        "      batch_first=True\n",
        "    )\n",
        "    self.rnn2 = nn.LSTM(\n",
        "      input_size=self.hidden_dim,\n",
        "      hidden_size=embedding_dim,\n",
        "      num_layers=1,\n",
        "      batch_first=True\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    #x = x.reshape((10, self.seq_len, self.n_features))\n",
        "   # print('In Encoder')\n",
        "   # print(x.shape)\n",
        "    x = self.conv(x)\n",
        "    x, (_, _) = self.rnn1(x)\n",
        "    x, (hidden_n, _) = self.rnn2(x)\n",
        "    return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkW-A8TzZdGT"
      },
      "source": [
        "class Decoder(torch.nn.Module):\n",
        "  def __init__(self, seq_len, embedding_dim=64, n_features=48):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.seq_len, self.embedding_dim = seq_len, embedding_dim\n",
        "    self.hidden_dim, self.n_features = 2 * embedding_dim, n_features\n",
        "    self.rnn1 = nn.LSTM(\n",
        "      input_size=embedding_dim,\n",
        "      hidden_size=embedding_dim,\n",
        "      num_layers=1,\n",
        "      batch_first=True\n",
        "    )\n",
        "    self.rnn2 = nn.LSTM(\n",
        "      input_size=embedding_dim,\n",
        "      hidden_size=self.hidden_dim,\n",
        "      num_layers=1,\n",
        "      batch_first=True\n",
        "    )\n",
        "    self.output_layer = nn.Linear(self.hidden_dim * self.seq_len, n_features * self.seq_len)\n",
        "  def forward(self, x):\n",
        "    x, (hidden_n, cell_n) = self.rnn1(x)\n",
        "    x, (hidden_n, cell_n) = self.rnn2(x)\n",
        "    #print(\"in decoder\", x.shape)\n",
        "    x = x.contiguous()\n",
        "    x = x.view(x.shape[0], -1)\n",
        "    x = self.output_layer(x)\n",
        "    return x.reshape(x.shape[0],self.seq_len, self.n_features)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mqrvU5MZfEA"
      },
      "source": [
        "class RecurrentAutoencoder(torch.nn.Module):\n",
        "  def __init__(self, seq_len, n_features, embedding_dim=64, device='cpu'):\n",
        "    super(RecurrentAutoencoder, self).__init__()\n",
        "    self.encoder = Encoder(seq_len, n_features, embedding_dim).to(device)\n",
        "    self.decoder = Decoder(seq_len, embedding_dim, n_features).to(device)\n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)\n",
        "    x = self.decoder(x)\n",
        "    return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRQ-t9aNZiUD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32faf639-f62e-4b5d-8392-f560c47d8c18"
      },
      "source": [
        "x = torch.randn(10, 48, 400)\n",
        "print(x.shape)\n",
        "x = x.permute(0, 2, 1)\n",
        "print(x.shape)\n",
        "\n",
        "encoder = Encoder(400, 48, embedding_dim=64, kernel_size=3, stride=1)\n",
        "encoded = encoder(x)\n",
        "print(encoded.shape)\n",
        "\n",
        "decoder = Decoder(400, 64, 48)\n",
        "decoded = decoder(encoded)\n",
        "print(decoded.shape)\n",
        "\n",
        "rae = RecurrentAutoencoder(400, 48, 64)\n",
        "output = rae(x)\n",
        "\n",
        "print(output.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 48, 400])\n",
            "torch.Size([10, 400, 48])\n",
            "torch.Size([10, 400, 64])\n",
            "torch.Size([10, 400, 48])\n",
            "torch.Size([10, 400, 48])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDpec6ICZnKN"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data.sampler import SequentialSampler\n",
        "\n",
        "class CustomDatasetMel(Dataset):\n",
        "\n",
        "    def __init__(self, dataList):\n",
        "        self.data = dataList\n",
        "        #self.labels = labelList\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        import numpy as np\n",
        "        fileName = self.data[index]\n",
        "        \n",
        "        mel_spect = np.load(fileName)\n",
        "        data = torch.tensor(mel_spect[:,:200], dtype=torch.float)\n",
        "        data = data.permute(1, 0)\n",
        "        #data = torch.unsqueeze(data, dim =0)\n",
        "\n",
        "        #label = torch.tensor(self.labels[index])\n",
        "        return data"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uAv1gLc6Zwwr",
        "outputId": "ecbda68c-73b7-4c0d-e92f-77069835f076"
      },
      "source": [
        "import copy\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "class TrainingWrapper:\n",
        "\n",
        "  def __init__(self, config, training_loader, test_loader, device, val_loader=None, cross=10):\n",
        "    self.config = config\n",
        "    self.training_loader = training_loader\n",
        "    self.test_loader = test_loader\n",
        "    self.val_loader = val_loader\n",
        "    self.device = device\n",
        "    self.model = RecurrentAutoencoder(self.config.seq_len, self.config.input_dim, self.config.embedding_dim, device=self.device)\n",
        "    self.model = self.model.to(self.device)\n",
        "    self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.lr)\n",
        "    self.criterion = self.config.loss_function.to(self.device)\n",
        "    self.history = dict(train=[], val=[], cross_val=[])\n",
        "    self.best_model_wts = copy.deepcopy(self.model.state_dict())\n",
        "    self.best_loss = 10000.0\n",
        "    #print(self.config.base_dir + self.config.model_file)\n",
        "    torch.save(self.model.state_dict(),  self.config.model_file)\n",
        "    self.cross = cross\n",
        "    \n",
        "\n",
        "  def combine_images(self, generated_images):\n",
        "    num = generated_images.shape[0]\n",
        "    width = int(math.sqrt(num))\n",
        "    height = int(math.ceil(float(num)/width))\n",
        "    shape = generated_images.shape[1:3]\n",
        "    image = np.zeros((height*shape[0], width*shape[1]),\n",
        "                     dtype=generated_images.dtype)\n",
        "    for index, img in enumerate(generated_images):\n",
        "        i = int(index/width)\n",
        "        j = index % width\n",
        "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
        "            img[:, :, 0]\n",
        "    return image\n",
        "\n",
        "\n",
        "  def show_reconstruction(self, test_loader, n_images):\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    from PIL import Image\n",
        "    import numpy as np\n",
        "\n",
        "    self.model.eval()\n",
        "    for x, _ in self.test_loader:\n",
        "        x = x[:min(n_images, x.size(0))].to(self,device)\n",
        "        _, x_recon = self.model(x)\n",
        "        data = np.concatenate([x.data.cpu(), x_recon.data.cpu()])\n",
        "        img = self.combine_images(np.transpose(data, [0, 2, 3, 1]))\n",
        "        image = img * 255\n",
        "        Image.fromarray(image.astype(np.uint8)).save(self.config.base_dir + \"/real_and_recon.png\")\n",
        "        print()\n",
        "        print('Reconstructed images are saved to %s/real_and_recon.png' % self.config.base_dir)\n",
        "        print('-' * 70)\n",
        "        plt.imshow(plt.imread(self.config.base_dir + \"/real_and_recon.png\", ))\n",
        "        plt.show()\n",
        "        break\n",
        "\n",
        "  def visualizeTraining(self, epoch, trn_losses, tst_losses, val_losses, save_dir,cross):\n",
        "    # visualize the loss as the network trained\n",
        "    import matplotlib.pyplot as plt\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    plt.plot(range(0, len(trn_losses)), trn_losses, label='Training Loss')\n",
        "    if tst_losses:\n",
        "      plt.plot(range(0, len(tst_losses)), tst_losses, label='Validation Loss')\n",
        "    if val_losses:\n",
        "      plt.plot(range(0, len(val_losses)), val_losses, label='Cross Validation Loss')\n",
        "\n",
        "    minposs = tst_losses.index(min(tst_losses))\n",
        "    plt.axvline(minposs, linestyle='--', color='r', label='Early Stopping Checkpoint')\n",
        "\n",
        "\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('loss')\n",
        "    # plt.ylim(0, 0.5)  # consistent scale\n",
        "    # plt.xlim(0, len(trn_losses))  # consistent scale\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    fig.savefig(os.path.join(save_dir , 'loss_plot_{}.png'.format(cross)), bbox_inches='tight')\n",
        "\n",
        "  def train(self):\n",
        "    #self.model.load_state_dict(torch.load(config.checkpoint_model_file))\n",
        "    for epoch in range(1, self.config.n_epochs + 1):\n",
        "      self.model = self.model.train()\n",
        "      train_losses = []\n",
        "      for i, data in enumerate(self.training_loader,0):\n",
        "        x = data\n",
        "        self.optimizer.zero_grad()\n",
        "        x = x.to(self.device)\n",
        "\n",
        "        output = self.model(x)\n",
        "        loss = self.criterion(output, x)\n",
        "\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        train_losses.append(loss.item())\n",
        "        print(\"in training loop, epoch {}, step {}, the loss is {}\".format(epoch, i, loss.item()))\n",
        "\n",
        "      val_losses = []\n",
        "      self.model = self.model.eval()\n",
        "      with torch.no_grad():\n",
        "        for i, data in enumerate(self.test_loader):\n",
        "          x = data\n",
        "          x = x.to(device)\n",
        "          output = self.model(x)\n",
        "          loss = self.criterion(output, x)\n",
        "          val_losses.append(loss.item())\n",
        "\n",
        "\n",
        "      cross_val_losses = []\n",
        "      self.model = self.model.eval()\n",
        "      with torch.no_grad():\n",
        "        for i, data in enumerate(self.val_loader):\n",
        "          x = data\n",
        "          x = x.to(device)\n",
        "          output = self.model(x)\n",
        "          loss = self.criterion(output, x)\n",
        "          cross_val_losses.append(loss.item())\n",
        "\n",
        "      train_loss = np.mean(train_losses)\n",
        "      val_loss = np.mean(val_losses)\n",
        "      cross_val_loss = np.mean(cross_val_losses)\n",
        "\n",
        "\n",
        "      self.history['train'].append(train_loss)\n",
        "      self.history['val'].append(val_loss)\n",
        "      self.history['cross_val'].append(cross_val_loss)\n",
        "\n",
        "      if val_loss < self.best_loss:\n",
        "        self.best_loss = val_loss\n",
        "        self.best_model_wts = copy.deepcopy(self.model.state_dict())\n",
        "      torch.save(self.model.state_dict(),  self.config.checkpoint_model_file)\n",
        "      if epoch % 2 == 0:\n",
        "        self.visualizeTraining(epoch, trn_losses= self.history['train'], tst_losses=self.history['val'], val_losses =self.history['cross_val'], save_dir=self.config.base_dir + \"/results\",cross=fold)\n",
        "      print(f'k-fold {fold}:: Epoch {epoch}: train loss {train_loss} val loss {val_loss}')\n",
        "    self.model.load_state_dict(self.best_model_wts)\n",
        "    torch.save(self.model.state_dict(), self.config.model_file)\n",
        "    return self.model.eval(), self.history\n",
        "\n",
        "  \n",
        "class TestingWrapper:\n",
        "  def __init__(self, config, device):\n",
        "    self.config = config\n",
        "    self.device = device\n",
        "    self.model = RecurrentAutoencoder(self.config.seq_len, self.config.input_dim, self.config.embedding_dim, device=self.device)\n",
        "    PATH =  self.config.checkpoint_model_file\n",
        "    print(PATH)\n",
        "    self.model.load_state_dict(torch.load(PATH, map_location=self.device))\n",
        "    self.model = self.model.to(self.device)\n",
        "\n",
        "  def combine_images(self, generated_images):\n",
        "    num = generated_images.shape[0]\n",
        "    width = int(math.sqrt(num))\n",
        "    height = int(math.ceil(float(num)/width))\n",
        "    shape = generated_images.shape[1:3]\n",
        "    image = np.zeros((height*shape[0], width*shape[1]),\n",
        "                     dtype=generated_images.dtype)\n",
        "    for index, img in enumerate(generated_images):\n",
        "        i = int(index/width)\n",
        "        j = index % width\n",
        "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
        "            img[:, :, 0]\n",
        "    return image\n",
        "\n",
        "\n",
        "  def show_reconstruction(self, test_loader, n_images):\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    from PIL import Image\n",
        "    import numpy as np\n",
        "\n",
        "    self.model.eval()\n",
        "    for x, _ in test_loader:\n",
        "      x = x[:min(n_images, x.size(0))].to(self.device)\n",
        "      x_recon = self.model(x)\n",
        "      data = np.concatenate([x.data.cpu(), x_recon.data.cpu()])\n",
        "      img = self.combine_images(np.transpose(data, [0, 2, 3, 1]))\n",
        "      image = img * 255\n",
        "      Image.fromarray(image.astype(np.uint8)).save(self.config.base_dir + \"/real_and_recon.png\")\n",
        "      print()\n",
        "      print('Reconstructed images are saved to %s/real_and_recon.png' % self.config.base_dir)\n",
        "      print('-' * 70)\n",
        "      plt.imshow(plt.imread(self.config.base_dir + \"/real_and_recon.png\", ))\n",
        "      plt.show()\n",
        "      break\n",
        "\n",
        "  def save_reconstruction(self, test_loader):\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    from PIL import Image\n",
        "    import numpy as np\n",
        "    import uuid\n",
        "\n",
        "\n",
        "    self.model.eval()\n",
        "    with torch.no_grad(): \n",
        "      fileCount = 0\n",
        "      for x, _ in test_loader:\n",
        "        x = x.to(self.device)\n",
        "        x_recon = self.model(x)\n",
        "        x_recon = x_recon.data.cpu().detach().numpy()\n",
        "        for mel in x_recon:\n",
        "          #print(mel.shape)\n",
        "          unique_filename = str(uuid.uuid4())\n",
        "          filename = self.config.base_dir + \"/reconstruction/\" + unique_filename + \".npy\"\n",
        "          np.save(filename, mel)\n",
        "          fileCount = fileCount + 1\n",
        "          print(\"saving file {} at index {}\".format(filename, fileCount))\n",
        "\n",
        "mode = 'train'\n",
        "data = \"mel\"\n",
        "#data = \"mnist\"\n",
        "config = Configuration()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import random\n",
        "import os\n",
        "\n",
        "def seed_everything(seed=1234):\n",
        "  \n",
        "  random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  np.random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def visualizeTraining(epoch, trn_losses, tst_losses, val_losses, save_dir):\n",
        "    # visualize the loss as the network trained\n",
        "    import matplotlib.pyplot as plt\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    plt.plot(range(0, len(trn_losses)), trn_losses, label='Training Loss')\n",
        "    #if tst_losses:\n",
        "    plt.plot(range(0, len(tst_losses)), tst_losses, label='Validation Loss')\n",
        "    #if val_losses:\n",
        "    plt.plot(range(0, len(val_losses)), val_losses, label='Cross Validation Loss')\n",
        "\n",
        "    #minposs = tst_losses.index(min(tst_losses))\n",
        "    #plt.axvline(minposs, linestyle='--', color='r', label='Early Stopping Checkpoint')\n",
        "\n",
        "\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('loss')\n",
        "    # plt.ylim(0, 0.5)  # consistent scale\n",
        "    # plt.xlim(0, len(trn_losses))  # consistent scale\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    fig.savefig(os.path.join(save_dir , 'loss_plot_{}.png'.format(\"MEAN\")), bbox_inches='tight')\n",
        "\n",
        "seed_everything()\n",
        "\n",
        "train_data = bal_western_files\n",
        "#labels = [1] * len(bal_western_files)\n",
        "\n",
        "val_data = indian_files\n",
        "#val_labels = [1] * len(indian_files)\n",
        "\n",
        "train_loss_mean_list = []\n",
        "test_loss_mean_list = []\n",
        "val_loss_mean_list = []\n",
        "\n",
        "# Cross validation runs\n",
        "# use sklearn KFolds\n",
        "from sklearn.model_selection import KFold\n",
        "kfold = KFold(n_splits=config.k_folds , shuffle=True)\n",
        "\n",
        "train_dataset = CustomDatasetMel(train_data)\n",
        "val_dataset = CustomDatasetMel(val_data)\n",
        "#Load the cross val dataset which is Full Indian dataset\n",
        "#It is identical for all K-folds\n",
        "crossval_loader = torch.utils.data.DataLoader(\n",
        "                      val_dataset,\n",
        "                      batch_size=config.batch_size, \n",
        "                      sampler=SequentialSampler(val_dataset), \n",
        "                      drop_last=False)  \n",
        "\n",
        "for fold, (train_ids, test_ids) in enumerate(kfold.split(train_dataset)):\n",
        "    # Print\n",
        "  print(f'FOLD {fold}')\n",
        "  print('--------------------------------')\n",
        "    \n",
        "    # Sample elements randomly from a given list of ids, no replacement.\n",
        "  train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "  test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
        "  \n",
        "    # Define data loaders for training and testing data in this fold\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "                      train_dataset, \n",
        "                      batch_size=config.batch_size,\n",
        "                      sampler=train_subsampler,\n",
        "                      drop_last=False)\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "                      train_dataset,\n",
        "                      batch_size=config.batch_size,\n",
        "                      sampler=test_subsampler,\n",
        "                      drop_last=False)\n",
        "    \n",
        "  print(\"length of of train_loader is {} & length of traindataset is {}\".format(len(train_loader),len(train_dataset)))\n",
        "  print(\"length of of test_loader is {}\".format(len(test_loader)))\n",
        "  print(\"length of of val_loader is {}\".format(len(crossval_loader)))\n",
        "    \n",
        "  if mode==\"train\":\n",
        "    trainingWrapper = TrainingWrapper(config=config, training_loader=train_loader, test_loader=test_loader, device=device, val_loader=crossval_loader, cross=fold)\n",
        "    model, history = trainingWrapper.train()\n",
        "    np.append(train_loss_mean_list,history['train'])\n",
        "    np.append(test_loss_mean_list,history['val'])\n",
        "    np.append(val_loss_mean_list,history['cross_val'])\n",
        "    \n",
        "\n",
        "    if data==\"mnist\":\n",
        "      #trainingWrapper.show_reconstruction(test_loader=test_loader, n_images=50)\n",
        "      pass\n",
        "\n",
        "  elif mode==\"test\":\n",
        "    testWrapper = TestingWrapper(config=config, device=device)\n",
        "    testWrapper.save_reconstruction(test_loader)\n",
        "  \n",
        "  train_loss_mean_list = np.mean(train_loss_mean_list, axis=0)\n",
        "  test_loss_mean_list = np.mean(test_loss_mean_list, axis=0)\n",
        "  val_loss_mean_list = np.mean(val_loss_mean_list, axis=0)\n",
        "  print(\"train_loss_mean_list\",train_loss_mean_list)\n",
        "  print(\"test_loss_mean_list\",test_loss_mean_list)\n",
        "  print(\"val_loss_mean_list\",val_loss_mean_list)\n",
        "  visualizeTraining(0, train_loss_mean_list, test_loss_mean_list, val_loss_mean_list, save_dir = config.base_dir + \"/results\")\n",
        "\n",
        "  "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOLD 0\n",
            "--------------------------------\n",
            "length of of train_loader is 904 & length of traindataset is 2008\n",
            "length of of test_loader is 101\n",
            "length of of val_loader is 1004\n",
            "in training loop, epoch 1, step 0, the loss is 32727054.0\n",
            "in training loop, epoch 1, step 1, the loss is 46148744.0\n",
            "in training loop, epoch 1, step 2, the loss is 26997360.0\n",
            "in training loop, epoch 1, step 3, the loss is 27906264.0\n",
            "in training loop, epoch 1, step 4, the loss is 41901684.0\n",
            "in training loop, epoch 1, step 5, the loss is 11342757.0\n",
            "in training loop, epoch 1, step 6, the loss is 9847688.0\n",
            "in training loop, epoch 1, step 7, the loss is 3625516.25\n",
            "in training loop, epoch 1, step 8, the loss is 3696511.0\n",
            "in training loop, epoch 1, step 9, the loss is 11523430.0\n",
            "in training loop, epoch 1, step 10, the loss is 3983450.25\n",
            "in training loop, epoch 1, step 11, the loss is 5419811.0\n",
            "in training loop, epoch 1, step 12, the loss is 2204262.25\n",
            "in training loop, epoch 1, step 13, the loss is 1010644.375\n",
            "in training loop, epoch 1, step 14, the loss is 8027172.0\n",
            "in training loop, epoch 1, step 15, the loss is 3655198.75\n",
            "in training loop, epoch 1, step 16, the loss is 12322657.0\n",
            "in training loop, epoch 1, step 17, the loss is 8114769.0\n",
            "in training loop, epoch 1, step 18, the loss is 3033431.75\n",
            "in training loop, epoch 1, step 19, the loss is 14586214.0\n",
            "in training loop, epoch 1, step 20, the loss is 5817924.0\n",
            "in training loop, epoch 1, step 21, the loss is 13956744.0\n",
            "in training loop, epoch 1, step 22, the loss is 13789721.0\n",
            "in training loop, epoch 1, step 23, the loss is 6632686.5\n",
            "in training loop, epoch 1, step 24, the loss is 10654616.0\n",
            "in training loop, epoch 1, step 25, the loss is 4212719.0\n",
            "in training loop, epoch 1, step 26, the loss is 9209750.0\n",
            "in training loop, epoch 1, step 27, the loss is 6068775.5\n",
            "in training loop, epoch 1, step 28, the loss is 7877877.5\n",
            "in training loop, epoch 1, step 29, the loss is 5876898.0\n",
            "in training loop, epoch 1, step 30, the loss is 1731254.0\n",
            "in training loop, epoch 1, step 31, the loss is 6059846.0\n",
            "in training loop, epoch 1, step 32, the loss is 2425648.0\n",
            "in training loop, epoch 1, step 33, the loss is 3503612.5\n",
            "in training loop, epoch 1, step 34, the loss is 2191660.25\n",
            "in training loop, epoch 1, step 35, the loss is 1726181.0\n",
            "in training loop, epoch 1, step 36, the loss is 11588779.0\n",
            "in training loop, epoch 1, step 37, the loss is 5142250.5\n",
            "in training loop, epoch 1, step 38, the loss is 2276436.0\n",
            "in training loop, epoch 1, step 39, the loss is 1290068.125\n",
            "in training loop, epoch 1, step 40, the loss is 1539476.75\n",
            "in training loop, epoch 1, step 41, the loss is 2313378.75\n",
            "in training loop, epoch 1, step 42, the loss is 1920844.375\n",
            "in training loop, epoch 1, step 43, the loss is 6845064.0\n",
            "in training loop, epoch 1, step 44, the loss is 1545975.875\n",
            "in training loop, epoch 1, step 45, the loss is 2171756.5\n",
            "in training loop, epoch 1, step 46, the loss is 4314680.0\n",
            "in training loop, epoch 1, step 47, the loss is 2646980.25\n",
            "in training loop, epoch 1, step 48, the loss is 1831778.5\n",
            "in training loop, epoch 1, step 49, the loss is 1797428.0\n",
            "in training loop, epoch 1, step 50, the loss is 4374268.5\n",
            "in training loop, epoch 1, step 51, the loss is 3494087.5\n",
            "in training loop, epoch 1, step 52, the loss is 2458586.25\n",
            "in training loop, epoch 1, step 53, the loss is 1700540.75\n",
            "in training loop, epoch 1, step 54, the loss is 1724326.875\n",
            "in training loop, epoch 1, step 55, the loss is 2337676.25\n",
            "in training loop, epoch 1, step 56, the loss is 4902731.5\n",
            "in training loop, epoch 1, step 57, the loss is 2597287.25\n",
            "in training loop, epoch 1, step 58, the loss is 1235518.625\n",
            "in training loop, epoch 1, step 59, the loss is 1353484.25\n",
            "in training loop, epoch 1, step 60, the loss is 2123338.5\n",
            "in training loop, epoch 1, step 61, the loss is 21486300.0\n",
            "in training loop, epoch 1, step 62, the loss is 3411395.5\n",
            "in training loop, epoch 1, step 63, the loss is 1572145.875\n",
            "in training loop, epoch 1, step 64, the loss is 1537693.625\n",
            "in training loop, epoch 1, step 65, the loss is 5744518.0\n",
            "in training loop, epoch 1, step 66, the loss is 6047779.0\n",
            "in training loop, epoch 1, step 67, the loss is 2253653.5\n",
            "in training loop, epoch 1, step 68, the loss is 2060205.25\n",
            "in training loop, epoch 1, step 69, the loss is 2905135.0\n",
            "in training loop, epoch 1, step 70, the loss is 2214267.75\n",
            "in training loop, epoch 1, step 71, the loss is 4857508.5\n",
            "in training loop, epoch 1, step 72, the loss is 1255811.625\n",
            "in training loop, epoch 1, step 73, the loss is 2525747.5\n",
            "in training loop, epoch 1, step 74, the loss is 4259764.5\n",
            "in training loop, epoch 1, step 75, the loss is 1134218.0\n",
            "in training loop, epoch 1, step 76, the loss is 2124803.75\n",
            "in training loop, epoch 1, step 77, the loss is 1069322.5\n",
            "in training loop, epoch 1, step 78, the loss is 1651258.625\n",
            "in training loop, epoch 1, step 79, the loss is 2923961.75\n",
            "in training loop, epoch 1, step 80, the loss is 4688307.0\n",
            "in training loop, epoch 1, step 81, the loss is 4243070.0\n",
            "in training loop, epoch 1, step 82, the loss is 3618761.0\n",
            "in training loop, epoch 1, step 83, the loss is 1364326.125\n",
            "in training loop, epoch 1, step 84, the loss is 8924000.0\n",
            "in training loop, epoch 1, step 85, the loss is 1990475.5\n",
            "in training loop, epoch 1, step 86, the loss is 1569429.25\n",
            "in training loop, epoch 1, step 87, the loss is 2986460.25\n",
            "in training loop, epoch 1, step 88, the loss is 10082083.0\n",
            "in training loop, epoch 1, step 89, the loss is 6265856.0\n",
            "in training loop, epoch 1, step 90, the loss is 3862790.5\n",
            "in training loop, epoch 1, step 91, the loss is 5224783.0\n",
            "in training loop, epoch 1, step 92, the loss is 5880842.0\n",
            "in training loop, epoch 1, step 93, the loss is 2208352.5\n",
            "in training loop, epoch 1, step 94, the loss is 2578456.0\n",
            "in training loop, epoch 1, step 95, the loss is 1236753.75\n",
            "in training loop, epoch 1, step 96, the loss is 8600762.0\n",
            "in training loop, epoch 1, step 97, the loss is 2893528.5\n",
            "in training loop, epoch 1, step 98, the loss is 5214521.0\n",
            "in training loop, epoch 1, step 99, the loss is 902608.4375\n",
            "in training loop, epoch 1, step 100, the loss is 9167265.0\n",
            "in training loop, epoch 1, step 101, the loss is 4877618.5\n",
            "in training loop, epoch 1, step 102, the loss is 1789054.75\n",
            "in training loop, epoch 1, step 103, the loss is 4338626.5\n",
            "in training loop, epoch 1, step 104, the loss is 5677814.0\n",
            "in training loop, epoch 1, step 105, the loss is 1058231.375\n",
            "in training loop, epoch 1, step 106, the loss is 2763624.5\n",
            "in training loop, epoch 1, step 107, the loss is 2764533.0\n",
            "in training loop, epoch 1, step 108, the loss is 2022515.0\n",
            "in training loop, epoch 1, step 109, the loss is 9101549.0\n",
            "in training loop, epoch 1, step 110, the loss is 2841233.0\n",
            "in training loop, epoch 1, step 111, the loss is 4178871.5\n",
            "in training loop, epoch 1, step 112, the loss is 4701161.0\n",
            "in training loop, epoch 1, step 113, the loss is 3128218.0\n",
            "in training loop, epoch 1, step 114, the loss is 3091700.75\n",
            "in training loop, epoch 1, step 115, the loss is 2801108.0\n",
            "in training loop, epoch 1, step 116, the loss is 2619332.0\n",
            "in training loop, epoch 1, step 117, the loss is 4897568.0\n",
            "in training loop, epoch 1, step 118, the loss is 6890301.5\n",
            "in training loop, epoch 1, step 119, the loss is 2892179.0\n",
            "in training loop, epoch 1, step 120, the loss is 2304923.75\n",
            "in training loop, epoch 1, step 121, the loss is 2272604.5\n",
            "in training loop, epoch 1, step 122, the loss is 1606496.125\n",
            "in training loop, epoch 1, step 123, the loss is 3818706.75\n",
            "in training loop, epoch 1, step 124, the loss is 1098021.375\n",
            "in training loop, epoch 1, step 125, the loss is 3008471.5\n",
            "in training loop, epoch 1, step 126, the loss is 1476207.875\n",
            "in training loop, epoch 1, step 127, the loss is 1012442.125\n",
            "in training loop, epoch 1, step 128, the loss is 1779626.875\n",
            "in training loop, epoch 1, step 129, the loss is 1512572.125\n",
            "in training loop, epoch 1, step 130, the loss is 3877697.5\n",
            "in training loop, epoch 1, step 131, the loss is 885116.3125\n",
            "in training loop, epoch 1, step 132, the loss is 1593969.25\n",
            "in training loop, epoch 1, step 133, the loss is 3942115.5\n",
            "in training loop, epoch 1, step 134, the loss is 1120264.375\n",
            "in training loop, epoch 1, step 135, the loss is 3379624.25\n",
            "in training loop, epoch 1, step 136, the loss is 1163011.875\n",
            "in training loop, epoch 1, step 137, the loss is 1470117.125\n",
            "in training loop, epoch 1, step 138, the loss is 2262559.75\n",
            "in training loop, epoch 1, step 139, the loss is 1852470.125\n",
            "in training loop, epoch 1, step 140, the loss is 2665454.75\n",
            "in training loop, epoch 1, step 141, the loss is 2575556.5\n",
            "in training loop, epoch 1, step 142, the loss is 1925572.5\n",
            "in training loop, epoch 1, step 143, the loss is 2394553.5\n",
            "in training loop, epoch 1, step 144, the loss is 1242181.125\n",
            "in training loop, epoch 1, step 145, the loss is 3161771.75\n",
            "in training loop, epoch 1, step 146, the loss is 1415961.625\n",
            "in training loop, epoch 1, step 147, the loss is 2152109.75\n",
            "in training loop, epoch 1, step 148, the loss is 3129983.5\n",
            "in training loop, epoch 1, step 149, the loss is 3304303.0\n",
            "in training loop, epoch 1, step 150, the loss is 1515912.625\n",
            "in training loop, epoch 1, step 151, the loss is 2724372.0\n",
            "in training loop, epoch 1, step 152, the loss is 5218474.0\n",
            "in training loop, epoch 1, step 153, the loss is 1625508.875\n",
            "in training loop, epoch 1, step 154, the loss is 6720956.5\n",
            "in training loop, epoch 1, step 155, the loss is 2572693.0\n",
            "in training loop, epoch 1, step 156, the loss is 1057957.125\n",
            "in training loop, epoch 1, step 157, the loss is 4520848.0\n",
            "in training loop, epoch 1, step 158, the loss is 1971748.5\n",
            "in training loop, epoch 1, step 159, the loss is 3074998.5\n",
            "in training loop, epoch 1, step 160, the loss is 1783028.0\n",
            "in training loop, epoch 1, step 161, the loss is 2881090.25\n",
            "in training loop, epoch 1, step 162, the loss is 2806206.75\n",
            "in training loop, epoch 1, step 163, the loss is 2329653.75\n",
            "in training loop, epoch 1, step 164, the loss is 2925002.5\n",
            "in training loop, epoch 1, step 165, the loss is 2498247.5\n",
            "in training loop, epoch 1, step 166, the loss is 2356254.0\n",
            "in training loop, epoch 1, step 167, the loss is 3865267.25\n",
            "in training loop, epoch 1, step 168, the loss is 2534204.5\n",
            "in training loop, epoch 1, step 169, the loss is 2939880.75\n",
            "in training loop, epoch 1, step 170, the loss is 2249598.75\n",
            "in training loop, epoch 1, step 171, the loss is 2700734.0\n",
            "in training loop, epoch 1, step 172, the loss is 3731343.0\n",
            "in training loop, epoch 1, step 173, the loss is 1012712.625\n",
            "in training loop, epoch 1, step 174, the loss is 1696733.25\n",
            "in training loop, epoch 1, step 175, the loss is 2431000.0\n",
            "in training loop, epoch 1, step 176, the loss is 809141.625\n",
            "in training loop, epoch 1, step 177, the loss is 1003263.3125\n",
            "in training loop, epoch 1, step 178, the loss is 3028176.25\n",
            "in training loop, epoch 1, step 179, the loss is 2527330.25\n",
            "in training loop, epoch 1, step 180, the loss is 3021507.75\n",
            "in training loop, epoch 1, step 181, the loss is 3972399.25\n",
            "in training loop, epoch 1, step 182, the loss is 3639972.0\n",
            "in training loop, epoch 1, step 183, the loss is 3020203.0\n",
            "in training loop, epoch 1, step 184, the loss is 2649438.0\n",
            "in training loop, epoch 1, step 185, the loss is 4911463.0\n",
            "in training loop, epoch 1, step 186, the loss is 2137042.5\n",
            "in training loop, epoch 1, step 187, the loss is 3760365.5\n",
            "in training loop, epoch 1, step 188, the loss is 1183421.0\n",
            "in training loop, epoch 1, step 189, the loss is 4183423.0\n",
            "in training loop, epoch 1, step 190, the loss is 2165633.5\n",
            "in training loop, epoch 1, step 191, the loss is 4036984.0\n",
            "in training loop, epoch 1, step 192, the loss is 3601053.5\n",
            "in training loop, epoch 1, step 193, the loss is 3292748.5\n",
            "in training loop, epoch 1, step 194, the loss is 4485838.0\n",
            "in training loop, epoch 1, step 195, the loss is 2001188.5\n",
            "in training loop, epoch 1, step 196, the loss is 2841909.75\n",
            "in training loop, epoch 1, step 197, the loss is 4132912.5\n",
            "in training loop, epoch 1, step 198, the loss is 2808450.25\n",
            "in training loop, epoch 1, step 199, the loss is 3580617.75\n",
            "in training loop, epoch 1, step 200, the loss is 1837561.75\n",
            "in training loop, epoch 1, step 201, the loss is 4724595.0\n",
            "in training loop, epoch 1, step 202, the loss is 2956078.5\n",
            "in training loop, epoch 1, step 203, the loss is 1273154.125\n",
            "in training loop, epoch 1, step 204, the loss is 2190262.75\n",
            "in training loop, epoch 1, step 205, the loss is 3467286.0\n",
            "in training loop, epoch 1, step 206, the loss is 2899411.0\n",
            "in training loop, epoch 1, step 207, the loss is 9103672.0\n",
            "in training loop, epoch 1, step 208, the loss is 1687769.25\n",
            "in training loop, epoch 1, step 209, the loss is 1897784.25\n",
            "in training loop, epoch 1, step 210, the loss is 1192416.25\n",
            "in training loop, epoch 1, step 211, the loss is 1841982.25\n",
            "in training loop, epoch 1, step 212, the loss is 2507998.0\n",
            "in training loop, epoch 1, step 213, the loss is 1307382.5\n",
            "in training loop, epoch 1, step 214, the loss is 1436520.25\n",
            "in training loop, epoch 1, step 215, the loss is 3392480.25\n",
            "in training loop, epoch 1, step 216, the loss is 2135155.0\n",
            "in training loop, epoch 1, step 217, the loss is 1838351.75\n",
            "in training loop, epoch 1, step 218, the loss is 2791917.75\n",
            "in training loop, epoch 1, step 219, the loss is 2341007.75\n",
            "in training loop, epoch 1, step 220, the loss is 2552195.0\n",
            "in training loop, epoch 1, step 221, the loss is 2429581.75\n",
            "in training loop, epoch 1, step 222, the loss is 2038548.25\n",
            "in training loop, epoch 1, step 223, the loss is 1516129.125\n",
            "in training loop, epoch 1, step 224, the loss is 2171918.75\n",
            "in training loop, epoch 1, step 225, the loss is 1392624.375\n",
            "in training loop, epoch 1, step 226, the loss is 1370896.375\n",
            "in training loop, epoch 1, step 227, the loss is 1268202.375\n",
            "in training loop, epoch 1, step 228, the loss is 3466353.5\n",
            "in training loop, epoch 1, step 229, the loss is 5585442.0\n",
            "in training loop, epoch 1, step 230, the loss is 1056185.375\n",
            "in training loop, epoch 1, step 231, the loss is 2246503.25\n",
            "in training loop, epoch 1, step 232, the loss is 1806455.125\n",
            "in training loop, epoch 1, step 233, the loss is 2773657.25\n",
            "in training loop, epoch 1, step 234, the loss is 3140211.25\n",
            "in training loop, epoch 1, step 235, the loss is 2175276.5\n",
            "in training loop, epoch 1, step 236, the loss is 2365256.25\n",
            "in training loop, epoch 1, step 237, the loss is 2509052.0\n",
            "in training loop, epoch 1, step 238, the loss is 1126701.125\n",
            "in training loop, epoch 1, step 239, the loss is 868970.5625\n",
            "in training loop, epoch 1, step 240, the loss is 1138024.75\n",
            "in training loop, epoch 1, step 241, the loss is 4934964.0\n",
            "in training loop, epoch 1, step 242, the loss is 2117720.0\n",
            "in training loop, epoch 1, step 243, the loss is 901612.3125\n",
            "in training loop, epoch 1, step 244, the loss is 1357341.875\n",
            "in training loop, epoch 1, step 245, the loss is 1009088.375\n",
            "in training loop, epoch 1, step 246, the loss is 1475657.25\n",
            "in training loop, epoch 1, step 247, the loss is 2123436.0\n",
            "in training loop, epoch 1, step 248, the loss is 1303618.75\n",
            "in training loop, epoch 1, step 249, the loss is 1474336.375\n",
            "in training loop, epoch 1, step 250, the loss is 2756784.25\n",
            "in training loop, epoch 1, step 251, the loss is 3083801.5\n",
            "in training loop, epoch 1, step 252, the loss is 1929517.625\n",
            "in training loop, epoch 1, step 253, the loss is 2925575.0\n",
            "in training loop, epoch 1, step 254, the loss is 2267985.5\n",
            "in training loop, epoch 1, step 255, the loss is 1200364.375\n",
            "in training loop, epoch 1, step 256, the loss is 905615.125\n",
            "in training loop, epoch 1, step 257, the loss is 2483999.75\n",
            "in training loop, epoch 1, step 258, the loss is 1955432.5\n",
            "in training loop, epoch 1, step 259, the loss is 1773726.375\n",
            "in training loop, epoch 1, step 260, the loss is 1489010.0\n",
            "in training loop, epoch 1, step 261, the loss is 732569.75\n",
            "in training loop, epoch 1, step 262, the loss is 1933849.125\n",
            "in training loop, epoch 1, step 263, the loss is 1452517.125\n",
            "in training loop, epoch 1, step 264, the loss is 1257634.625\n",
            "in training loop, epoch 1, step 265, the loss is 1417943.0\n",
            "in training loop, epoch 1, step 266, the loss is 878067.875\n",
            "in training loop, epoch 1, step 267, the loss is 1320002.75\n",
            "in training loop, epoch 1, step 268, the loss is 1133511.5\n",
            "in training loop, epoch 1, step 269, the loss is 1002782.75\n",
            "in training loop, epoch 1, step 270, the loss is 1907467.5\n",
            "in training loop, epoch 1, step 271, the loss is 1930812.375\n",
            "in training loop, epoch 1, step 272, the loss is 1058318.5\n",
            "in training loop, epoch 1, step 273, the loss is 1342541.875\n",
            "in training loop, epoch 1, step 274, the loss is 2392738.5\n",
            "in training loop, epoch 1, step 275, the loss is 1492798.625\n",
            "in training loop, epoch 1, step 276, the loss is 1637152.875\n",
            "in training loop, epoch 1, step 277, the loss is 1915718.25\n",
            "in training loop, epoch 1, step 278, the loss is 939128.25\n",
            "in training loop, epoch 1, step 279, the loss is 1525383.0\n",
            "in training loop, epoch 1, step 280, the loss is 1272778.75\n",
            "in training loop, epoch 1, step 281, the loss is 1083942.625\n",
            "in training loop, epoch 1, step 282, the loss is 1496317.25\n",
            "in training loop, epoch 1, step 283, the loss is 1739997.0\n",
            "in training loop, epoch 1, step 284, the loss is 2366989.75\n",
            "in training loop, epoch 1, step 285, the loss is 1148011.625\n",
            "in training loop, epoch 1, step 286, the loss is 850377.8125\n",
            "in training loop, epoch 1, step 287, the loss is 1338781.5\n",
            "in training loop, epoch 1, step 288, the loss is 3603910.0\n",
            "in training loop, epoch 1, step 289, the loss is 1055604.5\n",
            "in training loop, epoch 1, step 290, the loss is 994466.25\n",
            "in training loop, epoch 1, step 291, the loss is 1038884.9375\n",
            "in training loop, epoch 1, step 292, the loss is 1708323.875\n",
            "in training loop, epoch 1, step 293, the loss is 4089980.0\n",
            "in training loop, epoch 1, step 294, the loss is 1078344.5\n",
            "in training loop, epoch 1, step 295, the loss is 913957.5625\n",
            "in training loop, epoch 1, step 296, the loss is 1207588.875\n",
            "in training loop, epoch 1, step 297, the loss is 895482.5625\n",
            "in training loop, epoch 1, step 298, the loss is 1108890.875\n",
            "in training loop, epoch 1, step 299, the loss is 2346018.0\n",
            "in training loop, epoch 1, step 300, the loss is 877431.6875\n",
            "in training loop, epoch 1, step 301, the loss is 758886.625\n",
            "in training loop, epoch 1, step 302, the loss is 921712.75\n",
            "in training loop, epoch 1, step 303, the loss is 967563.5\n",
            "in training loop, epoch 1, step 304, the loss is 1378229.5\n",
            "in training loop, epoch 1, step 305, the loss is 682841.125\n",
            "in training loop, epoch 1, step 306, the loss is 1433713.875\n",
            "in training loop, epoch 1, step 307, the loss is 2304170.25\n",
            "in training loop, epoch 1, step 308, the loss is 1335550.75\n",
            "in training loop, epoch 1, step 309, the loss is 3543710.5\n",
            "in training loop, epoch 1, step 310, the loss is 6232022.0\n",
            "in training loop, epoch 1, step 311, the loss is 841990.0\n",
            "in training loop, epoch 1, step 312, the loss is 1547161.0\n",
            "in training loop, epoch 1, step 313, the loss is 1221869.125\n",
            "in training loop, epoch 1, step 314, the loss is 1213912.5\n",
            "in training loop, epoch 1, step 315, the loss is 849841.0625\n",
            "in training loop, epoch 1, step 316, the loss is 3094308.5\n",
            "in training loop, epoch 1, step 317, the loss is 1498277.0\n",
            "in training loop, epoch 1, step 318, the loss is 2820991.25\n",
            "in training loop, epoch 1, step 319, the loss is 1405961.5\n",
            "in training loop, epoch 1, step 320, the loss is 1619707.5\n",
            "in training loop, epoch 1, step 321, the loss is 1470978.0\n",
            "in training loop, epoch 1, step 322, the loss is 1514159.75\n",
            "in training loop, epoch 1, step 323, the loss is 807980.25\n",
            "in training loop, epoch 1, step 324, the loss is 2731564.75\n",
            "in training loop, epoch 1, step 325, the loss is 1728297.5\n",
            "in training loop, epoch 1, step 326, the loss is 1086821.5\n",
            "in training loop, epoch 1, step 327, the loss is 863384.25\n",
            "in training loop, epoch 1, step 328, the loss is 1464368.625\n",
            "in training loop, epoch 1, step 329, the loss is 1850736.25\n",
            "in training loop, epoch 1, step 330, the loss is 2394581.0\n",
            "in training loop, epoch 1, step 331, the loss is 1338487.875\n",
            "in training loop, epoch 1, step 332, the loss is 1728245.5\n",
            "in training loop, epoch 1, step 333, the loss is 2386509.5\n",
            "in training loop, epoch 1, step 334, the loss is 2714556.25\n",
            "in training loop, epoch 1, step 335, the loss is 1277701.0\n",
            "in training loop, epoch 1, step 336, the loss is 882459.6875\n",
            "in training loop, epoch 1, step 337, the loss is 1308766.25\n",
            "in training loop, epoch 1, step 338, the loss is 1683067.5\n",
            "in training loop, epoch 1, step 339, the loss is 2558967.5\n",
            "in training loop, epoch 1, step 340, the loss is 1376770.625\n",
            "in training loop, epoch 1, step 341, the loss is 1394512.625\n",
            "in training loop, epoch 1, step 342, the loss is 1304666.375\n",
            "in training loop, epoch 1, step 343, the loss is 1412988.25\n",
            "in training loop, epoch 1, step 344, the loss is 2432174.75\n",
            "in training loop, epoch 1, step 345, the loss is 972465.25\n",
            "in training loop, epoch 1, step 346, the loss is 1970164.75\n",
            "in training loop, epoch 1, step 347, the loss is 1698068.125\n",
            "in training loop, epoch 1, step 348, the loss is 1823764.375\n",
            "in training loop, epoch 1, step 349, the loss is 1474600.625\n",
            "in training loop, epoch 1, step 350, the loss is 1765599.625\n",
            "in training loop, epoch 1, step 351, the loss is 1968024.75\n",
            "in training loop, epoch 1, step 352, the loss is 1251650.75\n",
            "in training loop, epoch 1, step 353, the loss is 1212927.0\n",
            "in training loop, epoch 1, step 354, the loss is 1479460.0\n",
            "in training loop, epoch 1, step 355, the loss is 1654467.25\n",
            "in training loop, epoch 1, step 356, the loss is 1484766.0\n",
            "in training loop, epoch 1, step 357, the loss is 838871.6875\n",
            "in training loop, epoch 1, step 358, the loss is 1754853.625\n",
            "in training loop, epoch 1, step 359, the loss is 2593350.5\n",
            "in training loop, epoch 1, step 360, the loss is 815804.5625\n",
            "in training loop, epoch 1, step 361, the loss is 1425796.0\n",
            "in training loop, epoch 1, step 362, the loss is 1179004.25\n",
            "in training loop, epoch 1, step 363, the loss is 1372547.875\n",
            "in training loop, epoch 1, step 364, the loss is 740732.25\n",
            "in training loop, epoch 1, step 365, the loss is 1060030.125\n",
            "in training loop, epoch 1, step 366, the loss is 2183889.0\n",
            "in training loop, epoch 1, step 367, the loss is 1834630.625\n",
            "in training loop, epoch 1, step 368, the loss is 1415172.75\n",
            "in training loop, epoch 1, step 369, the loss is 1038851.375\n",
            "in training loop, epoch 1, step 370, the loss is 1229639.875\n",
            "in training loop, epoch 1, step 371, the loss is 2814109.5\n",
            "in training loop, epoch 1, step 372, the loss is 624792.8125\n",
            "in training loop, epoch 1, step 373, the loss is 2602529.75\n",
            "in training loop, epoch 1, step 374, the loss is 1706691.875\n",
            "in training loop, epoch 1, step 375, the loss is 1061391.375\n",
            "in training loop, epoch 1, step 376, the loss is 1140700.875\n",
            "in training loop, epoch 1, step 377, the loss is 1640284.0\n",
            "in training loop, epoch 1, step 378, the loss is 1493585.25\n",
            "in training loop, epoch 1, step 379, the loss is 1592788.75\n",
            "in training loop, epoch 1, step 380, the loss is 1258610.125\n",
            "in training loop, epoch 1, step 381, the loss is 3256671.75\n",
            "in training loop, epoch 1, step 382, the loss is 3522509.75\n",
            "in training loop, epoch 1, step 383, the loss is 833663.25\n",
            "in training loop, epoch 1, step 384, the loss is 1594666.625\n",
            "in training loop, epoch 1, step 385, the loss is 1368220.5\n",
            "in training loop, epoch 1, step 386, the loss is 1050558.75\n",
            "in training loop, epoch 1, step 387, the loss is 2023094.75\n",
            "in training loop, epoch 1, step 388, the loss is 2460861.25\n",
            "in training loop, epoch 1, step 389, the loss is 3353473.0\n",
            "in training loop, epoch 1, step 390, the loss is 1726629.5\n",
            "in training loop, epoch 1, step 391, the loss is 1209677.875\n",
            "in training loop, epoch 1, step 392, the loss is 970203.3125\n",
            "in training loop, epoch 1, step 393, the loss is 1682434.5\n",
            "in training loop, epoch 1, step 394, the loss is 2048392.75\n",
            "in training loop, epoch 1, step 395, the loss is 2467648.0\n",
            "in training loop, epoch 1, step 396, the loss is 659723.8125\n",
            "in training loop, epoch 1, step 397, the loss is 1401129.125\n",
            "in training loop, epoch 1, step 398, the loss is 870365.5625\n",
            "in training loop, epoch 1, step 399, the loss is 1500162.625\n",
            "in training loop, epoch 1, step 400, the loss is 1491963.875\n",
            "in training loop, epoch 1, step 401, the loss is 995534.1875\n",
            "in training loop, epoch 1, step 402, the loss is 863339.5\n",
            "in training loop, epoch 1, step 403, the loss is 1297004.0\n",
            "in training loop, epoch 1, step 404, the loss is 1765325.375\n",
            "in training loop, epoch 1, step 405, the loss is 1995834.25\n",
            "in training loop, epoch 1, step 406, the loss is 787326.375\n",
            "in training loop, epoch 1, step 407, the loss is 1106143.5\n",
            "in training loop, epoch 1, step 408, the loss is 954272.125\n",
            "in training loop, epoch 1, step 409, the loss is 1681932.0\n",
            "in training loop, epoch 1, step 410, the loss is 3380875.25\n",
            "in training loop, epoch 1, step 411, the loss is 1317340.5\n",
            "in training loop, epoch 1, step 412, the loss is 1220240.375\n",
            "in training loop, epoch 1, step 413, the loss is 1272783.875\n",
            "in training loop, epoch 1, step 414, the loss is 2059232.75\n",
            "in training loop, epoch 1, step 415, the loss is 659238.75\n",
            "in training loop, epoch 1, step 416, the loss is 1310436.0\n",
            "in training loop, epoch 1, step 417, the loss is 1434797.875\n",
            "in training loop, epoch 1, step 418, the loss is 1401959.875\n",
            "in training loop, epoch 1, step 419, the loss is 1054740.125\n",
            "in training loop, epoch 1, step 420, the loss is 557990.9375\n",
            "in training loop, epoch 1, step 421, the loss is 1344079.25\n",
            "in training loop, epoch 1, step 422, the loss is 1525178.625\n",
            "in training loop, epoch 1, step 423, the loss is 1075511.75\n",
            "in training loop, epoch 1, step 424, the loss is 1503145.25\n",
            "in training loop, epoch 1, step 425, the loss is 2002398.375\n",
            "in training loop, epoch 1, step 426, the loss is 1903389.375\n",
            "in training loop, epoch 1, step 427, the loss is 1319214.0\n",
            "in training loop, epoch 1, step 428, the loss is 816870.5\n",
            "in training loop, epoch 1, step 429, the loss is 1941040.375\n",
            "in training loop, epoch 1, step 430, the loss is 2305145.25\n",
            "in training loop, epoch 1, step 431, the loss is 1313433.625\n",
            "in training loop, epoch 1, step 432, the loss is 1149667.625\n",
            "in training loop, epoch 1, step 433, the loss is 1690886.5\n",
            "in training loop, epoch 1, step 434, the loss is 3034502.5\n",
            "in training loop, epoch 1, step 435, the loss is 1765714.75\n",
            "in training loop, epoch 1, step 436, the loss is 2399168.0\n",
            "in training loop, epoch 1, step 437, the loss is 868097.25\n",
            "in training loop, epoch 1, step 438, the loss is 3835453.25\n",
            "in training loop, epoch 1, step 439, the loss is 785803.0\n",
            "in training loop, epoch 1, step 440, the loss is 1031147.875\n",
            "in training loop, epoch 1, step 441, the loss is 2349889.75\n",
            "in training loop, epoch 1, step 442, the loss is 987069.125\n",
            "in training loop, epoch 1, step 443, the loss is 1185640.875\n",
            "in training loop, epoch 1, step 444, the loss is 1445523.25\n",
            "in training loop, epoch 1, step 445, the loss is 527963.1875\n",
            "in training loop, epoch 1, step 446, the loss is 1714065.625\n",
            "in training loop, epoch 1, step 447, the loss is 577489.5625\n",
            "in training loop, epoch 1, step 448, the loss is 1511567.0\n",
            "in training loop, epoch 1, step 449, the loss is 1633095.625\n",
            "in training loop, epoch 1, step 450, the loss is 1448252.25\n",
            "in training loop, epoch 1, step 451, the loss is 1964897.5\n",
            "in training loop, epoch 1, step 452, the loss is 2119045.25\n",
            "in training loop, epoch 1, step 453, the loss is 1072151.375\n",
            "in training loop, epoch 1, step 454, the loss is 2340451.5\n",
            "in training loop, epoch 1, step 455, the loss is 968600.75\n",
            "in training loop, epoch 1, step 456, the loss is 790527.0625\n",
            "in training loop, epoch 1, step 457, the loss is 1534636.75\n",
            "in training loop, epoch 1, step 458, the loss is 1725173.25\n",
            "in training loop, epoch 1, step 459, the loss is 1351334.125\n",
            "in training loop, epoch 1, step 460, the loss is 827710.75\n",
            "in training loop, epoch 1, step 461, the loss is 1075854.375\n",
            "in training loop, epoch 1, step 462, the loss is 1949912.0\n",
            "in training loop, epoch 1, step 463, the loss is 1893600.125\n",
            "in training loop, epoch 1, step 464, the loss is 788478.375\n",
            "in training loop, epoch 1, step 465, the loss is 1816465.875\n",
            "in training loop, epoch 1, step 466, the loss is 3579095.5\n",
            "in training loop, epoch 1, step 467, the loss is 1496688.5\n",
            "in training loop, epoch 1, step 468, the loss is 1321746.75\n",
            "in training loop, epoch 1, step 469, the loss is 1314750.875\n",
            "in training loop, epoch 1, step 470, the loss is 580560.6875\n",
            "in training loop, epoch 1, step 471, the loss is 1314311.0\n",
            "in training loop, epoch 1, step 472, the loss is 2169588.25\n",
            "in training loop, epoch 1, step 473, the loss is 966056.375\n",
            "in training loop, epoch 1, step 474, the loss is 3164534.25\n",
            "in training loop, epoch 1, step 475, the loss is 1625314.875\n",
            "in training loop, epoch 1, step 476, the loss is 1517109.0\n",
            "in training loop, epoch 1, step 477, the loss is 504788.375\n",
            "in training loop, epoch 1, step 478, the loss is 1448046.75\n",
            "in training loop, epoch 1, step 479, the loss is 1999580.5\n",
            "in training loop, epoch 1, step 480, the loss is 3584726.0\n",
            "in training loop, epoch 1, step 481, the loss is 732503.125\n",
            "in training loop, epoch 1, step 482, the loss is 683145.125\n",
            "in training loop, epoch 1, step 483, the loss is 698364.75\n",
            "in training loop, epoch 1, step 484, the loss is 863967.125\n",
            "in training loop, epoch 1, step 485, the loss is 2181581.75\n",
            "in training loop, epoch 1, step 486, the loss is 727861.25\n",
            "in training loop, epoch 1, step 487, the loss is 1128879.125\n",
            "in training loop, epoch 1, step 488, the loss is 600267.75\n",
            "in training loop, epoch 1, step 489, the loss is 2021922.875\n",
            "in training loop, epoch 1, step 490, the loss is 2974622.5\n",
            "in training loop, epoch 1, step 491, the loss is 1493410.625\n",
            "in training loop, epoch 1, step 492, the loss is 1907487.75\n",
            "in training loop, epoch 1, step 493, the loss is 1625870.375\n",
            "in training loop, epoch 1, step 494, the loss is 1151168.625\n",
            "in training loop, epoch 1, step 495, the loss is 3060330.75\n",
            "in training loop, epoch 1, step 496, the loss is 754354.375\n",
            "in training loop, epoch 1, step 497, the loss is 3225718.0\n",
            "in training loop, epoch 1, step 498, the loss is 1270735.5\n",
            "in training loop, epoch 1, step 499, the loss is 1337387.0\n",
            "in training loop, epoch 1, step 500, the loss is 2242215.75\n",
            "in training loop, epoch 1, step 501, the loss is 757163.75\n",
            "in training loop, epoch 1, step 502, the loss is 829209.5\n",
            "in training loop, epoch 1, step 503, the loss is 1854071.0\n",
            "in training loop, epoch 1, step 504, the loss is 1647834.0\n",
            "in training loop, epoch 1, step 505, the loss is 824969.4375\n",
            "in training loop, epoch 1, step 506, the loss is 1915382.25\n",
            "in training loop, epoch 1, step 507, the loss is 829980.3125\n",
            "in training loop, epoch 1, step 508, the loss is 1627027.75\n",
            "in training loop, epoch 1, step 509, the loss is 1057229.5\n",
            "in training loop, epoch 1, step 510, the loss is 1715124.0\n",
            "in training loop, epoch 1, step 511, the loss is 1396517.5\n",
            "in training loop, epoch 1, step 512, the loss is 614030.0625\n",
            "in training loop, epoch 1, step 513, the loss is 1871387.25\n",
            "in training loop, epoch 1, step 514, the loss is 1722561.0\n",
            "in training loop, epoch 1, step 515, the loss is 1508234.0\n",
            "in training loop, epoch 1, step 516, the loss is 1472568.625\n",
            "in training loop, epoch 1, step 517, the loss is 1177917.5\n",
            "in training loop, epoch 1, step 518, the loss is 2708539.0\n",
            "in training loop, epoch 1, step 519, the loss is 1258370.5\n",
            "in training loop, epoch 1, step 520, the loss is 963299.625\n",
            "in training loop, epoch 1, step 521, the loss is 1501707.75\n",
            "in training loop, epoch 1, step 522, the loss is 1257563.5\n",
            "in training loop, epoch 1, step 523, the loss is 1464933.5\n",
            "in training loop, epoch 1, step 524, the loss is 1634005.5\n",
            "in training loop, epoch 1, step 525, the loss is 884103.625\n",
            "in training loop, epoch 1, step 526, the loss is 1684249.25\n",
            "in training loop, epoch 1, step 527, the loss is 2390563.75\n",
            "in training loop, epoch 1, step 528, the loss is 1003448.6875\n",
            "in training loop, epoch 1, step 529, the loss is 2385470.25\n",
            "in training loop, epoch 1, step 530, the loss is 1301757.5\n",
            "in training loop, epoch 1, step 531, the loss is 2334956.75\n",
            "in training loop, epoch 1, step 532, the loss is 1226686.875\n",
            "in training loop, epoch 1, step 533, the loss is 815652.5\n",
            "in training loop, epoch 1, step 534, the loss is 1309661.0\n",
            "in training loop, epoch 1, step 535, the loss is 1739033.0\n",
            "in training loop, epoch 1, step 536, the loss is 1322818.5\n",
            "in training loop, epoch 1, step 537, the loss is 1381868.25\n",
            "in training loop, epoch 1, step 538, the loss is 1535180.0\n",
            "in training loop, epoch 1, step 539, the loss is 1396931.0\n",
            "in training loop, epoch 1, step 540, the loss is 1663115.25\n",
            "in training loop, epoch 1, step 541, the loss is 984513.625\n",
            "in training loop, epoch 1, step 542, the loss is 760398.25\n",
            "in training loop, epoch 1, step 543, the loss is 1831675.0\n",
            "in training loop, epoch 1, step 544, the loss is 1520051.0\n",
            "in training loop, epoch 1, step 545, the loss is 1428618.75\n",
            "in training loop, epoch 1, step 546, the loss is 1846986.375\n",
            "in training loop, epoch 1, step 547, the loss is 1981531.0\n",
            "in training loop, epoch 1, step 548, the loss is 866480.875\n",
            "in training loop, epoch 1, step 549, the loss is 1180499.5\n",
            "in training loop, epoch 1, step 550, the loss is 2352651.25\n",
            "in training loop, epoch 1, step 551, the loss is 1253653.5\n",
            "in training loop, epoch 1, step 552, the loss is 987563.625\n",
            "in training loop, epoch 1, step 553, the loss is 754691.375\n",
            "in training loop, epoch 1, step 554, the loss is 950073.4375\n",
            "in training loop, epoch 1, step 555, the loss is 1171534.375\n",
            "in training loop, epoch 1, step 556, the loss is 1250546.0\n",
            "in training loop, epoch 1, step 557, the loss is 1298893.625\n",
            "in training loop, epoch 1, step 558, the loss is 1522092.25\n",
            "in training loop, epoch 1, step 559, the loss is 844687.6875\n",
            "in training loop, epoch 1, step 560, the loss is 1890209.5\n",
            "in training loop, epoch 1, step 561, the loss is 1583036.125\n",
            "in training loop, epoch 1, step 562, the loss is 3427519.5\n",
            "in training loop, epoch 1, step 563, the loss is 1014183.0\n",
            "in training loop, epoch 1, step 564, the loss is 978663.875\n",
            "in training loop, epoch 1, step 565, the loss is 1021322.3125\n",
            "in training loop, epoch 1, step 566, the loss is 1127399.75\n",
            "in training loop, epoch 1, step 567, the loss is 1519048.0\n",
            "in training loop, epoch 1, step 568, the loss is 1846950.0\n",
            "in training loop, epoch 1, step 569, the loss is 985574.75\n",
            "in training loop, epoch 1, step 570, the loss is 1980528.625\n",
            "in training loop, epoch 1, step 571, the loss is 1643598.125\n",
            "in training loop, epoch 1, step 572, the loss is 3138662.5\n",
            "in training loop, epoch 1, step 573, the loss is 1566930.25\n",
            "in training loop, epoch 1, step 574, the loss is 1127181.5\n",
            "in training loop, epoch 1, step 575, the loss is 2421485.0\n",
            "in training loop, epoch 1, step 576, the loss is 1963195.875\n",
            "in training loop, epoch 1, step 577, the loss is 2731957.25\n",
            "in training loop, epoch 1, step 578, the loss is 2449083.75\n",
            "in training loop, epoch 1, step 579, the loss is 1804366.75\n",
            "in training loop, epoch 1, step 580, the loss is 706859.0625\n",
            "in training loop, epoch 1, step 581, the loss is 1059586.75\n",
            "in training loop, epoch 1, step 582, the loss is 1330725.625\n",
            "in training loop, epoch 1, step 583, the loss is 485117.4375\n",
            "in training loop, epoch 1, step 584, the loss is 1634005.75\n",
            "in training loop, epoch 1, step 585, the loss is 555314.625\n",
            "in training loop, epoch 1, step 586, the loss is 2391383.75\n",
            "in training loop, epoch 1, step 587, the loss is 1604748.25\n",
            "in training loop, epoch 1, step 588, the loss is 1196179.625\n",
            "in training loop, epoch 1, step 589, the loss is 1796609.0\n",
            "in training loop, epoch 1, step 590, the loss is 2714133.5\n",
            "in training loop, epoch 1, step 591, the loss is 1833820.75\n",
            "in training loop, epoch 1, step 592, the loss is 2134792.25\n",
            "in training loop, epoch 1, step 593, the loss is 1201233.125\n",
            "in training loop, epoch 1, step 594, the loss is 1531652.375\n",
            "in training loop, epoch 1, step 595, the loss is 1920643.5\n",
            "in training loop, epoch 1, step 596, the loss is 1997751.125\n",
            "in training loop, epoch 1, step 597, the loss is 1647253.125\n",
            "in training loop, epoch 1, step 598, the loss is 1042917.5\n",
            "in training loop, epoch 1, step 599, the loss is 1982962.375\n",
            "in training loop, epoch 1, step 600, the loss is 1153811.125\n",
            "in training loop, epoch 1, step 601, the loss is 1480936.5\n",
            "in training loop, epoch 1, step 602, the loss is 860160.3125\n",
            "in training loop, epoch 1, step 603, the loss is 947820.8125\n",
            "in training loop, epoch 1, step 604, the loss is 870697.6875\n",
            "in training loop, epoch 1, step 605, the loss is 2067914.125\n",
            "in training loop, epoch 1, step 606, the loss is 1139876.25\n",
            "in training loop, epoch 1, step 607, the loss is 1414549.75\n",
            "in training loop, epoch 1, step 608, the loss is 2446614.25\n",
            "in training loop, epoch 1, step 609, the loss is 1410826.625\n",
            "in training loop, epoch 1, step 610, the loss is 1588901.625\n",
            "in training loop, epoch 1, step 611, the loss is 1096038.0\n",
            "in training loop, epoch 1, step 612, the loss is 1650230.0\n",
            "in training loop, epoch 1, step 613, the loss is 829074.125\n",
            "in training loop, epoch 1, step 614, the loss is 1792073.125\n",
            "in training loop, epoch 1, step 615, the loss is 1636970.25\n",
            "in training loop, epoch 1, step 616, the loss is 784844.9375\n",
            "in training loop, epoch 1, step 617, the loss is 563456.75\n",
            "in training loop, epoch 1, step 618, the loss is 2910549.25\n",
            "in training loop, epoch 1, step 619, the loss is 980169.25\n",
            "in training loop, epoch 1, step 620, the loss is 1674981.625\n",
            "in training loop, epoch 1, step 621, the loss is 2086050.875\n",
            "in training loop, epoch 1, step 622, the loss is 1649174.875\n",
            "in training loop, epoch 1, step 623, the loss is 1431557.25\n",
            "in training loop, epoch 1, step 624, the loss is 3784767.25\n",
            "in training loop, epoch 1, step 625, the loss is 1879403.75\n",
            "in training loop, epoch 1, step 626, the loss is 1497418.625\n",
            "in training loop, epoch 1, step 627, the loss is 1924912.25\n",
            "in training loop, epoch 1, step 628, the loss is 862012.625\n",
            "in training loop, epoch 1, step 629, the loss is 985895.25\n",
            "in training loop, epoch 1, step 630, the loss is 983994.625\n",
            "in training loop, epoch 1, step 631, the loss is 2719178.0\n",
            "in training loop, epoch 1, step 632, the loss is 1658199.625\n",
            "in training loop, epoch 1, step 633, the loss is 1889960.375\n",
            "in training loop, epoch 1, step 634, the loss is 2760303.5\n",
            "in training loop, epoch 1, step 635, the loss is 1089997.125\n",
            "in training loop, epoch 1, step 636, the loss is 958718.8125\n",
            "in training loop, epoch 1, step 637, the loss is 2007698.875\n",
            "in training loop, epoch 1, step 638, the loss is 1271555.625\n",
            "in training loop, epoch 1, step 639, the loss is 1177212.125\n",
            "in training loop, epoch 1, step 640, the loss is 1207833.75\n",
            "in training loop, epoch 1, step 641, the loss is 1731706.875\n",
            "in training loop, epoch 1, step 642, the loss is 719680.0\n",
            "in training loop, epoch 1, step 643, the loss is 1409883.625\n",
            "in training loop, epoch 1, step 644, the loss is 1426058.0\n",
            "in training loop, epoch 1, step 645, the loss is 672152.25\n",
            "in training loop, epoch 1, step 646, the loss is 1043502.375\n",
            "in training loop, epoch 1, step 647, the loss is 1660564.125\n",
            "in training loop, epoch 1, step 648, the loss is 807446.5\n",
            "in training loop, epoch 1, step 649, the loss is 2300286.5\n",
            "in training loop, epoch 1, step 650, the loss is 1245680.125\n",
            "in training loop, epoch 1, step 651, the loss is 1338255.375\n",
            "in training loop, epoch 1, step 652, the loss is 6256350.5\n",
            "in training loop, epoch 1, step 653, the loss is 845968.8125\n",
            "in training loop, epoch 1, step 654, the loss is 2035191.0\n",
            "in training loop, epoch 1, step 655, the loss is 840345.9375\n",
            "in training loop, epoch 1, step 656, the loss is 3427433.0\n",
            "in training loop, epoch 1, step 657, the loss is 1867343.125\n",
            "in training loop, epoch 1, step 658, the loss is 1156606.75\n",
            "in training loop, epoch 1, step 659, the loss is 1606246.0\n",
            "in training loop, epoch 1, step 660, the loss is 2181232.5\n",
            "in training loop, epoch 1, step 661, the loss is 1441532.25\n",
            "in training loop, epoch 1, step 662, the loss is 806937.0625\n",
            "in training loop, epoch 1, step 663, the loss is 2089740.625\n",
            "in training loop, epoch 1, step 664, the loss is 1492462.5\n",
            "in training loop, epoch 1, step 665, the loss is 2048018.25\n",
            "in training loop, epoch 1, step 666, the loss is 1807493.75\n",
            "in training loop, epoch 1, step 667, the loss is 2100652.0\n",
            "in training loop, epoch 1, step 668, the loss is 973656.125\n",
            "in training loop, epoch 1, step 669, the loss is 2433976.5\n",
            "in training loop, epoch 1, step 670, the loss is 1323007.5\n",
            "in training loop, epoch 1, step 671, the loss is 2531663.75\n",
            "in training loop, epoch 1, step 672, the loss is 1637973.25\n",
            "in training loop, epoch 1, step 673, the loss is 1159538.75\n",
            "in training loop, epoch 1, step 674, the loss is 1163474.375\n",
            "in training loop, epoch 1, step 675, the loss is 1651888.5\n",
            "in training loop, epoch 1, step 676, the loss is 1688772.0\n",
            "in training loop, epoch 1, step 677, the loss is 807521.75\n",
            "in training loop, epoch 1, step 678, the loss is 1750806.0\n",
            "in training loop, epoch 1, step 679, the loss is 1110576.75\n",
            "in training loop, epoch 1, step 680, the loss is 1509979.25\n",
            "in training loop, epoch 1, step 681, the loss is 1255594.5\n",
            "in training loop, epoch 1, step 682, the loss is 912301.0625\n",
            "in training loop, epoch 1, step 683, the loss is 1986754.875\n",
            "in training loop, epoch 1, step 684, the loss is 1845679.625\n",
            "in training loop, epoch 1, step 685, the loss is 1921656.25\n",
            "in training loop, epoch 1, step 686, the loss is 1354338.125\n",
            "in training loop, epoch 1, step 687, the loss is 1411621.5\n",
            "in training loop, epoch 1, step 688, the loss is 1074051.25\n",
            "in training loop, epoch 1, step 689, the loss is 1520069.75\n",
            "in training loop, epoch 1, step 690, the loss is 735753.125\n",
            "in training loop, epoch 1, step 691, the loss is 1166014.5\n",
            "in training loop, epoch 1, step 692, the loss is 894674.875\n",
            "in training loop, epoch 1, step 693, the loss is 1011997.1875\n",
            "in training loop, epoch 1, step 694, the loss is 2586013.0\n",
            "in training loop, epoch 1, step 695, the loss is 1162434.5\n",
            "in training loop, epoch 1, step 696, the loss is 1820670.125\n",
            "in training loop, epoch 1, step 697, the loss is 1684903.875\n",
            "in training loop, epoch 1, step 698, the loss is 2168616.0\n",
            "in training loop, epoch 1, step 699, the loss is 1184566.0\n",
            "in training loop, epoch 1, step 700, the loss is 992526.75\n",
            "in training loop, epoch 1, step 701, the loss is 1379311.375\n",
            "in training loop, epoch 1, step 702, the loss is 1239165.0\n",
            "in training loop, epoch 1, step 703, the loss is 1211320.625\n",
            "in training loop, epoch 1, step 704, the loss is 1045265.625\n",
            "in training loop, epoch 1, step 705, the loss is 1797708.625\n",
            "in training loop, epoch 1, step 706, the loss is 1152338.875\n",
            "in training loop, epoch 1, step 707, the loss is 1119812.5\n",
            "in training loop, epoch 1, step 708, the loss is 661841.375\n",
            "in training loop, epoch 1, step 709, the loss is 3056572.5\n",
            "in training loop, epoch 1, step 710, the loss is 1027527.875\n",
            "in training loop, epoch 1, step 711, the loss is 1031247.1875\n",
            "in training loop, epoch 1, step 712, the loss is 2083027.875\n",
            "in training loop, epoch 1, step 713, the loss is 637393.875\n",
            "in training loop, epoch 1, step 714, the loss is 3650496.25\n",
            "in training loop, epoch 1, step 715, the loss is 1735168.5\n",
            "in training loop, epoch 1, step 716, the loss is 924532.625\n",
            "in training loop, epoch 1, step 717, the loss is 1789190.625\n",
            "in training loop, epoch 1, step 718, the loss is 1165450.375\n",
            "in training loop, epoch 1, step 719, the loss is 1669543.5\n",
            "in training loop, epoch 1, step 720, the loss is 1460142.375\n",
            "in training loop, epoch 1, step 721, the loss is 1167204.0\n",
            "in training loop, epoch 1, step 722, the loss is 862487.8125\n",
            "in training loop, epoch 1, step 723, the loss is 853678.25\n",
            "in training loop, epoch 1, step 724, the loss is 3614437.75\n",
            "in training loop, epoch 1, step 725, the loss is 1772274.75\n",
            "in training loop, epoch 1, step 726, the loss is 4441336.0\n",
            "in training loop, epoch 1, step 727, the loss is 1402696.25\n",
            "in training loop, epoch 1, step 728, the loss is 1113877.625\n",
            "in training loop, epoch 1, step 729, the loss is 1457061.125\n",
            "in training loop, epoch 1, step 730, the loss is 2661315.5\n",
            "in training loop, epoch 1, step 731, the loss is 1190658.0\n",
            "in training loop, epoch 1, step 732, the loss is 2101218.25\n",
            "in training loop, epoch 1, step 733, the loss is 1065914.875\n",
            "in training loop, epoch 1, step 734, the loss is 1039299.375\n",
            "in training loop, epoch 1, step 735, the loss is 2367554.5\n",
            "in training loop, epoch 1, step 736, the loss is 823992.75\n",
            "in training loop, epoch 1, step 737, the loss is 1485546.25\n",
            "in training loop, epoch 1, step 738, the loss is 2325109.75\n",
            "in training loop, epoch 1, step 739, the loss is 1156116.375\n",
            "in training loop, epoch 1, step 740, the loss is 1353891.0\n",
            "in training loop, epoch 1, step 741, the loss is 1253101.125\n",
            "in training loop, epoch 1, step 742, the loss is 2005021.25\n",
            "in training loop, epoch 1, step 743, the loss is 2343654.75\n",
            "in training loop, epoch 1, step 744, the loss is 1215734.25\n",
            "in training loop, epoch 1, step 745, the loss is 1514088.5\n",
            "in training loop, epoch 1, step 746, the loss is 2544666.0\n",
            "in training loop, epoch 1, step 747, the loss is 1347745.125\n",
            "in training loop, epoch 1, step 748, the loss is 2929587.0\n",
            "in training loop, epoch 1, step 749, the loss is 1182335.5\n",
            "in training loop, epoch 1, step 750, the loss is 875876.0\n",
            "in training loop, epoch 1, step 751, the loss is 969646.3125\n",
            "in training loop, epoch 1, step 752, the loss is 968499.375\n",
            "in training loop, epoch 1, step 753, the loss is 4957939.5\n",
            "in training loop, epoch 1, step 754, the loss is 1352964.0\n",
            "in training loop, epoch 1, step 755, the loss is 2039198.375\n",
            "in training loop, epoch 1, step 756, the loss is 1910570.125\n",
            "in training loop, epoch 1, step 757, the loss is 1751885.375\n",
            "in training loop, epoch 1, step 758, the loss is 1305726.125\n",
            "in training loop, epoch 1, step 759, the loss is 1404612.125\n",
            "in training loop, epoch 1, step 760, the loss is 4742171.5\n",
            "in training loop, epoch 1, step 761, the loss is 1787573.0\n",
            "in training loop, epoch 1, step 762, the loss is 1596089.375\n",
            "in training loop, epoch 1, step 763, the loss is 1115169.75\n",
            "in training loop, epoch 1, step 764, the loss is 1609336.75\n",
            "in training loop, epoch 1, step 765, the loss is 1058074.375\n",
            "in training loop, epoch 1, step 766, the loss is 1181986.5\n",
            "in training loop, epoch 1, step 767, the loss is 2510070.5\n",
            "in training loop, epoch 1, step 768, the loss is 3185201.5\n",
            "in training loop, epoch 1, step 769, the loss is 870393.25\n",
            "in training loop, epoch 1, step 770, the loss is 800000.9375\n",
            "in training loop, epoch 1, step 771, the loss is 1283042.75\n",
            "in training loop, epoch 1, step 772, the loss is 1439694.375\n",
            "in training loop, epoch 1, step 773, the loss is 995940.5\n",
            "in training loop, epoch 1, step 774, the loss is 1182302.25\n",
            "in training loop, epoch 1, step 775, the loss is 3975846.5\n",
            "in training loop, epoch 1, step 776, the loss is 1467621.875\n",
            "in training loop, epoch 1, step 777, the loss is 1956400.5\n",
            "in training loop, epoch 1, step 778, the loss is 1612629.125\n",
            "in training loop, epoch 1, step 779, the loss is 1061929.25\n",
            "in training loop, epoch 1, step 780, the loss is 1876112.0\n",
            "in training loop, epoch 1, step 781, the loss is 1754692.125\n",
            "in training loop, epoch 1, step 782, the loss is 909206.375\n",
            "in training loop, epoch 1, step 783, the loss is 1145343.5\n",
            "in training loop, epoch 1, step 784, the loss is 2150816.0\n",
            "in training loop, epoch 1, step 785, the loss is 1739215.5\n",
            "in training loop, epoch 1, step 786, the loss is 1185944.875\n",
            "in training loop, epoch 1, step 787, the loss is 1607606.75\n",
            "in training loop, epoch 1, step 788, the loss is 2898281.25\n",
            "in training loop, epoch 1, step 789, the loss is 1495716.75\n",
            "in training loop, epoch 1, step 790, the loss is 1154065.125\n",
            "in training loop, epoch 1, step 791, the loss is 1001590.875\n",
            "in training loop, epoch 1, step 792, the loss is 838813.8125\n",
            "in training loop, epoch 1, step 793, the loss is 2829102.0\n",
            "in training loop, epoch 1, step 794, the loss is 2904385.0\n",
            "in training loop, epoch 1, step 795, the loss is 1325628.75\n",
            "in training loop, epoch 1, step 796, the loss is 3044616.25\n",
            "in training loop, epoch 1, step 797, the loss is 1648282.625\n",
            "in training loop, epoch 1, step 798, the loss is 1095906.875\n",
            "in training loop, epoch 1, step 799, the loss is 1096860.125\n",
            "in training loop, epoch 1, step 800, the loss is 1215791.75\n",
            "in training loop, epoch 1, step 801, the loss is 2058942.25\n",
            "in training loop, epoch 1, step 802, the loss is 888467.8125\n",
            "in training loop, epoch 1, step 803, the loss is 701016.875\n",
            "in training loop, epoch 1, step 804, the loss is 757654.375\n",
            "in training loop, epoch 1, step 805, the loss is 1024948.5625\n",
            "in training loop, epoch 1, step 806, the loss is 1188326.875\n",
            "in training loop, epoch 1, step 807, the loss is 1566994.25\n",
            "in training loop, epoch 1, step 808, the loss is 1513638.0\n",
            "in training loop, epoch 1, step 809, the loss is 1382040.0\n",
            "in training loop, epoch 1, step 810, the loss is 1512860.125\n",
            "in training loop, epoch 1, step 811, the loss is 823409.0\n",
            "in training loop, epoch 1, step 812, the loss is 1235488.5\n",
            "in training loop, epoch 1, step 813, the loss is 966699.5\n",
            "in training loop, epoch 1, step 814, the loss is 1225349.125\n",
            "in training loop, epoch 1, step 815, the loss is 963802.0625\n",
            "in training loop, epoch 1, step 816, the loss is 1670612.875\n",
            "in training loop, epoch 1, step 817, the loss is 1056881.875\n",
            "in training loop, epoch 1, step 818, the loss is 1055543.5\n",
            "in training loop, epoch 1, step 819, the loss is 1434792.75\n",
            "in training loop, epoch 1, step 820, the loss is 1119041.0\n",
            "in training loop, epoch 1, step 821, the loss is 1747855.125\n",
            "in training loop, epoch 1, step 822, the loss is 1503506.0\n",
            "in training loop, epoch 1, step 823, the loss is 800026.375\n",
            "in training loop, epoch 1, step 824, the loss is 1535877.75\n",
            "in training loop, epoch 1, step 825, the loss is 3310985.75\n",
            "in training loop, epoch 1, step 826, the loss is 1405716.25\n",
            "in training loop, epoch 1, step 827, the loss is 691598.125\n",
            "in training loop, epoch 1, step 828, the loss is 1798910.625\n",
            "in training loop, epoch 1, step 829, the loss is 1037236.9375\n",
            "in training loop, epoch 1, step 830, the loss is 981332.8125\n",
            "in training loop, epoch 1, step 831, the loss is 1138266.75\n",
            "in training loop, epoch 1, step 832, the loss is 2508559.25\n",
            "in training loop, epoch 1, step 833, the loss is 1625174.25\n",
            "in training loop, epoch 1, step 834, the loss is 1576402.0\n",
            "in training loop, epoch 1, step 835, the loss is 1213526.375\n",
            "in training loop, epoch 1, step 836, the loss is 2466966.75\n",
            "in training loop, epoch 1, step 837, the loss is 1344046.75\n",
            "in training loop, epoch 1, step 838, the loss is 1905141.0\n",
            "in training loop, epoch 1, step 839, the loss is 1005060.875\n",
            "in training loop, epoch 1, step 840, the loss is 1538285.375\n",
            "in training loop, epoch 1, step 841, the loss is 1694812.875\n",
            "in training loop, epoch 1, step 842, the loss is 1637207.5\n",
            "in training loop, epoch 1, step 843, the loss is 867641.625\n",
            "in training loop, epoch 1, step 844, the loss is 2518539.5\n",
            "in training loop, epoch 1, step 845, the loss is 1599953.0\n",
            "in training loop, epoch 1, step 846, the loss is 1251107.25\n",
            "in training loop, epoch 1, step 847, the loss is 657563.125\n",
            "in training loop, epoch 1, step 848, the loss is 1550931.5\n",
            "in training loop, epoch 1, step 849, the loss is 1488127.25\n",
            "in training loop, epoch 1, step 850, the loss is 2463539.75\n",
            "in training loop, epoch 1, step 851, the loss is 1548069.625\n",
            "in training loop, epoch 1, step 852, the loss is 1741784.375\n",
            "in training loop, epoch 1, step 853, the loss is 1420472.25\n",
            "in training loop, epoch 1, step 854, the loss is 1657715.5\n",
            "in training loop, epoch 1, step 855, the loss is 1923455.75\n",
            "in training loop, epoch 1, step 856, the loss is 1517204.5\n",
            "in training loop, epoch 1, step 857, the loss is 1196958.5\n",
            "in training loop, epoch 1, step 858, the loss is 1104165.375\n",
            "in training loop, epoch 1, step 859, the loss is 1472543.125\n",
            "in training loop, epoch 1, step 860, the loss is 2153984.5\n",
            "in training loop, epoch 1, step 861, the loss is 458986.25\n",
            "in training loop, epoch 1, step 862, the loss is 1466219.375\n",
            "in training loop, epoch 1, step 863, the loss is 1688906.0\n",
            "in training loop, epoch 1, step 864, the loss is 1737457.25\n",
            "in training loop, epoch 1, step 865, the loss is 1027152.9375\n",
            "in training loop, epoch 1, step 866, the loss is 788178.125\n",
            "in training loop, epoch 1, step 867, the loss is 292449.21875\n",
            "in training loop, epoch 1, step 868, the loss is 3442532.75\n",
            "in training loop, epoch 1, step 869, the loss is 1842912.25\n",
            "in training loop, epoch 1, step 870, the loss is 1307774.0\n",
            "in training loop, epoch 1, step 871, the loss is 1246269.5\n",
            "in training loop, epoch 1, step 872, the loss is 638045.4375\n",
            "in training loop, epoch 1, step 873, the loss is 1431640.625\n",
            "in training loop, epoch 1, step 874, the loss is 1576779.875\n",
            "in training loop, epoch 1, step 875, the loss is 852883.25\n",
            "in training loop, epoch 1, step 876, the loss is 1244075.125\n",
            "in training loop, epoch 1, step 877, the loss is 2622979.5\n",
            "in training loop, epoch 1, step 878, the loss is 612420.25\n",
            "in training loop, epoch 1, step 879, the loss is 827718.375\n",
            "in training loop, epoch 1, step 880, the loss is 5338016.0\n",
            "in training loop, epoch 1, step 881, the loss is 1009458.625\n",
            "in training loop, epoch 1, step 882, the loss is 1465275.875\n",
            "in training loop, epoch 1, step 883, the loss is 1532751.25\n",
            "in training loop, epoch 1, step 884, the loss is 1396021.375\n",
            "in training loop, epoch 1, step 885, the loss is 2414251.5\n",
            "in training loop, epoch 1, step 886, the loss is 2996250.5\n",
            "in training loop, epoch 1, step 887, the loss is 1020187.4375\n",
            "in training loop, epoch 1, step 888, the loss is 2931757.0\n",
            "in training loop, epoch 1, step 889, the loss is 1054946.5\n",
            "in training loop, epoch 1, step 890, the loss is 798806.875\n",
            "in training loop, epoch 1, step 891, the loss is 1333061.375\n",
            "in training loop, epoch 1, step 892, the loss is 2397275.75\n",
            "in training loop, epoch 1, step 893, the loss is 2292409.25\n",
            "in training loop, epoch 1, step 894, the loss is 1463759.875\n",
            "in training loop, epoch 1, step 895, the loss is 1465080.25\n",
            "in training loop, epoch 1, step 896, the loss is 1463609.5\n",
            "in training loop, epoch 1, step 897, the loss is 1499159.0\n",
            "in training loop, epoch 1, step 898, the loss is 2125346.75\n",
            "in training loop, epoch 1, step 899, the loss is 3128842.5\n",
            "in training loop, epoch 1, step 900, the loss is 2604866.0\n",
            "in training loop, epoch 1, step 901, the loss is 536057.3125\n",
            "in training loop, epoch 1, step 902, the loss is 1982149.875\n",
            "in training loop, epoch 1, step 903, the loss is 320322.46875\n",
            "k-fold 0:: Epoch 1: train loss 2267803.508711283 val loss 1565953.6417079207\n",
            "in training loop, epoch 2, step 0, the loss is 1363940.75\n",
            "in training loop, epoch 2, step 1, the loss is 1757658.25\n",
            "in training loop, epoch 2, step 2, the loss is 989515.25\n",
            "in training loop, epoch 2, step 3, the loss is 1063248.0\n",
            "in training loop, epoch 2, step 4, the loss is 1923840.0\n",
            "in training loop, epoch 2, step 5, the loss is 3648010.25\n",
            "in training loop, epoch 2, step 6, the loss is 1730207.5\n",
            "in training loop, epoch 2, step 7, the loss is 3096768.75\n",
            "in training loop, epoch 2, step 8, the loss is 1427165.5\n",
            "in training loop, epoch 2, step 9, the loss is 3164142.5\n",
            "in training loop, epoch 2, step 10, the loss is 1350801.25\n",
            "in training loop, epoch 2, step 11, the loss is 1869208.75\n",
            "in training loop, epoch 2, step 12, the loss is 1621783.75\n",
            "in training loop, epoch 2, step 13, the loss is 1981203.875\n",
            "in training loop, epoch 2, step 14, the loss is 965504.3125\n",
            "in training loop, epoch 2, step 15, the loss is 1084546.625\n",
            "in training loop, epoch 2, step 16, the loss is 2396726.5\n",
            "in training loop, epoch 2, step 17, the loss is 2480111.5\n",
            "in training loop, epoch 2, step 18, the loss is 704898.375\n",
            "in training loop, epoch 2, step 19, the loss is 1780327.75\n",
            "in training loop, epoch 2, step 20, the loss is 1357539.5\n",
            "in training loop, epoch 2, step 21, the loss is 1377574.875\n",
            "in training loop, epoch 2, step 22, the loss is 1241491.625\n",
            "in training loop, epoch 2, step 23, the loss is 952285.375\n",
            "in training loop, epoch 2, step 24, the loss is 1436240.375\n",
            "in training loop, epoch 2, step 25, the loss is 698323.1875\n",
            "in training loop, epoch 2, step 26, the loss is 939945.625\n",
            "in training loop, epoch 2, step 27, the loss is 1329125.5\n",
            "in training loop, epoch 2, step 28, the loss is 1757165.375\n",
            "in training loop, epoch 2, step 29, the loss is 1905014.5\n",
            "in training loop, epoch 2, step 30, the loss is 756784.6875\n",
            "in training loop, epoch 2, step 31, the loss is 931550.0\n",
            "in training loop, epoch 2, step 32, the loss is 1280362.25\n",
            "in training loop, epoch 2, step 33, the loss is 550081.125\n",
            "in training loop, epoch 2, step 34, the loss is 2019637.0\n",
            "in training loop, epoch 2, step 35, the loss is 998571.0\n",
            "in training loop, epoch 2, step 36, the loss is 851193.3125\n",
            "in training loop, epoch 2, step 37, the loss is 1055436.0\n",
            "in training loop, epoch 2, step 38, the loss is 1788103.0\n",
            "in training loop, epoch 2, step 39, the loss is 1849561.0\n",
            "in training loop, epoch 2, step 40, the loss is 2451066.5\n",
            "in training loop, epoch 2, step 41, the loss is 1377352.25\n",
            "in training loop, epoch 2, step 42, the loss is 1166632.25\n",
            "in training loop, epoch 2, step 43, the loss is 1806280.75\n",
            "in training loop, epoch 2, step 44, the loss is 1517851.75\n",
            "in training loop, epoch 2, step 45, the loss is 2051535.125\n",
            "in training loop, epoch 2, step 46, the loss is 1864179.25\n",
            "in training loop, epoch 2, step 47, the loss is 2316547.5\n",
            "in training loop, epoch 2, step 48, the loss is 1778907.625\n",
            "in training loop, epoch 2, step 49, the loss is 1836688.25\n",
            "in training loop, epoch 2, step 50, the loss is 1495768.875\n",
            "in training loop, epoch 2, step 51, the loss is 1469151.625\n",
            "in training loop, epoch 2, step 52, the loss is 985139.5\n",
            "in training loop, epoch 2, step 53, the loss is 1882962.375\n",
            "in training loop, epoch 2, step 54, the loss is 1601832.875\n",
            "in training loop, epoch 2, step 55, the loss is 1445213.875\n",
            "in training loop, epoch 2, step 56, the loss is 1188985.25\n",
            "in training loop, epoch 2, step 57, the loss is 2624456.0\n",
            "in training loop, epoch 2, step 58, the loss is 732050.375\n",
            "in training loop, epoch 2, step 59, the loss is 2528734.25\n",
            "in training loop, epoch 2, step 60, the loss is 799259.25\n",
            "in training loop, epoch 2, step 61, the loss is 1381857.125\n",
            "in training loop, epoch 2, step 62, the loss is 2713493.25\n",
            "in training loop, epoch 2, step 63, the loss is 999769.125\n",
            "in training loop, epoch 2, step 64, the loss is 1047647.9375\n",
            "in training loop, epoch 2, step 65, the loss is 1477549.0\n",
            "in training loop, epoch 2, step 66, the loss is 2644995.0\n",
            "in training loop, epoch 2, step 67, the loss is 2198279.0\n",
            "in training loop, epoch 2, step 68, the loss is 1448291.625\n",
            "in training loop, epoch 2, step 69, the loss is 2381221.0\n",
            "in training loop, epoch 2, step 70, the loss is 3553660.75\n",
            "in training loop, epoch 2, step 71, the loss is 1705993.625\n",
            "in training loop, epoch 2, step 72, the loss is 1547010.625\n",
            "in training loop, epoch 2, step 73, the loss is 647336.5\n",
            "in training loop, epoch 2, step 74, the loss is 1016561.375\n",
            "in training loop, epoch 2, step 75, the loss is 1482896.5\n",
            "in training loop, epoch 2, step 76, the loss is 943472.25\n",
            "in training loop, epoch 2, step 77, the loss is 1382695.0\n",
            "in training loop, epoch 2, step 78, the loss is 1151322.0\n",
            "in training loop, epoch 2, step 79, the loss is 856605.375\n",
            "in training loop, epoch 2, step 80, the loss is 1582057.125\n",
            "in training loop, epoch 2, step 81, the loss is 1104038.5\n",
            "in training loop, epoch 2, step 82, the loss is 1058663.5\n",
            "in training loop, epoch 2, step 83, the loss is 1290689.5\n",
            "in training loop, epoch 2, step 84, the loss is 2907555.0\n",
            "in training loop, epoch 2, step 85, the loss is 1207019.0\n",
            "in training loop, epoch 2, step 86, the loss is 1592126.5\n",
            "in training loop, epoch 2, step 87, the loss is 1751622.5\n",
            "in training loop, epoch 2, step 88, the loss is 1384570.125\n",
            "in training loop, epoch 2, step 89, the loss is 5172778.0\n",
            "in training loop, epoch 2, step 90, the loss is 745270.9375\n",
            "in training loop, epoch 2, step 91, the loss is 1306143.25\n",
            "in training loop, epoch 2, step 92, the loss is 910414.5625\n",
            "in training loop, epoch 2, step 93, the loss is 925532.625\n",
            "in training loop, epoch 2, step 94, the loss is 1208468.375\n",
            "in training loop, epoch 2, step 95, the loss is 1377664.125\n",
            "in training loop, epoch 2, step 96, the loss is 1838209.375\n",
            "in training loop, epoch 2, step 97, the loss is 818903.0625\n",
            "in training loop, epoch 2, step 98, the loss is 966305.9375\n",
            "in training loop, epoch 2, step 99, the loss is 1962378.0\n",
            "in training loop, epoch 2, step 100, the loss is 1324124.75\n",
            "in training loop, epoch 2, step 101, the loss is 1243526.0\n",
            "in training loop, epoch 2, step 102, the loss is 1787752.5\n",
            "in training loop, epoch 2, step 103, the loss is 1048060.0625\n",
            "in training loop, epoch 2, step 104, the loss is 904016.0625\n",
            "in training loop, epoch 2, step 105, the loss is 2490711.5\n",
            "in training loop, epoch 2, step 106, the loss is 2126361.25\n",
            "in training loop, epoch 2, step 107, the loss is 1181342.5\n",
            "in training loop, epoch 2, step 108, the loss is 1229178.75\n",
            "in training loop, epoch 2, step 109, the loss is 2059531.125\n",
            "in training loop, epoch 2, step 110, the loss is 923245.5625\n",
            "in training loop, epoch 2, step 111, the loss is 2159697.5\n",
            "in training loop, epoch 2, step 112, the loss is 1277706.125\n",
            "in training loop, epoch 2, step 113, the loss is 2251832.5\n",
            "in training loop, epoch 2, step 114, the loss is 975222.9375\n",
            "in training loop, epoch 2, step 115, the loss is 827826.625\n",
            "in training loop, epoch 2, step 116, the loss is 865539.9375\n",
            "in training loop, epoch 2, step 117, the loss is 1873154.125\n",
            "in training loop, epoch 2, step 118, the loss is 968491.3125\n",
            "in training loop, epoch 2, step 119, the loss is 1290749.0\n",
            "in training loop, epoch 2, step 120, the loss is 2201226.0\n",
            "in training loop, epoch 2, step 121, the loss is 2031909.75\n",
            "in training loop, epoch 2, step 122, the loss is 2343721.75\n",
            "in training loop, epoch 2, step 123, the loss is 668438.9375\n",
            "in training loop, epoch 2, step 124, the loss is 1477436.625\n",
            "in training loop, epoch 2, step 125, the loss is 838067.25\n",
            "in training loop, epoch 2, step 126, the loss is 992414.5\n",
            "in training loop, epoch 2, step 127, the loss is 1147960.25\n",
            "in training loop, epoch 2, step 128, the loss is 1277613.25\n",
            "in training loop, epoch 2, step 129, the loss is 2151675.75\n",
            "in training loop, epoch 2, step 130, the loss is 1522486.625\n",
            "in training loop, epoch 2, step 131, the loss is 2456550.25\n",
            "in training loop, epoch 2, step 132, the loss is 1467237.5\n",
            "in training loop, epoch 2, step 133, the loss is 1252480.875\n",
            "in training loop, epoch 2, step 134, the loss is 1704556.625\n",
            "in training loop, epoch 2, step 135, the loss is 859650.75\n",
            "in training loop, epoch 2, step 136, the loss is 1272225.75\n",
            "in training loop, epoch 2, step 137, the loss is 929244.75\n",
            "in training loop, epoch 2, step 138, the loss is 2883027.0\n",
            "in training loop, epoch 2, step 139, the loss is 1309857.5\n",
            "in training loop, epoch 2, step 140, the loss is 1983028.75\n",
            "in training loop, epoch 2, step 141, the loss is 1334433.125\n",
            "in training loop, epoch 2, step 142, the loss is 902919.9375\n",
            "in training loop, epoch 2, step 143, the loss is 1885533.75\n",
            "in training loop, epoch 2, step 144, the loss is 2126812.5\n",
            "in training loop, epoch 2, step 145, the loss is 622435.4375\n",
            "in training loop, epoch 2, step 146, the loss is 926679.6875\n",
            "in training loop, epoch 2, step 147, the loss is 2491186.25\n",
            "in training loop, epoch 2, step 148, the loss is 711338.25\n",
            "in training loop, epoch 2, step 149, the loss is 3319961.0\n",
            "in training loop, epoch 2, step 150, the loss is 1141655.25\n",
            "in training loop, epoch 2, step 151, the loss is 1407973.375\n",
            "in training loop, epoch 2, step 152, the loss is 1212932.0\n",
            "in training loop, epoch 2, step 153, the loss is 2035568.25\n",
            "in training loop, epoch 2, step 154, the loss is 1317148.5\n",
            "in training loop, epoch 2, step 155, the loss is 1229621.875\n",
            "in training loop, epoch 2, step 156, the loss is 621591.5\n",
            "in training loop, epoch 2, step 157, the loss is 2564628.25\n",
            "in training loop, epoch 2, step 158, the loss is 1144049.625\n",
            "in training loop, epoch 2, step 159, the loss is 1216137.0\n",
            "in training loop, epoch 2, step 160, the loss is 2680644.5\n",
            "in training loop, epoch 2, step 161, the loss is 2092611.625\n",
            "in training loop, epoch 2, step 162, the loss is 887510.625\n",
            "in training loop, epoch 2, step 163, the loss is 843819.4375\n",
            "in training loop, epoch 2, step 164, the loss is 570650.9375\n",
            "in training loop, epoch 2, step 165, the loss is 1104575.625\n",
            "in training loop, epoch 2, step 166, the loss is 788184.9375\n",
            "in training loop, epoch 2, step 167, the loss is 1250732.75\n",
            "in training loop, epoch 2, step 168, the loss is 1806919.625\n",
            "in training loop, epoch 2, step 169, the loss is 4150842.5\n",
            "in training loop, epoch 2, step 170, the loss is 1032091.4375\n",
            "in training loop, epoch 2, step 171, the loss is 2151978.0\n",
            "in training loop, epoch 2, step 172, the loss is 1706395.25\n",
            "in training loop, epoch 2, step 173, the loss is 1446237.875\n",
            "in training loop, epoch 2, step 174, the loss is 2401611.0\n",
            "in training loop, epoch 2, step 175, the loss is 2172288.75\n",
            "in training loop, epoch 2, step 176, the loss is 805551.625\n",
            "in training loop, epoch 2, step 177, the loss is 1666913.0\n",
            "in training loop, epoch 2, step 178, the loss is 1490580.0\n",
            "in training loop, epoch 2, step 179, the loss is 1874106.75\n",
            "in training loop, epoch 2, step 180, the loss is 1968640.875\n",
            "in training loop, epoch 2, step 181, the loss is 2237750.5\n",
            "in training loop, epoch 2, step 182, the loss is 2108922.75\n",
            "in training loop, epoch 2, step 183, the loss is 1895733.5\n",
            "in training loop, epoch 2, step 184, the loss is 944254.375\n",
            "in training loop, epoch 2, step 185, the loss is 2989138.25\n",
            "in training loop, epoch 2, step 186, the loss is 2473836.5\n",
            "in training loop, epoch 2, step 187, the loss is 876079.125\n",
            "in training loop, epoch 2, step 188, the loss is 1723477.25\n",
            "in training loop, epoch 2, step 189, the loss is 1076267.125\n",
            "in training loop, epoch 2, step 190, the loss is 2475381.5\n",
            "in training loop, epoch 2, step 191, the loss is 1545238.25\n",
            "in training loop, epoch 2, step 192, the loss is 1694721.75\n",
            "in training loop, epoch 2, step 193, the loss is 579294.8125\n",
            "in training loop, epoch 2, step 194, the loss is 1131991.125\n",
            "in training loop, epoch 2, step 195, the loss is 689060.25\n",
            "in training loop, epoch 2, step 196, the loss is 916282.375\n",
            "in training loop, epoch 2, step 197, the loss is 1551160.0\n",
            "in training loop, epoch 2, step 198, the loss is 1555421.375\n",
            "in training loop, epoch 2, step 199, the loss is 1437477.5\n",
            "in training loop, epoch 2, step 200, the loss is 1044546.875\n",
            "in training loop, epoch 2, step 201, the loss is 1130228.25\n",
            "in training loop, epoch 2, step 202, the loss is 1616509.0\n",
            "in training loop, epoch 2, step 203, the loss is 1179504.5\n",
            "in training loop, epoch 2, step 204, the loss is 1428549.75\n",
            "in training loop, epoch 2, step 205, the loss is 1193903.625\n",
            "in training loop, epoch 2, step 206, the loss is 1414889.875\n",
            "in training loop, epoch 2, step 207, the loss is 1060421.0\n",
            "in training loop, epoch 2, step 208, the loss is 1666115.75\n",
            "in training loop, epoch 2, step 209, the loss is 1789634.875\n",
            "in training loop, epoch 2, step 210, the loss is 1528356.0\n",
            "in training loop, epoch 2, step 211, the loss is 1422030.25\n",
            "in training loop, epoch 2, step 212, the loss is 1866184.25\n",
            "in training loop, epoch 2, step 213, the loss is 1806813.25\n",
            "in training loop, epoch 2, step 214, the loss is 852130.125\n",
            "in training loop, epoch 2, step 215, the loss is 1328496.375\n",
            "in training loop, epoch 2, step 216, the loss is 922359.625\n",
            "in training loop, epoch 2, step 217, the loss is 1962302.25\n",
            "in training loop, epoch 2, step 218, the loss is 812770.5\n",
            "in training loop, epoch 2, step 219, the loss is 1395974.625\n",
            "in training loop, epoch 2, step 220, the loss is 1455003.625\n",
            "in training loop, epoch 2, step 221, the loss is 1161876.125\n",
            "in training loop, epoch 2, step 222, the loss is 897959.0\n",
            "in training loop, epoch 2, step 223, the loss is 1245933.75\n",
            "in training loop, epoch 2, step 224, the loss is 1774846.875\n",
            "in training loop, epoch 2, step 225, the loss is 1003389.3125\n",
            "in training loop, epoch 2, step 226, the loss is 856795.875\n",
            "in training loop, epoch 2, step 227, the loss is 1666304.625\n",
            "in training loop, epoch 2, step 228, the loss is 1469587.0\n",
            "in training loop, epoch 2, step 229, the loss is 1339478.75\n",
            "in training loop, epoch 2, step 230, the loss is 1800114.125\n",
            "in training loop, epoch 2, step 231, the loss is 623896.75\n",
            "in training loop, epoch 2, step 232, the loss is 1404338.125\n",
            "in training loop, epoch 2, step 233, the loss is 5048598.5\n",
            "in training loop, epoch 2, step 234, the loss is 1641774.0\n",
            "in training loop, epoch 2, step 235, the loss is 1250975.625\n",
            "in training loop, epoch 2, step 236, the loss is 1549868.125\n",
            "in training loop, epoch 2, step 237, the loss is 787575.0\n",
            "in training loop, epoch 2, step 238, the loss is 2044621.0\n",
            "in training loop, epoch 2, step 239, the loss is 1367783.625\n",
            "in training loop, epoch 2, step 240, the loss is 1930625.75\n",
            "in training loop, epoch 2, step 241, the loss is 1718159.625\n",
            "in training loop, epoch 2, step 242, the loss is 973860.5625\n",
            "in training loop, epoch 2, step 243, the loss is 1755649.25\n",
            "in training loop, epoch 2, step 244, the loss is 1090748.0\n",
            "in training loop, epoch 2, step 245, the loss is 1096778.75\n",
            "in training loop, epoch 2, step 246, the loss is 1658600.0\n",
            "in training loop, epoch 2, step 247, the loss is 959901.6875\n",
            "in training loop, epoch 2, step 248, the loss is 2196706.75\n",
            "in training loop, epoch 2, step 249, the loss is 1213662.375\n",
            "in training loop, epoch 2, step 250, the loss is 1953837.875\n",
            "in training loop, epoch 2, step 251, the loss is 775205.9375\n",
            "in training loop, epoch 2, step 252, the loss is 1761984.25\n",
            "in training loop, epoch 2, step 253, the loss is 1549393.125\n",
            "in training loop, epoch 2, step 254, the loss is 2418248.25\n",
            "in training loop, epoch 2, step 255, the loss is 1061883.5\n",
            "in training loop, epoch 2, step 256, the loss is 2043397.625\n",
            "in training loop, epoch 2, step 257, the loss is 1147879.75\n",
            "in training loop, epoch 2, step 258, the loss is 1511434.0\n",
            "in training loop, epoch 2, step 259, the loss is 1265056.75\n",
            "in training loop, epoch 2, step 260, the loss is 1351184.5\n",
            "in training loop, epoch 2, step 261, the loss is 1013900.25\n",
            "in training loop, epoch 2, step 262, the loss is 1731819.75\n",
            "in training loop, epoch 2, step 263, the loss is 1375353.375\n",
            "in training loop, epoch 2, step 264, the loss is 1621586.25\n",
            "in training loop, epoch 2, step 265, the loss is 796260.4375\n",
            "in training loop, epoch 2, step 266, the loss is 1170409.25\n",
            "in training loop, epoch 2, step 267, the loss is 914315.0625\n",
            "in training loop, epoch 2, step 268, the loss is 943693.0625\n",
            "in training loop, epoch 2, step 269, the loss is 844347.875\n",
            "in training loop, epoch 2, step 270, the loss is 606913.875\n",
            "in training loop, epoch 2, step 271, the loss is 3599993.5\n",
            "in training loop, epoch 2, step 272, the loss is 1290487.625\n",
            "in training loop, epoch 2, step 273, the loss is 2287012.0\n",
            "in training loop, epoch 2, step 274, the loss is 751246.875\n",
            "in training loop, epoch 2, step 275, the loss is 5702782.5\n",
            "in training loop, epoch 2, step 276, the loss is 2203484.5\n",
            "in training loop, epoch 2, step 277, the loss is 1013192.125\n",
            "in training loop, epoch 2, step 278, the loss is 2142739.5\n",
            "in training loop, epoch 2, step 279, the loss is 449747.40625\n",
            "in training loop, epoch 2, step 280, the loss is 1134189.0\n",
            "in training loop, epoch 2, step 281, the loss is 1690231.0\n",
            "in training loop, epoch 2, step 282, the loss is 1569877.0\n",
            "in training loop, epoch 2, step 283, the loss is 1162751.25\n",
            "in training loop, epoch 2, step 284, the loss is 808037.375\n",
            "in training loop, epoch 2, step 285, the loss is 3298218.5\n",
            "in training loop, epoch 2, step 286, the loss is 1616028.875\n",
            "in training loop, epoch 2, step 287, the loss is 1722766.25\n",
            "in training loop, epoch 2, step 288, the loss is 919140.5\n",
            "in training loop, epoch 2, step 289, the loss is 1496186.125\n",
            "in training loop, epoch 2, step 290, the loss is 1219533.375\n",
            "in training loop, epoch 2, step 291, the loss is 859654.1875\n",
            "in training loop, epoch 2, step 292, the loss is 1463518.875\n",
            "in training loop, epoch 2, step 293, the loss is 771795.75\n",
            "in training loop, epoch 2, step 294, the loss is 1453818.125\n",
            "in training loop, epoch 2, step 295, the loss is 1842870.25\n",
            "in training loop, epoch 2, step 296, the loss is 1255021.375\n",
            "in training loop, epoch 2, step 297, the loss is 1176738.625\n",
            "in training loop, epoch 2, step 298, the loss is 1124632.375\n",
            "in training loop, epoch 2, step 299, the loss is 3181766.0\n",
            "in training loop, epoch 2, step 300, the loss is 2400124.0\n",
            "in training loop, epoch 2, step 301, the loss is 1094022.625\n",
            "in training loop, epoch 2, step 302, the loss is 645765.9375\n",
            "in training loop, epoch 2, step 303, the loss is 1676493.0\n",
            "in training loop, epoch 2, step 304, the loss is 2224790.5\n",
            "in training loop, epoch 2, step 305, the loss is 1296802.625\n",
            "in training loop, epoch 2, step 306, the loss is 1619331.875\n",
            "in training loop, epoch 2, step 307, the loss is 736820.0625\n",
            "in training loop, epoch 2, step 308, the loss is 1040807.125\n",
            "in training loop, epoch 2, step 309, the loss is 3146981.0\n",
            "in training loop, epoch 2, step 310, the loss is 1905655.5\n",
            "in training loop, epoch 2, step 311, the loss is 1288318.125\n",
            "in training loop, epoch 2, step 312, the loss is 1396283.875\n",
            "in training loop, epoch 2, step 313, the loss is 759506.375\n",
            "in training loop, epoch 2, step 314, the loss is 1133344.625\n",
            "in training loop, epoch 2, step 315, the loss is 1124896.125\n",
            "in training loop, epoch 2, step 316, the loss is 1199236.375\n",
            "in training loop, epoch 2, step 317, the loss is 1962305.75\n",
            "in training loop, epoch 2, step 318, the loss is 1136150.0\n",
            "in training loop, epoch 2, step 319, the loss is 1415199.0\n",
            "in training loop, epoch 2, step 320, the loss is 1033214.875\n",
            "in training loop, epoch 2, step 321, the loss is 748957.875\n",
            "in training loop, epoch 2, step 322, the loss is 1670271.5\n",
            "in training loop, epoch 2, step 323, the loss is 1459024.375\n",
            "in training loop, epoch 2, step 324, the loss is 1160268.5\n",
            "in training loop, epoch 2, step 325, the loss is 835579.75\n",
            "in training loop, epoch 2, step 326, the loss is 979708.25\n",
            "in training loop, epoch 2, step 327, the loss is 2460584.0\n",
            "in training loop, epoch 2, step 328, the loss is 823676.5\n",
            "in training loop, epoch 2, step 329, the loss is 466121.375\n",
            "in training loop, epoch 2, step 330, the loss is 747714.125\n",
            "in training loop, epoch 2, step 331, the loss is 1376326.375\n",
            "in training loop, epoch 2, step 332, the loss is 802645.6875\n",
            "in training loop, epoch 2, step 333, the loss is 1981303.5\n",
            "in training loop, epoch 2, step 334, the loss is 538719.625\n",
            "in training loop, epoch 2, step 335, the loss is 522521.21875\n",
            "in training loop, epoch 2, step 336, the loss is 1009069.1875\n",
            "in training loop, epoch 2, step 337, the loss is 577127.5\n",
            "in training loop, epoch 2, step 338, the loss is 1261754.125\n",
            "in training loop, epoch 2, step 339, the loss is 1941833.0\n",
            "in training loop, epoch 2, step 340, the loss is 1614109.25\n",
            "in training loop, epoch 2, step 341, the loss is 2828384.0\n",
            "in training loop, epoch 2, step 342, the loss is 2808159.5\n",
            "in training loop, epoch 2, step 343, the loss is 933967.875\n",
            "in training loop, epoch 2, step 344, the loss is 1340301.5\n",
            "in training loop, epoch 2, step 345, the loss is 1422033.5\n",
            "in training loop, epoch 2, step 346, the loss is 924048.4375\n",
            "in training loop, epoch 2, step 347, the loss is 655066.9375\n",
            "in training loop, epoch 2, step 348, the loss is 1211901.0\n",
            "in training loop, epoch 2, step 349, the loss is 823526.25\n",
            "in training loop, epoch 2, step 350, the loss is 1396075.0\n",
            "in training loop, epoch 2, step 351, the loss is 1167639.75\n",
            "in training loop, epoch 2, step 352, the loss is 806892.5\n",
            "in training loop, epoch 2, step 353, the loss is 1032073.1875\n",
            "in training loop, epoch 2, step 354, the loss is 1299392.0\n",
            "in training loop, epoch 2, step 355, the loss is 724626.625\n",
            "in training loop, epoch 2, step 356, the loss is 2154948.5\n",
            "in training loop, epoch 2, step 357, the loss is 1142219.875\n",
            "in training loop, epoch 2, step 358, the loss is 1216690.25\n",
            "in training loop, epoch 2, step 359, the loss is 3999714.75\n",
            "in training loop, epoch 2, step 360, the loss is 1260593.0\n",
            "in training loop, epoch 2, step 361, the loss is 1219135.0\n",
            "in training loop, epoch 2, step 362, the loss is 1160249.875\n",
            "in training loop, epoch 2, step 363, the loss is 1453659.125\n",
            "in training loop, epoch 2, step 364, the loss is 1403192.0\n",
            "in training loop, epoch 2, step 365, the loss is 1101840.875\n",
            "in training loop, epoch 2, step 366, the loss is 1266800.125\n",
            "in training loop, epoch 2, step 367, the loss is 1247658.0\n",
            "in training loop, epoch 2, step 368, the loss is 2668560.25\n",
            "in training loop, epoch 2, step 369, the loss is 1118657.0\n",
            "in training loop, epoch 2, step 370, the loss is 1228198.875\n",
            "in training loop, epoch 2, step 371, the loss is 1841715.125\n",
            "in training loop, epoch 2, step 372, the loss is 528053.75\n",
            "in training loop, epoch 2, step 373, the loss is 761970.25\n",
            "in training loop, epoch 2, step 374, the loss is 1305359.75\n",
            "in training loop, epoch 2, step 375, the loss is 810324.25\n",
            "in training loop, epoch 2, step 376, the loss is 895695.5625\n",
            "in training loop, epoch 2, step 377, the loss is 1088084.5\n",
            "in training loop, epoch 2, step 378, the loss is 2209281.0\n",
            "in training loop, epoch 2, step 379, the loss is 1224707.75\n",
            "in training loop, epoch 2, step 380, the loss is 514300.15625\n",
            "in training loop, epoch 2, step 381, the loss is 1124359.75\n",
            "in training loop, epoch 2, step 382, the loss is 1266013.75\n",
            "in training loop, epoch 2, step 383, the loss is 3504356.5\n",
            "in training loop, epoch 2, step 384, the loss is 955961.1875\n",
            "in training loop, epoch 2, step 385, the loss is 671871.0\n",
            "in training loop, epoch 2, step 386, the loss is 736387.0\n",
            "in training loop, epoch 2, step 387, the loss is 1518559.5\n",
            "in training loop, epoch 2, step 388, the loss is 668698.1875\n",
            "in training loop, epoch 2, step 389, the loss is 645658.25\n",
            "in training loop, epoch 2, step 390, the loss is 2290384.0\n",
            "in training loop, epoch 2, step 391, the loss is 1010340.5625\n",
            "in training loop, epoch 2, step 392, the loss is 1778052.125\n",
            "in training loop, epoch 2, step 393, the loss is 1040895.125\n",
            "in training loop, epoch 2, step 394, the loss is 1644570.75\n",
            "in training loop, epoch 2, step 395, the loss is 1624162.5\n",
            "in training loop, epoch 2, step 396, the loss is 885336.875\n",
            "in training loop, epoch 2, step 397, the loss is 1092291.375\n",
            "in training loop, epoch 2, step 398, the loss is 1656234.5\n",
            "in training loop, epoch 2, step 399, the loss is 1256052.375\n",
            "in training loop, epoch 2, step 400, the loss is 1021346.75\n",
            "in training loop, epoch 2, step 401, the loss is 722079.1875\n",
            "in training loop, epoch 2, step 402, the loss is 955612.375\n",
            "in training loop, epoch 2, step 403, the loss is 598012.5\n",
            "in training loop, epoch 2, step 404, the loss is 1109208.875\n",
            "in training loop, epoch 2, step 405, the loss is 1287318.125\n",
            "in training loop, epoch 2, step 406, the loss is 792834.5625\n",
            "in training loop, epoch 2, step 407, the loss is 910431.5625\n",
            "in training loop, epoch 2, step 408, the loss is 999074.4375\n",
            "in training loop, epoch 2, step 409, the loss is 917630.875\n",
            "in training loop, epoch 2, step 410, the loss is 594012.9375\n",
            "in training loop, epoch 2, step 411, the loss is 2325168.25\n",
            "in training loop, epoch 2, step 412, the loss is 1321379.25\n",
            "in training loop, epoch 2, step 413, the loss is 1252491.375\n",
            "in training loop, epoch 2, step 414, the loss is 1149900.25\n",
            "in training loop, epoch 2, step 415, the loss is 1401484.75\n",
            "in training loop, epoch 2, step 416, the loss is 1000266.1875\n",
            "in training loop, epoch 2, step 417, the loss is 1269980.75\n",
            "in training loop, epoch 2, step 418, the loss is 1074620.625\n",
            "in training loop, epoch 2, step 419, the loss is 716624.875\n",
            "in training loop, epoch 2, step 420, the loss is 1152783.875\n",
            "in training loop, epoch 2, step 421, the loss is 770369.6875\n",
            "in training loop, epoch 2, step 422, the loss is 3287578.5\n",
            "in training loop, epoch 2, step 423, the loss is 1602208.75\n",
            "in training loop, epoch 2, step 424, the loss is 1813661.625\n",
            "in training loop, epoch 2, step 425, the loss is 1029211.5\n",
            "in training loop, epoch 2, step 426, the loss is 1627285.125\n",
            "in training loop, epoch 2, step 427, the loss is 2551813.0\n",
            "in training loop, epoch 2, step 428, the loss is 1558216.25\n",
            "in training loop, epoch 2, step 429, the loss is 1050137.125\n",
            "in training loop, epoch 2, step 430, the loss is 940639.875\n",
            "in training loop, epoch 2, step 431, the loss is 1358576.625\n",
            "in training loop, epoch 2, step 432, the loss is 2001257.875\n",
            "in training loop, epoch 2, step 433, the loss is 1895438.75\n",
            "in training loop, epoch 2, step 434, the loss is 542304.9375\n",
            "in training loop, epoch 2, step 435, the loss is 828092.375\n",
            "in training loop, epoch 2, step 436, the loss is 1378545.5\n",
            "in training loop, epoch 2, step 437, the loss is 1250528.625\n",
            "in training loop, epoch 2, step 438, the loss is 1864913.75\n",
            "in training loop, epoch 2, step 439, the loss is 1177503.0\n",
            "in training loop, epoch 2, step 440, the loss is 1258199.5\n",
            "in training loop, epoch 2, step 441, the loss is 1744390.0\n",
            "in training loop, epoch 2, step 442, the loss is 1552880.5\n",
            "in training loop, epoch 2, step 443, the loss is 1003237.625\n",
            "in training loop, epoch 2, step 444, the loss is 1854783.5\n",
            "in training loop, epoch 2, step 445, the loss is 1031319.25\n",
            "in training loop, epoch 2, step 446, the loss is 1686339.75\n",
            "in training loop, epoch 2, step 447, the loss is 2380840.0\n",
            "in training loop, epoch 2, step 448, the loss is 1087407.25\n",
            "in training loop, epoch 2, step 449, the loss is 1672562.75\n",
            "in training loop, epoch 2, step 450, the loss is 1830550.0\n",
            "in training loop, epoch 2, step 451, the loss is 1265714.875\n",
            "in training loop, epoch 2, step 452, the loss is 1345477.75\n",
            "in training loop, epoch 2, step 453, the loss is 737489.5\n",
            "in training loop, epoch 2, step 454, the loss is 1547245.5\n",
            "in training loop, epoch 2, step 455, the loss is 1062618.625\n",
            "in training loop, epoch 2, step 456, the loss is 894861.875\n",
            "in training loop, epoch 2, step 457, the loss is 1749792.5\n",
            "in training loop, epoch 2, step 458, the loss is 1924813.5\n",
            "in training loop, epoch 2, step 459, the loss is 1012845.5\n",
            "in training loop, epoch 2, step 460, the loss is 744163.0625\n",
            "in training loop, epoch 2, step 461, the loss is 1267128.0\n",
            "in training loop, epoch 2, step 462, the loss is 2345070.75\n",
            "in training loop, epoch 2, step 463, the loss is 1968335.75\n",
            "in training loop, epoch 2, step 464, the loss is 1543564.125\n",
            "in training loop, epoch 2, step 465, the loss is 710454.875\n",
            "in training loop, epoch 2, step 466, the loss is 783415.625\n",
            "in training loop, epoch 2, step 467, the loss is 1852018.0\n",
            "in training loop, epoch 2, step 468, the loss is 1157681.375\n",
            "in training loop, epoch 2, step 469, the loss is 1260165.75\n",
            "in training loop, epoch 2, step 470, the loss is 768215.9375\n",
            "in training loop, epoch 2, step 471, the loss is 1089687.375\n",
            "in training loop, epoch 2, step 472, the loss is 1873676.375\n",
            "in training loop, epoch 2, step 473, the loss is 1798090.875\n",
            "in training loop, epoch 2, step 474, the loss is 1230697.0\n",
            "in training loop, epoch 2, step 475, the loss is 1866025.5\n",
            "in training loop, epoch 2, step 476, the loss is 1569504.0\n",
            "in training loop, epoch 2, step 477, the loss is 1824349.5\n",
            "in training loop, epoch 2, step 478, the loss is 406307.25\n",
            "in training loop, epoch 2, step 479, the loss is 829964.625\n",
            "in training loop, epoch 2, step 480, the loss is 653191.4375\n",
            "in training loop, epoch 2, step 481, the loss is 1546711.875\n",
            "in training loop, epoch 2, step 482, the loss is 1186673.625\n",
            "in training loop, epoch 2, step 483, the loss is 1922543.125\n",
            "in training loop, epoch 2, step 484, the loss is 1178900.25\n",
            "in training loop, epoch 2, step 485, the loss is 1229773.25\n",
            "in training loop, epoch 2, step 486, the loss is 965646.375\n",
            "in training loop, epoch 2, step 487, the loss is 978266.125\n",
            "in training loop, epoch 2, step 488, the loss is 701020.0\n",
            "in training loop, epoch 2, step 489, the loss is 802351.4375\n",
            "in training loop, epoch 2, step 490, the loss is 1391217.125\n",
            "in training loop, epoch 2, step 491, the loss is 967041.25\n",
            "in training loop, epoch 2, step 492, the loss is 1311618.75\n",
            "in training loop, epoch 2, step 493, the loss is 902402.25\n",
            "in training loop, epoch 2, step 494, the loss is 1298137.625\n",
            "in training loop, epoch 2, step 495, the loss is 1077755.25\n",
            "in training loop, epoch 2, step 496, the loss is 2097818.5\n",
            "in training loop, epoch 2, step 497, the loss is 1294061.75\n",
            "in training loop, epoch 2, step 498, the loss is 2725044.25\n",
            "in training loop, epoch 2, step 499, the loss is 523151.1875\n",
            "in training loop, epoch 2, step 500, the loss is 807614.8125\n",
            "in training loop, epoch 2, step 501, the loss is 1215745.75\n",
            "in training loop, epoch 2, step 502, the loss is 2036888.875\n",
            "in training loop, epoch 2, step 503, the loss is 1127901.125\n",
            "in training loop, epoch 2, step 504, the loss is 1161209.875\n",
            "in training loop, epoch 2, step 505, the loss is 961106.1875\n",
            "in training loop, epoch 2, step 506, the loss is 870603.0625\n",
            "in training loop, epoch 2, step 507, the loss is 911509.5625\n",
            "in training loop, epoch 2, step 508, the loss is 1275234.125\n",
            "in training loop, epoch 2, step 509, the loss is 2994911.75\n",
            "in training loop, epoch 2, step 510, the loss is 1231158.625\n",
            "in training loop, epoch 2, step 511, the loss is 1356696.375\n",
            "in training loop, epoch 2, step 512, the loss is 513065.1875\n",
            "in training loop, epoch 2, step 513, the loss is 1297931.625\n",
            "in training loop, epoch 2, step 514, the loss is 1758346.25\n",
            "in training loop, epoch 2, step 515, the loss is 1513097.125\n",
            "in training loop, epoch 2, step 516, the loss is 2332663.5\n",
            "in training loop, epoch 2, step 517, the loss is 1050850.375\n",
            "in training loop, epoch 2, step 518, the loss is 1096773.5\n",
            "in training loop, epoch 2, step 519, the loss is 1295727.0\n",
            "in training loop, epoch 2, step 520, the loss is 1491659.875\n",
            "in training loop, epoch 2, step 521, the loss is 985129.375\n",
            "in training loop, epoch 2, step 522, the loss is 1772611.25\n",
            "in training loop, epoch 2, step 523, the loss is 861806.375\n",
            "in training loop, epoch 2, step 524, the loss is 3195860.5\n",
            "in training loop, epoch 2, step 525, the loss is 754193.25\n",
            "in training loop, epoch 2, step 526, the loss is 1367923.875\n",
            "in training loop, epoch 2, step 527, the loss is 1081075.875\n",
            "in training loop, epoch 2, step 528, the loss is 788433.75\n",
            "in training loop, epoch 2, step 529, the loss is 993909.375\n",
            "in training loop, epoch 2, step 530, the loss is 1740627.5\n",
            "in training loop, epoch 2, step 531, the loss is 1100092.625\n",
            "in training loop, epoch 2, step 532, the loss is 3332268.5\n",
            "in training loop, epoch 2, step 533, the loss is 785727.25\n",
            "in training loop, epoch 2, step 534, the loss is 2099755.25\n",
            "in training loop, epoch 2, step 535, the loss is 2860070.0\n",
            "in training loop, epoch 2, step 536, the loss is 992184.625\n",
            "in training loop, epoch 2, step 537, the loss is 1711251.5\n",
            "in training loop, epoch 2, step 538, the loss is 1240479.25\n",
            "in training loop, epoch 2, step 539, the loss is 1649397.125\n",
            "in training loop, epoch 2, step 540, the loss is 1703779.25\n",
            "in training loop, epoch 2, step 541, the loss is 1173708.25\n",
            "in training loop, epoch 2, step 542, the loss is 2411588.5\n",
            "in training loop, epoch 2, step 543, the loss is 1075170.75\n",
            "in training loop, epoch 2, step 544, the loss is 1379518.25\n",
            "in training loop, epoch 2, step 545, the loss is 759437.625\n",
            "in training loop, epoch 2, step 546, the loss is 1388783.625\n",
            "in training loop, epoch 2, step 547, the loss is 1683044.75\n",
            "in training loop, epoch 2, step 548, the loss is 1079543.5\n",
            "in training loop, epoch 2, step 549, the loss is 1713555.25\n",
            "in training loop, epoch 2, step 550, the loss is 1115805.125\n",
            "in training loop, epoch 2, step 551, the loss is 1458489.5\n",
            "in training loop, epoch 2, step 552, the loss is 871482.0625\n",
            "in training loop, epoch 2, step 553, the loss is 659378.125\n",
            "in training loop, epoch 2, step 554, the loss is 741163.5\n",
            "in training loop, epoch 2, step 555, the loss is 1826151.75\n",
            "in training loop, epoch 2, step 556, the loss is 524345.6875\n",
            "in training loop, epoch 2, step 557, the loss is 1843234.0\n",
            "in training loop, epoch 2, step 558, the loss is 1365183.25\n",
            "in training loop, epoch 2, step 559, the loss is 1656203.5\n",
            "in training loop, epoch 2, step 560, the loss is 1612763.875\n",
            "in training loop, epoch 2, step 561, the loss is 799416.75\n",
            "in training loop, epoch 2, step 562, the loss is 766509.1875\n",
            "in training loop, epoch 2, step 563, the loss is 1108530.0\n",
            "in training loop, epoch 2, step 564, the loss is 878256.8125\n",
            "in training loop, epoch 2, step 565, the loss is 1222467.5\n",
            "in training loop, epoch 2, step 566, the loss is 722929.5625\n",
            "in training loop, epoch 2, step 567, the loss is 1506822.0\n",
            "in training loop, epoch 2, step 568, the loss is 765984.6875\n",
            "in training loop, epoch 2, step 569, the loss is 721796.1875\n",
            "in training loop, epoch 2, step 570, the loss is 744990.0\n",
            "in training loop, epoch 2, step 571, the loss is 2388898.25\n",
            "in training loop, epoch 2, step 572, the loss is 724328.0\n",
            "in training loop, epoch 2, step 573, the loss is 1353478.875\n",
            "in training loop, epoch 2, step 574, the loss is 940205.75\n",
            "in training loop, epoch 2, step 575, the loss is 1092222.875\n",
            "in training loop, epoch 2, step 576, the loss is 696933.625\n",
            "in training loop, epoch 2, step 577, the loss is 1687350.125\n",
            "in training loop, epoch 2, step 578, the loss is 1176751.25\n",
            "in training loop, epoch 2, step 579, the loss is 609877.75\n",
            "in training loop, epoch 2, step 580, the loss is 2177238.5\n",
            "in training loop, epoch 2, step 581, the loss is 979607.25\n",
            "in training loop, epoch 2, step 582, the loss is 988612.1875\n",
            "in training loop, epoch 2, step 583, the loss is 987785.0\n",
            "in training loop, epoch 2, step 584, the loss is 1324870.75\n",
            "in training loop, epoch 2, step 585, the loss is 1404862.875\n",
            "in training loop, epoch 2, step 586, the loss is 1527670.5\n",
            "in training loop, epoch 2, step 587, the loss is 855182.875\n",
            "in training loop, epoch 2, step 588, the loss is 816682.625\n",
            "in training loop, epoch 2, step 589, the loss is 1810111.0\n",
            "in training loop, epoch 2, step 590, the loss is 1128840.75\n",
            "in training loop, epoch 2, step 591, the loss is 1296892.375\n",
            "in training loop, epoch 2, step 592, the loss is 896251.875\n",
            "in training loop, epoch 2, step 593, the loss is 1092786.5\n",
            "in training loop, epoch 2, step 594, the loss is 1350603.625\n",
            "in training loop, epoch 2, step 595, the loss is 1171386.125\n",
            "in training loop, epoch 2, step 596, the loss is 1208613.5\n",
            "in training loop, epoch 2, step 597, the loss is 987980.875\n",
            "in training loop, epoch 2, step 598, the loss is 464122.46875\n",
            "in training loop, epoch 2, step 599, the loss is 721340.125\n",
            "in training loop, epoch 2, step 600, the loss is 1171295.125\n",
            "in training loop, epoch 2, step 601, the loss is 1002284.375\n",
            "in training loop, epoch 2, step 602, the loss is 693648.6875\n",
            "in training loop, epoch 2, step 603, the loss is 951117.375\n",
            "in training loop, epoch 2, step 604, the loss is 1910382.75\n",
            "in training loop, epoch 2, step 605, the loss is 1170541.375\n",
            "in training loop, epoch 2, step 606, the loss is 1324444.125\n",
            "in training loop, epoch 2, step 607, the loss is 1546589.875\n",
            "in training loop, epoch 2, step 608, the loss is 1139231.0\n",
            "in training loop, epoch 2, step 609, the loss is 648577.5\n",
            "in training loop, epoch 2, step 610, the loss is 1656869.875\n",
            "in training loop, epoch 2, step 611, the loss is 1680735.125\n",
            "in training loop, epoch 2, step 612, the loss is 1454169.875\n",
            "in training loop, epoch 2, step 613, the loss is 816465.5\n",
            "in training loop, epoch 2, step 614, the loss is 1097249.0\n",
            "in training loop, epoch 2, step 615, the loss is 1530680.125\n",
            "in training loop, epoch 2, step 616, the loss is 910021.75\n",
            "in training loop, epoch 2, step 617, the loss is 1690446.0\n",
            "in training loop, epoch 2, step 618, the loss is 2095405.5\n",
            "in training loop, epoch 2, step 619, the loss is 1500324.5\n",
            "in training loop, epoch 2, step 620, the loss is 1076549.875\n",
            "in training loop, epoch 2, step 621, the loss is 391026.0625\n",
            "in training loop, epoch 2, step 622, the loss is 904882.25\n",
            "in training loop, epoch 2, step 623, the loss is 937520.625\n",
            "in training loop, epoch 2, step 624, the loss is 1797185.5\n",
            "in training loop, epoch 2, step 625, the loss is 1782866.0\n",
            "in training loop, epoch 2, step 626, the loss is 1034444.9375\n",
            "in training loop, epoch 2, step 627, the loss is 745226.0625\n",
            "in training loop, epoch 2, step 628, the loss is 1437001.25\n",
            "in training loop, epoch 2, step 629, the loss is 1935101.0\n",
            "in training loop, epoch 2, step 630, the loss is 327167.5625\n",
            "in training loop, epoch 2, step 631, the loss is 1343778.5\n",
            "in training loop, epoch 2, step 632, the loss is 1104509.75\n",
            "in training loop, epoch 2, step 633, the loss is 1061573.75\n",
            "in training loop, epoch 2, step 634, the loss is 744198.5\n",
            "in training loop, epoch 2, step 635, the loss is 916858.75\n",
            "in training loop, epoch 2, step 636, the loss is 1056891.875\n",
            "in training loop, epoch 2, step 637, the loss is 1188695.75\n",
            "in training loop, epoch 2, step 638, the loss is 1076958.75\n",
            "in training loop, epoch 2, step 639, the loss is 613865.6875\n",
            "in training loop, epoch 2, step 640, the loss is 466931.125\n",
            "in training loop, epoch 2, step 641, the loss is 411028.28125\n",
            "in training loop, epoch 2, step 642, the loss is 1547300.125\n",
            "in training loop, epoch 2, step 643, the loss is 797034.0625\n",
            "in training loop, epoch 2, step 644, the loss is 860436.8125\n",
            "in training loop, epoch 2, step 645, the loss is 1436352.25\n",
            "in training loop, epoch 2, step 646, the loss is 1802579.0\n",
            "in training loop, epoch 2, step 647, the loss is 1060307.0\n",
            "in training loop, epoch 2, step 648, the loss is 1561349.125\n",
            "in training loop, epoch 2, step 649, the loss is 1686990.0\n",
            "in training loop, epoch 2, step 650, the loss is 1097307.5\n",
            "in training loop, epoch 2, step 651, the loss is 1107186.125\n",
            "in training loop, epoch 2, step 652, the loss is 615758.375\n",
            "in training loop, epoch 2, step 653, the loss is 1770289.125\n",
            "in training loop, epoch 2, step 654, the loss is 806365.625\n",
            "in training loop, epoch 2, step 655, the loss is 1434931.25\n",
            "in training loop, epoch 2, step 656, the loss is 711662.25\n",
            "in training loop, epoch 2, step 657, the loss is 876604.875\n",
            "in training loop, epoch 2, step 658, the loss is 1665117.5\n",
            "in training loop, epoch 2, step 659, the loss is 1208502.125\n",
            "in training loop, epoch 2, step 660, the loss is 1143848.5\n",
            "in training loop, epoch 2, step 661, the loss is 1276126.125\n",
            "in training loop, epoch 2, step 662, the loss is 1308872.5\n",
            "in training loop, epoch 2, step 663, the loss is 1232202.375\n",
            "in training loop, epoch 2, step 664, the loss is 842639.875\n",
            "in training loop, epoch 2, step 665, the loss is 1266233.625\n",
            "in training loop, epoch 2, step 666, the loss is 707649.75\n",
            "in training loop, epoch 2, step 667, the loss is 802368.75\n",
            "in training loop, epoch 2, step 668, the loss is 818163.9375\n",
            "in training loop, epoch 2, step 669, the loss is 1320025.5\n",
            "in training loop, epoch 2, step 670, the loss is 1671680.5\n",
            "in training loop, epoch 2, step 671, the loss is 737909.5625\n",
            "in training loop, epoch 2, step 672, the loss is 840974.625\n",
            "in training loop, epoch 2, step 673, the loss is 627732.3125\n",
            "in training loop, epoch 2, step 674, the loss is 1074766.75\n",
            "in training loop, epoch 2, step 675, the loss is 762349.875\n",
            "in training loop, epoch 2, step 676, the loss is 1273850.125\n",
            "in training loop, epoch 2, step 677, the loss is 2050918.5\n",
            "in training loop, epoch 2, step 678, the loss is 1355988.75\n",
            "in training loop, epoch 2, step 679, the loss is 1760560.625\n",
            "in training loop, epoch 2, step 680, the loss is 1199706.375\n",
            "in training loop, epoch 2, step 681, the loss is 851940.1875\n",
            "in training loop, epoch 2, step 682, the loss is 1179791.0\n",
            "in training loop, epoch 2, step 683, the loss is 1803422.5\n",
            "in training loop, epoch 2, step 684, the loss is 458115.8125\n",
            "in training loop, epoch 2, step 685, the loss is 567997.9375\n",
            "in training loop, epoch 2, step 686, the loss is 554664.5625\n",
            "in training loop, epoch 2, step 687, the loss is 1343418.75\n",
            "in training loop, epoch 2, step 688, the loss is 899946.625\n",
            "in training loop, epoch 2, step 689, the loss is 842045.8125\n",
            "in training loop, epoch 2, step 690, the loss is 802459.875\n",
            "in training loop, epoch 2, step 691, the loss is 1118925.375\n",
            "in training loop, epoch 2, step 692, the loss is 1064475.625\n",
            "in training loop, epoch 2, step 693, the loss is 888279.875\n",
            "in training loop, epoch 2, step 694, the loss is 1332052.0\n",
            "in training loop, epoch 2, step 695, the loss is 1619473.375\n",
            "in training loop, epoch 2, step 696, the loss is 1696028.0\n",
            "in training loop, epoch 2, step 697, the loss is 1536619.625\n",
            "in training loop, epoch 2, step 698, the loss is 1000446.5625\n",
            "in training loop, epoch 2, step 699, the loss is 1141892.125\n",
            "in training loop, epoch 2, step 700, the loss is 1566834.875\n",
            "in training loop, epoch 2, step 701, the loss is 475549.8125\n",
            "in training loop, epoch 2, step 702, the loss is 1464790.125\n",
            "in training loop, epoch 2, step 703, the loss is 912865.625\n",
            "in training loop, epoch 2, step 704, the loss is 766721.375\n",
            "in training loop, epoch 2, step 705, the loss is 1018362.0\n",
            "in training loop, epoch 2, step 706, the loss is 613436.125\n",
            "in training loop, epoch 2, step 707, the loss is 966045.8125\n",
            "in training loop, epoch 2, step 708, the loss is 978904.0\n",
            "in training loop, epoch 2, step 709, the loss is 1824183.5\n",
            "in training loop, epoch 2, step 710, the loss is 1870859.75\n",
            "in training loop, epoch 2, step 711, the loss is 868210.6875\n",
            "in training loop, epoch 2, step 712, the loss is 1141589.875\n",
            "in training loop, epoch 2, step 713, the loss is 964169.375\n",
            "in training loop, epoch 2, step 714, the loss is 1786275.75\n",
            "in training loop, epoch 2, step 715, the loss is 1232610.5\n",
            "in training loop, epoch 2, step 716, the loss is 685955.625\n",
            "in training loop, epoch 2, step 717, the loss is 1611535.625\n",
            "in training loop, epoch 2, step 718, the loss is 793023.9375\n",
            "in training loop, epoch 2, step 719, the loss is 1999976.125\n",
            "in training loop, epoch 2, step 720, the loss is 645982.0\n",
            "in training loop, epoch 2, step 721, the loss is 771277.875\n",
            "in training loop, epoch 2, step 722, the loss is 1293238.75\n",
            "in training loop, epoch 2, step 723, the loss is 1072764.625\n",
            "in training loop, epoch 2, step 724, the loss is 916754.6875\n",
            "in training loop, epoch 2, step 725, the loss is 1332600.25\n",
            "in training loop, epoch 2, step 726, the loss is 1392128.0\n",
            "in training loop, epoch 2, step 727, the loss is 1355976.5\n",
            "in training loop, epoch 2, step 728, the loss is 1393489.625\n",
            "in training loop, epoch 2, step 729, the loss is 1834205.625\n",
            "in training loop, epoch 2, step 730, the loss is 471197.84375\n",
            "in training loop, epoch 2, step 731, the loss is 1364763.75\n",
            "in training loop, epoch 2, step 732, the loss is 593914.0\n",
            "in training loop, epoch 2, step 733, the loss is 1240739.125\n",
            "in training loop, epoch 2, step 734, the loss is 1263825.375\n",
            "in training loop, epoch 2, step 735, the loss is 1188653.75\n",
            "in training loop, epoch 2, step 736, the loss is 1591759.5\n",
            "in training loop, epoch 2, step 737, the loss is 864503.1875\n",
            "in training loop, epoch 2, step 738, the loss is 923873.4375\n",
            "in training loop, epoch 2, step 739, the loss is 1328911.125\n",
            "in training loop, epoch 2, step 740, the loss is 832101.6875\n",
            "in training loop, epoch 2, step 741, the loss is 1700059.0\n",
            "in training loop, epoch 2, step 742, the loss is 1500984.875\n",
            "in training loop, epoch 2, step 743, the loss is 1245321.375\n",
            "in training loop, epoch 2, step 744, the loss is 1044699.0\n",
            "in training loop, epoch 2, step 745, the loss is 1227181.5\n",
            "in training loop, epoch 2, step 746, the loss is 1057356.625\n",
            "in training loop, epoch 2, step 747, the loss is 753866.3125\n",
            "in training loop, epoch 2, step 748, the loss is 945356.625\n",
            "in training loop, epoch 2, step 749, the loss is 1090847.375\n",
            "in training loop, epoch 2, step 750, the loss is 1601045.0\n",
            "in training loop, epoch 2, step 751, the loss is 341588.125\n",
            "in training loop, epoch 2, step 752, the loss is 635146.875\n",
            "in training loop, epoch 2, step 753, the loss is 1108509.125\n",
            "in training loop, epoch 2, step 754, the loss is 809463.5625\n",
            "in training loop, epoch 2, step 755, the loss is 2014863.5\n",
            "in training loop, epoch 2, step 756, the loss is 2037253.5\n",
            "in training loop, epoch 2, step 757, the loss is 1968258.0\n",
            "in training loop, epoch 2, step 758, the loss is 1831249.0\n",
            "in training loop, epoch 2, step 759, the loss is 861420.25\n",
            "in training loop, epoch 2, step 760, the loss is 898960.625\n",
            "in training loop, epoch 2, step 761, the loss is 669273.4375\n",
            "in training loop, epoch 2, step 762, the loss is 771209.375\n",
            "in training loop, epoch 2, step 763, the loss is 1120516.875\n",
            "in training loop, epoch 2, step 764, the loss is 1017616.0\n",
            "in training loop, epoch 2, step 765, the loss is 767891.1875\n",
            "in training loop, epoch 2, step 766, the loss is 830800.25\n",
            "in training loop, epoch 2, step 767, the loss is 774602.4375\n",
            "in training loop, epoch 2, step 768, the loss is 787111.5\n",
            "in training loop, epoch 2, step 769, the loss is 1516699.25\n",
            "in training loop, epoch 2, step 770, the loss is 2653966.5\n",
            "in training loop, epoch 2, step 771, the loss is 2441080.5\n",
            "in training loop, epoch 2, step 772, the loss is 689047.375\n",
            "in training loop, epoch 2, step 773, the loss is 1103350.125\n",
            "in training loop, epoch 2, step 774, the loss is 1074605.875\n",
            "in training loop, epoch 2, step 775, the loss is 1681154.75\n",
            "in training loop, epoch 2, step 776, the loss is 1170275.75\n",
            "in training loop, epoch 2, step 777, the loss is 716770.625\n",
            "in training loop, epoch 2, step 778, the loss is 998729.375\n",
            "in training loop, epoch 2, step 779, the loss is 940380.0\n",
            "in training loop, epoch 2, step 780, the loss is 1731060.75\n",
            "in training loop, epoch 2, step 781, the loss is 989473.9375\n",
            "in training loop, epoch 2, step 782, the loss is 1123936.125\n",
            "in training loop, epoch 2, step 783, the loss is 2404394.25\n",
            "in training loop, epoch 2, step 784, the loss is 1881932.0\n",
            "in training loop, epoch 2, step 785, the loss is 1276001.375\n",
            "in training loop, epoch 2, step 786, the loss is 991321.5625\n",
            "in training loop, epoch 2, step 787, the loss is 610542.25\n",
            "in training loop, epoch 2, step 788, the loss is 774678.1875\n",
            "in training loop, epoch 2, step 789, the loss is 752525.375\n",
            "in training loop, epoch 2, step 790, the loss is 1544951.0\n",
            "in training loop, epoch 2, step 791, the loss is 1107091.75\n",
            "in training loop, epoch 2, step 792, the loss is 834452.9375\n",
            "in training loop, epoch 2, step 793, the loss is 1071250.0\n",
            "in training loop, epoch 2, step 794, the loss is 1430873.25\n",
            "in training loop, epoch 2, step 795, the loss is 1840009.5\n",
            "in training loop, epoch 2, step 796, the loss is 931878.625\n",
            "in training loop, epoch 2, step 797, the loss is 1802743.125\n",
            "in training loop, epoch 2, step 798, the loss is 1355897.625\n",
            "in training loop, epoch 2, step 799, the loss is 1571535.75\n",
            "in training loop, epoch 2, step 800, the loss is 930851.0\n",
            "in training loop, epoch 2, step 801, the loss is 1889701.75\n",
            "in training loop, epoch 2, step 802, the loss is 890098.25\n",
            "in training loop, epoch 2, step 803, the loss is 1647875.625\n",
            "in training loop, epoch 2, step 804, the loss is 720810.8125\n",
            "in training loop, epoch 2, step 805, the loss is 974083.5625\n",
            "in training loop, epoch 2, step 806, the loss is 1646595.125\n",
            "in training loop, epoch 2, step 807, the loss is 769015.375\n",
            "in training loop, epoch 2, step 808, the loss is 2370095.0\n",
            "in training loop, epoch 2, step 809, the loss is 890615.25\n",
            "in training loop, epoch 2, step 810, the loss is 1426409.875\n",
            "in training loop, epoch 2, step 811, the loss is 952129.3125\n",
            "in training loop, epoch 2, step 812, the loss is 804427.6875\n",
            "in training loop, epoch 2, step 813, the loss is 1472434.75\n",
            "in training loop, epoch 2, step 814, the loss is 1634133.875\n",
            "in training loop, epoch 2, step 815, the loss is 1772843.25\n",
            "in training loop, epoch 2, step 816, the loss is 583316.8125\n",
            "in training loop, epoch 2, step 817, the loss is 1637888.5\n",
            "in training loop, epoch 2, step 818, the loss is 1561631.875\n",
            "in training loop, epoch 2, step 819, the loss is 685697.25\n",
            "in training loop, epoch 2, step 820, the loss is 951762.6875\n",
            "in training loop, epoch 2, step 821, the loss is 1117735.5\n",
            "in training loop, epoch 2, step 822, the loss is 541648.625\n",
            "in training loop, epoch 2, step 823, the loss is 953193.125\n",
            "in training loop, epoch 2, step 824, the loss is 574592.25\n",
            "in training loop, epoch 2, step 825, the loss is 814640.875\n",
            "in training loop, epoch 2, step 826, the loss is 1793993.875\n",
            "in training loop, epoch 2, step 827, the loss is 964766.8125\n",
            "in training loop, epoch 2, step 828, the loss is 1728782.625\n",
            "in training loop, epoch 2, step 829, the loss is 792035.4375\n",
            "in training loop, epoch 2, step 830, the loss is 1909874.0\n",
            "in training loop, epoch 2, step 831, the loss is 751228.75\n",
            "in training loop, epoch 2, step 832, the loss is 784396.75\n",
            "in training loop, epoch 2, step 833, the loss is 1062358.625\n",
            "in training loop, epoch 2, step 834, the loss is 1122258.75\n",
            "in training loop, epoch 2, step 835, the loss is 1506715.75\n",
            "in training loop, epoch 2, step 836, the loss is 2300653.0\n",
            "in training loop, epoch 2, step 837, the loss is 1600595.0\n",
            "in training loop, epoch 2, step 838, the loss is 1483814.25\n",
            "in training loop, epoch 2, step 839, the loss is 1509484.125\n",
            "in training loop, epoch 2, step 840, the loss is 1079730.25\n",
            "in training loop, epoch 2, step 841, the loss is 891686.75\n",
            "in training loop, epoch 2, step 842, the loss is 1830438.75\n",
            "in training loop, epoch 2, step 843, the loss is 1103018.75\n",
            "in training loop, epoch 2, step 844, the loss is 901435.875\n",
            "in training loop, epoch 2, step 845, the loss is 1525279.0\n",
            "in training loop, epoch 2, step 846, the loss is 1658853.75\n",
            "in training loop, epoch 2, step 847, the loss is 826936.8125\n",
            "in training loop, epoch 2, step 848, the loss is 702601.125\n",
            "in training loop, epoch 2, step 849, the loss is 930987.875\n",
            "in training loop, epoch 2, step 850, the loss is 1368543.625\n",
            "in training loop, epoch 2, step 851, the loss is 1030288.0\n",
            "in training loop, epoch 2, step 852, the loss is 829933.25\n",
            "in training loop, epoch 2, step 853, the loss is 1831929.25\n",
            "in training loop, epoch 2, step 854, the loss is 802323.5\n",
            "in training loop, epoch 2, step 855, the loss is 1003907.3125\n",
            "in training loop, epoch 2, step 856, the loss is 662670.875\n",
            "in training loop, epoch 2, step 857, the loss is 786054.625\n",
            "in training loop, epoch 2, step 858, the loss is 1140545.75\n",
            "in training loop, epoch 2, step 859, the loss is 1330718.25\n",
            "in training loop, epoch 2, step 860, the loss is 1575802.0\n",
            "in training loop, epoch 2, step 861, the loss is 1323604.75\n",
            "in training loop, epoch 2, step 862, the loss is 1494649.0\n",
            "in training loop, epoch 2, step 863, the loss is 833693.875\n",
            "in training loop, epoch 2, step 864, the loss is 1208369.75\n",
            "in training loop, epoch 2, step 865, the loss is 1414202.875\n",
            "in training loop, epoch 2, step 866, the loss is 617295.5\n",
            "in training loop, epoch 2, step 867, the loss is 1113071.5\n",
            "in training loop, epoch 2, step 868, the loss is 1060980.25\n",
            "in training loop, epoch 2, step 869, the loss is 1223052.125\n",
            "in training loop, epoch 2, step 870, the loss is 724994.875\n",
            "in training loop, epoch 2, step 871, the loss is 1589181.25\n",
            "in training loop, epoch 2, step 872, the loss is 1431638.625\n",
            "in training loop, epoch 2, step 873, the loss is 894490.875\n",
            "in training loop, epoch 2, step 874, the loss is 1214453.375\n",
            "in training loop, epoch 2, step 875, the loss is 1380901.125\n",
            "in training loop, epoch 2, step 876, the loss is 1373379.5\n",
            "in training loop, epoch 2, step 877, the loss is 1996757.5\n",
            "in training loop, epoch 2, step 878, the loss is 619855.375\n",
            "in training loop, epoch 2, step 879, the loss is 1061329.875\n",
            "in training loop, epoch 2, step 880, the loss is 1578336.5\n",
            "in training loop, epoch 2, step 881, the loss is 806507.8125\n",
            "in training loop, epoch 2, step 882, the loss is 1646558.75\n",
            "in training loop, epoch 2, step 883, the loss is 1038395.9375\n",
            "in training loop, epoch 2, step 884, the loss is 1011660.5\n",
            "in training loop, epoch 2, step 885, the loss is 1302086.5\n",
            "in training loop, epoch 2, step 886, the loss is 1407956.0\n",
            "in training loop, epoch 2, step 887, the loss is 1002387.625\n",
            "in training loop, epoch 2, step 888, the loss is 743412.6875\n",
            "in training loop, epoch 2, step 889, the loss is 1421219.0\n",
            "in training loop, epoch 2, step 890, the loss is 694785.1875\n",
            "in training loop, epoch 2, step 891, the loss is 1463712.75\n",
            "in training loop, epoch 2, step 892, the loss is 1238915.375\n",
            "in training loop, epoch 2, step 893, the loss is 1704722.625\n",
            "in training loop, epoch 2, step 894, the loss is 1358109.75\n",
            "in training loop, epoch 2, step 895, the loss is 1327341.875\n",
            "in training loop, epoch 2, step 896, the loss is 2018831.125\n",
            "in training loop, epoch 2, step 897, the loss is 992692.375\n",
            "in training loop, epoch 2, step 898, the loss is 1639277.0\n",
            "in training loop, epoch 2, step 899, the loss is 723799.75\n",
            "in training loop, epoch 2, step 900, the loss is 1097305.0\n",
            "in training loop, epoch 2, step 901, the loss is 659594.3125\n",
            "in training loop, epoch 2, step 902, the loss is 884391.875\n",
            "in training loop, epoch 2, step 903, the loss is 1116609.125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "k-fold 0:: Epoch 2: train loss 1343657.3213495575 val loss 1148390.6376856435\n",
            "in training loop, epoch 3, step 0, the loss is 761402.3125\n",
            "in training loop, epoch 3, step 1, the loss is 1028230.0\n",
            "in training loop, epoch 3, step 2, the loss is 651300.875\n",
            "in training loop, epoch 3, step 3, the loss is 915390.875\n",
            "in training loop, epoch 3, step 4, the loss is 581243.4375\n",
            "in training loop, epoch 3, step 5, the loss is 1540089.875\n",
            "in training loop, epoch 3, step 6, the loss is 927899.0\n",
            "in training loop, epoch 3, step 7, the loss is 2023275.75\n",
            "in training loop, epoch 3, step 8, the loss is 766954.0625\n",
            "in training loop, epoch 3, step 9, the loss is 1476865.125\n",
            "in training loop, epoch 3, step 10, the loss is 1467023.125\n",
            "in training loop, epoch 3, step 11, the loss is 1707694.25\n",
            "in training loop, epoch 3, step 12, the loss is 1130393.75\n",
            "in training loop, epoch 3, step 13, the loss is 850623.375\n",
            "in training loop, epoch 3, step 14, the loss is 1116202.25\n",
            "in training loop, epoch 3, step 15, the loss is 953615.1875\n",
            "in training loop, epoch 3, step 16, the loss is 1425095.875\n",
            "in training loop, epoch 3, step 17, the loss is 1396318.0\n",
            "in training loop, epoch 3, step 18, the loss is 1047747.25\n",
            "in training loop, epoch 3, step 19, the loss is 1160658.375\n",
            "in training loop, epoch 3, step 20, the loss is 610095.875\n",
            "in training loop, epoch 3, step 21, the loss is 697477.8125\n",
            "in training loop, epoch 3, step 22, the loss is 680181.8125\n",
            "in training loop, epoch 3, step 23, the loss is 1534381.0\n",
            "in training loop, epoch 3, step 24, the loss is 862938.1875\n",
            "in training loop, epoch 3, step 25, the loss is 725261.8125\n",
            "in training loop, epoch 3, step 26, the loss is 972095.125\n",
            "in training loop, epoch 3, step 27, the loss is 582242.9375\n",
            "in training loop, epoch 3, step 28, the loss is 1412312.75\n",
            "in training loop, epoch 3, step 29, the loss is 1165831.0\n",
            "in training loop, epoch 3, step 30, the loss is 941950.6875\n",
            "in training loop, epoch 3, step 31, the loss is 1054195.625\n",
            "in training loop, epoch 3, step 32, the loss is 472697.125\n",
            "in training loop, epoch 3, step 33, the loss is 752516.4375\n",
            "in training loop, epoch 3, step 34, the loss is 1302222.875\n",
            "in training loop, epoch 3, step 35, the loss is 2039206.75\n",
            "in training loop, epoch 3, step 36, the loss is 1186945.75\n",
            "in training loop, epoch 3, step 37, the loss is 930026.25\n",
            "in training loop, epoch 3, step 38, the loss is 914890.3125\n",
            "in training loop, epoch 3, step 39, the loss is 1089672.375\n",
            "in training loop, epoch 3, step 40, the loss is 1250616.875\n",
            "in training loop, epoch 3, step 41, the loss is 1015659.375\n",
            "in training loop, epoch 3, step 42, the loss is 1232532.375\n",
            "in training loop, epoch 3, step 43, the loss is 1593764.0\n",
            "in training loop, epoch 3, step 44, the loss is 3227686.25\n",
            "in training loop, epoch 3, step 45, the loss is 1811717.75\n",
            "in training loop, epoch 3, step 46, the loss is 1361761.75\n",
            "in training loop, epoch 3, step 47, the loss is 1445862.625\n",
            "in training loop, epoch 3, step 48, the loss is 947776.0\n",
            "in training loop, epoch 3, step 49, the loss is 1056598.0\n",
            "in training loop, epoch 3, step 50, the loss is 1421140.75\n",
            "in training loop, epoch 3, step 51, the loss is 1152444.0\n",
            "in training loop, epoch 3, step 52, the loss is 1246718.5\n",
            "in training loop, epoch 3, step 53, the loss is 624870.0625\n",
            "in training loop, epoch 3, step 54, the loss is 655918.0\n",
            "in training loop, epoch 3, step 55, the loss is 1144041.25\n",
            "in training loop, epoch 3, step 56, the loss is 813090.3125\n",
            "in training loop, epoch 3, step 57, the loss is 730161.875\n",
            "in training loop, epoch 3, step 58, the loss is 499846.75\n",
            "in training loop, epoch 3, step 59, the loss is 622888.5\n",
            "in training loop, epoch 3, step 60, the loss is 2346652.0\n",
            "in training loop, epoch 3, step 61, the loss is 1390029.5\n",
            "in training loop, epoch 3, step 62, the loss is 1604235.625\n",
            "in training loop, epoch 3, step 63, the loss is 1311589.0\n",
            "in training loop, epoch 3, step 64, the loss is 638082.0625\n",
            "in training loop, epoch 3, step 65, the loss is 743687.375\n",
            "in training loop, epoch 3, step 66, the loss is 1809141.625\n",
            "in training loop, epoch 3, step 67, the loss is 1343253.625\n",
            "in training loop, epoch 3, step 68, the loss is 1836259.625\n",
            "in training loop, epoch 3, step 69, the loss is 1396504.125\n",
            "in training loop, epoch 3, step 70, the loss is 2260996.0\n",
            "in training loop, epoch 3, step 71, the loss is 933167.25\n",
            "in training loop, epoch 3, step 72, the loss is 779596.375\n",
            "in training loop, epoch 3, step 73, the loss is 1364663.125\n",
            "in training loop, epoch 3, step 74, the loss is 1221355.0\n",
            "in training loop, epoch 3, step 75, the loss is 2859232.0\n",
            "in training loop, epoch 3, step 76, the loss is 1989061.25\n",
            "in training loop, epoch 3, step 77, the loss is 1295266.125\n",
            "in training loop, epoch 3, step 78, the loss is 1570857.375\n",
            "in training loop, epoch 3, step 79, the loss is 1214288.875\n",
            "in training loop, epoch 3, step 80, the loss is 856178.5625\n",
            "in training loop, epoch 3, step 81, the loss is 1065949.25\n",
            "in training loop, epoch 3, step 82, the loss is 1233113.75\n",
            "in training loop, epoch 3, step 83, the loss is 1196244.625\n",
            "in training loop, epoch 3, step 84, the loss is 1116321.25\n",
            "in training loop, epoch 3, step 85, the loss is 771630.0\n",
            "in training loop, epoch 3, step 86, the loss is 1412213.375\n",
            "in training loop, epoch 3, step 87, the loss is 681232.125\n",
            "in training loop, epoch 3, step 88, the loss is 1426644.75\n",
            "in training loop, epoch 3, step 89, the loss is 1442938.875\n",
            "in training loop, epoch 3, step 90, the loss is 848823.625\n",
            "in training loop, epoch 3, step 91, the loss is 1014779.25\n",
            "in training loop, epoch 3, step 92, the loss is 1327075.625\n",
            "in training loop, epoch 3, step 93, the loss is 3053701.25\n",
            "in training loop, epoch 3, step 94, the loss is 1028396.5\n",
            "in training loop, epoch 3, step 95, the loss is 1430821.125\n",
            "in training loop, epoch 3, step 96, the loss is 1330163.125\n",
            "in training loop, epoch 3, step 97, the loss is 1035201.5\n",
            "in training loop, epoch 3, step 98, the loss is 1409498.375\n",
            "in training loop, epoch 3, step 99, the loss is 1288751.875\n",
            "in training loop, epoch 3, step 100, the loss is 1788875.625\n",
            "in training loop, epoch 3, step 101, the loss is 2156755.5\n",
            "in training loop, epoch 3, step 102, the loss is 1232836.875\n",
            "in training loop, epoch 3, step 103, the loss is 1339870.0\n",
            "in training loop, epoch 3, step 104, the loss is 890486.25\n",
            "in training loop, epoch 3, step 105, the loss is 1652726.0\n",
            "in training loop, epoch 3, step 106, the loss is 873532.375\n",
            "in training loop, epoch 3, step 107, the loss is 1478600.625\n",
            "in training loop, epoch 3, step 108, the loss is 1002841.125\n",
            "in training loop, epoch 3, step 109, the loss is 1037480.0625\n",
            "in training loop, epoch 3, step 110, the loss is 1955645.25\n",
            "in training loop, epoch 3, step 111, the loss is 1454003.875\n",
            "in training loop, epoch 3, step 112, the loss is 1014684.0625\n",
            "in training loop, epoch 3, step 113, the loss is 1052212.0\n",
            "in training loop, epoch 3, step 114, the loss is 2050177.25\n",
            "in training loop, epoch 3, step 115, the loss is 1031423.375\n",
            "in training loop, epoch 3, step 116, the loss is 768619.625\n",
            "in training loop, epoch 3, step 117, the loss is 1230510.25\n",
            "in training loop, epoch 3, step 118, the loss is 640065.5\n",
            "in training loop, epoch 3, step 119, the loss is 2565640.5\n",
            "in training loop, epoch 3, step 120, the loss is 2237621.0\n",
            "in training loop, epoch 3, step 121, the loss is 337441.09375\n",
            "in training loop, epoch 3, step 122, the loss is 831040.8125\n",
            "in training loop, epoch 3, step 123, the loss is 961750.4375\n",
            "in training loop, epoch 3, step 124, the loss is 1361258.375\n",
            "in training loop, epoch 3, step 125, the loss is 633010.3125\n",
            "in training loop, epoch 3, step 126, the loss is 949826.9375\n",
            "in training loop, epoch 3, step 127, the loss is 792392.3125\n",
            "in training loop, epoch 3, step 128, the loss is 1460552.5\n",
            "in training loop, epoch 3, step 129, the loss is 1008169.125\n",
            "in training loop, epoch 3, step 130, the loss is 1004271.0\n",
            "in training loop, epoch 3, step 131, the loss is 1197610.125\n",
            "in training loop, epoch 3, step 132, the loss is 1439690.5\n",
            "in training loop, epoch 3, step 133, the loss is 637186.3125\n",
            "in training loop, epoch 3, step 134, the loss is 333102.78125\n",
            "in training loop, epoch 3, step 135, the loss is 456571.6875\n",
            "in training loop, epoch 3, step 136, the loss is 3069034.75\n",
            "in training loop, epoch 3, step 137, the loss is 1227174.75\n",
            "in training loop, epoch 3, step 138, the loss is 1060589.25\n",
            "in training loop, epoch 3, step 139, the loss is 571946.5\n",
            "in training loop, epoch 3, step 140, the loss is 1330699.875\n",
            "in training loop, epoch 3, step 141, the loss is 688735.0625\n",
            "in training loop, epoch 3, step 142, the loss is 1215610.75\n",
            "in training loop, epoch 3, step 143, the loss is 785760.3125\n",
            "in training loop, epoch 3, step 144, the loss is 1778514.375\n",
            "in training loop, epoch 3, step 145, the loss is 773029.5\n",
            "in training loop, epoch 3, step 146, the loss is 970797.5625\n",
            "in training loop, epoch 3, step 147, the loss is 436779.8125\n",
            "in training loop, epoch 3, step 148, the loss is 1969237.875\n",
            "in training loop, epoch 3, step 149, the loss is 661577.625\n",
            "in training loop, epoch 3, step 150, the loss is 1334008.75\n",
            "in training loop, epoch 3, step 151, the loss is 974962.9375\n",
            "in training loop, epoch 3, step 152, the loss is 779396.1875\n",
            "in training loop, epoch 3, step 153, the loss is 738392.75\n",
            "in training loop, epoch 3, step 154, the loss is 1223065.375\n",
            "in training loop, epoch 3, step 155, the loss is 280596.1875\n",
            "in training loop, epoch 3, step 156, the loss is 1032023.75\n",
            "in training loop, epoch 3, step 157, the loss is 548568.6875\n",
            "in training loop, epoch 3, step 158, the loss is 659464.1875\n",
            "in training loop, epoch 3, step 159, the loss is 1069940.5\n",
            "in training loop, epoch 3, step 160, the loss is 750574.1875\n",
            "in training loop, epoch 3, step 161, the loss is 1529983.875\n",
            "in training loop, epoch 3, step 162, the loss is 847816.75\n",
            "in training loop, epoch 3, step 163, the loss is 971139.5625\n",
            "in training loop, epoch 3, step 164, the loss is 1303455.875\n",
            "in training loop, epoch 3, step 165, the loss is 1161861.5\n",
            "in training loop, epoch 3, step 166, the loss is 719316.0\n",
            "in training loop, epoch 3, step 167, the loss is 1303175.375\n",
            "in training loop, epoch 3, step 168, the loss is 694324.0625\n",
            "in training loop, epoch 3, step 169, the loss is 729656.75\n",
            "in training loop, epoch 3, step 170, the loss is 775208.5625\n",
            "in training loop, epoch 3, step 171, the loss is 1201988.5\n",
            "in training loop, epoch 3, step 172, the loss is 794615.75\n",
            "in training loop, epoch 3, step 173, the loss is 948141.75\n",
            "in training loop, epoch 3, step 174, the loss is 719488.375\n",
            "in training loop, epoch 3, step 175, the loss is 983714.375\n",
            "in training loop, epoch 3, step 176, the loss is 1678256.75\n",
            "in training loop, epoch 3, step 177, the loss is 551326.75\n",
            "in training loop, epoch 3, step 178, the loss is 688581.9375\n",
            "in training loop, epoch 3, step 179, the loss is 1080769.625\n",
            "in training loop, epoch 3, step 180, the loss is 1117198.5\n",
            "in training loop, epoch 3, step 181, the loss is 1446199.875\n",
            "in training loop, epoch 3, step 182, the loss is 1552648.0\n",
            "in training loop, epoch 3, step 183, the loss is 920756.875\n",
            "in training loop, epoch 3, step 184, the loss is 431288.71875\n",
            "in training loop, epoch 3, step 185, the loss is 1007321.625\n",
            "in training loop, epoch 3, step 186, the loss is 2146677.0\n",
            "in training loop, epoch 3, step 187, the loss is 1052284.875\n",
            "in training loop, epoch 3, step 188, the loss is 1115898.375\n",
            "in training loop, epoch 3, step 189, the loss is 1031575.8125\n",
            "in training loop, epoch 3, step 190, the loss is 736949.3125\n",
            "in training loop, epoch 3, step 191, the loss is 1213994.0\n",
            "in training loop, epoch 3, step 192, the loss is 1539078.625\n",
            "in training loop, epoch 3, step 193, the loss is 1294998.0\n",
            "in training loop, epoch 3, step 194, the loss is 990729.6875\n",
            "in training loop, epoch 3, step 195, the loss is 1172815.5\n",
            "in training loop, epoch 3, step 196, the loss is 682683.125\n",
            "in training loop, epoch 3, step 197, the loss is 1536416.25\n",
            "in training loop, epoch 3, step 198, the loss is 1141958.75\n",
            "in training loop, epoch 3, step 199, the loss is 1016904.5\n",
            "in training loop, epoch 3, step 200, the loss is 1215883.25\n",
            "in training loop, epoch 3, step 201, the loss is 399100.03125\n",
            "in training loop, epoch 3, step 202, the loss is 1651069.875\n",
            "in training loop, epoch 3, step 203, the loss is 1070102.875\n",
            "in training loop, epoch 3, step 204, the loss is 883165.5\n",
            "in training loop, epoch 3, step 205, the loss is 1324181.75\n",
            "in training loop, epoch 3, step 206, the loss is 1673066.625\n",
            "in training loop, epoch 3, step 207, the loss is 869382.5625\n",
            "in training loop, epoch 3, step 208, the loss is 821983.1875\n",
            "in training loop, epoch 3, step 209, the loss is 1161801.5\n",
            "in training loop, epoch 3, step 210, the loss is 1359548.625\n",
            "in training loop, epoch 3, step 211, the loss is 1243387.375\n",
            "in training loop, epoch 3, step 212, the loss is 1262195.375\n",
            "in training loop, epoch 3, step 213, the loss is 1061030.125\n",
            "in training loop, epoch 3, step 214, the loss is 1623682.5\n",
            "in training loop, epoch 3, step 215, the loss is 1028379.4375\n",
            "in training loop, epoch 3, step 216, the loss is 1652837.5\n",
            "in training loop, epoch 3, step 217, the loss is 939128.0\n",
            "in training loop, epoch 3, step 218, the loss is 934794.5\n",
            "in training loop, epoch 3, step 219, the loss is 1200233.875\n",
            "in training loop, epoch 3, step 220, the loss is 1648561.5\n",
            "in training loop, epoch 3, step 221, the loss is 942178.6875\n",
            "in training loop, epoch 3, step 222, the loss is 1804448.875\n",
            "in training loop, epoch 3, step 223, the loss is 1248657.25\n",
            "in training loop, epoch 3, step 224, the loss is 1061879.375\n",
            "in training loop, epoch 3, step 225, the loss is 906535.0\n",
            "in training loop, epoch 3, step 226, the loss is 1585590.5\n",
            "in training loop, epoch 3, step 227, the loss is 1150719.0\n",
            "in training loop, epoch 3, step 228, the loss is 2201105.75\n",
            "in training loop, epoch 3, step 229, the loss is 958903.8125\n",
            "in training loop, epoch 3, step 230, the loss is 1497574.25\n",
            "in training loop, epoch 3, step 231, the loss is 1217562.875\n",
            "in training loop, epoch 3, step 232, the loss is 1221986.0\n",
            "in training loop, epoch 3, step 233, the loss is 493554.78125\n",
            "in training loop, epoch 3, step 234, the loss is 1437567.0\n",
            "in training loop, epoch 3, step 235, the loss is 791257.0625\n",
            "in training loop, epoch 3, step 236, the loss is 1263432.25\n",
            "in training loop, epoch 3, step 237, the loss is 661034.75\n",
            "in training loop, epoch 3, step 238, the loss is 944684.875\n",
            "in training loop, epoch 3, step 239, the loss is 386961.0625\n",
            "in training loop, epoch 3, step 240, the loss is 607060.3125\n",
            "in training loop, epoch 3, step 241, the loss is 1836106.75\n",
            "in training loop, epoch 3, step 242, the loss is 640869.375\n",
            "in training loop, epoch 3, step 243, the loss is 1676659.125\n",
            "in training loop, epoch 3, step 244, the loss is 843926.75\n",
            "in training loop, epoch 3, step 245, the loss is 694099.6875\n",
            "in training loop, epoch 3, step 246, the loss is 654402.6875\n",
            "in training loop, epoch 3, step 247, the loss is 377206.8125\n",
            "in training loop, epoch 3, step 248, the loss is 766848.5625\n",
            "in training loop, epoch 3, step 249, the loss is 1667287.125\n",
            "in training loop, epoch 3, step 250, the loss is 1436729.5\n",
            "in training loop, epoch 3, step 251, the loss is 820873.5625\n",
            "in training loop, epoch 3, step 252, the loss is 1419345.875\n",
            "in training loop, epoch 3, step 253, the loss is 1295067.625\n",
            "in training loop, epoch 3, step 254, the loss is 1149404.5\n",
            "in training loop, epoch 3, step 255, the loss is 681385.5\n",
            "in training loop, epoch 3, step 256, the loss is 613465.5\n",
            "in training loop, epoch 3, step 257, the loss is 874475.4375\n",
            "in training loop, epoch 3, step 258, the loss is 1006950.125\n",
            "in training loop, epoch 3, step 259, the loss is 906355.875\n",
            "in training loop, epoch 3, step 260, the loss is 861360.8125\n",
            "in training loop, epoch 3, step 261, the loss is 1108502.875\n",
            "in training loop, epoch 3, step 262, the loss is 1195036.875\n",
            "in training loop, epoch 3, step 263, the loss is 2611195.5\n",
            "in training loop, epoch 3, step 264, the loss is 878584.125\n",
            "in training loop, epoch 3, step 265, the loss is 1537461.0\n",
            "in training loop, epoch 3, step 266, the loss is 947340.0\n",
            "in training loop, epoch 3, step 267, the loss is 875211.25\n",
            "in training loop, epoch 3, step 268, the loss is 1047005.9375\n",
            "in training loop, epoch 3, step 269, the loss is 1188345.0\n",
            "in training loop, epoch 3, step 270, the loss is 1834558.25\n",
            "in training loop, epoch 3, step 271, the loss is 1051856.125\n",
            "in training loop, epoch 3, step 272, the loss is 948171.0\n",
            "in training loop, epoch 3, step 273, the loss is 824861.625\n",
            "in training loop, epoch 3, step 274, the loss is 941653.1875\n",
            "in training loop, epoch 3, step 275, the loss is 689406.0\n",
            "in training loop, epoch 3, step 276, the loss is 822990.3125\n",
            "in training loop, epoch 3, step 277, the loss is 1130357.625\n",
            "in training loop, epoch 3, step 278, the loss is 1839980.75\n",
            "in training loop, epoch 3, step 279, the loss is 1042928.25\n",
            "in training loop, epoch 3, step 280, the loss is 891224.625\n",
            "in training loop, epoch 3, step 281, the loss is 461754.875\n",
            "in training loop, epoch 3, step 282, the loss is 2527903.75\n",
            "in training loop, epoch 3, step 283, the loss is 1149513.625\n",
            "in training loop, epoch 3, step 284, the loss is 844951.75\n",
            "in training loop, epoch 3, step 285, the loss is 998246.0625\n",
            "in training loop, epoch 3, step 286, the loss is 760634.625\n",
            "in training loop, epoch 3, step 287, the loss is 574841.125\n",
            "in training loop, epoch 3, step 288, the loss is 1399102.25\n",
            "in training loop, epoch 3, step 289, the loss is 2137876.0\n",
            "in training loop, epoch 3, step 290, the loss is 1018198.4375\n",
            "in training loop, epoch 3, step 291, the loss is 1243890.5\n",
            "in training loop, epoch 3, step 292, the loss is 710947.625\n",
            "in training loop, epoch 3, step 293, the loss is 1760906.75\n",
            "in training loop, epoch 3, step 294, the loss is 1629207.125\n",
            "in training loop, epoch 3, step 295, the loss is 540639.5625\n",
            "in training loop, epoch 3, step 296, the loss is 873651.25\n",
            "in training loop, epoch 3, step 297, the loss is 617971.1875\n",
            "in training loop, epoch 3, step 298, the loss is 2017437.25\n",
            "in training loop, epoch 3, step 299, the loss is 643607.4375\n",
            "in training loop, epoch 3, step 300, the loss is 768082.6875\n",
            "in training loop, epoch 3, step 301, the loss is 995010.8125\n",
            "in training loop, epoch 3, step 302, the loss is 982521.125\n",
            "in training loop, epoch 3, step 303, the loss is 1003954.0\n",
            "in training loop, epoch 3, step 304, the loss is 763746.6875\n",
            "in training loop, epoch 3, step 305, the loss is 1090497.0\n",
            "in training loop, epoch 3, step 306, the loss is 2712172.5\n",
            "in training loop, epoch 3, step 307, the loss is 2279550.5\n",
            "in training loop, epoch 3, step 308, the loss is 1068601.125\n",
            "in training loop, epoch 3, step 309, the loss is 1745686.5\n",
            "in training loop, epoch 3, step 310, the loss is 1228436.375\n",
            "in training loop, epoch 3, step 311, the loss is 705333.75\n",
            "in training loop, epoch 3, step 312, the loss is 1181109.25\n",
            "in training loop, epoch 3, step 313, the loss is 1254184.25\n",
            "in training loop, epoch 3, step 314, the loss is 1838631.0\n",
            "in training loop, epoch 3, step 315, the loss is 1609665.25\n",
            "in training loop, epoch 3, step 316, the loss is 1152854.5\n",
            "in training loop, epoch 3, step 317, the loss is 1658005.25\n",
            "in training loop, epoch 3, step 318, the loss is 1184146.0\n",
            "in training loop, epoch 3, step 319, the loss is 1284711.25\n",
            "in training loop, epoch 3, step 320, the loss is 1319961.625\n",
            "in training loop, epoch 3, step 321, the loss is 1442656.125\n",
            "in training loop, epoch 3, step 322, the loss is 646114.75\n",
            "in training loop, epoch 3, step 323, the loss is 1223296.75\n",
            "in training loop, epoch 3, step 324, the loss is 1081501.75\n",
            "in training loop, epoch 3, step 325, the loss is 929269.0\n",
            "in training loop, epoch 3, step 326, the loss is 970415.3125\n",
            "in training loop, epoch 3, step 327, the loss is 913684.3125\n",
            "in training loop, epoch 3, step 328, the loss is 675762.9375\n",
            "in training loop, epoch 3, step 329, the loss is 997882.9375\n",
            "in training loop, epoch 3, step 330, the loss is 1381618.875\n",
            "in training loop, epoch 3, step 331, the loss is 1257191.875\n",
            "in training loop, epoch 3, step 332, the loss is 788700.625\n",
            "in training loop, epoch 3, step 333, the loss is 1192327.75\n",
            "in training loop, epoch 3, step 334, the loss is 1020061.5\n",
            "in training loop, epoch 3, step 335, the loss is 1733604.125\n",
            "in training loop, epoch 3, step 336, the loss is 959231.375\n",
            "in training loop, epoch 3, step 337, the loss is 2001125.875\n",
            "in training loop, epoch 3, step 338, the loss is 595920.625\n",
            "in training loop, epoch 3, step 339, the loss is 1099588.125\n",
            "in training loop, epoch 3, step 340, the loss is 1239398.125\n",
            "in training loop, epoch 3, step 341, the loss is 1256586.75\n",
            "in training loop, epoch 3, step 342, the loss is 794618.75\n",
            "in training loop, epoch 3, step 343, the loss is 1584492.875\n",
            "in training loop, epoch 3, step 344, the loss is 2046098.75\n",
            "in training loop, epoch 3, step 345, the loss is 2469121.0\n",
            "in training loop, epoch 3, step 346, the loss is 676891.125\n",
            "in training loop, epoch 3, step 347, the loss is 988352.0\n",
            "in training loop, epoch 3, step 348, the loss is 1297272.25\n",
            "in training loop, epoch 3, step 349, the loss is 906505.8125\n",
            "in training loop, epoch 3, step 350, the loss is 1047418.1875\n",
            "in training loop, epoch 3, step 351, the loss is 818975.625\n",
            "in training loop, epoch 3, step 352, the loss is 1351092.75\n",
            "in training loop, epoch 3, step 353, the loss is 1434088.0\n",
            "in training loop, epoch 3, step 354, the loss is 570001.0625\n",
            "in training loop, epoch 3, step 355, the loss is 1084815.25\n",
            "in training loop, epoch 3, step 356, the loss is 740572.25\n",
            "in training loop, epoch 3, step 357, the loss is 1341796.5\n",
            "in training loop, epoch 3, step 358, the loss is 683284.125\n",
            "in training loop, epoch 3, step 359, the loss is 433057.25\n",
            "in training loop, epoch 3, step 360, the loss is 965965.375\n",
            "in training loop, epoch 3, step 361, the loss is 681566.625\n",
            "in training loop, epoch 3, step 362, the loss is 741339.9375\n",
            "in training loop, epoch 3, step 363, the loss is 974875.375\n",
            "in training loop, epoch 3, step 364, the loss is 1373586.25\n",
            "in training loop, epoch 3, step 365, the loss is 1553412.75\n",
            "in training loop, epoch 3, step 366, the loss is 758492.75\n",
            "in training loop, epoch 3, step 367, the loss is 1983376.375\n",
            "in training loop, epoch 3, step 368, the loss is 512152.8125\n",
            "in training loop, epoch 3, step 369, the loss is 1178354.875\n",
            "in training loop, epoch 3, step 370, the loss is 669627.8125\n",
            "in training loop, epoch 3, step 371, the loss is 894499.25\n",
            "in training loop, epoch 3, step 372, the loss is 813441.625\n",
            "in training loop, epoch 3, step 373, the loss is 681306.875\n",
            "in training loop, epoch 3, step 374, the loss is 792211.25\n",
            "in training loop, epoch 3, step 375, the loss is 1027837.75\n",
            "in training loop, epoch 3, step 376, the loss is 967858.4375\n",
            "in training loop, epoch 3, step 377, the loss is 1676823.625\n",
            "in training loop, epoch 3, step 378, the loss is 1573192.0\n",
            "in training loop, epoch 3, step 379, the loss is 893705.75\n",
            "in training loop, epoch 3, step 380, the loss is 1789234.75\n",
            "in training loop, epoch 3, step 381, the loss is 1744856.5\n",
            "in training loop, epoch 3, step 382, the loss is 975805.875\n",
            "in training loop, epoch 3, step 383, the loss is 1196232.625\n",
            "in training loop, epoch 3, step 384, the loss is 1162247.25\n",
            "in training loop, epoch 3, step 385, the loss is 964190.875\n",
            "in training loop, epoch 3, step 386, the loss is 823493.75\n",
            "in training loop, epoch 3, step 387, the loss is 1292105.25\n",
            "in training loop, epoch 3, step 388, the loss is 1467710.0\n",
            "in training loop, epoch 3, step 389, the loss is 986951.0\n",
            "in training loop, epoch 3, step 390, the loss is 1762753.125\n",
            "in training loop, epoch 3, step 391, the loss is 1039436.125\n",
            "in training loop, epoch 3, step 392, the loss is 1857843.25\n",
            "in training loop, epoch 3, step 393, the loss is 657844.0\n",
            "in training loop, epoch 3, step 394, the loss is 1273318.875\n",
            "in training loop, epoch 3, step 395, the loss is 924741.3125\n",
            "in training loop, epoch 3, step 396, the loss is 1081546.875\n",
            "in training loop, epoch 3, step 397, the loss is 1175337.75\n",
            "in training loop, epoch 3, step 398, the loss is 1229635.25\n",
            "in training loop, epoch 3, step 399, the loss is 1896570.125\n",
            "in training loop, epoch 3, step 400, the loss is 2593123.0\n",
            "in training loop, epoch 3, step 401, the loss is 1937040.375\n",
            "in training loop, epoch 3, step 402, the loss is 1205552.875\n",
            "in training loop, epoch 3, step 403, the loss is 1100786.25\n",
            "in training loop, epoch 3, step 404, the loss is 2001680.75\n",
            "in training loop, epoch 3, step 405, the loss is 1779172.0\n",
            "in training loop, epoch 3, step 406, the loss is 490432.15625\n",
            "in training loop, epoch 3, step 407, the loss is 730662.4375\n",
            "in training loop, epoch 3, step 408, the loss is 2083582.25\n",
            "in training loop, epoch 3, step 409, the loss is 1665633.25\n",
            "in training loop, epoch 3, step 410, the loss is 763474.125\n",
            "in training loop, epoch 3, step 411, the loss is 772150.8125\n",
            "in training loop, epoch 3, step 412, the loss is 1421909.5\n",
            "in training loop, epoch 3, step 413, the loss is 2264395.25\n",
            "in training loop, epoch 3, step 414, the loss is 1595550.5\n",
            "in training loop, epoch 3, step 415, the loss is 593162.375\n",
            "in training loop, epoch 3, step 416, the loss is 1715776.5\n",
            "in training loop, epoch 3, step 417, the loss is 1430162.625\n",
            "in training loop, epoch 3, step 418, the loss is 1657575.5\n",
            "in training loop, epoch 3, step 419, the loss is 1343670.875\n",
            "in training loop, epoch 3, step 420, the loss is 707514.5\n",
            "in training loop, epoch 3, step 421, the loss is 1180026.25\n",
            "in training loop, epoch 3, step 422, the loss is 1144848.75\n",
            "in training loop, epoch 3, step 423, the loss is 1092200.125\n",
            "in training loop, epoch 3, step 424, the loss is 916032.0\n",
            "in training loop, epoch 3, step 425, the loss is 1055517.75\n",
            "in training loop, epoch 3, step 426, the loss is 666031.9375\n",
            "in training loop, epoch 3, step 427, the loss is 981460.0\n",
            "in training loop, epoch 3, step 428, the loss is 1291926.25\n",
            "in training loop, epoch 3, step 429, the loss is 1377081.375\n",
            "in training loop, epoch 3, step 430, the loss is 1401580.25\n",
            "in training loop, epoch 3, step 431, the loss is 1067400.875\n",
            "in training loop, epoch 3, step 432, the loss is 1000091.0\n",
            "in training loop, epoch 3, step 433, the loss is 1002766.3125\n",
            "in training loop, epoch 3, step 434, the loss is 726247.875\n",
            "in training loop, epoch 3, step 435, the loss is 1154991.625\n",
            "in training loop, epoch 3, step 436, the loss is 879147.25\n",
            "in training loop, epoch 3, step 437, the loss is 1018787.125\n",
            "in training loop, epoch 3, step 438, the loss is 654090.6875\n",
            "in training loop, epoch 3, step 439, the loss is 1128605.625\n",
            "in training loop, epoch 3, step 440, the loss is 1614211.75\n",
            "in training loop, epoch 3, step 441, the loss is 499577.65625\n",
            "in training loop, epoch 3, step 442, the loss is 3945036.25\n",
            "in training loop, epoch 3, step 443, the loss is 1373907.375\n",
            "in training loop, epoch 3, step 444, the loss is 932157.5\n",
            "in training loop, epoch 3, step 445, the loss is 919406.25\n",
            "in training loop, epoch 3, step 446, the loss is 755674.0\n",
            "in training loop, epoch 3, step 447, the loss is 890651.5625\n",
            "in training loop, epoch 3, step 448, the loss is 308103.8125\n",
            "in training loop, epoch 3, step 449, the loss is 1265065.375\n",
            "in training loop, epoch 3, step 450, the loss is 1085552.0\n",
            "in training loop, epoch 3, step 451, the loss is 988005.25\n",
            "in training loop, epoch 3, step 452, the loss is 786224.625\n",
            "in training loop, epoch 3, step 453, the loss is 378938.53125\n",
            "in training loop, epoch 3, step 454, the loss is 605332.625\n",
            "in training loop, epoch 3, step 455, the loss is 733980.875\n",
            "in training loop, epoch 3, step 456, the loss is 1770357.25\n",
            "in training loop, epoch 3, step 457, the loss is 1179151.0\n",
            "in training loop, epoch 3, step 458, the loss is 865950.3125\n",
            "in training loop, epoch 3, step 459, the loss is 983319.8125\n",
            "in training loop, epoch 3, step 460, the loss is 1120238.625\n",
            "in training loop, epoch 3, step 461, the loss is 686889.25\n",
            "in training loop, epoch 3, step 462, the loss is 1265868.375\n",
            "in training loop, epoch 3, step 463, the loss is 2103973.75\n",
            "in training loop, epoch 3, step 464, the loss is 748119.1875\n",
            "in training loop, epoch 3, step 465, the loss is 559196.4375\n",
            "in training loop, epoch 3, step 466, the loss is 814575.125\n",
            "in training loop, epoch 3, step 467, the loss is 1380530.25\n",
            "in training loop, epoch 3, step 468, the loss is 1269773.375\n",
            "in training loop, epoch 3, step 469, the loss is 748600.0625\n",
            "in training loop, epoch 3, step 470, the loss is 1050292.375\n",
            "in training loop, epoch 3, step 471, the loss is 1193881.625\n",
            "in training loop, epoch 3, step 472, the loss is 974236.625\n",
            "in training loop, epoch 3, step 473, the loss is 1265425.625\n",
            "in training loop, epoch 3, step 474, the loss is 515460.03125\n",
            "in training loop, epoch 3, step 475, the loss is 836329.625\n",
            "in training loop, epoch 3, step 476, the loss is 2022927.0\n",
            "in training loop, epoch 3, step 477, the loss is 672483.75\n",
            "in training loop, epoch 3, step 478, the loss is 1049953.0\n",
            "in training loop, epoch 3, step 479, the loss is 2176026.0\n",
            "in training loop, epoch 3, step 480, the loss is 731663.375\n",
            "in training loop, epoch 3, step 481, the loss is 933045.125\n",
            "in training loop, epoch 3, step 482, the loss is 1868595.5\n",
            "in training loop, epoch 3, step 483, the loss is 758584.9375\n",
            "in training loop, epoch 3, step 484, the loss is 824106.625\n",
            "in training loop, epoch 3, step 485, the loss is 409176.46875\n",
            "in training loop, epoch 3, step 486, the loss is 653036.5\n",
            "in training loop, epoch 3, step 487, the loss is 701150.25\n",
            "in training loop, epoch 3, step 488, the loss is 684838.0\n",
            "in training loop, epoch 3, step 489, the loss is 638795.3125\n",
            "in training loop, epoch 3, step 490, the loss is 1307939.125\n",
            "in training loop, epoch 3, step 491, the loss is 1171546.625\n",
            "in training loop, epoch 3, step 492, the loss is 1528051.75\n",
            "in training loop, epoch 3, step 493, the loss is 958869.8125\n",
            "in training loop, epoch 3, step 494, the loss is 1864566.25\n",
            "in training loop, epoch 3, step 495, the loss is 851656.8125\n",
            "in training loop, epoch 3, step 496, the loss is 1643699.875\n",
            "in training loop, epoch 3, step 497, the loss is 920438.6875\n",
            "in training loop, epoch 3, step 498, the loss is 740133.75\n",
            "in training loop, epoch 3, step 499, the loss is 897018.0\n",
            "in training loop, epoch 3, step 500, the loss is 917778.625\n",
            "in training loop, epoch 3, step 501, the loss is 609275.4375\n",
            "in training loop, epoch 3, step 502, the loss is 777691.8125\n",
            "in training loop, epoch 3, step 503, the loss is 1132830.125\n",
            "in training loop, epoch 3, step 504, the loss is 1614879.875\n",
            "in training loop, epoch 3, step 505, the loss is 556510.6875\n",
            "in training loop, epoch 3, step 506, the loss is 1188756.875\n",
            "in training loop, epoch 3, step 507, the loss is 796413.75\n",
            "in training loop, epoch 3, step 508, the loss is 1180967.5\n",
            "in training loop, epoch 3, step 509, the loss is 821213.125\n",
            "in training loop, epoch 3, step 510, the loss is 1217666.25\n",
            "in training loop, epoch 3, step 511, the loss is 723332.125\n",
            "in training loop, epoch 3, step 512, the loss is 785408.375\n",
            "in training loop, epoch 3, step 513, the loss is 772454.75\n",
            "in training loop, epoch 3, step 514, the loss is 682919.9375\n",
            "in training loop, epoch 3, step 515, the loss is 1000207.5625\n",
            "in training loop, epoch 3, step 516, the loss is 670518.0625\n",
            "in training loop, epoch 3, step 517, the loss is 955450.4375\n",
            "in training loop, epoch 3, step 518, the loss is 666328.125\n",
            "in training loop, epoch 3, step 519, the loss is 916595.625\n",
            "in training loop, epoch 3, step 520, the loss is 482582.15625\n",
            "in training loop, epoch 3, step 521, the loss is 864228.9375\n",
            "in training loop, epoch 3, step 522, the loss is 1588812.25\n",
            "in training loop, epoch 3, step 523, the loss is 509309.1875\n",
            "in training loop, epoch 3, step 524, the loss is 986958.4375\n",
            "in training loop, epoch 3, step 525, the loss is 764610.1875\n",
            "in training loop, epoch 3, step 526, the loss is 1049717.75\n",
            "in training loop, epoch 3, step 527, the loss is 1383784.125\n",
            "in training loop, epoch 3, step 528, the loss is 1358132.125\n",
            "in training loop, epoch 3, step 529, the loss is 1467935.25\n",
            "in training loop, epoch 3, step 530, the loss is 823992.375\n",
            "in training loop, epoch 3, step 531, the loss is 718370.625\n",
            "in training loop, epoch 3, step 532, the loss is 1210009.75\n",
            "in training loop, epoch 3, step 533, the loss is 801288.125\n",
            "in training loop, epoch 3, step 534, the loss is 938632.1875\n",
            "in training loop, epoch 3, step 535, the loss is 939479.0\n",
            "in training loop, epoch 3, step 536, the loss is 1218925.25\n",
            "in training loop, epoch 3, step 537, the loss is 1764974.5\n",
            "in training loop, epoch 3, step 538, the loss is 1134252.125\n",
            "in training loop, epoch 3, step 539, the loss is 1653649.25\n",
            "in training loop, epoch 3, step 540, the loss is 842971.125\n",
            "in training loop, epoch 3, step 541, the loss is 1101908.25\n",
            "in training loop, epoch 3, step 542, the loss is 1932561.75\n",
            "in training loop, epoch 3, step 543, the loss is 1414821.875\n",
            "in training loop, epoch 3, step 544, the loss is 966783.375\n",
            "in training loop, epoch 3, step 545, the loss is 889295.6875\n",
            "in training loop, epoch 3, step 546, the loss is 1028351.25\n",
            "in training loop, epoch 3, step 547, the loss is 1073966.75\n",
            "in training loop, epoch 3, step 548, the loss is 1501912.625\n",
            "in training loop, epoch 3, step 549, the loss is 1329569.5\n",
            "in training loop, epoch 3, step 550, the loss is 1027102.6875\n",
            "in training loop, epoch 3, step 551, the loss is 1667041.875\n",
            "in training loop, epoch 3, step 552, the loss is 1182852.875\n",
            "in training loop, epoch 3, step 553, the loss is 813894.3125\n",
            "in training loop, epoch 3, step 554, the loss is 1689730.75\n",
            "in training loop, epoch 3, step 555, the loss is 894322.1875\n",
            "in training loop, epoch 3, step 556, the loss is 764036.875\n",
            "in training loop, epoch 3, step 557, the loss is 1643876.75\n",
            "in training loop, epoch 3, step 558, the loss is 832283.9375\n",
            "in training loop, epoch 3, step 559, the loss is 972902.375\n",
            "in training loop, epoch 3, step 560, the loss is 1013541.875\n",
            "in training loop, epoch 3, step 561, the loss is 1096541.0\n",
            "in training loop, epoch 3, step 562, the loss is 1257463.25\n",
            "in training loop, epoch 3, step 563, the loss is 1323550.5\n",
            "in training loop, epoch 3, step 564, the loss is 1083538.5\n",
            "in training loop, epoch 3, step 565, the loss is 1256816.25\n",
            "in training loop, epoch 3, step 566, the loss is 875597.4375\n",
            "in training loop, epoch 3, step 567, the loss is 1472190.875\n",
            "in training loop, epoch 3, step 568, the loss is 559523.6875\n",
            "in training loop, epoch 3, step 569, the loss is 1278003.0\n",
            "in training loop, epoch 3, step 570, the loss is 648154.4375\n",
            "in training loop, epoch 3, step 571, the loss is 745651.0\n",
            "in training loop, epoch 3, step 572, the loss is 544244.75\n",
            "in training loop, epoch 3, step 573, the loss is 882538.5\n",
            "in training loop, epoch 3, step 574, the loss is 773198.125\n",
            "in training loop, epoch 3, step 575, the loss is 825297.3125\n",
            "in training loop, epoch 3, step 576, the loss is 844528.8125\n",
            "in training loop, epoch 3, step 577, the loss is 649919.3125\n",
            "in training loop, epoch 3, step 578, the loss is 842640.375\n",
            "in training loop, epoch 3, step 579, the loss is 646444.8125\n",
            "in training loop, epoch 3, step 580, the loss is 1966919.0\n",
            "in training loop, epoch 3, step 581, the loss is 498045.5625\n",
            "in training loop, epoch 3, step 582, the loss is 1370451.0\n",
            "in training loop, epoch 3, step 583, the loss is 1105298.375\n",
            "in training loop, epoch 3, step 584, the loss is 2887651.0\n",
            "in training loop, epoch 3, step 585, the loss is 736116.125\n",
            "in training loop, epoch 3, step 586, the loss is 852955.0\n",
            "in training loop, epoch 3, step 587, the loss is 1749227.5\n",
            "in training loop, epoch 3, step 588, the loss is 628649.5\n",
            "in training loop, epoch 3, step 589, the loss is 1319125.0\n",
            "in training loop, epoch 3, step 590, the loss is 779317.25\n",
            "in training loop, epoch 3, step 591, the loss is 978068.3125\n",
            "in training loop, epoch 3, step 592, the loss is 1648965.625\n",
            "in training loop, epoch 3, step 593, the loss is 774574.6875\n",
            "in training loop, epoch 3, step 594, the loss is 1137241.0\n",
            "in training loop, epoch 3, step 595, the loss is 1337131.875\n",
            "in training loop, epoch 3, step 596, the loss is 1101163.625\n",
            "in training loop, epoch 3, step 597, the loss is 1292347.625\n",
            "in training loop, epoch 3, step 598, the loss is 620412.5625\n",
            "in training loop, epoch 3, step 599, the loss is 1131489.875\n",
            "in training loop, epoch 3, step 600, the loss is 796246.75\n",
            "in training loop, epoch 3, step 601, the loss is 961637.25\n",
            "in training loop, epoch 3, step 602, the loss is 662346.5625\n",
            "in training loop, epoch 3, step 603, the loss is 994606.8125\n",
            "in training loop, epoch 3, step 604, the loss is 771565.9375\n",
            "in training loop, epoch 3, step 605, the loss is 1316524.875\n",
            "in training loop, epoch 3, step 606, the loss is 714180.0\n",
            "in training loop, epoch 3, step 607, the loss is 1069805.375\n",
            "in training loop, epoch 3, step 608, the loss is 1085013.5\n",
            "in training loop, epoch 3, step 609, the loss is 740808.1875\n",
            "in training loop, epoch 3, step 610, the loss is 646145.125\n",
            "in training loop, epoch 3, step 611, the loss is 1064908.25\n",
            "in training loop, epoch 3, step 612, the loss is 1491392.0\n",
            "in training loop, epoch 3, step 613, the loss is 701071.125\n",
            "in training loop, epoch 3, step 614, the loss is 945034.8125\n",
            "in training loop, epoch 3, step 615, the loss is 1089839.125\n",
            "in training loop, epoch 3, step 616, the loss is 2419629.5\n",
            "in training loop, epoch 3, step 617, the loss is 1157470.5\n",
            "in training loop, epoch 3, step 618, the loss is 964436.75\n",
            "in training loop, epoch 3, step 619, the loss is 1135030.0\n",
            "in training loop, epoch 3, step 620, the loss is 854254.5625\n",
            "in training loop, epoch 3, step 621, the loss is 966286.0\n",
            "in training loop, epoch 3, step 622, the loss is 1395866.875\n",
            "in training loop, epoch 3, step 623, the loss is 1462121.875\n",
            "in training loop, epoch 3, step 624, the loss is 875406.5\n",
            "in training loop, epoch 3, step 625, the loss is 1333021.625\n",
            "in training loop, epoch 3, step 626, the loss is 1794268.0\n",
            "in training loop, epoch 3, step 627, the loss is 912046.0625\n",
            "in training loop, epoch 3, step 628, the loss is 1163656.25\n",
            "in training loop, epoch 3, step 629, the loss is 1181312.125\n",
            "in training loop, epoch 3, step 630, the loss is 814939.3125\n",
            "in training loop, epoch 3, step 631, the loss is 1010388.625\n",
            "in training loop, epoch 3, step 632, the loss is 608019.0625\n",
            "in training loop, epoch 3, step 633, the loss is 1141337.0\n",
            "in training loop, epoch 3, step 634, the loss is 1198124.75\n",
            "in training loop, epoch 3, step 635, the loss is 1097540.375\n",
            "in training loop, epoch 3, step 636, the loss is 1088059.375\n",
            "in training loop, epoch 3, step 637, the loss is 991097.625\n",
            "in training loop, epoch 3, step 638, the loss is 1110778.875\n",
            "in training loop, epoch 3, step 639, the loss is 1124402.75\n",
            "in training loop, epoch 3, step 640, the loss is 1116730.125\n",
            "in training loop, epoch 3, step 641, the loss is 959901.125\n",
            "in training loop, epoch 3, step 642, the loss is 702820.875\n",
            "in training loop, epoch 3, step 643, the loss is 997341.0\n",
            "in training loop, epoch 3, step 644, the loss is 1078735.125\n",
            "in training loop, epoch 3, step 645, the loss is 970647.75\n",
            "in training loop, epoch 3, step 646, the loss is 1036091.625\n",
            "in training loop, epoch 3, step 647, the loss is 1996771.25\n",
            "in training loop, epoch 3, step 648, the loss is 2951325.25\n",
            "in training loop, epoch 3, step 649, the loss is 986593.3125\n",
            "in training loop, epoch 3, step 650, the loss is 1223827.0\n",
            "in training loop, epoch 3, step 651, the loss is 2010995.5\n",
            "in training loop, epoch 3, step 652, the loss is 854426.3125\n",
            "in training loop, epoch 3, step 653, the loss is 1528911.375\n",
            "in training loop, epoch 3, step 654, the loss is 442361.0\n",
            "in training loop, epoch 3, step 655, the loss is 1596810.5\n",
            "in training loop, epoch 3, step 656, the loss is 934117.625\n",
            "in training loop, epoch 3, step 657, the loss is 1046846.75\n",
            "in training loop, epoch 3, step 658, the loss is 769591.125\n",
            "in training loop, epoch 3, step 659, the loss is 1601873.25\n",
            "in training loop, epoch 3, step 660, the loss is 712986.75\n",
            "in training loop, epoch 3, step 661, the loss is 823806.375\n",
            "in training loop, epoch 3, step 662, the loss is 831969.3125\n",
            "in training loop, epoch 3, step 663, the loss is 894124.125\n",
            "in training loop, epoch 3, step 664, the loss is 1010896.6875\n",
            "in training loop, epoch 3, step 665, the loss is 1602181.75\n",
            "in training loop, epoch 3, step 666, the loss is 696752.8125\n",
            "in training loop, epoch 3, step 667, the loss is 1135143.25\n",
            "in training loop, epoch 3, step 668, the loss is 911846.375\n",
            "in training loop, epoch 3, step 669, the loss is 766152.8125\n",
            "in training loop, epoch 3, step 670, the loss is 876074.75\n",
            "in training loop, epoch 3, step 671, the loss is 1050486.5\n",
            "in training loop, epoch 3, step 672, the loss is 713507.5\n",
            "in training loop, epoch 3, step 673, the loss is 911750.5\n",
            "in training loop, epoch 3, step 674, the loss is 528897.5625\n",
            "in training loop, epoch 3, step 675, the loss is 511047.65625\n",
            "in training loop, epoch 3, step 676, the loss is 1012771.1875\n",
            "in training loop, epoch 3, step 677, the loss is 1212500.5\n",
            "in training loop, epoch 3, step 678, the loss is 1336651.125\n",
            "in training loop, epoch 3, step 679, the loss is 781044.4375\n",
            "in training loop, epoch 3, step 680, the loss is 850100.625\n",
            "in training loop, epoch 3, step 681, the loss is 692354.5\n",
            "in training loop, epoch 3, step 682, the loss is 1041775.0625\n",
            "in training loop, epoch 3, step 683, the loss is 1070266.875\n",
            "in training loop, epoch 3, step 684, the loss is 951789.5\n",
            "in training loop, epoch 3, step 685, the loss is 571086.625\n",
            "in training loop, epoch 3, step 686, the loss is 644788.625\n",
            "in training loop, epoch 3, step 687, the loss is 1293158.25\n",
            "in training loop, epoch 3, step 688, the loss is 876744.875\n",
            "in training loop, epoch 3, step 689, the loss is 1077994.375\n",
            "in training loop, epoch 3, step 690, the loss is 1216688.25\n",
            "in training loop, epoch 3, step 691, the loss is 658602.0\n",
            "in training loop, epoch 3, step 692, the loss is 1582984.875\n",
            "in training loop, epoch 3, step 693, the loss is 880906.625\n",
            "in training loop, epoch 3, step 694, the loss is 645727.625\n",
            "in training loop, epoch 3, step 695, the loss is 1065467.0\n",
            "in training loop, epoch 3, step 696, the loss is 1228461.375\n",
            "in training loop, epoch 3, step 697, the loss is 1108296.75\n",
            "in training loop, epoch 3, step 698, the loss is 880345.25\n",
            "in training loop, epoch 3, step 699, the loss is 516537.4375\n",
            "in training loop, epoch 3, step 700, the loss is 469738.21875\n",
            "in training loop, epoch 3, step 701, the loss is 1618703.125\n",
            "in training loop, epoch 3, step 702, the loss is 1417075.0\n",
            "in training loop, epoch 3, step 703, the loss is 856039.75\n",
            "in training loop, epoch 3, step 704, the loss is 1358693.75\n",
            "in training loop, epoch 3, step 705, the loss is 659152.4375\n",
            "in training loop, epoch 3, step 706, the loss is 952391.5\n",
            "in training loop, epoch 3, step 707, the loss is 690776.8125\n",
            "in training loop, epoch 3, step 708, the loss is 572127.1875\n",
            "in training loop, epoch 3, step 709, the loss is 1187246.0\n",
            "in training loop, epoch 3, step 710, the loss is 593179.0625\n",
            "in training loop, epoch 3, step 711, the loss is 1243164.625\n",
            "in training loop, epoch 3, step 712, the loss is 1063417.0\n",
            "in training loop, epoch 3, step 713, the loss is 1467516.25\n",
            "in training loop, epoch 3, step 714, the loss is 2071446.875\n",
            "in training loop, epoch 3, step 715, the loss is 1294614.625\n",
            "in training loop, epoch 3, step 716, the loss is 702717.6875\n",
            "in training loop, epoch 3, step 717, the loss is 620224.125\n",
            "in training loop, epoch 3, step 718, the loss is 707599.3125\n",
            "in training loop, epoch 3, step 719, the loss is 1795424.25\n",
            "in training loop, epoch 3, step 720, the loss is 739155.9375\n",
            "in training loop, epoch 3, step 721, the loss is 1156668.0\n",
            "in training loop, epoch 3, step 722, the loss is 765331.875\n",
            "in training loop, epoch 3, step 723, the loss is 997560.5625\n",
            "in training loop, epoch 3, step 724, the loss is 484968.96875\n",
            "in training loop, epoch 3, step 725, the loss is 413030.1875\n",
            "in training loop, epoch 3, step 726, the loss is 403769.03125\n",
            "in training loop, epoch 3, step 727, the loss is 1178113.75\n",
            "in training loop, epoch 3, step 728, the loss is 948559.375\n",
            "in training loop, epoch 3, step 729, the loss is 1892704.0\n",
            "in training loop, epoch 3, step 730, the loss is 1360268.875\n",
            "in training loop, epoch 3, step 731, the loss is 1013784.0\n",
            "in training loop, epoch 3, step 732, the loss is 686528.0\n",
            "in training loop, epoch 3, step 733, the loss is 1081888.625\n",
            "in training loop, epoch 3, step 734, the loss is 887771.375\n",
            "in training loop, epoch 3, step 735, the loss is 1074019.0\n",
            "in training loop, epoch 3, step 736, the loss is 2134315.25\n",
            "in training loop, epoch 3, step 737, the loss is 1134594.125\n",
            "in training loop, epoch 3, step 738, the loss is 794469.75\n",
            "in training loop, epoch 3, step 739, the loss is 1378159.875\n",
            "in training loop, epoch 3, step 740, the loss is 769154.5\n",
            "in training loop, epoch 3, step 741, the loss is 828889.0\n",
            "in training loop, epoch 3, step 742, the loss is 1374989.875\n",
            "in training loop, epoch 3, step 743, the loss is 1418205.375\n",
            "in training loop, epoch 3, step 744, the loss is 391960.75\n",
            "in training loop, epoch 3, step 745, the loss is 1186161.75\n",
            "in training loop, epoch 3, step 746, the loss is 2075698.375\n",
            "in training loop, epoch 3, step 747, the loss is 920137.1875\n",
            "in training loop, epoch 3, step 748, the loss is 996888.4375\n",
            "in training loop, epoch 3, step 749, the loss is 604364.125\n",
            "in training loop, epoch 3, step 750, the loss is 1841188.5\n",
            "in training loop, epoch 3, step 751, the loss is 737948.625\n",
            "in training loop, epoch 3, step 752, the loss is 637084.875\n",
            "in training loop, epoch 3, step 753, the loss is 1116259.5\n",
            "in training loop, epoch 3, step 754, the loss is 2007938.75\n",
            "in training loop, epoch 3, step 755, the loss is 1019436.0\n",
            "in training loop, epoch 3, step 756, the loss is 1081220.125\n",
            "in training loop, epoch 3, step 757, the loss is 1236360.5\n",
            "in training loop, epoch 3, step 758, the loss is 1286946.875\n",
            "in training loop, epoch 3, step 759, the loss is 1043446.125\n",
            "in training loop, epoch 3, step 760, the loss is 1601577.125\n",
            "in training loop, epoch 3, step 761, the loss is 2063370.25\n",
            "in training loop, epoch 3, step 762, the loss is 723914.6875\n",
            "in training loop, epoch 3, step 763, the loss is 838661.375\n",
            "in training loop, epoch 3, step 764, the loss is 1336737.25\n",
            "in training loop, epoch 3, step 765, the loss is 972629.375\n",
            "in training loop, epoch 3, step 766, the loss is 1790953.625\n",
            "in training loop, epoch 3, step 767, the loss is 1519421.25\n",
            "in training loop, epoch 3, step 768, the loss is 2643516.5\n",
            "in training loop, epoch 3, step 769, the loss is 2317990.75\n",
            "in training loop, epoch 3, step 770, the loss is 1045676.875\n",
            "in training loop, epoch 3, step 771, the loss is 1434056.375\n",
            "in training loop, epoch 3, step 772, the loss is 1004082.0\n",
            "in training loop, epoch 3, step 773, the loss is 667662.125\n",
            "in training loop, epoch 3, step 774, the loss is 865396.625\n",
            "in training loop, epoch 3, step 775, the loss is 1363332.5\n",
            "in training loop, epoch 3, step 776, the loss is 1103601.625\n",
            "in training loop, epoch 3, step 777, the loss is 1293298.375\n",
            "in training loop, epoch 3, step 778, the loss is 2349697.0\n",
            "in training loop, epoch 3, step 779, the loss is 683428.5625\n",
            "in training loop, epoch 3, step 780, the loss is 1660395.75\n",
            "in training loop, epoch 3, step 781, the loss is 2594107.5\n",
            "in training loop, epoch 3, step 782, the loss is 2065235.375\n",
            "in training loop, epoch 3, step 783, the loss is 900882.375\n",
            "in training loop, epoch 3, step 784, the loss is 1921297.125\n",
            "in training loop, epoch 3, step 785, the loss is 1833248.5\n",
            "in training loop, epoch 3, step 786, the loss is 1238087.875\n",
            "in training loop, epoch 3, step 787, the loss is 1020791.125\n",
            "in training loop, epoch 3, step 788, the loss is 4328020.5\n",
            "in training loop, epoch 3, step 789, the loss is 1249759.5\n",
            "in training loop, epoch 3, step 790, the loss is 2963978.75\n",
            "in training loop, epoch 3, step 791, the loss is 3433446.25\n",
            "in training loop, epoch 3, step 792, the loss is 1976501.75\n",
            "in training loop, epoch 3, step 793, the loss is 1823633.625\n",
            "in training loop, epoch 3, step 794, the loss is 1307543.125\n",
            "in training loop, epoch 3, step 795, the loss is 1874621.0\n",
            "in training loop, epoch 3, step 796, the loss is 737199.625\n",
            "in training loop, epoch 3, step 797, the loss is 1532459.0\n",
            "in training loop, epoch 3, step 798, the loss is 1374860.625\n",
            "in training loop, epoch 3, step 799, the loss is 1914910.0\n",
            "in training loop, epoch 3, step 800, the loss is 1518499.875\n",
            "in training loop, epoch 3, step 801, the loss is 1325805.625\n",
            "in training loop, epoch 3, step 802, the loss is 3445552.75\n",
            "in training loop, epoch 3, step 803, the loss is 1586659.875\n",
            "in training loop, epoch 3, step 804, the loss is 1017460.9375\n",
            "in training loop, epoch 3, step 805, the loss is 896235.8125\n",
            "in training loop, epoch 3, step 806, the loss is 2047027.25\n",
            "in training loop, epoch 3, step 807, the loss is 1163721.0\n",
            "in training loop, epoch 3, step 808, the loss is 417070.4375\n",
            "in training loop, epoch 3, step 809, the loss is 2698314.0\n",
            "in training loop, epoch 3, step 810, the loss is 677257.5\n",
            "in training loop, epoch 3, step 811, the loss is 2031866.625\n",
            "in training loop, epoch 3, step 812, the loss is 1474218.75\n",
            "in training loop, epoch 3, step 813, the loss is 1582415.375\n",
            "in training loop, epoch 3, step 814, the loss is 1336606.625\n",
            "in training loop, epoch 3, step 815, the loss is 1158614.25\n",
            "in training loop, epoch 3, step 816, the loss is 1002011.75\n",
            "in training loop, epoch 3, step 817, the loss is 828268.6875\n",
            "in training loop, epoch 3, step 818, the loss is 3308853.0\n",
            "in training loop, epoch 3, step 819, the loss is 982556.6875\n",
            "in training loop, epoch 3, step 820, the loss is 1038009.625\n",
            "in training loop, epoch 3, step 821, the loss is 860454.6875\n",
            "in training loop, epoch 3, step 822, the loss is 1077016.0\n",
            "in training loop, epoch 3, step 823, the loss is 671552.125\n",
            "in training loop, epoch 3, step 824, the loss is 954268.375\n",
            "in training loop, epoch 3, step 825, the loss is 902337.3125\n",
            "in training loop, epoch 3, step 826, the loss is 1218271.5\n",
            "in training loop, epoch 3, step 827, the loss is 1107300.875\n",
            "in training loop, epoch 3, step 828, the loss is 713626.0625\n",
            "in training loop, epoch 3, step 829, the loss is 813101.3125\n",
            "in training loop, epoch 3, step 830, the loss is 1326196.625\n",
            "in training loop, epoch 3, step 831, the loss is 890263.1875\n",
            "in training loop, epoch 3, step 832, the loss is 1000132.9375\n",
            "in training loop, epoch 3, step 833, the loss is 760566.1875\n",
            "in training loop, epoch 3, step 834, the loss is 903028.5\n",
            "in training loop, epoch 3, step 835, the loss is 714168.0\n",
            "in training loop, epoch 3, step 836, the loss is 1548226.625\n",
            "in training loop, epoch 3, step 837, the loss is 695567.6875\n",
            "in training loop, epoch 3, step 838, the loss is 1842261.0\n",
            "in training loop, epoch 3, step 839, the loss is 782352.125\n",
            "in training loop, epoch 3, step 840, the loss is 945497.0\n",
            "in training loop, epoch 3, step 841, the loss is 1098640.0\n",
            "in training loop, epoch 3, step 842, the loss is 1064810.625\n",
            "in training loop, epoch 3, step 843, the loss is 2573032.5\n",
            "in training loop, epoch 3, step 844, the loss is 1143765.375\n",
            "in training loop, epoch 3, step 845, the loss is 922258.8125\n",
            "in training loop, epoch 3, step 846, the loss is 1095263.25\n",
            "in training loop, epoch 3, step 847, the loss is 434847.25\n",
            "in training loop, epoch 3, step 848, the loss is 570974.75\n",
            "in training loop, epoch 3, step 849, the loss is 1901360.75\n",
            "in training loop, epoch 3, step 850, the loss is 767842.8125\n",
            "in training loop, epoch 3, step 851, the loss is 1362133.5\n",
            "in training loop, epoch 3, step 852, the loss is 564439.4375\n",
            "in training loop, epoch 3, step 853, the loss is 1144808.0\n",
            "in training loop, epoch 3, step 854, the loss is 1208745.0\n",
            "in training loop, epoch 3, step 855, the loss is 1423479.625\n",
            "in training loop, epoch 3, step 856, the loss is 618096.6875\n",
            "in training loop, epoch 3, step 857, the loss is 930869.125\n",
            "in training loop, epoch 3, step 858, the loss is 917331.625\n",
            "in training loop, epoch 3, step 859, the loss is 903114.375\n",
            "in training loop, epoch 3, step 860, the loss is 678195.5625\n",
            "in training loop, epoch 3, step 861, the loss is 1281107.25\n",
            "in training loop, epoch 3, step 862, the loss is 636509.3125\n",
            "in training loop, epoch 3, step 863, the loss is 770704.8125\n",
            "in training loop, epoch 3, step 864, the loss is 1137391.5\n",
            "in training loop, epoch 3, step 865, the loss is 1241370.25\n",
            "in training loop, epoch 3, step 866, the loss is 1302853.125\n",
            "in training loop, epoch 3, step 867, the loss is 669926.5\n",
            "in training loop, epoch 3, step 868, the loss is 1281956.25\n",
            "in training loop, epoch 3, step 869, the loss is 955053.5\n",
            "in training loop, epoch 3, step 870, the loss is 973697.25\n",
            "in training loop, epoch 3, step 871, the loss is 1108062.0\n",
            "in training loop, epoch 3, step 872, the loss is 961653.0625\n",
            "in training loop, epoch 3, step 873, the loss is 2399555.25\n",
            "in training loop, epoch 3, step 874, the loss is 1204570.0\n",
            "in training loop, epoch 3, step 875, the loss is 1218930.375\n",
            "in training loop, epoch 3, step 876, the loss is 1507800.0\n",
            "in training loop, epoch 3, step 877, the loss is 1056454.5\n",
            "in training loop, epoch 3, step 878, the loss is 1552716.25\n",
            "in training loop, epoch 3, step 879, the loss is 907399.1875\n",
            "in training loop, epoch 3, step 880, the loss is 1257337.125\n",
            "in training loop, epoch 3, step 881, the loss is 986451.8125\n",
            "in training loop, epoch 3, step 882, the loss is 1724601.0\n",
            "in training loop, epoch 3, step 883, the loss is 848374.625\n",
            "in training loop, epoch 3, step 884, the loss is 724961.375\n",
            "in training loop, epoch 3, step 885, the loss is 1470212.125\n",
            "in training loop, epoch 3, step 886, the loss is 1370661.125\n",
            "in training loop, epoch 3, step 887, the loss is 1773810.375\n",
            "in training loop, epoch 3, step 888, the loss is 659317.125\n",
            "in training loop, epoch 3, step 889, the loss is 1654840.75\n",
            "in training loop, epoch 3, step 890, the loss is 1478179.0\n",
            "in training loop, epoch 3, step 891, the loss is 1301020.875\n",
            "in training loop, epoch 3, step 892, the loss is 1229814.75\n",
            "in training loop, epoch 3, step 893, the loss is 1431330.5\n",
            "in training loop, epoch 3, step 894, the loss is 1045473.8125\n",
            "in training loop, epoch 3, step 895, the loss is 1336810.5\n",
            "in training loop, epoch 3, step 896, the loss is 1442635.875\n",
            "in training loop, epoch 3, step 897, the loss is 839327.125\n",
            "in training loop, epoch 3, step 898, the loss is 1283389.5\n",
            "in training loop, epoch 3, step 899, the loss is 1303437.0\n",
            "in training loop, epoch 3, step 900, the loss is 460089.78125\n",
            "in training loop, epoch 3, step 901, the loss is 661177.625\n",
            "in training loop, epoch 3, step 902, the loss is 1208361.875\n",
            "in training loop, epoch 3, step 903, the loss is 526312.4375\n",
            "k-fold 0:: Epoch 3: train loss 1150280.3972621681 val loss 1055940.0024752475\n",
            "in training loop, epoch 4, step 0, the loss is 1110170.25\n",
            "in training loop, epoch 4, step 1, the loss is 712497.25\n",
            "in training loop, epoch 4, step 2, the loss is 796446.8125\n",
            "in training loop, epoch 4, step 3, the loss is 901207.5\n",
            "in training loop, epoch 4, step 4, the loss is 1078812.25\n",
            "in training loop, epoch 4, step 5, the loss is 1223084.0\n",
            "in training loop, epoch 4, step 6, the loss is 985384.4375\n",
            "in training loop, epoch 4, step 7, the loss is 1286467.75\n",
            "in training loop, epoch 4, step 8, the loss is 490690.125\n",
            "in training loop, epoch 4, step 9, the loss is 835049.75\n",
            "in training loop, epoch 4, step 10, the loss is 1103771.75\n",
            "in training loop, epoch 4, step 11, the loss is 752410.0625\n",
            "in training loop, epoch 4, step 12, the loss is 745832.0\n",
            "in training loop, epoch 4, step 13, the loss is 1594938.5\n",
            "in training loop, epoch 4, step 14, the loss is 695316.0625\n",
            "in training loop, epoch 4, step 15, the loss is 1278568.0\n",
            "in training loop, epoch 4, step 16, the loss is 965671.3125\n",
            "in training loop, epoch 4, step 17, the loss is 904662.9375\n",
            "in training loop, epoch 4, step 18, the loss is 1783891.875\n",
            "in training loop, epoch 4, step 19, the loss is 853676.6875\n",
            "in training loop, epoch 4, step 20, the loss is 969119.125\n",
            "in training loop, epoch 4, step 21, the loss is 1018651.375\n",
            "in training loop, epoch 4, step 22, the loss is 1246205.0\n",
            "in training loop, epoch 4, step 23, the loss is 644119.0\n",
            "in training loop, epoch 4, step 24, the loss is 1133431.25\n",
            "in training loop, epoch 4, step 25, the loss is 450785.125\n",
            "in training loop, epoch 4, step 26, the loss is 930476.5625\n",
            "in training loop, epoch 4, step 27, the loss is 853253.125\n",
            "in training loop, epoch 4, step 28, the loss is 757169.125\n",
            "in training loop, epoch 4, step 29, the loss is 1070532.75\n",
            "in training loop, epoch 4, step 30, the loss is 644630.1875\n",
            "in training loop, epoch 4, step 31, the loss is 885751.5\n",
            "in training loop, epoch 4, step 32, the loss is 1150054.5\n",
            "in training loop, epoch 4, step 33, the loss is 591678.625\n",
            "in training loop, epoch 4, step 34, the loss is 570599.5\n",
            "in training loop, epoch 4, step 35, the loss is 1070522.5\n",
            "in training loop, epoch 4, step 36, the loss is 699996.25\n",
            "in training loop, epoch 4, step 37, the loss is 593018.125\n",
            "in training loop, epoch 4, step 38, the loss is 1842643.0\n",
            "in training loop, epoch 4, step 39, the loss is 910429.0625\n",
            "in training loop, epoch 4, step 40, the loss is 706064.4375\n",
            "in training loop, epoch 4, step 41, the loss is 1342317.125\n",
            "in training loop, epoch 4, step 42, the loss is 1081559.25\n",
            "in training loop, epoch 4, step 43, the loss is 1088097.0\n",
            "in training loop, epoch 4, step 44, the loss is 522276.15625\n",
            "in training loop, epoch 4, step 45, the loss is 770071.125\n",
            "in training loop, epoch 4, step 46, the loss is 739419.625\n",
            "in training loop, epoch 4, step 47, the loss is 1113560.875\n",
            "in training loop, epoch 4, step 48, the loss is 1094987.875\n",
            "in training loop, epoch 4, step 49, the loss is 1098680.75\n",
            "in training loop, epoch 4, step 50, the loss is 1174555.75\n",
            "in training loop, epoch 4, step 51, the loss is 1250212.0\n",
            "in training loop, epoch 4, step 52, the loss is 717887.0\n",
            "in training loop, epoch 4, step 53, the loss is 1481688.25\n",
            "in training loop, epoch 4, step 54, the loss is 932180.8125\n",
            "in training loop, epoch 4, step 55, the loss is 1031632.25\n",
            "in training loop, epoch 4, step 56, the loss is 878696.25\n",
            "in training loop, epoch 4, step 57, the loss is 824041.0\n",
            "in training loop, epoch 4, step 58, the loss is 776932.25\n",
            "in training loop, epoch 4, step 59, the loss is 510524.4375\n",
            "in training loop, epoch 4, step 60, the loss is 2314096.0\n",
            "in training loop, epoch 4, step 61, the loss is 672978.5625\n",
            "in training loop, epoch 4, step 62, the loss is 1398193.625\n",
            "in training loop, epoch 4, step 63, the loss is 1088797.75\n",
            "in training loop, epoch 4, step 64, the loss is 1421577.25\n",
            "in training loop, epoch 4, step 65, the loss is 676699.6875\n",
            "in training loop, epoch 4, step 66, the loss is 1476752.25\n",
            "in training loop, epoch 4, step 67, the loss is 965066.375\n",
            "in training loop, epoch 4, step 68, the loss is 949034.0625\n",
            "in training loop, epoch 4, step 69, the loss is 1158023.5\n",
            "in training loop, epoch 4, step 70, the loss is 1077769.25\n",
            "in training loop, epoch 4, step 71, the loss is 804391.375\n",
            "in training loop, epoch 4, step 72, the loss is 638645.375\n",
            "in training loop, epoch 4, step 73, the loss is 1408938.75\n",
            "in training loop, epoch 4, step 74, the loss is 1268010.875\n",
            "in training loop, epoch 4, step 75, the loss is 995753.0625\n",
            "in training loop, epoch 4, step 76, the loss is 672123.625\n",
            "in training loop, epoch 4, step 77, the loss is 1273501.75\n",
            "in training loop, epoch 4, step 78, the loss is 1107388.75\n",
            "in training loop, epoch 4, step 79, the loss is 2420939.25\n",
            "in training loop, epoch 4, step 80, the loss is 1535422.375\n",
            "in training loop, epoch 4, step 81, the loss is 1175841.125\n",
            "in training loop, epoch 4, step 82, the loss is 903382.3125\n",
            "in training loop, epoch 4, step 83, the loss is 591414.625\n",
            "in training loop, epoch 4, step 84, the loss is 388831.53125\n",
            "in training loop, epoch 4, step 85, the loss is 1134548.25\n",
            "in training loop, epoch 4, step 86, the loss is 652278.75\n",
            "in training loop, epoch 4, step 87, the loss is 1178338.25\n",
            "in training loop, epoch 4, step 88, the loss is 726546.3125\n",
            "in training loop, epoch 4, step 89, the loss is 533148.5\n",
            "in training loop, epoch 4, step 90, the loss is 1612152.5\n",
            "in training loop, epoch 4, step 91, the loss is 864081.5625\n",
            "in training loop, epoch 4, step 92, the loss is 418233.40625\n",
            "in training loop, epoch 4, step 93, the loss is 1222909.375\n",
            "in training loop, epoch 4, step 94, the loss is 833446.8125\n",
            "in training loop, epoch 4, step 95, the loss is 816773.1875\n",
            "in training loop, epoch 4, step 96, the loss is 902057.625\n",
            "in training loop, epoch 4, step 97, the loss is 567142.4375\n",
            "in training loop, epoch 4, step 98, the loss is 909668.4375\n",
            "in training loop, epoch 4, step 99, the loss is 1432288.125\n",
            "in training loop, epoch 4, step 100, the loss is 809210.125\n",
            "in training loop, epoch 4, step 101, the loss is 1426393.0\n",
            "in training loop, epoch 4, step 102, the loss is 582665.1875\n",
            "in training loop, epoch 4, step 103, the loss is 894090.5\n",
            "in training loop, epoch 4, step 104, the loss is 1175166.0\n",
            "in training loop, epoch 4, step 105, the loss is 1223805.625\n",
            "in training loop, epoch 4, step 106, the loss is 1179558.375\n",
            "in training loop, epoch 4, step 107, the loss is 502960.0\n",
            "in training loop, epoch 4, step 108, the loss is 1227512.375\n",
            "in training loop, epoch 4, step 109, the loss is 471048.0625\n",
            "in training loop, epoch 4, step 110, the loss is 642083.0625\n",
            "in training loop, epoch 4, step 111, the loss is 569439.5625\n",
            "in training loop, epoch 4, step 112, the loss is 864010.25\n",
            "in training loop, epoch 4, step 113, the loss is 1449272.75\n",
            "in training loop, epoch 4, step 114, the loss is 894125.375\n",
            "in training loop, epoch 4, step 115, the loss is 1553765.125\n",
            "in training loop, epoch 4, step 116, the loss is 1062481.75\n",
            "in training loop, epoch 4, step 117, the loss is 950047.8125\n",
            "in training loop, epoch 4, step 118, the loss is 859591.875\n",
            "in training loop, epoch 4, step 119, the loss is 711038.875\n",
            "in training loop, epoch 4, step 120, the loss is 782844.625\n",
            "in training loop, epoch 4, step 121, the loss is 683637.8125\n",
            "in training loop, epoch 4, step 122, the loss is 705741.1875\n",
            "in training loop, epoch 4, step 123, the loss is 1185714.625\n",
            "in training loop, epoch 4, step 124, the loss is 1028379.875\n",
            "in training loop, epoch 4, step 125, the loss is 1108337.375\n",
            "in training loop, epoch 4, step 126, the loss is 971434.125\n",
            "in training loop, epoch 4, step 127, the loss is 662867.375\n",
            "in training loop, epoch 4, step 128, the loss is 487665.9375\n",
            "in training loop, epoch 4, step 129, the loss is 1301926.5\n",
            "in training loop, epoch 4, step 130, the loss is 914876.75\n",
            "in training loop, epoch 4, step 131, the loss is 1034865.75\n",
            "in training loop, epoch 4, step 132, the loss is 704731.125\n",
            "in training loop, epoch 4, step 133, the loss is 1064938.75\n",
            "in training loop, epoch 4, step 134, the loss is 1246411.625\n",
            "in training loop, epoch 4, step 135, the loss is 1760643.625\n",
            "in training loop, epoch 4, step 136, the loss is 843234.8125\n",
            "in training loop, epoch 4, step 137, the loss is 886736.4375\n",
            "in training loop, epoch 4, step 138, the loss is 674801.0\n",
            "in training loop, epoch 4, step 139, the loss is 450767.0\n",
            "in training loop, epoch 4, step 140, the loss is 1656046.25\n",
            "in training loop, epoch 4, step 141, the loss is 1092851.625\n",
            "in training loop, epoch 4, step 142, the loss is 1292827.625\n",
            "in training loop, epoch 4, step 143, the loss is 1188459.75\n",
            "in training loop, epoch 4, step 144, the loss is 628690.625\n",
            "in training loop, epoch 4, step 145, the loss is 997208.75\n",
            "in training loop, epoch 4, step 146, the loss is 1044389.3125\n",
            "in training loop, epoch 4, step 147, the loss is 1377687.5\n",
            "in training loop, epoch 4, step 148, the loss is 398630.1875\n",
            "in training loop, epoch 4, step 149, the loss is 1647643.875\n",
            "in training loop, epoch 4, step 150, the loss is 1245196.5\n",
            "in training loop, epoch 4, step 151, the loss is 728182.5625\n",
            "in training loop, epoch 4, step 152, the loss is 1039757.25\n",
            "in training loop, epoch 4, step 153, the loss is 1382959.5\n",
            "in training loop, epoch 4, step 154, the loss is 909513.1875\n",
            "in training loop, epoch 4, step 155, the loss is 774396.75\n",
            "in training loop, epoch 4, step 156, the loss is 943881.0625\n",
            "in training loop, epoch 4, step 157, the loss is 865677.875\n",
            "in training loop, epoch 4, step 158, the loss is 1970828.375\n",
            "in training loop, epoch 4, step 159, the loss is 672832.75\n",
            "in training loop, epoch 4, step 160, the loss is 598270.0\n",
            "in training loop, epoch 4, step 161, the loss is 1394747.5\n",
            "in training loop, epoch 4, step 162, the loss is 1149512.5\n",
            "in training loop, epoch 4, step 163, the loss is 838013.375\n",
            "in training loop, epoch 4, step 164, the loss is 691821.5\n",
            "in training loop, epoch 4, step 165, the loss is 593390.125\n",
            "in training loop, epoch 4, step 166, the loss is 1323840.75\n",
            "in training loop, epoch 4, step 167, the loss is 1366200.875\n",
            "in training loop, epoch 4, step 168, the loss is 935393.375\n",
            "in training loop, epoch 4, step 169, the loss is 543380.875\n",
            "in training loop, epoch 4, step 170, the loss is 752215.4375\n",
            "in training loop, epoch 4, step 171, the loss is 1291570.5\n",
            "in training loop, epoch 4, step 172, the loss is 1700206.25\n",
            "in training loop, epoch 4, step 173, the loss is 964054.125\n",
            "in training loop, epoch 4, step 174, the loss is 433631.875\n",
            "in training loop, epoch 4, step 175, the loss is 1191990.375\n",
            "in training loop, epoch 4, step 176, the loss is 1142558.625\n",
            "in training loop, epoch 4, step 177, the loss is 1139930.375\n",
            "in training loop, epoch 4, step 178, the loss is 621888.0\n",
            "in training loop, epoch 4, step 179, the loss is 580521.875\n",
            "in training loop, epoch 4, step 180, the loss is 790293.25\n",
            "in training loop, epoch 4, step 181, the loss is 998266.875\n",
            "in training loop, epoch 4, step 182, the loss is 1638196.875\n",
            "in training loop, epoch 4, step 183, the loss is 877138.25\n",
            "in training loop, epoch 4, step 184, the loss is 1005957.625\n",
            "in training loop, epoch 4, step 185, the loss is 698838.75\n",
            "in training loop, epoch 4, step 186, the loss is 779347.125\n",
            "in training loop, epoch 4, step 187, the loss is 779514.125\n",
            "in training loop, epoch 4, step 188, the loss is 1027655.5625\n",
            "in training loop, epoch 4, step 189, the loss is 1167809.0\n",
            "in training loop, epoch 4, step 190, the loss is 1765989.0\n",
            "in training loop, epoch 4, step 191, the loss is 1058009.625\n",
            "in training loop, epoch 4, step 192, the loss is 725849.1875\n",
            "in training loop, epoch 4, step 193, the loss is 1425811.75\n",
            "in training loop, epoch 4, step 194, the loss is 714205.625\n",
            "in training loop, epoch 4, step 195, the loss is 593172.0\n",
            "in training loop, epoch 4, step 196, the loss is 846287.4375\n",
            "in training loop, epoch 4, step 197, the loss is 855086.8125\n",
            "in training loop, epoch 4, step 198, the loss is 840402.625\n",
            "in training loop, epoch 4, step 199, the loss is 1500125.625\n",
            "in training loop, epoch 4, step 200, the loss is 1011965.375\n",
            "in training loop, epoch 4, step 201, the loss is 3541227.25\n",
            "in training loop, epoch 4, step 202, the loss is 980250.9375\n",
            "in training loop, epoch 4, step 203, the loss is 873605.9375\n",
            "in training loop, epoch 4, step 204, the loss is 1778625.875\n",
            "in training loop, epoch 4, step 205, the loss is 1167175.0\n",
            "in training loop, epoch 4, step 206, the loss is 2193838.25\n",
            "in training loop, epoch 4, step 207, the loss is 1421343.0\n",
            "in training loop, epoch 4, step 208, the loss is 454480.125\n",
            "in training loop, epoch 4, step 209, the loss is 938851.8125\n",
            "in training loop, epoch 4, step 210, the loss is 1601942.0\n",
            "in training loop, epoch 4, step 211, the loss is 689154.25\n",
            "in training loop, epoch 4, step 212, the loss is 682727.875\n",
            "in training loop, epoch 4, step 213, the loss is 515437.6875\n",
            "in training loop, epoch 4, step 214, the loss is 1185247.75\n",
            "in training loop, epoch 4, step 215, the loss is 649289.0625\n",
            "in training loop, epoch 4, step 216, the loss is 611563.5625\n",
            "in training loop, epoch 4, step 217, the loss is 1428986.5\n",
            "in training loop, epoch 4, step 218, the loss is 1166326.125\n",
            "in training loop, epoch 4, step 219, the loss is 1277864.875\n",
            "in training loop, epoch 4, step 220, the loss is 1393174.125\n",
            "in training loop, epoch 4, step 221, the loss is 616751.8125\n",
            "in training loop, epoch 4, step 222, the loss is 471721.40625\n",
            "in training loop, epoch 4, step 223, the loss is 794869.75\n",
            "in training loop, epoch 4, step 224, the loss is 2478904.75\n",
            "in training loop, epoch 4, step 225, the loss is 1081597.75\n",
            "in training loop, epoch 4, step 226, the loss is 593309.375\n",
            "in training loop, epoch 4, step 227, the loss is 1391489.5\n",
            "in training loop, epoch 4, step 228, the loss is 1011031.25\n",
            "in training loop, epoch 4, step 229, the loss is 1388647.375\n",
            "in training loop, epoch 4, step 230, the loss is 1689202.875\n",
            "in training loop, epoch 4, step 231, the loss is 1362138.125\n",
            "in training loop, epoch 4, step 232, the loss is 945416.875\n",
            "in training loop, epoch 4, step 233, the loss is 883847.75\n",
            "in training loop, epoch 4, step 234, the loss is 1212290.875\n",
            "in training loop, epoch 4, step 235, the loss is 1546699.875\n",
            "in training loop, epoch 4, step 236, the loss is 1010999.625\n",
            "in training loop, epoch 4, step 237, the loss is 1164789.75\n",
            "in training loop, epoch 4, step 238, the loss is 1492731.0\n",
            "in training loop, epoch 4, step 239, the loss is 1956288.125\n",
            "in training loop, epoch 4, step 240, the loss is 1389295.125\n",
            "in training loop, epoch 4, step 241, the loss is 951200.0625\n",
            "in training loop, epoch 4, step 242, the loss is 848863.125\n",
            "in training loop, epoch 4, step 243, the loss is 525000.5\n",
            "in training loop, epoch 4, step 244, the loss is 1192775.0\n",
            "in training loop, epoch 4, step 245, the loss is 674306.4375\n",
            "in training loop, epoch 4, step 246, the loss is 1343677.75\n",
            "in training loop, epoch 4, step 247, the loss is 504318.09375\n",
            "in training loop, epoch 4, step 248, the loss is 1208495.25\n",
            "in training loop, epoch 4, step 249, the loss is 464747.5\n",
            "in training loop, epoch 4, step 250, the loss is 1272772.0\n",
            "in training loop, epoch 4, step 251, the loss is 1020313.5\n",
            "in training loop, epoch 4, step 252, the loss is 442831.59375\n",
            "in training loop, epoch 4, step 253, the loss is 776256.5\n",
            "in training loop, epoch 4, step 254, the loss is 706328.0625\n",
            "in training loop, epoch 4, step 255, the loss is 584197.3125\n",
            "in training loop, epoch 4, step 256, the loss is 1265997.75\n",
            "in training loop, epoch 4, step 257, the loss is 974943.6875\n",
            "in training loop, epoch 4, step 258, the loss is 651485.625\n",
            "in training loop, epoch 4, step 259, the loss is 882535.0\n",
            "in training loop, epoch 4, step 260, the loss is 1095297.0\n",
            "in training loop, epoch 4, step 261, the loss is 597583.8125\n",
            "in training loop, epoch 4, step 262, the loss is 788441.125\n",
            "in training loop, epoch 4, step 263, the loss is 1036131.0625\n",
            "in training loop, epoch 4, step 264, the loss is 2214032.75\n",
            "in training loop, epoch 4, step 265, the loss is 985129.5\n",
            "in training loop, epoch 4, step 266, the loss is 935014.5625\n",
            "in training loop, epoch 4, step 267, the loss is 721234.25\n",
            "in training loop, epoch 4, step 268, the loss is 1763976.5\n",
            "in training loop, epoch 4, step 269, the loss is 2273535.5\n",
            "in training loop, epoch 4, step 270, the loss is 1142774.25\n",
            "in training loop, epoch 4, step 271, the loss is 719185.375\n",
            "in training loop, epoch 4, step 272, the loss is 802021.5625\n",
            "in training loop, epoch 4, step 273, the loss is 1980808.0\n",
            "in training loop, epoch 4, step 274, the loss is 969010.875\n",
            "in training loop, epoch 4, step 275, the loss is 1496531.5\n",
            "in training loop, epoch 4, step 276, the loss is 1150398.875\n",
            "in training loop, epoch 4, step 277, the loss is 766823.875\n",
            "in training loop, epoch 4, step 278, the loss is 550216.9375\n",
            "in training loop, epoch 4, step 279, the loss is 931122.625\n",
            "in training loop, epoch 4, step 280, the loss is 2066357.25\n",
            "in training loop, epoch 4, step 281, the loss is 1334553.0\n",
            "in training loop, epoch 4, step 282, the loss is 1271708.125\n",
            "in training loop, epoch 4, step 283, the loss is 880538.375\n",
            "in training loop, epoch 4, step 284, the loss is 722134.75\n",
            "in training loop, epoch 4, step 285, the loss is 1144920.0\n",
            "in training loop, epoch 4, step 286, the loss is 1257383.375\n",
            "in training loop, epoch 4, step 287, the loss is 2844968.5\n",
            "in training loop, epoch 4, step 288, the loss is 681916.0\n",
            "in training loop, epoch 4, step 289, the loss is 1087737.25\n",
            "in training loop, epoch 4, step 290, the loss is 1687620.25\n",
            "in training loop, epoch 4, step 291, the loss is 873658.9375\n",
            "in training loop, epoch 4, step 292, the loss is 1100207.125\n",
            "in training loop, epoch 4, step 293, the loss is 569047.25\n",
            "in training loop, epoch 4, step 294, the loss is 1411355.75\n",
            "in training loop, epoch 4, step 295, the loss is 1423312.125\n",
            "in training loop, epoch 4, step 296, the loss is 837055.4375\n",
            "in training loop, epoch 4, step 297, the loss is 981748.125\n",
            "in training loop, epoch 4, step 298, the loss is 1764579.5\n",
            "in training loop, epoch 4, step 299, the loss is 1367020.125\n",
            "in training loop, epoch 4, step 300, the loss is 1103603.0\n",
            "in training loop, epoch 4, step 301, the loss is 1448368.25\n",
            "in training loop, epoch 4, step 302, the loss is 842279.25\n",
            "in training loop, epoch 4, step 303, the loss is 731207.125\n",
            "in training loop, epoch 4, step 304, the loss is 528031.875\n",
            "in training loop, epoch 4, step 305, the loss is 811753.375\n",
            "in training loop, epoch 4, step 306, the loss is 757591.625\n",
            "in training loop, epoch 4, step 307, the loss is 1946483.0\n",
            "in training loop, epoch 4, step 308, the loss is 853383.1875\n",
            "in training loop, epoch 4, step 309, the loss is 670799.6875\n",
            "in training loop, epoch 4, step 310, the loss is 796762.5625\n",
            "in training loop, epoch 4, step 311, the loss is 1450862.25\n",
            "in training loop, epoch 4, step 312, the loss is 1283802.75\n",
            "in training loop, epoch 4, step 313, the loss is 908244.5\n",
            "in training loop, epoch 4, step 314, the loss is 1355399.25\n",
            "in training loop, epoch 4, step 315, the loss is 1589431.875\n",
            "in training loop, epoch 4, step 316, the loss is 863871.75\n",
            "in training loop, epoch 4, step 317, the loss is 1134031.5\n",
            "in training loop, epoch 4, step 318, the loss is 2073556.125\n",
            "in training loop, epoch 4, step 319, the loss is 582706.375\n",
            "in training loop, epoch 4, step 320, the loss is 939082.6875\n",
            "in training loop, epoch 4, step 321, the loss is 947673.0\n",
            "in training loop, epoch 4, step 322, the loss is 907946.25\n",
            "in training loop, epoch 4, step 323, the loss is 1389636.125\n",
            "in training loop, epoch 4, step 324, the loss is 1181769.25\n",
            "in training loop, epoch 4, step 325, the loss is 1212181.5\n",
            "in training loop, epoch 4, step 326, the loss is 725369.9375\n",
            "in training loop, epoch 4, step 327, the loss is 800398.25\n",
            "in training loop, epoch 4, step 328, the loss is 1307070.0\n",
            "in training loop, epoch 4, step 329, the loss is 806875.5625\n",
            "in training loop, epoch 4, step 330, the loss is 721483.375\n",
            "in training loop, epoch 4, step 331, the loss is 852554.5625\n",
            "in training loop, epoch 4, step 332, the loss is 949879.5625\n",
            "in training loop, epoch 4, step 333, the loss is 1035998.125\n",
            "in training loop, epoch 4, step 334, the loss is 2475245.25\n",
            "in training loop, epoch 4, step 335, the loss is 918887.0\n",
            "in training loop, epoch 4, step 336, the loss is 840284.75\n",
            "in training loop, epoch 4, step 337, the loss is 646909.0\n",
            "in training loop, epoch 4, step 338, the loss is 909216.1875\n",
            "in training loop, epoch 4, step 339, the loss is 780734.625\n",
            "in training loop, epoch 4, step 340, the loss is 1453905.25\n",
            "in training loop, epoch 4, step 341, the loss is 1173796.375\n",
            "in training loop, epoch 4, step 342, the loss is 703651.125\n",
            "in training loop, epoch 4, step 343, the loss is 1270405.5\n",
            "in training loop, epoch 4, step 344, the loss is 1270451.25\n",
            "in training loop, epoch 4, step 345, the loss is 1471760.375\n",
            "in training loop, epoch 4, step 346, the loss is 502164.03125\n",
            "in training loop, epoch 4, step 347, the loss is 1500401.25\n",
            "in training loop, epoch 4, step 348, the loss is 1077621.0\n",
            "in training loop, epoch 4, step 349, the loss is 658807.875\n",
            "in training loop, epoch 4, step 350, the loss is 744296.125\n",
            "in training loop, epoch 4, step 351, the loss is 1013106.3125\n",
            "in training loop, epoch 4, step 352, the loss is 901403.5625\n",
            "in training loop, epoch 4, step 353, the loss is 1003975.375\n",
            "in training loop, epoch 4, step 354, the loss is 870748.75\n",
            "in training loop, epoch 4, step 355, the loss is 1374544.25\n",
            "in training loop, epoch 4, step 356, the loss is 581118.875\n",
            "in training loop, epoch 4, step 357, the loss is 1419425.375\n",
            "in training loop, epoch 4, step 358, the loss is 1117198.25\n",
            "in training loop, epoch 4, step 359, the loss is 455096.53125\n",
            "in training loop, epoch 4, step 360, the loss is 1310012.625\n",
            "in training loop, epoch 4, step 361, the loss is 481175.84375\n",
            "in training loop, epoch 4, step 362, the loss is 1521548.875\n",
            "in training loop, epoch 4, step 363, the loss is 1031016.875\n",
            "in training loop, epoch 4, step 364, the loss is 1417354.5\n",
            "in training loop, epoch 4, step 365, the loss is 528464.5625\n",
            "in training loop, epoch 4, step 366, the loss is 1354482.875\n",
            "in training loop, epoch 4, step 367, the loss is 706713.75\n",
            "in training loop, epoch 4, step 368, the loss is 763768.0\n",
            "in training loop, epoch 4, step 369, the loss is 620195.875\n",
            "in training loop, epoch 4, step 370, the loss is 1105652.5\n",
            "in training loop, epoch 4, step 371, the loss is 1330714.5\n",
            "in training loop, epoch 4, step 372, the loss is 396151.65625\n",
            "in training loop, epoch 4, step 373, the loss is 1064852.25\n",
            "in training loop, epoch 4, step 374, the loss is 692686.0625\n",
            "in training loop, epoch 4, step 375, the loss is 629335.0625\n",
            "in training loop, epoch 4, step 376, the loss is 879466.375\n",
            "in training loop, epoch 4, step 377, the loss is 834470.4375\n",
            "in training loop, epoch 4, step 378, the loss is 1291827.0\n",
            "in training loop, epoch 4, step 379, the loss is 766736.75\n",
            "in training loop, epoch 4, step 380, the loss is 1132011.0\n",
            "in training loop, epoch 4, step 381, the loss is 1019257.9375\n",
            "in training loop, epoch 4, step 382, the loss is 1437437.625\n",
            "in training loop, epoch 4, step 383, the loss is 795944.375\n",
            "in training loop, epoch 4, step 384, the loss is 1024704.0625\n",
            "in training loop, epoch 4, step 385, the loss is 1337179.625\n",
            "in training loop, epoch 4, step 386, the loss is 1005046.125\n",
            "in training loop, epoch 4, step 387, the loss is 1685334.5\n",
            "in training loop, epoch 4, step 388, the loss is 571800.5\n",
            "in training loop, epoch 4, step 389, the loss is 711295.8125\n",
            "in training loop, epoch 4, step 390, the loss is 596125.0\n",
            "in training loop, epoch 4, step 391, the loss is 1736880.125\n",
            "in training loop, epoch 4, step 392, the loss is 1593320.875\n",
            "in training loop, epoch 4, step 393, the loss is 819114.25\n",
            "in training loop, epoch 4, step 394, the loss is 1055729.875\n",
            "in training loop, epoch 4, step 395, the loss is 1050501.375\n",
            "in training loop, epoch 4, step 396, the loss is 543928.0625\n",
            "in training loop, epoch 4, step 397, the loss is 787330.125\n",
            "in training loop, epoch 4, step 398, the loss is 696376.125\n",
            "in training loop, epoch 4, step 399, the loss is 1565376.0\n",
            "in training loop, epoch 4, step 400, the loss is 683328.9375\n",
            "in training loop, epoch 4, step 401, the loss is 1135850.5\n",
            "in training loop, epoch 4, step 402, the loss is 2163923.5\n",
            "in training loop, epoch 4, step 403, the loss is 898870.375\n",
            "in training loop, epoch 4, step 404, the loss is 693031.875\n",
            "in training loop, epoch 4, step 405, the loss is 660146.25\n",
            "in training loop, epoch 4, step 406, the loss is 888381.625\n",
            "in training loop, epoch 4, step 407, the loss is 1103900.0\n",
            "in training loop, epoch 4, step 408, the loss is 891911.0\n",
            "in training loop, epoch 4, step 409, the loss is 1009689.3125\n",
            "in training loop, epoch 4, step 410, the loss is 899453.9375\n",
            "in training loop, epoch 4, step 411, the loss is 599632.5\n",
            "in training loop, epoch 4, step 412, the loss is 782396.0\n",
            "in training loop, epoch 4, step 413, the loss is 1297408.25\n",
            "in training loop, epoch 4, step 414, the loss is 652366.125\n",
            "in training loop, epoch 4, step 415, the loss is 1599082.25\n",
            "in training loop, epoch 4, step 416, the loss is 1079655.25\n",
            "in training loop, epoch 4, step 417, the loss is 1398075.0\n",
            "in training loop, epoch 4, step 418, the loss is 1170010.375\n",
            "in training loop, epoch 4, step 419, the loss is 1087547.125\n",
            "in training loop, epoch 4, step 420, the loss is 1459694.25\n",
            "in training loop, epoch 4, step 421, the loss is 867627.75\n",
            "in training loop, epoch 4, step 422, the loss is 770949.25\n",
            "in training loop, epoch 4, step 423, the loss is 1714804.25\n",
            "in training loop, epoch 4, step 424, the loss is 455506.375\n",
            "in training loop, epoch 4, step 425, the loss is 1093712.375\n",
            "in training loop, epoch 4, step 426, the loss is 1313139.375\n",
            "in training loop, epoch 4, step 427, the loss is 1373306.5\n",
            "in training loop, epoch 4, step 428, the loss is 1406901.125\n",
            "in training loop, epoch 4, step 429, the loss is 866248.0\n",
            "in training loop, epoch 4, step 430, the loss is 671567.75\n",
            "in training loop, epoch 4, step 431, the loss is 584511.0625\n",
            "in training loop, epoch 4, step 432, the loss is 358282.25\n",
            "in training loop, epoch 4, step 433, the loss is 988085.4375\n",
            "in training loop, epoch 4, step 434, the loss is 615574.875\n",
            "in training loop, epoch 4, step 435, the loss is 654236.1875\n",
            "in training loop, epoch 4, step 436, the loss is 746133.125\n",
            "in training loop, epoch 4, step 437, the loss is 689991.25\n",
            "in training loop, epoch 4, step 438, the loss is 1003120.8125\n",
            "in training loop, epoch 4, step 439, the loss is 635295.875\n",
            "in training loop, epoch 4, step 440, the loss is 1151966.625\n",
            "in training loop, epoch 4, step 441, the loss is 892752.75\n",
            "in training loop, epoch 4, step 442, the loss is 397137.4375\n",
            "in training loop, epoch 4, step 443, the loss is 700933.6875\n",
            "in training loop, epoch 4, step 444, the loss is 1278051.375\n",
            "in training loop, epoch 4, step 445, the loss is 854554.6875\n",
            "in training loop, epoch 4, step 446, the loss is 960857.875\n",
            "in training loop, epoch 4, step 447, the loss is 870348.0\n",
            "in training loop, epoch 4, step 448, the loss is 1000590.125\n",
            "in training loop, epoch 4, step 449, the loss is 1157921.5\n",
            "in training loop, epoch 4, step 450, the loss is 937134.9375\n",
            "in training loop, epoch 4, step 451, the loss is 1021525.375\n",
            "in training loop, epoch 4, step 452, the loss is 907644.4375\n",
            "in training loop, epoch 4, step 453, the loss is 1530966.75\n",
            "in training loop, epoch 4, step 454, the loss is 1022665.875\n",
            "in training loop, epoch 4, step 455, the loss is 952733.5625\n",
            "in training loop, epoch 4, step 456, the loss is 574729.125\n",
            "in training loop, epoch 4, step 457, the loss is 699919.9375\n",
            "in training loop, epoch 4, step 458, the loss is 993558.75\n",
            "in training loop, epoch 4, step 459, the loss is 1154552.875\n",
            "in training loop, epoch 4, step 460, the loss is 897631.1875\n",
            "in training loop, epoch 4, step 461, the loss is 1503214.75\n",
            "in training loop, epoch 4, step 462, the loss is 725627.5625\n",
            "in training loop, epoch 4, step 463, the loss is 959704.3125\n",
            "in training loop, epoch 4, step 464, the loss is 630718.4375\n",
            "in training loop, epoch 4, step 465, the loss is 404593.8125\n",
            "in training loop, epoch 4, step 466, the loss is 1065439.375\n",
            "in training loop, epoch 4, step 467, the loss is 993479.75\n",
            "in training loop, epoch 4, step 468, the loss is 564106.625\n",
            "in training loop, epoch 4, step 469, the loss is 830850.9375\n",
            "in training loop, epoch 4, step 470, the loss is 955202.0625\n",
            "in training loop, epoch 4, step 471, the loss is 734615.375\n",
            "in training loop, epoch 4, step 472, the loss is 539006.875\n",
            "in training loop, epoch 4, step 473, the loss is 978701.375\n",
            "in training loop, epoch 4, step 474, the loss is 1841657.75\n",
            "in training loop, epoch 4, step 475, the loss is 750240.875\n",
            "in training loop, epoch 4, step 476, the loss is 822654.25\n",
            "in training loop, epoch 4, step 477, the loss is 767854.0\n",
            "in training loop, epoch 4, step 478, the loss is 859254.9375\n",
            "in training loop, epoch 4, step 479, the loss is 737174.5625\n",
            "in training loop, epoch 4, step 480, the loss is 813535.375\n",
            "in training loop, epoch 4, step 481, the loss is 635293.8125\n",
            "in training loop, epoch 4, step 482, the loss is 787347.125\n",
            "in training loop, epoch 4, step 483, the loss is 1100440.5\n",
            "in training loop, epoch 4, step 484, the loss is 558210.75\n",
            "in training loop, epoch 4, step 485, the loss is 1446480.5\n",
            "in training loop, epoch 4, step 486, the loss is 870265.9375\n",
            "in training loop, epoch 4, step 487, the loss is 689211.375\n",
            "in training loop, epoch 4, step 488, the loss is 987800.625\n",
            "in training loop, epoch 4, step 489, the loss is 822657.25\n",
            "in training loop, epoch 4, step 490, the loss is 893053.375\n",
            "in training loop, epoch 4, step 491, the loss is 886952.6875\n",
            "in training loop, epoch 4, step 492, the loss is 1029017.375\n",
            "in training loop, epoch 4, step 493, the loss is 714149.75\n",
            "in training loop, epoch 4, step 494, the loss is 732680.375\n",
            "in training loop, epoch 4, step 495, the loss is 737372.0625\n",
            "in training loop, epoch 4, step 496, the loss is 760964.4375\n",
            "in training loop, epoch 4, step 497, the loss is 1149090.75\n",
            "in training loop, epoch 4, step 498, the loss is 859963.8125\n",
            "in training loop, epoch 4, step 499, the loss is 1457711.75\n",
            "in training loop, epoch 4, step 500, the loss is 865136.4375\n",
            "in training loop, epoch 4, step 501, the loss is 939580.375\n",
            "in training loop, epoch 4, step 502, the loss is 724317.8125\n",
            "in training loop, epoch 4, step 503, the loss is 1076482.875\n",
            "in training loop, epoch 4, step 504, the loss is 1052360.875\n",
            "in training loop, epoch 4, step 505, the loss is 917707.25\n",
            "in training loop, epoch 4, step 506, the loss is 790120.25\n",
            "in training loop, epoch 4, step 507, the loss is 1133260.5\n",
            "in training loop, epoch 4, step 508, the loss is 820002.8125\n",
            "in training loop, epoch 4, step 509, the loss is 936547.5\n",
            "in training loop, epoch 4, step 510, the loss is 685578.0625\n",
            "in training loop, epoch 4, step 511, the loss is 598795.0625\n",
            "in training loop, epoch 4, step 512, the loss is 854764.0625\n",
            "in training loop, epoch 4, step 513, the loss is 652224.3125\n",
            "in training loop, epoch 4, step 514, the loss is 442035.5\n",
            "in training loop, epoch 4, step 515, the loss is 1405352.75\n",
            "in training loop, epoch 4, step 516, the loss is 579770.125\n",
            "in training loop, epoch 4, step 517, the loss is 1534106.75\n",
            "in training loop, epoch 4, step 518, the loss is 1483714.5\n",
            "in training loop, epoch 4, step 519, the loss is 1028875.1875\n",
            "in training loop, epoch 4, step 520, the loss is 776558.25\n",
            "in training loop, epoch 4, step 521, the loss is 634211.375\n",
            "in training loop, epoch 4, step 522, the loss is 198516.421875\n",
            "in training loop, epoch 4, step 523, the loss is 1120455.125\n",
            "in training loop, epoch 4, step 524, the loss is 1290519.5\n",
            "in training loop, epoch 4, step 525, the loss is 784198.9375\n",
            "in training loop, epoch 4, step 526, the loss is 797580.875\n",
            "in training loop, epoch 4, step 527, the loss is 777324.625\n",
            "in training loop, epoch 4, step 528, the loss is 898367.0\n",
            "in training loop, epoch 4, step 529, the loss is 2202741.0\n",
            "in training loop, epoch 4, step 530, the loss is 523847.78125\n",
            "in training loop, epoch 4, step 531, the loss is 1019793.1875\n",
            "in training loop, epoch 4, step 532, the loss is 1512099.75\n",
            "in training loop, epoch 4, step 533, the loss is 1236857.25\n",
            "in training loop, epoch 4, step 534, the loss is 813896.25\n",
            "in training loop, epoch 4, step 535, the loss is 1507736.875\n",
            "in training loop, epoch 4, step 536, the loss is 957869.5625\n",
            "in training loop, epoch 4, step 537, the loss is 1502306.5\n",
            "in training loop, epoch 4, step 538, the loss is 910171.25\n",
            "in training loop, epoch 4, step 539, the loss is 927394.75\n",
            "in training loop, epoch 4, step 540, the loss is 1013561.3125\n",
            "in training loop, epoch 4, step 541, the loss is 1779558.125\n",
            "in training loop, epoch 4, step 542, the loss is 1092415.5\n",
            "in training loop, epoch 4, step 543, the loss is 563591.625\n",
            "in training loop, epoch 4, step 544, the loss is 1139596.625\n",
            "in training loop, epoch 4, step 545, the loss is 1240658.625\n",
            "in training loop, epoch 4, step 546, the loss is 949915.1875\n",
            "in training loop, epoch 4, step 547, the loss is 1596410.75\n",
            "in training loop, epoch 4, step 548, the loss is 1216410.0\n",
            "in training loop, epoch 4, step 549, the loss is 2101053.75\n",
            "in training loop, epoch 4, step 550, the loss is 1175816.625\n",
            "in training loop, epoch 4, step 551, the loss is 645466.25\n",
            "in training loop, epoch 4, step 552, the loss is 594729.75\n",
            "in training loop, epoch 4, step 553, the loss is 1003385.75\n",
            "in training loop, epoch 4, step 554, the loss is 938538.3125\n",
            "in training loop, epoch 4, step 555, the loss is 896077.9375\n",
            "in training loop, epoch 4, step 556, the loss is 1613765.0\n",
            "in training loop, epoch 4, step 557, the loss is 952551.1875\n",
            "in training loop, epoch 4, step 558, the loss is 1013926.75\n",
            "in training loop, epoch 4, step 559, the loss is 1252823.625\n",
            "in training loop, epoch 4, step 560, the loss is 541764.0625\n",
            "in training loop, epoch 4, step 561, the loss is 863219.0625\n",
            "in training loop, epoch 4, step 562, the loss is 642251.75\n",
            "in training loop, epoch 4, step 563, the loss is 847948.0\n",
            "in training loop, epoch 4, step 564, the loss is 946444.75\n",
            "in training loop, epoch 4, step 565, the loss is 938484.125\n",
            "in training loop, epoch 4, step 566, the loss is 648550.875\n",
            "in training loop, epoch 4, step 567, the loss is 867321.625\n",
            "in training loop, epoch 4, step 568, the loss is 1182664.25\n",
            "in training loop, epoch 4, step 569, the loss is 952077.375\n",
            "in training loop, epoch 4, step 570, the loss is 802079.375\n",
            "in training loop, epoch 4, step 571, the loss is 1452557.25\n",
            "in training loop, epoch 4, step 572, the loss is 602197.0\n",
            "in training loop, epoch 4, step 573, the loss is 1850012.25\n",
            "in training loop, epoch 4, step 574, the loss is 676317.5\n",
            "in training loop, epoch 4, step 575, the loss is 546436.125\n",
            "in training loop, epoch 4, step 576, the loss is 2432956.0\n",
            "in training loop, epoch 4, step 577, the loss is 809129.625\n",
            "in training loop, epoch 4, step 578, the loss is 778760.875\n",
            "in training loop, epoch 4, step 579, the loss is 1141026.75\n",
            "in training loop, epoch 4, step 580, the loss is 1025919.75\n",
            "in training loop, epoch 4, step 581, the loss is 1153121.375\n",
            "in training loop, epoch 4, step 582, the loss is 631344.1875\n",
            "in training loop, epoch 4, step 583, the loss is 1830284.875\n",
            "in training loop, epoch 4, step 584, the loss is 1241618.75\n",
            "in training loop, epoch 4, step 585, the loss is 1061942.375\n",
            "in training loop, epoch 4, step 586, the loss is 850198.375\n",
            "in training loop, epoch 4, step 587, the loss is 728756.8125\n",
            "in training loop, epoch 4, step 588, the loss is 930991.0\n",
            "in training loop, epoch 4, step 589, the loss is 919335.875\n",
            "in training loop, epoch 4, step 590, the loss is 923547.4375\n",
            "in training loop, epoch 4, step 591, the loss is 990171.5625\n",
            "in training loop, epoch 4, step 592, the loss is 633973.0625\n",
            "in training loop, epoch 4, step 593, the loss is 1458191.25\n",
            "in training loop, epoch 4, step 594, the loss is 855355.6875\n",
            "in training loop, epoch 4, step 595, the loss is 564985.5625\n",
            "in training loop, epoch 4, step 596, the loss is 717935.625\n",
            "in training loop, epoch 4, step 597, the loss is 718270.5\n",
            "in training loop, epoch 4, step 598, the loss is 729366.8125\n",
            "in training loop, epoch 4, step 599, the loss is 1470094.0\n",
            "in training loop, epoch 4, step 600, the loss is 1008153.1875\n",
            "in training loop, epoch 4, step 601, the loss is 672076.6875\n",
            "in training loop, epoch 4, step 602, the loss is 2792762.0\n",
            "in training loop, epoch 4, step 603, the loss is 879026.5\n",
            "in training loop, epoch 4, step 604, the loss is 1336170.125\n",
            "in training loop, epoch 4, step 605, the loss is 1436289.25\n",
            "in training loop, epoch 4, step 606, the loss is 467347.1875\n",
            "in training loop, epoch 4, step 607, the loss is 881553.3125\n",
            "in training loop, epoch 4, step 608, the loss is 694378.1875\n",
            "in training loop, epoch 4, step 609, the loss is 1423240.0\n",
            "in training loop, epoch 4, step 610, the loss is 505615.03125\n",
            "in training loop, epoch 4, step 611, the loss is 1218238.875\n",
            "in training loop, epoch 4, step 612, the loss is 649154.1875\n",
            "in training loop, epoch 4, step 613, the loss is 1214313.75\n",
            "in training loop, epoch 4, step 614, the loss is 1278537.5\n",
            "in training loop, epoch 4, step 615, the loss is 653845.375\n",
            "in training loop, epoch 4, step 616, the loss is 1066199.25\n",
            "in training loop, epoch 4, step 617, the loss is 1295083.0\n",
            "in training loop, epoch 4, step 618, the loss is 999151.375\n",
            "in training loop, epoch 4, step 619, the loss is 806226.4375\n",
            "in training loop, epoch 4, step 620, the loss is 675904.1875\n",
            "in training loop, epoch 4, step 621, the loss is 1082012.0\n",
            "in training loop, epoch 4, step 622, the loss is 228349.46875\n",
            "in training loop, epoch 4, step 623, the loss is 508003.625\n",
            "in training loop, epoch 4, step 624, the loss is 669772.9375\n",
            "in training loop, epoch 4, step 625, the loss is 1488398.875\n",
            "in training loop, epoch 4, step 626, the loss is 1603011.125\n",
            "in training loop, epoch 4, step 627, the loss is 690767.1875\n",
            "in training loop, epoch 4, step 628, the loss is 739684.9375\n",
            "in training loop, epoch 4, step 629, the loss is 377495.3125\n",
            "in training loop, epoch 4, step 630, the loss is 1213688.75\n",
            "in training loop, epoch 4, step 631, the loss is 1047286.0\n",
            "in training loop, epoch 4, step 632, the loss is 747819.5625\n",
            "in training loop, epoch 4, step 633, the loss is 1025583.75\n",
            "in training loop, epoch 4, step 634, the loss is 1091960.625\n",
            "in training loop, epoch 4, step 635, the loss is 678833.5625\n",
            "in training loop, epoch 4, step 636, the loss is 885850.5\n",
            "in training loop, epoch 4, step 637, the loss is 976516.1875\n",
            "in training loop, epoch 4, step 638, the loss is 878426.75\n",
            "in training loop, epoch 4, step 639, the loss is 716894.1875\n",
            "in training loop, epoch 4, step 640, the loss is 993016.5\n",
            "in training loop, epoch 4, step 641, the loss is 1125682.0\n",
            "in training loop, epoch 4, step 642, the loss is 970320.1875\n",
            "in training loop, epoch 4, step 643, the loss is 733018.0\n",
            "in training loop, epoch 4, step 644, the loss is 703415.4375\n",
            "in training loop, epoch 4, step 645, the loss is 1999478.5\n",
            "in training loop, epoch 4, step 646, the loss is 802380.75\n",
            "in training loop, epoch 4, step 647, the loss is 435088.6875\n",
            "in training loop, epoch 4, step 648, the loss is 478266.4375\n",
            "in training loop, epoch 4, step 649, the loss is 815864.5\n",
            "in training loop, epoch 4, step 650, the loss is 1349978.25\n",
            "in training loop, epoch 4, step 651, the loss is 1304178.125\n",
            "in training loop, epoch 4, step 652, the loss is 865250.4375\n",
            "in training loop, epoch 4, step 653, the loss is 1109030.125\n",
            "in training loop, epoch 4, step 654, the loss is 744711.625\n",
            "in training loop, epoch 4, step 655, the loss is 562880.75\n",
            "in training loop, epoch 4, step 656, the loss is 886124.0\n",
            "in training loop, epoch 4, step 657, the loss is 1008638.125\n",
            "in training loop, epoch 4, step 658, the loss is 864032.125\n",
            "in training loop, epoch 4, step 659, the loss is 1054612.0\n",
            "in training loop, epoch 4, step 660, the loss is 833767.3125\n",
            "in training loop, epoch 4, step 661, the loss is 964561.625\n",
            "in training loop, epoch 4, step 662, the loss is 1364559.25\n",
            "in training loop, epoch 4, step 663, the loss is 1910425.5\n",
            "in training loop, epoch 4, step 664, the loss is 1293422.5\n",
            "in training loop, epoch 4, step 665, the loss is 1315473.0\n",
            "in training loop, epoch 4, step 666, the loss is 1939716.875\n",
            "in training loop, epoch 4, step 667, the loss is 743092.9375\n",
            "in training loop, epoch 4, step 668, the loss is 422439.53125\n",
            "in training loop, epoch 4, step 669, the loss is 1474145.75\n",
            "in training loop, epoch 4, step 670, the loss is 1136060.5\n",
            "in training loop, epoch 4, step 671, the loss is 754713.75\n",
            "in training loop, epoch 4, step 672, the loss is 1082062.875\n",
            "in training loop, epoch 4, step 673, the loss is 752362.6875\n",
            "in training loop, epoch 4, step 674, the loss is 1467951.25\n",
            "in training loop, epoch 4, step 675, the loss is 875559.4375\n",
            "in training loop, epoch 4, step 676, the loss is 661057.875\n",
            "in training loop, epoch 4, step 677, the loss is 758765.5625\n",
            "in training loop, epoch 4, step 678, the loss is 971555.25\n",
            "in training loop, epoch 4, step 679, the loss is 1842309.0\n",
            "in training loop, epoch 4, step 680, the loss is 518013.1875\n",
            "in training loop, epoch 4, step 681, the loss is 1375573.625\n",
            "in training loop, epoch 4, step 682, the loss is 536012.1875\n",
            "in training loop, epoch 4, step 683, the loss is 637312.875\n",
            "in training loop, epoch 4, step 684, the loss is 1192043.375\n",
            "in training loop, epoch 4, step 685, the loss is 755508.875\n",
            "in training loop, epoch 4, step 686, the loss is 968166.0625\n",
            "in training loop, epoch 4, step 687, the loss is 780375.625\n",
            "in training loop, epoch 4, step 688, the loss is 448548.3125\n",
            "in training loop, epoch 4, step 689, the loss is 1065806.625\n",
            "in training loop, epoch 4, step 690, the loss is 876449.6875\n",
            "in training loop, epoch 4, step 691, the loss is 1083273.0\n",
            "in training loop, epoch 4, step 692, the loss is 928634.4375\n",
            "in training loop, epoch 4, step 693, the loss is 647873.5\n",
            "in training loop, epoch 4, step 694, the loss is 685695.3125\n",
            "in training loop, epoch 4, step 695, the loss is 1383125.75\n",
            "in training loop, epoch 4, step 696, the loss is 631010.1875\n",
            "in training loop, epoch 4, step 697, the loss is 949740.0625\n",
            "in training loop, epoch 4, step 698, the loss is 1304711.0\n",
            "in training loop, epoch 4, step 699, the loss is 645122.625\n",
            "in training loop, epoch 4, step 700, the loss is 573686.0\n",
            "in training loop, epoch 4, step 701, the loss is 1087565.125\n",
            "in training loop, epoch 4, step 702, the loss is 703141.25\n",
            "in training loop, epoch 4, step 703, the loss is 727216.6875\n",
            "in training loop, epoch 4, step 704, the loss is 1555330.5\n",
            "in training loop, epoch 4, step 705, the loss is 1257146.5\n",
            "in training loop, epoch 4, step 706, the loss is 901325.4375\n",
            "in training loop, epoch 4, step 707, the loss is 518462.09375\n",
            "in training loop, epoch 4, step 708, the loss is 806007.375\n",
            "in training loop, epoch 4, step 709, the loss is 602640.0\n",
            "in training loop, epoch 4, step 710, the loss is 882523.875\n",
            "in training loop, epoch 4, step 711, the loss is 1332054.125\n",
            "in training loop, epoch 4, step 712, the loss is 538227.75\n",
            "in training loop, epoch 4, step 713, the loss is 979261.25\n",
            "in training loop, epoch 4, step 714, the loss is 609206.1875\n",
            "in training loop, epoch 4, step 715, the loss is 938561.875\n",
            "in training loop, epoch 4, step 716, the loss is 464382.1875\n",
            "in training loop, epoch 4, step 717, the loss is 1341666.5\n",
            "in training loop, epoch 4, step 718, the loss is 634807.8125\n",
            "in training loop, epoch 4, step 719, the loss is 454262.3125\n",
            "in training loop, epoch 4, step 720, the loss is 333006.625\n",
            "in training loop, epoch 4, step 721, the loss is 833838.625\n",
            "in training loop, epoch 4, step 722, the loss is 713449.4375\n",
            "in training loop, epoch 4, step 723, the loss is 1245066.25\n",
            "in training loop, epoch 4, step 724, the loss is 600763.9375\n",
            "in training loop, epoch 4, step 725, the loss is 2122613.0\n",
            "in training loop, epoch 4, step 726, the loss is 699446.0\n",
            "in training loop, epoch 4, step 727, the loss is 820668.4375\n",
            "in training loop, epoch 4, step 728, the loss is 551479.125\n",
            "in training loop, epoch 4, step 729, the loss is 801153.0625\n",
            "in training loop, epoch 4, step 730, the loss is 924317.8125\n",
            "in training loop, epoch 4, step 731, the loss is 1268273.125\n",
            "in training loop, epoch 4, step 732, the loss is 793415.625\n",
            "in training loop, epoch 4, step 733, the loss is 599908.75\n",
            "in training loop, epoch 4, step 734, the loss is 554648.5625\n",
            "in training loop, epoch 4, step 735, the loss is 1107039.125\n",
            "in training loop, epoch 4, step 736, the loss is 925339.4375\n",
            "in training loop, epoch 4, step 737, the loss is 901499.375\n",
            "in training loop, epoch 4, step 738, the loss is 789682.3125\n",
            "in training loop, epoch 4, step 739, the loss is 630849.5625\n",
            "in training loop, epoch 4, step 740, the loss is 1144937.0\n",
            "in training loop, epoch 4, step 741, the loss is 1092297.625\n",
            "in training loop, epoch 4, step 742, the loss is 915119.1875\n",
            "in training loop, epoch 4, step 743, the loss is 1590611.25\n",
            "in training loop, epoch 4, step 744, the loss is 1153609.25\n",
            "in training loop, epoch 4, step 745, the loss is 1163941.0\n",
            "in training loop, epoch 4, step 746, the loss is 622248.9375\n",
            "in training loop, epoch 4, step 747, the loss is 673114.5\n",
            "in training loop, epoch 4, step 748, the loss is 608872.0\n",
            "in training loop, epoch 4, step 749, the loss is 634733.6875\n",
            "in training loop, epoch 4, step 750, the loss is 837004.0625\n",
            "in training loop, epoch 4, step 751, the loss is 759300.625\n",
            "in training loop, epoch 4, step 752, the loss is 727548.75\n",
            "in training loop, epoch 4, step 753, the loss is 899174.0\n",
            "in training loop, epoch 4, step 754, the loss is 771741.75\n",
            "in training loop, epoch 4, step 755, the loss is 1246693.5\n",
            "in training loop, epoch 4, step 756, the loss is 523383.53125\n",
            "in training loop, epoch 4, step 757, the loss is 713657.75\n",
            "in training loop, epoch 4, step 758, the loss is 1317971.25\n",
            "in training loop, epoch 4, step 759, the loss is 511866.28125\n",
            "in training loop, epoch 4, step 760, the loss is 1438995.625\n",
            "in training loop, epoch 4, step 761, the loss is 854662.6875\n",
            "in training loop, epoch 4, step 762, the loss is 1438366.625\n",
            "in training loop, epoch 4, step 763, the loss is 754508.0625\n",
            "in training loop, epoch 4, step 764, the loss is 604780.25\n",
            "in training loop, epoch 4, step 765, the loss is 765284.75\n",
            "in training loop, epoch 4, step 766, the loss is 411157.875\n",
            "in training loop, epoch 4, step 767, the loss is 520672.15625\n",
            "in training loop, epoch 4, step 768, the loss is 737027.5625\n",
            "in training loop, epoch 4, step 769, the loss is 814971.625\n",
            "in training loop, epoch 4, step 770, the loss is 1015737.125\n",
            "in training loop, epoch 4, step 771, the loss is 793735.375\n",
            "in training loop, epoch 4, step 772, the loss is 861784.125\n",
            "in training loop, epoch 4, step 773, the loss is 492152.25\n",
            "in training loop, epoch 4, step 774, the loss is 1556349.75\n",
            "in training loop, epoch 4, step 775, the loss is 1522407.375\n",
            "in training loop, epoch 4, step 776, the loss is 605242.0625\n",
            "in training loop, epoch 4, step 777, the loss is 1011204.3125\n",
            "in training loop, epoch 4, step 778, the loss is 789032.125\n",
            "in training loop, epoch 4, step 779, the loss is 2186589.0\n",
            "in training loop, epoch 4, step 780, the loss is 624292.125\n",
            "in training loop, epoch 4, step 781, the loss is 910528.0625\n",
            "in training loop, epoch 4, step 782, the loss is 1806724.125\n",
            "in training loop, epoch 4, step 783, the loss is 1146353.25\n",
            "in training loop, epoch 4, step 784, the loss is 845771.6875\n",
            "in training loop, epoch 4, step 785, the loss is 902350.125\n",
            "in training loop, epoch 4, step 786, the loss is 507474.875\n",
            "in training loop, epoch 4, step 787, the loss is 1269331.5\n",
            "in training loop, epoch 4, step 788, the loss is 873008.875\n",
            "in training loop, epoch 4, step 789, the loss is 578998.0\n",
            "in training loop, epoch 4, step 790, the loss is 602579.6875\n",
            "in training loop, epoch 4, step 791, the loss is 637221.9375\n",
            "in training loop, epoch 4, step 792, the loss is 1229239.75\n",
            "in training loop, epoch 4, step 793, the loss is 799557.5625\n",
            "in training loop, epoch 4, step 794, the loss is 960634.4375\n",
            "in training loop, epoch 4, step 795, the loss is 1365825.125\n",
            "in training loop, epoch 4, step 796, the loss is 724707.9375\n",
            "in training loop, epoch 4, step 797, the loss is 1164330.5\n",
            "in training loop, epoch 4, step 798, the loss is 992226.625\n",
            "in training loop, epoch 4, step 799, the loss is 332011.1875\n",
            "in training loop, epoch 4, step 800, the loss is 929742.75\n",
            "in training loop, epoch 4, step 801, the loss is 660166.875\n",
            "in training loop, epoch 4, step 802, the loss is 800401.375\n",
            "in training loop, epoch 4, step 803, the loss is 804443.5\n",
            "in training loop, epoch 4, step 804, the loss is 536928.5625\n",
            "in training loop, epoch 4, step 805, the loss is 732151.5\n",
            "in training loop, epoch 4, step 806, the loss is 1271281.875\n",
            "in training loop, epoch 4, step 807, the loss is 447006.5625\n",
            "in training loop, epoch 4, step 808, the loss is 509583.4375\n",
            "in training loop, epoch 4, step 809, the loss is 1379187.75\n",
            "in training loop, epoch 4, step 810, the loss is 628079.625\n",
            "in training loop, epoch 4, step 811, the loss is 617748.25\n",
            "in training loop, epoch 4, step 812, the loss is 788174.5625\n",
            "in training loop, epoch 4, step 813, the loss is 1266978.125\n",
            "in training loop, epoch 4, step 814, the loss is 1657164.0\n",
            "in training loop, epoch 4, step 815, the loss is 871030.5\n",
            "in training loop, epoch 4, step 816, the loss is 974221.3125\n",
            "in training loop, epoch 4, step 817, the loss is 682419.5\n",
            "in training loop, epoch 4, step 818, the loss is 1234982.625\n",
            "in training loop, epoch 4, step 819, the loss is 1004589.3125\n",
            "in training loop, epoch 4, step 820, the loss is 1300960.5\n",
            "in training loop, epoch 4, step 821, the loss is 626379.9375\n",
            "in training loop, epoch 4, step 822, the loss is 1197320.25\n",
            "in training loop, epoch 4, step 823, the loss is 645458.875\n",
            "in training loop, epoch 4, step 824, the loss is 769124.25\n",
            "in training loop, epoch 4, step 825, the loss is 966350.375\n",
            "in training loop, epoch 4, step 826, the loss is 946152.875\n",
            "in training loop, epoch 4, step 827, the loss is 643944.5\n",
            "in training loop, epoch 4, step 828, the loss is 782654.9375\n",
            "in training loop, epoch 4, step 829, the loss is 1186357.375\n",
            "in training loop, epoch 4, step 830, the loss is 1206030.75\n",
            "in training loop, epoch 4, step 831, the loss is 1673089.75\n",
            "in training loop, epoch 4, step 832, the loss is 1847294.125\n",
            "in training loop, epoch 4, step 833, the loss is 824122.125\n",
            "in training loop, epoch 4, step 834, the loss is 782072.8125\n",
            "in training loop, epoch 4, step 835, the loss is 277530.1875\n",
            "in training loop, epoch 4, step 836, the loss is 1396723.25\n",
            "in training loop, epoch 4, step 837, the loss is 1302523.125\n",
            "in training loop, epoch 4, step 838, the loss is 1206353.25\n",
            "in training loop, epoch 4, step 839, the loss is 1142061.0\n",
            "in training loop, epoch 4, step 840, the loss is 805297.6875\n",
            "in training loop, epoch 4, step 841, the loss is 762619.125\n",
            "in training loop, epoch 4, step 842, the loss is 737646.375\n",
            "in training loop, epoch 4, step 843, the loss is 1344494.75\n",
            "in training loop, epoch 4, step 844, the loss is 1094491.875\n",
            "in training loop, epoch 4, step 845, the loss is 903181.4375\n",
            "in training loop, epoch 4, step 846, the loss is 1308407.0\n",
            "in training loop, epoch 4, step 847, the loss is 1538535.75\n",
            "in training loop, epoch 4, step 848, the loss is 779389.9375\n",
            "in training loop, epoch 4, step 849, the loss is 1063847.25\n",
            "in training loop, epoch 4, step 850, the loss is 952596.3125\n",
            "in training loop, epoch 4, step 851, the loss is 796097.8125\n",
            "in training loop, epoch 4, step 852, the loss is 665243.8125\n",
            "in training loop, epoch 4, step 853, the loss is 1323275.5\n",
            "in training loop, epoch 4, step 854, the loss is 856030.1875\n",
            "in training loop, epoch 4, step 855, the loss is 1318546.5\n",
            "in training loop, epoch 4, step 856, the loss is 602217.5625\n",
            "in training loop, epoch 4, step 857, the loss is 701954.375\n",
            "in training loop, epoch 4, step 858, the loss is 809928.0625\n",
            "in training loop, epoch 4, step 859, the loss is 745164.875\n",
            "in training loop, epoch 4, step 860, the loss is 1438517.0\n",
            "in training loop, epoch 4, step 861, the loss is 652567.625\n",
            "in training loop, epoch 4, step 862, the loss is 1036807.625\n",
            "in training loop, epoch 4, step 863, the loss is 1109609.0\n",
            "in training loop, epoch 4, step 864, the loss is 753640.25\n",
            "in training loop, epoch 4, step 865, the loss is 2262800.25\n",
            "in training loop, epoch 4, step 866, the loss is 1633326.875\n",
            "in training loop, epoch 4, step 867, the loss is 1126553.875\n",
            "in training loop, epoch 4, step 868, the loss is 993085.0\n",
            "in training loop, epoch 4, step 869, the loss is 625861.875\n",
            "in training loop, epoch 4, step 870, the loss is 868403.0\n",
            "in training loop, epoch 4, step 871, the loss is 710638.0\n",
            "in training loop, epoch 4, step 872, the loss is 759536.375\n",
            "in training loop, epoch 4, step 873, the loss is 1689093.25\n",
            "in training loop, epoch 4, step 874, the loss is 2038179.25\n",
            "in training loop, epoch 4, step 875, the loss is 778369.75\n",
            "in training loop, epoch 4, step 876, the loss is 1027266.875\n",
            "in training loop, epoch 4, step 877, the loss is 1163192.0\n",
            "in training loop, epoch 4, step 878, the loss is 493971.0\n",
            "in training loop, epoch 4, step 879, the loss is 420690.125\n",
            "in training loop, epoch 4, step 880, the loss is 493763.8125\n",
            "in training loop, epoch 4, step 881, the loss is 425750.875\n",
            "in training loop, epoch 4, step 882, the loss is 737740.25\n",
            "in training loop, epoch 4, step 883, the loss is 1135395.0\n",
            "in training loop, epoch 4, step 884, the loss is 1854129.5\n",
            "in training loop, epoch 4, step 885, the loss is 610964.625\n",
            "in training loop, epoch 4, step 886, the loss is 371688.40625\n",
            "in training loop, epoch 4, step 887, the loss is 640603.125\n",
            "in training loop, epoch 4, step 888, the loss is 561231.3125\n",
            "in training loop, epoch 4, step 889, the loss is 770213.8125\n",
            "in training loop, epoch 4, step 890, the loss is 948871.375\n",
            "in training loop, epoch 4, step 891, the loss is 780295.6875\n",
            "in training loop, epoch 4, step 892, the loss is 1089598.25\n",
            "in training loop, epoch 4, step 893, the loss is 670082.375\n",
            "in training loop, epoch 4, step 894, the loss is 538736.875\n",
            "in training loop, epoch 4, step 895, the loss is 839703.25\n",
            "in training loop, epoch 4, step 896, the loss is 609492.125\n",
            "in training loop, epoch 4, step 897, the loss is 487540.9375\n",
            "in training loop, epoch 4, step 898, the loss is 710196.3125\n",
            "in training loop, epoch 4, step 899, the loss is 555555.125\n",
            "in training loop, epoch 4, step 900, the loss is 950381.625\n",
            "in training loop, epoch 4, step 901, the loss is 883441.9375\n",
            "in training loop, epoch 4, step 902, the loss is 922972.125\n",
            "in training loop, epoch 4, step 903, the loss is 365185.15625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAAI4CAYAAAB3OR9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3iV5eH/8c+dHUgIEEgIJBD2SBgKooJgGMpeddU6WxXRWqVqq7JEGWodX+pCcVGtA6uylwpGEFALFkjCCHtvCBAIIeP5/RF+3y/HMgLk5H7OOe/XdXG1Jiec99Wnen08POc+xnEcAQAAACgRZDsAAAAAcBMGMgAAAHAaBjIAAABwGgYyAAAAcBoGMgAAAHAaBjIAAABwGp8cyMaY940xe40xmaV8/M3GmFXGmCxjzCfe7gMAAIDvMr54DrIxpqOkXEkfOo6Tep7HNpT0uaTOjuMcMsbEOY6ztzw6AQAA4Ht88hVkx3EWSDp4+teMMfWNMXOMMcuMMQuNMU1Ofes+SW84jnPo1M8yjgEAAHBWPjmQz2KCpD85jtNa0uOS3jz19UaSGhljFhljfjTGdLdWCAAAANcLsR1QFowxUZLaSfqXMeb/fzn81H+GSGooKU1SoqQFxpjmjuPklHcnAAAA3M8vBrJKXgnPcRyn1Rm+t13ST47jFEjaZIzJVslg/nd5BgIAAMA3+MUtFo7jHFHJ+L1JkkyJlqe+PUUlrx7LGFNNJbdcbLTRCQAAAPfzyYFsjPlU0hJJjY0x240x90i6TdI9xpgVkrIk9Tv18LmSDhhjVkn6TtJfHMc5YKMbAAAA7ueTx7wBAAAA3uKTryADAAAA3uJzb9KrVq2ak5ycXO7Pe+zYMVWsWLHcnxdnxvVwH66Ju3A93IXr4S5cD5dZu1ZFRUUKbtas3J962bJl+x3Hqf7rr/vcQE5OTtbSpUvL/XnT09OVlpZW7s+LM+N6uA/XxF24Hu7C9XAXrofLpKUpJydHlS3sO2PMljN9nVssAAAAgNMwkAEAAIDT+NwtFgAAAPAjb7+ttT/9pCttd5yGgQwAAAB7GjdW3q5dtis8cIsFAAAA7Jk+XbGLF9uu8MBABgAAgD0vv6ykzz+3XeGBgQwAAACchoEMAAAAnIaBDAAAAJyGgQwAAACchmPeAAAAYM9HH2n1kiW62nbHaXgFGQAAAPYkJSk/Ls52hQcGMgAAAOyZNEnV58+3XeGBgQwAAAB7xo9XrWnTbFd4YCADAAAAp2EgAwAAAKdhIAMAAACnYSADAAAAp+EcZAAAANjzxRfKWrRI7W13nIZXkAEAAGBPtWoqiImxXeGBgVwKjuNo3/Fi2xkAAAD+Z+JE1Zgzx3aFBwZyKbz8dbZGLM7T3qMnbKcAAAD4FwaybxpweS0VFEljZq62nQIAAAAvYyCXQv3qUepVL1RTl+/UD+v2284BAACAFzGQS6lXvVDVia2g4VMzdaKgyHYOAAAAvISBXEphwUaj+6dq0/5jGp++wXYOAAAAvISBfAE6NKyuvi1ranz6Bm3cl2s7BwAAwPfNmqWVzz9vu8IDA/kCDevdVOGhQRo+NVOO49jOAQAA8G0VKqg4IsJ2hQcG8gWKi47QX7s11qL1BzRtxU7bOQAAAL7tzTdVc8oU2xUeGMgX4XdX1lHLpMoaNWOVDh8vsJ0DAADguz7/XHHp6bYrPDCQL0JwkNGY/qk6eOyk/jZ3je0cAAAAlCEG8kVKrRWju9vV1Sc/b9UvWw/ZzgEAAEAZYSBfgkevb6T46AgNnZypwqJi2zkAAAAoAwzkSxAVHqKRfZtp9a4jmrh4s+0cAAAAlAEG8iXqllJDnZvE6ZVvsrUzJ892DgAAgG9JT9fyceNsV3hgIF8iY4ye6ZuiYsfRM9OzbOcAAADgEjGQy0BS1Qp6uEtDzc3ao29X7bGdAwAA4DteeklJkybZrvDAQC4j93Wop0bxUXp6WpaOnyy0nQMAAOAbZsxQ7JIltis8MJDLSGhwkEb3b64dOXn6+7x1tnMAAABwkRjIZaht3aq6uU2i3lu4SWt2H7GdAwAAgIvAQC5jT/VoquiIEA2dnKniYsd2DgAAAC4QA7mMVakYpiE9m2rZlkP6fOk22zkAAADuFhmpovBw2xUevDaQjTFJxpjvjDGrjDFZxphHzvCY24wxK40xGcaYxcaYlt7qKU83tk5U27pV9dzsNTqQm287BwAAwL1mz1bGCy/YrvDgzVeQCyU95jhOM0lXSfqjMabZrx6zSdK1juM0lzRK0gQv9pQbY4zG9E/V8ZOFGjtrje0cAAAAXACvDWTHcXY5jvPLqf9+VNJqSbV+9ZjFjuMcOvWXP0pK9FZPeWsYH62BHevpy1+2a8mGA7ZzAAAA3GnUKNX58EPbFR6M43j/jWTGmGRJCySlOo5zxuMdjDGPS2riOM69Z/jeQEkDJSk+Pr71Z5995r3Ys8jNzVVUVNQF/Ux+kaNhP+QpOEga1T5SoUHGS3WB52KuB7yLa+IuXA934Xq4C9fDXVoNHqyioiJlvPZauT93p06dljmO0+bXXw/x9hMbY6IkfSlp8DnGcSdJ90i65kzfdxxngk7dftGmTRsnLS3NO7HnkJ6erot53rBae/X7if/WWiXqobSGZR8WoC72esB7uCbuwvVwF66Hu3A9XKZyZeXk5Ljqmnj1FAtjTKhKxvHHjuN8dZbHtJD0rqR+juP43b0InZrEqWfzGnpt/nptOXDMdg4AAADOw5unWBhJ70la7TjOK2d5TG1JX0m6w3GcbG+12Daid4pCg4M0bEqmyuOWFgAAAFw8b76C3F7SHZI6G2OWn/rV0xgzyBgz6NRjRkiKlfTmqe8v9WKPNTViIvTY9Y20cN1+zczYZTsHAADAPWJjVVCpku0KD167B9lxnB8knfNdaafekPdfb8rzR3denawvf9muZ6evUsdG1VUpItR2EgAAgH1ffqms9HSl2e44DZ+kV06Cg4zGDmiufbn5ennuWts5AAAAOAsGcjlqkVhZd15VRx/+uEUrt+fYzgEAALDvqadU9513bFd4YCCXs8e6NVa1qHANmZyhomLesAcAAALckiWKycqyXeGBgVzOKkWEakTvZsrccUQfLdlsOwcAAAC/wkC2oHeLBHVsVF0vfZ2t3YdP2M4BAADAaRjIFhhjNKpfik4WFWvUjFW2cwAAAHAaBrIldWIr6k+dGmhmxi59t3av7RwAAAA7EhOVX7267QoPDGSLBl5bT/WqV9SIqZk6UVBkOwcAAKD8/fOfWj10qO0KDwxki8JDgjWmf3NtO5in1+avs50DAAAAMZCtu7p+rH5zeS1NWLBR6/YctZ0DAABQvgYPVoPXX7dd4YGB7AJDejZVhbAQDZ2SKcfhbGQAABBAli9X1Pr1tis8MJBdoFpUuJ7s0UQ/bzqoL5Ztt50DAAAQ0BjILnFLmyS1rlNFY2et1qFjJ23nAAAABCwGsksEBRmNGZCqIycK9fzsNbZzAAAAAhYD2UWa1Kike6+pq0lLt+nfmw/azgEAAPC+Ro10PDHRdoUHBrLLPNK1oWpVjtTQyRk6WVhsOwcAAMC7JkxQ9uOP267wwEB2mQphIXqmb4qy9+TqvR822c4BAAAIOAxkF+raLF7XN4vX3+dla9vB47ZzAAAAvGfgQDV66SXbFR4YyC41sm+KgozR09OyOBsZAAD4r+xsVdjurmNuGcguVbNypB69rpHmr9mruVm7becAAAAEDAayi93dLllNEypp5LRVys0vtJ0DAAAQEBjILhYSHKQxA1K15+gJ/c832bZzAAAAAgID2eUur11Ft7atrQ8WbVLmjsO2cwAAAMpWq1bKbdDAdoUHBrIPeKJbE1WtGKahkzNUVMwb9gAAgB8ZN07rH3rIdoUHBrIPiKkQqmG9mmnF9sP65OettnMAAAD8GgPZR/RrVVPtG8Tqb3PWaO/RE7ZzAAAAysbtt6vpmDG2KzwwkH2EMUaj+qUqv6BYo2estp0DAABQNrZvV/i+fbYrPDCQfUi96lF6IK2+pq3YqYXr3PV/JAAAAH/BQPYxD6TVV3JsBQ2fkqkTBUW2cwAAAPwOA9nHRIQGa3T/5tp84LjGp2+wnQMAAOB3GMg+6JqG1dSvVU2NT9+gDftybecAAABcvKuv1uGUFNsVHhjIPmpor6YKDw3S8CmZchzORgYAAD7quee06b77bFd4YCD7qLjoCP21exMt3nBAU5fvtJ0DAADgNxjIPux3bWurZVJljZ65SoePF9jOAQAAuHA33KCUESNsV3hgIPuw4CCjsQNSdfDYSb0wd43tHAAAgAt34IBCjxyxXeGBgezjUmrG6Pft6+qTn7Zq2ZZDtnMAAAB8HgPZD/z5ukZKiInQ0MkZKiwqtp0DAADg0xjIfiAqPERP90nRmt1H9cGizbZzAAAAfBoD2U90S4lXlyZx+p9vs7UjJ892DgAAQOl06aJDl19uu8IDA9lPGGM0sm+Kih1Hz0zLsp0DAABQOsOHa8udd9qu8MBA9iNJVSvokS6N9PWqPfpm1R7bOQAAAD6Jgexn7u1QV43iozRyWpaOnyy0nQMAAHBuPXqo+RNP2K7wwED2M6HBQRozoLl25OTp79+us50DAABwbnl5Cs7Pt13hgYHsh65Irqpb2iTp3R82ac1udx28DQAA4HYMZD/1ZI8miokM1dDJmSoudmznAAAA+AwGsp+qUjFMQ3o21bIthzRp6TbbOQAAAD6DgezHbri8lq6sW1XPz16j/bnuurcHAABAktS7tw5cfbXtCg8MZD9mjNGYAak6frJQY2ettp0DAADw3x5/XNtuucV2hQcGsp9rEBetgR3r6atfdmjxhv22cwAAAFyPgRwA/tS5oWpXraBhUzKVX1hkOwcAAOD/pKWp1eDBtis8MJADQERosJ7tl6KN+45pwvcbbecAAAC4GgM5QKQ1jlOv5gl67bv12rz/mO0cAAAA12IgB5ARfZopLDhIw6dmynE4GxkAAOBMGMgBJL5ShB6/vpEWrtuvGSt32c4BAABwJQZygLnj6mQ1rxWjZ2es0pETBbZzAABAoLv5Zu1NS7Nd4YGBHGCCg0rORj6Qm6+X5661nQMAAALdgw9qZ//+tis8MJADUIvEyrrz6mR9+OMWrdiWYzsHAAAEsuPHFXTihO0KDwzkAPXo9Y1UPSpcQyZnqLCo2HYOAAAIVD17qsWTT9qu8MBADlCVIkI1ok8zZe08oo9+3GI7BwAAwDUYyAGsV/MEdWxUXS9/na3dh931RxsAAAC2MJADmDFGo/qlqKCoWM/OyLKdAwAA4ApeG8jGmCRjzHfGmFXGmCxjzCNneIwxxrxqjFlvjFlpjLncWz04szqxFfWnzg00K2O3vluz13YOAACAdd58BblQ0mOO4zSTdJWkPxpjmv3qMT0kNTz1a6Ck8V7swVnc17Ge6levqBHTMpV3ssh2DgAACCR3363d3bvbrvDgtYHsOM4ux3F+OfXfj0paLanWrx7WT9KHTokfJVU2xiR4qwlnFh4SrDEDmmvbwTy9Nn+d7RwAABBIAmkgn84YkyzpMkk//epbtSRtO+2vt+u/RzTKwVX1YnXD5YmasGCjsvcctZ0DAAACxf79Cj182HaFhxBvP4ExJkrSl5IGO45z5CJ/j4EquQVD8fHxSk9PL7vAUsrNzbXyvOXp2sqO5gQ7emjiD3qqbYSMMbaTzioQroev4Zq4C9fDXbge7sL1cJdWgwerSVGR0mNibKf8L68OZGNMqErG8ceO43x1hofskJR02l8nnvqaB8dxJkiaIElt2rRx0ix8Xnd6erpsPG95O155q578KkP7oxvopjZJ5/8BSwLlevgSrom7cD3chevhLlwPl6lcWTk5Oa66Jt48xcJIek/SasdxXjnLw6ZJuvPUaRZXSTrsOM4ubzXh/G5uk6Q2dapo7KzVOnjspO0cAACAcufNe5DbS7pDUmdjzPJTv3oaYwYZYwadeswsSRslrZf0jqQHvdiDUggKMho9IFVHTxTq+dmrbecAAACUO6/dYuE4zg+SznkTq+M4jqQ/eqsBF6dJjUq6p0Ndvf39Rt3YOklt61a1nQQAAFBu+CQ9nNEjXRqqVuVIDZuSoZOFxbZzAACAv3rgAe3o29d2hQcGMs6oQliInu2Xouw9uXr3h422cwAAgL+65Rbt69zZdoUHBjLOqkvTeHVLider89Zp28HjtnMAAIA/2rZN4Xv32q7wwEDGOT3dJ0XBxmjE1EyV3DIOAABQhu64Q03HjrVd4YGBjHOqWTlSf76ukb5bu09zs3bbzgEAAPA6BjLO6+52yWqaUEkjp61Sbn6h7RwAAACvYiDjvEKCgzR2QKr2HD2hV77Otp0DAADgVQxklMpltavod21ra+LiTcrccdh2DgAAgNcwkFFqf+3eRFUrhmno5AwVFfOGPQAAUAYee0zbbr7ZdoUHBjJKLSYyVMN7N9OK7Yf1yU9bbOcAAAB/0KePDrRrZ7vCAwMZF6Rvy5pq3yBWf5uzVnuPnrCdAwAAfN3atYrcutV2hQcGMi6IMUaj+qUqv7BYo2estp0DAAB83f33q/Err9iu8MBAxgWrVz1KD3aqr2krdmpB9j7bOQAAAGWKgYyLMuja+qpbraKGT83UiYIi2zkAAABlhoGMixIRGqxR/VK15cBxvZm+wXYOAABAmWEg46Jd07Ca+rWqqbfSN2jDvlzbOQAAAGWCgYxLMqxXM4WHBmn4lEw5DmcjAwCACzRsmLbccYftCg8MZFyS6tHheqJ7Ey3ecEBTlu+wnQMAAHxN16461Lq17QoPDGRcst+1ra1WSZU1esZqHT5eYDsHAAD4kuXLFbV+ve0KDwxkXLKgIKOxA5orJ69Az89ZYzsHAAD4ksGD1eD1121XeGAgo0w0q1lJv2+XrE9/3qplWw7azgEAALhoDGSUmcHXNVJCTISGTs5UQVGx7RwAAICLwkBGmYkKD9HTfVK0ZvdRTVy02XYOAADARWEgo0x1S4lX16ZxeuWbbO3IybOdAwAAcMEYyChTxhiN7JsiSRo5LctyDQAAcL2xY7Xx3nttV3hgIKPMJVapoEe6NtQ3q/bo66zdtnMAAICbtWunI6mptis8MJDhFfdcU1eN46M1clqWjuUX2s4BAAButXixKmVm2q7wwECGV4QGB2nMgFTtPHxCf5+3znYOAABwqyFDVO/dd21XeGAgw2vaJFfVb69I0ns/bNLqXUds5wAAAJQKAxle9UT3JoqJDNXQyRkqLnZs5wAAAJwXAxleVaVimIb2bKpftuZo0tJttnMAAADOi4EMr/vN5bV0Zd2qen72Gu3PzbedAwAAcE4MZHidMUZjBqTq+MlCjZ252nYOAABwk3HjtP6hh2xXeGAgo1w0iIvW/R3r66v/7NDiDftt5wAAALdo1Uq5DRrYrvDAQEa5eahzA9WuWkHDJmcqv7DIdg4AAHCDb79VlWXLbFd4YCCj3ESEBuvZfinauP+Y3v5+o+0cAADgBqNHq85HH9mu8MBARrlKaxynXi0S9Pp367V5/zHbOQAAAP+FgYxyN6J3M4UHB2n41Ew5DmcjAwAAd2Ego9zFV4rQ490aa+G6/Zq+cpftHAAAAA8MZFhx+1V11LxWjEbNWKXDeQW2cwAAAP4XAxlWBAcZjR3QXAdy8/Xy12tt5wAAAFvefltrH33UdoUHBjKsaZ4YozuvTtZHP27Rim05tnMAAIANjRsrr3Zt2xUeGMiw6rHrG6l6VLiGTM5QYVGx7RwAAFDepk9X7OLFtis8MJBhVXREqJ7uk6KsnUf04ZIttnMAAEB5e/llJX3+ue0KDwxkWNezeQ1d26i6Xv56rXYfPmE7BwAABDgGMqwzxmhUv1QVFjt6ZnqW7RwAABDgGMhwhdqxFfRwl4aanblb89fssZ0DAAACGAMZrnFfh3pqEBelEVOzlHeyyHYOAAAIUAxkuEZYSJBG90/V9kN5em3+Ots5AACgPHz0kVYPGWK7wgMDGa5yVb1Y3dg6URMWbFT2nqO2cwAAgLclJSk/Ls52hQcGMlznqR5NFBURoqGTM1Rc7NjOAQAA3jRpkqrPn2+7wgMDGa4TGxWup3o00b83H9IXv2y3nQMAALxp/HjVmjbNdoUHBjJc6abWSWpTp4qem7VaB4+dtJ0DAAACCAMZrhQUZDRmQHMdPVGo52attp0DAAACCAMZrtW4RrTu7VBP/1q2XT9tPGA7BwAABAgGMlzt4S4NVKtypIZNydTJwmLbOQAAIAAwkOFqFcJCNKp/itbtzdW7P2y0nQMAAMraF18o65lnbFd4YCDD9To3iVf3lBp6dd46bTt43HYOAAAoS9WqqSAmxnaFBwYyfMLTfZsp2BgNn5opx+FsZAAA/MbEiaoxZ47tCg8MZPiEhJhI/fm6Rkpfu09zMnfbzgEAAGWFgQxcvLvbJatZQiWNnJ6lvEJeRQYAAN7BQIbPCAkO0pgBqdp7NF9frePDQwAAgHd4bSAbY943xuw1xmSe5fsxxpjpxpgVxpgsY8zvvdUC/3FZ7Sq67cra+nZLoTJ3HLadAwAA/JA3X0GeKKn7Ob7/R0mrHMdpKSlN0svGmDAv9sBP/KVbE0WHGQ2dnKGiYm61AAAAZctrA9lxnAWSDp7rIZKijTFGUtSpxxZ6qwf+IyYyVL9rEqYV2w/r45+22M4BAACXYtYsrXz+edsVHow3j8wyxiRLmuE4TuoZvhctaZqkJpKiJd3iOM7Ms/w+AyUNlKT4+PjWn332mbeSzyo3N1dRUVHl/rw4s6NHc/XWmmBtPFys566JVOUIbqe3jb9H3IXr4S5cD3fheriPrWvSqVOnZY7jtPn1120O5BsltZf0qKT6kr6R1NJxnCPn+j3btGnjLF26tOxjzyM9PV1paWnl/rw4s/T0dNVJvULdxi1Qt5Qaeu3Wy2wnBTz+HnEXroe7cD3chevhMm++qezsbDUaN67cn9oYc8aBbPNlt99L+sopsV7SJpW8mgyUSt1qFfXHtAaavmKnFmTvs50DAAAuxuefKy493XaFB5sDeaukLpJkjImX1FjSRos98EGD0uqpXrWKGj41UycKimznAAAAP+DNY94+lbREUmNjzHZjzD3GmEHGmEGnHjJKUjtjTIakeZKecBxnv7d64J/CQ4I1qn+qthw4rje/W287BwAA+IEQb/3GjuPcep7v75R0vbeeH4GjfYNq6t+qpsZ/v0F9W9VSgzjeeAEAAC4eb/2HXxjaq5kiQ4M1fEqmvPnGUwAA4P8YyPAL1aPD9USPJlqy8YAm/2eH7RwAAFBa6elabuEEi3NhIMNv3HpFbV1Wu7LGzFytnOMnbecAAAAfxUCG3wgKMhrTv7ly8gr0wpy1tnMAAEBpvPSSkiZNsl3hgYEMv9KsZiX9oX2yPv15q5ZtOdcnnQMAAFeYMUOxS5bYrvDAQIbfGdy1kRJiIjR0cqYKiopt5wAAAB/DQIbfqRgeopF9U7Rm91F9sGiT7RwAAOBjGMjwS91Saqhr03j9zzfrtCMnz3YOAADwIQxk+K2RfZtJkp6emmW5BAAAnFVkpIrCw21XeGAgw28lVqmgwV0b6tvVe/R11m7bOQAA4Exmz1bGCy/YrvDAQIZf+8M1ddWkRrRGTsvSsfxC2zkAAMAHMJDh10KDgzRmQKp2Hj6hv89bZzsHAAD82qhRqvPhh7YrPDCQ4fda16mqW9sm6b0fNmnVziO2cwAAwOnmzVOVX36xXeGBgYyA8ET3JoqJDNXQKRkqLnZs5wAAABdjICMgVK4QpqE9m+o/W3P02b+32c4BAAAuxkBGwPjN5bV0Vb2qen72au07mm87BwAAuBQDGQHDGKPR/Zsrr6BIY2ettp0DAAAkKTZWBZUq2a7wwEBGQGkQF6VB19bX5P/s0OL1+23nAACAL79U1rPP2q7wwEBGwPljpwaqE1tBw6ZkKr+wyHYOAABwGQYyAk5EaLCe7ZeqjfuP6a30jbZzAAAIbE89pbrvvGO7wgMDGQHp2kbV1btFgt5IX69N+4/ZzgEAIHAtWaKYrCzbFR4YyAhYI3o3U3hwkEZMzZTjcDYyAAAowUBGwIqrFKG/dG+shev2a/rKXbZzAACASzCQEdBuu7KOWiTG6Nnpq3Q4r8B2DgAAcAEGMgJacJDRmP7NdfBYvl6au9Z2DgAAgScxUfnVq9uu8MBARsBrnhijO69O1j9/2qLl23Js5wAAEFj++U+tHjrUdoUHBjIg6bHrGykuOlxDJ2eosKjYdg4AALCIgQxIio4I1dN9UpS184j+sWSL7RwAAALH4MFq8Prrtis8MJCBU3qk1lBa4+p65eu12nU4z3YOAACBYflyRa1fb7vCAwMZOMUYo2f7pqqw2NGz01fZzgEAAJYwkIHT1I6toIe7NNTszN2av2aP7RwAAGABAxn4lfs61FODuCgNn5KlvJNFtnMAAEA5YyADvxIWEqQx/VO1IydPr85fZzsHAAD/1qiRjicm2q7wwEAGzuDKerG6qXWi3lmwUdl7jtrOAQDAf02YoOzHH7dd4YGBDJzFUz2bKioiREMnZ6i42LGdAwAAygkDGTiLqhXDNKRHU/178yF9sWy77RwAAPzTwIFq9NJLtis8MJCBc7ixdaKuSK6isbNX6+Cxk7ZzAADwP9nZqrDdXS9EMZCBcwgKMhozoLlyTxTquVmrbecAAIBywEAGzqNRfLTu61hP/1q2XT9tPGA7BwAAeBkDGSiFhzs3VGKVSA2dkqmThcW2cwAAgBcxkIFSiAwL1rP9UrR+b67eWbjRdg4AAP6jVSvlNmhgu8IDAxkopc5N4tUjtYZenbdOWw8ct50DAIB/GDdO6x96yHaFBwYycAFG9GmmkCCjEdMy5TicjQwAgD9iIAMXICEmUo9e31jpa/dpduZu2zkAAPi+229X0zFjbFd4YCADF+iuq+uoWUIlPTM9S0dPFNjOAQDAt23frvB9+2xXeGAgAxcoJDhIY3/TXHuP5uvlr7Nt5wAAgDLGQAYuQqukyrr9yjr6cMlmZWw/bDsHAACUIQYycJEe79ZYsVHhGjolQ0XFvGEPAAB/wUAGLlJMZKiG926mldsP6+OfttjOAQDAN119tQ6npPMo6CQAACAASURBVNiu8MBABi5BnxYJ6tCwml6cs1Z7jpywnQMAgO957jltuu8+2xUeGMjAJTDG6Nl+qcovKtaoGats5wAAgDLAQAYuUd1qFfXHtAaasXKXvs921zE1AAC43g03KGXECNsVHhjIQBkYlFZP9apV1IipmTpRUGQ7BwAA33HggEKPHLFd4YGBDJSB8JBgje6fqi0HjuuN79bbzgEAAJeAgQyUkXYNqmnAZbX01vcbtH5vru0cAABwkRjIQBka0rOpIkODNWxKhhyHs5EBAPBFpRrIxphHjDGVTIn3jDG/GGOu93Yc4GuqR4fryR5N9ePGg/rqlx22cwAAcL8uXXTo8sttV3go7SvIf3Ac54ik6yVVkXSHpOe9VgX4sN9ekaTLalfWmFmrlXP8pO0cAADcbfhwbbnzTtsVHko7kM2p/+wp6SPHcbJO+xqA0wQFGY0d0FyH8wr0wpw1tnMAAMAFKu1AXmaM+VolA3muMSZaUrH3sgDf1jShku65pq4+/Xmblm05aDsHAAD36tFDzZ94wnaFh9IO5HskPSnpCsdxjksKlfR7r1UBfuCRLg1VMyZCQ77KVEER/z4JAMAZ5eUpOD/fdoWH0g7kqyWtdRwnxxhzu6Rhkg6f6weMMe8bY/YaYzLP8Zg0Y8xyY0yWMeb70mcD7lcxPEQj+6Zo7Z6jev+HTbZzAABAKZV2II+XdNwY01LSY5I2SPrwPD8zUVL3s33TGFNZ0puS+jqOkyLpplK2AD7j+pQa6to0XuO+Xafth47bzgEAAKVQ2oFc6JQc6tpP0uuO47whKfpcP+A4zgJJ57r58neSvnIcZ+upx+8tZQvgU57plyJJGjltleUSAABQGqY0H2Zw6vaHOZL+IKmDpL2SVjiO0/w8P5csaYbjOKln+N44ldzLnKKSsf13x3HO+Kq0MWagpIGSFB8f3/qzzz47b3NZy83NVVRUVLk/L87M167H7E0FmrT2pP50Wbhax4fYzvEKX7sm/o7r4S5cD3fherhL0qRJys/P114LR7116tRpmeM4bX799dIO5BoqecX3347jLDTG1JaUdrZBe9rPJevsA/l1SW0kdZEUKWmJpF6O42Sf6/ds06aNs3Tp0vM2l7X09HSlpaWV+/PizHztehQUFavPaz/ocF6Bvn30WlUM97+R7GvXxN9xPdyF6+EuXA/3sXVNjDFnHMilusXCcZzdkj6WFGOM6S3pxPnGcSlslzTXcZxjjuPsl7RAUstL/D0BVwoNDtKYAanadfiExn17zn8HBAAAlpX2o6ZvlvSzSt5Id7Okn4wxN17ic0+VdI0xJsQYU0HSlZJWX+LvCbhW6zpVdWvb2np/0Wat2nnEdg4AAO6QlqZWgwfbrvBQ2jfpDVXJGch3OY5zp6S2koaf6weMMZ+q5LaJxsaY7caYe4wxg4wxgyTJcZzVKrmveaVKxve7juOc9Ug4wB880b2xKkeGauiUDBUXn//2JgAAUP5KeyNk0K9OmTig84xrx3FuPd9v6jjOi5JeLGUD4PMqVwjT0F5N9ejnK/Tpv7fqtivr2E4CAAC/UtpXkOcYY+YaY+42xtwtaaakWd7LAvzXgMtq6ep6sXph9hrtO+quTw4CAAClf5PeXyRNkNTi1K8JjuO460OzAR9hjNHoAak6UVCsMTM5GxkAALcp9VlTjuN8KelLL7YAAaN+9SgNuraeXp2/Xje1SVL7BtVsJwEAYMfNN2tvdrYq2+44zTlfQTbGHDXGHDnDr6PGGN6GD1yCBzs1UJ3YCho2JVMnCops5wAAYMeDD2pn//62Kzyc74120Y7jVDrDr2jHcSqVV6R1jiNTXGC7An4mIjRYo/qlatP+Y3r7+422cwAAsOP4cQWdOGG7wkNp36QX2JZ/rDZLH5X2ckwzylbHRtXVp2VNvZG+Xpv2H7OdAwBA+evZUy2efNJ2hQcGcmlEJyi04LA0IU1a+r5Uio/nBkpreK+mCg8O0vApmSrNR78DAADvYiCXRoMuWtrm71Ltq6UZf5b+dZeUd8h2FfxEXKUI/aV7Y/2wfr+mrdhpOwcAgIDHQC6lk+FVpNu/kro+I62ZKb3VQdr6k+0s+InbrqyjlokxGjVjtQ7ncb87AAA2MZAvRFCQdM1g6Q9zJRMkfdBDWvCSVMwJBLg0wUFGYwY018Fj+Xpx7hrbOQAABDQG8sVIbCMNWig16yfNHyV91F86sst2FXxcaq0Y3dUuWR//tFX/2cotPACAAHH33drdvbvtCg8M5IsVESPd+L7U93Vp+1LprfZS9lzbVfBxj17XSHHR4Ro6OVOFRcW2cwAA8D4Gsp8xRrr8DmlguhSdIH1yszRniFSYb7sMPio6IlQj+6Ro1a4jmrh4s+0cAAC8b/9+hR4+bLvCAwO5LFRvLN07T7riPunHN6T3rpMObLBdBR/VPbWGOjWurle+ydbOnDzbOQAAeNeNNyrl6adtV3hgIJeV0Aip10vSLR9Lh7ZIb3eUVnxmuwo+yBijZ/ulqthx9Oz0VbZzAAAIOAzksta0t/TAIqlGC2ny/dJX90v5R21XwcckVa2gh7s01Jys3Zq3eo/tHAAAAgoD2RtiEqW7pkvXPillfC69fa20c7ntKviYe6+pp4ZxURoxNUvHTxbazgEAIGAwkL0lOETq9FTJUC7Ik97tKi15k4+pRqmFhQRpdP9U7cjJ06vz1tvOAQAgYDCQvS35mpJbLhpeJ819SvrkFunYfttV8BFX1ovVTa0T9e7CjVq7m1t1AAB+6IEHtKNvX9sVHhjI5aFCVem3n0g9XpQ2fieNby9tWmC7Cj7iqZ5NFR0RomFTMlRczJ9AAAD8zC23aF/nzrYrPDCQy4sx0pUDS46DC4+W/tFXmjdKKuLeUpxb1YpheqpnU/178yH9a9k22zkAAJStbdsUvnev7QoPDOTyltBCuv97qdVt0sKXpIk9pZyttqvgcjdenqi2yVX13Ow1OpDLB9EAAPzIHXeo6dixtis8MJBtCKso9X9DuuE9ac8q6a1rpFVTbVfBxYKCjEYPSFXuiUI9N3uN7RwAAPwaA9mm5jdKgxZIVetLn98pTR9ccuIFcAaN4qM1sGM9fbFsu37ceMB2DgAAfouBbFvVetIf5krtHpaWfSC901nau9p2FVzqT50bKrFKpIZNydTJwmLbOQAA+CUGshuEhEnXj5Ju/1I6tk+a0Ela+gFnJuO/RIYFa1S/VK3fm6t3Fm60nQMAgF9iILtJg67SoEVS7aukGYOlf90l5eXYroLLdGoSp57Na+jVeeu09cBx2zkAAFyaxx7Ttptvtl3hgYHsNtHx0u1fSV2fkdbMlN7qIG372XYVXGZE7xSFBBkNn5ophz9pAAD4sj59dKBdO9sVHhjIbhQUJF0zuOTeZCPp/e7Sgpek4iLbZXCJGjEReuz6xvo+e59mZey2nQMAwMVbu1aRW9115C0D2c0S20iDfpCa9ZPmj5I+GiAd2WW7Ci5x59V1lFKzkp6ZnqWjJwps5wAAcHHuv1+NX3nFdoUHBrLbRcRIN74v9X2t5FaLt9pL2V/broILhAQHaeyA5tqXm6+Xv862nQMAgN9gIPsCY6TL7yz5BL7oBOmTm6S5Q6XCk7bLYFnLpMq646o6+nDJZmVsP2w7BwAAv8BA9iXVG0v3zpOuuE9a8rr03nXSgQ22q2DZ490aKzYqXEMmZ6iomDfsAQBwqRjIviY0Qur1knTLx9KhzdLbHaUVk2xXwaJKEaEa3ruZMnYc1j9/3GI7BwAAn8dA9lVNe0sPLJJqtJAmD5QmD5Lyc21XwZI+LRLUoWE1vTh3rfYcOWE7BwCA0hs2TFvuuMN2hQcGsi+LSZTumi5d+4S0clLJq8k7l9uuggXGGI3ql6qTRcV6dsYq2zkAAJRe16461Lq17QoPDGRfFxwidRpSMpQL8kruS17yJh9THYCSq1XUQ50aaObKXUpfu9d2DgAApbN8uaLWr7dd4YGB7C+Sryk5M7l+F2nuU9Int0jH9tuuQjm7/9p6qle9okZMzdKJAj5YBgDgAwYPVoPXX7dd4YGB7E8qxkq3fir1+Ju08TtpfHtp0wLbVShH4SHBGt0/VVsPHtcb37nr38YBAPAVDGR/Y4x05f0lx8GFR0v/6CvNHy0VFdouQzlpV7+afnNZLb31/Qat33vUdg4AAD6HgeyvElpIA9OlVr+TFrwoTewl5bjrc87hPUN6NVWFsBANnZwph/vRAQC4IAxkfxYeJfV/U/rNu9KeLOmta6RV02xXoRxUiwrXkz2a6KdNB/XVLzts5wAA4FMYyIGgxU3SoAVS1XrS53dIM/5ccuIF/NotbZJ0ee3KGjNrtQ4d42PJAQAuNXasNt57r+0KDwzkQFG1nvSHr6V2f5KWvi+901nau9p2FbwoKMhozIDmOpxXoBfmrLGdAwDAmbVrpyOpqbYrPDCQA0lImHT9aOm2L6Vj+6QJnaSlH3Bmsh9rmlBJ91xTV5/9e5uWbj5oOwcAgP+2eLEqZWbarvDAQA5EDbtKgxZJta+UZgyW/nW3lJdjuwpeMrhrQ9WqHKmhkzNVUFRsOwcAAE9Dhqjeu+/arvDAQA5U0fHS7ZOlriOlNTOktzpI2362XQUvqBAWopF9U7R2z1G998Mm2zkAALgeAzmQBQVJ1/xZ+v0cyUh6v7u08GWpmFcZ/c11zeJ1XbN4jfs2W9sOHredAwCAqzGQISVdId2/UGrWV5r3rPRRf+nobttVKGMj+6YoyBiNnJbF2cgAAJwDAxklIitLN34g9X2t5FaL8e2ldd/YrkIZqlU5Un/u2kjz1uzV16v22M4BAMC1GMj4P8ZIl99Z8gl8UfHSxzdKc4dKhZyh6y/ubp+sJjWiNXJalnLz+fhxAIALjBun9Q89ZLvCAwMZ/y2uiXTfPOmKe6Ulr0vvXScd2GC7CmUgNDhIYwY01+4jJzTum2zbOQAASK1aKbdBA9sVHhjIOLPQSKnXy9It/5QObZbe7iitmGS7CmWgdZ0qurVtbX2weLOydh62nQMACHTffqsqy5bZrvDAQMa5Ne0jDfpBqtFcmjxQmvyAlJ9ruwqX6IluTVQ5MlRDJ2eqqJg37AEALBo9WnU++sh2hQcGMs6vcpJ01wzp2iekFZ9KE66Vdq2wXYVLEFMhVMN6N9XybTn69OettnMAAHAVBjJKJzhE6jREumu6dPK49G5X6cfxfEy1D+vfqpba1Y/VC3PWaN/RfNs5AAC4BgMZF6Zuh5JbLup3keY8KX36W+nYAdtVuAjGGI3qn6r8gmKNmbnKdg4AAK7BQMaFqxgr3fqp1P0FacN86a320qaFtqtwEepXj9KgtPqasnynfli333YOAACuwEDGxTFGumqQdO+3UlhF6R99pPmjpSLO1vU1D6bVV53YCho+NVMnCops5wAAAs3bb2vto4/arvDAQMalSWgpDfxeavU7acGL0sReUs4221W4ABGhwRrdP1Wb9h/TW99z3jUAoJw1bqy82rVtV3hgIOPShUdJ/d+UfvOutCer5JaLVdNsV+ECdGhYXX1b1tSb323Qpv3HbOcAAALJ9OmKXbzYdoUHrw1kY8z7xpi9xpjM8zzuCmNMoTHmRm+1oJy0uEkatECqWk/6/A5pxqNSQZ7tKpTSsN5NFR4apGFTMuRwOgkAoLy8/LKSPv/cdoUHb76CPFFS93M9wBgTLOkFSV97sQPlqWo96Q9fS1c/JC19T3qni7R3je0qlEJcdIT+2q2xFq0/oGkrdtrOAQDAGq8NZMdxFkg6eJ6H/UnSl5L2eqsDFoSESd3GSLd9IeXukSakScsmcmayD/jdlXXUMqmyRs1YpcPHC2znAABghfHmH6UaY5IlzXAcJ/UM36sl6RNJnSS9f+pxX5zl9xkoaaAkxcfHt/7ss8+8lXxWubm5ioqKKvfn9XVh+QfVZM04VT20Qnurt1d2owdVGHrp/ztyPbxny5EijVx8Qp2SQnRnSnipf45r4i5cD3fhergL18NdWg0erKKiImW89lq5P3enTp2WOY7T5tdfDyn3kv8zTtITjuMUG2PO+UDHcSZImiBJbdq0cdLS0rxf9yvp6emy8bx+4br+0qJxips/WnEF26Qb3pOS2l7Sb8n18K4tZpU+WLxJf+rTVpfVrlKqn+GauAvXw124Hu7C9XCZypWVk5Pjqmti8xSLNpI+M8ZslnSjpDeNMf0t9sBbgoKkDo9Kf5hb8tfvd5cWviwVF9vtwlk9en0jxUdHaMjkTBUWcZ0AAF700UdaPWSI7QoP1gay4zh1HcdJdhwnWdIXkh50HGeKrR6Ug6QrpPsXSs36SvOelT7qLx3dbbsKZxAVHqKRfZtp9a4jmrh4s+0cAIA/S0pSflyc7QoP3jzm7VNJSyQ1NsZsN8bcY4wZZIwZ5K3nhA+IrCzd+IHU51Vp28/S+PbSum9tV+EMuqXUUOcmcXrlm2ztzOG4PgCAl0yapOrz59uu8ODNUyxudRwnwXGcUMdxEh3Hec9xnLccx3nrDI+9+2xv0IMfMkZqfZc0MF2KipM+vkGaO1QqPGm7DKcxxuiZvikqdhw9Mz3Ldg4AwF+NH69a09z1AWN8kh7siWsi3TdfuuJeacnr0vvXSwf4qGM3SapaQQ93aai5WXv07ao9tnMAACgXDGTYFRop9XpZuuWf0sGN0tsdpZXu+jSdQHdfh3pqFB+lp6dl6fjJQts5AAB4HQMZ7tC0jzRokRSfKn11nzT5ASk/13YVJIUGB2l0/+bakZOnv89bZzsHAACvYyDDPSonSXfPlDr+VVrxqTThWmnXCttVkNS2blXd3CZR7y3cpDW7j9jOAQDAqxjIcJfgEKnzUOmu6dLJY9K7XaUfx/Mx1S7wVI+mio4I0bDJmSou5noAAMrIF18o65lnbFd4YCDDnep2KLnlon5nac6T0qe3SscO2K4KaFUqhmlIz6ZauuWQ/rVsm+0cAIC/qFZNBTExtis8MJDhXhVjpVs/k7q/IG2YJ73VXtq00HZVQLuxdaLa1q2q52av0YHcfNs5AAB/MHGiasyZY7vCAwMZ7maMdNUg6d5vpbCK0j/6SPPHyBQX2S4LSMYYjemfqmP5hRo7a43tHACAP2AgAxcpoaU08Hup1e+kBX9TyxXDpBz+mN+GhvHRGtixnr78ZbuWbOC2FwCA/2Egw3eER0n935R+846icjdKb10jrZ5uuyogPdSpoZKqRmrYlAzlF/JqPgDAvzCQ4Xta3KxlrcdJVZKlSbdLMx6VCvJsVwWUyLBgPds3VRv2HdM7CzbazgEAoEwxkOGT8iokSPd8I139kLT0PemdLtJe7oktT52axKln8xp6bf56bTlwzHYOAABlhoEM3xUSJnUbI932hZS7R5qQJi37B2cml6MRvVMUGhykEVOz5PC/OwDgYsyapZXPP2+7wgMDGb6v4XXSA4ukpLbS9IelL34v5eXYrgoINWIi9Nj1jfR99j69uSJf36zawz3JAIALU6GCiiMibFd4CLEdAJSJ6BrSHVOkRf8jzR8j7Vgm3fC+lHSF7TK/d+fVydp68Lg+/3mz7vtwqaIjQnR9sxrq3SJB7RtUU1gI/x4OADiHN99UzexsKS3Ndsn/YiDDfwQFSR0ek5I7SF/cI73fTeo8TGo/uOR78IrgIKOn+6SofcW9Cq6Vopkrd2lu1m59+ct2xUSGqntKDfVqkaB29WMVEsx1AAD8yuefKy7HXX/yy0CG/0lqKw1aKE1/RJr3jLTpe2nABCk63naZXwsJMkprHKdOjeM0ZkCqFmbv18yMXZqZsUuTlm5T1Yph6pZSQ31aJOjKerEKDjK2kwEAOCMGMvxTZGXpponSL/+QZj8pjW8nDXhbatjVdllACA8JVtdm8eraLF4nCor0ffY+zVi5S1OX79CnP29Vtahw9WxeQ72aJ+iK5KoKYiwDAFyEgQz/ZYzU+m4p6aqSN+59fEPJsXBdni45AQPlIiI0WN1SaqhbSg3lnSzS/DV7NTNjpz5fuk0fLtmi+Erh6pGaoD4tE3RZUhXGMgDAOgYy/F9cE+m++dLcodKS16Uti6Qb3pNi69suCziRYcHq1SJBvVok6Fh+oeat2asZK3bqk5+3auLizaoZE6GezRPUu2VNtUyMkTGMZQBA+WMgIzCERkq9X5HqpUnTHpLevlbq/T9Si5tslwWsiuEh6tuypvq2rKmjJwr07eo9mrFil/6xZLPe/WGTEqtEqleLBPVpUVMpNSsxlgHAX6Wna3l6utJsd5yGgYzA0qyvVLOV9OV90lf3Shu/k3r8TQqPsl0W0KIjQjXgskQNuCxRh48X6OtVuzVj5S69t3CT3v5+o5JjK6hXiwT1blFTTWpEM5YBAF7FQEbgqVxbunum9P0L0oIXpW0/STd+ICW0sF0GSTEVQnVTmyTd1CZJh46d1NyskrE8Pn2D3vhug+pXr6heLWqqT4sENYyPtp0LALhUL72kpA0bOAcZsC44ROo8VKrbQfpqoPRuF+m6UdKV95e8uQ+uUKVimH7btrZ+27a29ufma07mbs1YuVOvzV+nV+etU+P46FOvLCeoXnX+FAAAfNKMGYrlHGTARep2lAYtkqY+KM15QtqYLvV7Q6oYa7sMv1ItKly3X1VHt19VR3uPntDsjJKx/Mo32Xrlm2w1Taik3qfGcp3YirZzAQA+jIEMVIyVbv1M+ukt6ZsR0lvtpd+8U/LqMlwpLjpCd7VL1l3tkrX78AnNzNilGSt36sW5a/Xi3LVqXitGvU+dlpFYpYLtXACAj+FzXwGp5LaKqx6Q7vlGCq0g/aOPNH+MVFRouwznUSMmQvdcU1eTH2yvH57opCE9m8gY6bnZa3TNC9+p/xuL9O7Cjdp1OM92KgDAR/AKMnC6mq2k+xdIs/4iLfibtHmhdMO7Ukyi7TKUQmKVChrYsb4GdqyvrQeOa0bGTs1cuUujZ67W6Jmr1aZOFfVukaCezRMUVynCdi4AQJIiI1WU564XMRjIwK+FR0kDxpecmTzzUWl8e6nf61LTPrbLcAFqx1bQg2kN9GBaA23cl6uZK3dpZsYujZy+Ss/MWKW2yVXVu2VN9UitoWpR4bZzASBwzZ6tDJedg8wtFsDZtLyl5NXkKsnSpNulmY9JBe76N1yUTr3qUfpTl4aaM7ijvvlzRz3cuaH25+Zr+JRMtR3zrW5790d9+vNWHTx20nYqAMAFeAUZOJfY+iX3Jc975tTHVC+RbvpAqt7YdhkuUsP4aP35umgN7tpQa/cc1YwVJW/we+qrDA2bkqn2Daqpd4sEdWtWQzEVQm3nAoD/GzVKdTZt4hxkwKeEhEndxpTccjF5UMnHVPd4Qbr8Ts5M9mHGGDWpUUlNalTSY9c3UtbOI5qxcpdmZuzUX79YqaHBGerQsLp6t0hQ12bxqhTBWAYAr5g3T1U4BxnwUQ2vkx5YVPLBItMfLjkzuc84KSLGdhkukTFGqbVilForRk90b6yV2w9rZsYuzVy5S/PX7FVYcJCubVwylrs0jVdUOP/oBAB/xj/lgQsRXUO6Y7K0aFzJMXA7lko3vC8lXWG7DGXEGKOWSZXVMqmynuzeRP/ZlnPqDX479c2qPQoPCVLnJnHq3aKmOjWprgph/GMUAPwN/2QHLlRQsNThMSm5g/TFPdIH3aXOw6R2j0hBvO/VnwQFGbWuU0Wt61TRsF5NtXTLIc1cuVMzM3ZrduZuRYYGq0vTOPVukaC0xnGKCA22nQwAKAMMZOBiJbWVBi2Upj8ifTuy5JaLAROk6HjbZfCCoCCjtnWrqm3dqhrRJ0U/bTqgmSt3aXbmbs1YuUsVw4J1XbN49Wrx/9i78/ioyrP/458zayYzk3WyTPY9BEjCEgIGSCYiggIVf25orXtd2keqrfWxi6191GKrba2tFbV1V3AtlU3qwgRkEZAlYUvYSUgCCRjIZCHb/P44ySQhKCiByXK9fZ0XcM6ZmTtMMN/cue/riiA3xYZRJ2FZCCHOSnAwzW1t3h5FNxKQhTgXpgC45hX48hX46CF4LgeufB6SL/H2yMR5pNUo5CTayEm08bvvDWPNXjUsf7StkgWby7H66Lh0aDjTM+yMT7Jh0MlPFoQQ4mu9/z7b+lgdZAnIQpwrRYGsWyFmHLx3G7x5FeTcCxf/Rq2AIQY0nVbDxOQQJiaH8OjM4azaXc2iwgqWbavk/Y1l+Jv0TBkWxvSMCHISg9FpJSwLIURfJwFZiN4SmgY//AyW/RJW/w32r4Kr/wVBCd4embhA9FoNjtRQHKmhPH7lcD7fpYblJUWVvLOhjCCzgSnDwpmRYWdsQjBajZQJFEIIfvEL4g8elDrIQgxYehNM/4taM/nDe2FurvrnjGu8PTJxgRl1WialhTEpLYzG5lYKSqpYVFjBfzYfYt66g9gsRi4bri7DGBMXhEbCshBisFqzBn+pgyzEIDD0CogYCe//ED64A/Yuh8v+CEaLt0cmvMBHr2XKsHCmDAunoamV5cVHWFRYzrtflvL62gOEWo1cnm5nRqadkdGBEpaFEMLLJCALcb4ExMAti6HgCVjxFJSug6tfAnuGt0cmvMhk0HJ5up3L0+3UnWzh051HWLSlnLfWHeSV1fuJ8Pfh8nQ70zMjyIzyR5FujUIIccFJQBbifNLq1BrJ8blqB75/ToJLH4PsO6VNtcBs1PG9zAi+lxlBbWMzn+w4zKItFby6Zj///HwfUYEmpmXYmZERwbAIPwnLQghxgUhAFuJCiM+Fuz+HBT+CpQ+qNZOveBZ8g7w9MtFHWH30XDkyiitHRnG8vpn/blfrK/9r5T6eL9hLXLAv0zLsTM+IYEi4VcKyEGLgiIripF7v7VF0IwFZiAvFbIMb3oYv5sLHv4HnxsNVL0LcBG+PTPQx/r56rsmK5pqsaL6qa2LZNjUsP+fcw7PL95AYYmZaRgQzMuwkh1m9PVwhhDg3b7zBDqeTvtRm4ot5KgAAIABJREFUSwKyEBeSosC4eyDmIrVm8qszIPfnkPuguhxDiFMEmg3Myo5hVnYM1a6TfLS1kkWF5fzts1088+kuUsIsTM+IYHqGnYQQ2QQqhBC9Qb4iC+ENESPgrgJY8nMo+APsW6nOJvtHeXtkog+zWYzcOC6WG8fFcqS2kaVFalj+88cl/PnjEtLsfkzPsDM9w+7toQohxNm77z6SysqkDrIQAjBa4cq5kJAPi3+qLrm44llIm+7tkYl+INTqw805cdycE0fl8UYWF1WwqLCcJ5cV8+SyYuL8NFyv7OHydDvRQb7eHq4QQny9zZuxSB1kIUQ3mddBVJa65OLt78OYO+DSx0Hv4+2RiX4i3N+H2yfEc/uEeMq+qmdJUQXzVpUwZ+lO5izdyYjoAKZn2JmWYcfub/L2cIUQos+TgCxEXxCcCLd/DJ/+Dtb8HQ6uVWsmh6R6e2Sin4kK9OXO3ERS2kpJSM9mUVE5iwsreGzxDh5bvIOs2ECmZ6h1mEP95JswIYQ4HY23ByCEaKczwJTH4YZ3obYCXnDAxtfA7fb2yEQ/FRPsy48cSSyePZHPfpbHzyan4DrZwiMLtzN2zqdc9/waXl97gGrXSW8PVQgh+hQJyEL0NSmXwt2r1GUXH96rLr1oPO7tUYl+LiHEwr2Tkvnovlw++Wkusy9Optp1kocXbCX78U/4/j/XMm/dQY7VNXl7qEKIwSYlhfqovrVJXZZYCNEX+dnhBwtg1dPw2eNw6Et1yUVUlrdHJgaApFAr90+2ct8lyRQfrmXRFnWD3y8+KOLXC7YyPsnG9HQ7U4aF4+/bt4r3CyEGoBdeoMTpJMLb4+hCArIQfZVGCxN/BrET4P074KUpatvqnJ+ARn74I86doigMCfdjSLgfP7s0hW3lJzzVMB58v5BfLShiYnII09LtTB4Whp+PhGUhxOAgAVmIvi5mLNy9EhbOhk8egb0FcOXzYO1LPYdEf6coCsMj/Rke6c+DU1IpLDvO4qIKFhdW8NnOIxg+0JCXGsL0DDuT0sKwGOXLhxCil9x5Jynl5VIHWQjxLZkC4JpX4ctX4KOHYO54tYZy0iXeHpkYgBRFITM6gMzoAB6aOoRNpTUsLqxgcVE5H28/jFGnIT81lOmZdi4eEoqvQb6UCCHOQUkJvlIHWQjxnSgKZN0KMePg3VvhjasgZzZc/LBaAUOI80CjURgdG8jo2EB+PS2NDQe+YnFhOYuLKvloWyUmvZZJaaFMz7DjSA3FR6/19pCFEOKcSUA+Cx/t/4jXjrzGsV3HyI3KxWayeXtIYjALTYM7l8OyX8LqZ2D/53D1vyAowdsjEwOcRqOQHR9EdnwQv5kxjC/2HWVxYQVLt1ayqLACs0HL5KFhTMuIIDfFhlEnYVkI0T9JQD4LDc0NlDWV8dvVv0VBIT0kHUeUA0e0g6SAJBRF8fYQxWCjN8H0v0CCQy0FNzcXZjwN6Vd7e2RikNBqFHISbeQk2vjd94axdu8xFhWW89G2ShZsLsdq1DF5WBgzMiIYn2TDoJONpUKI/kMC8lm4MvlKAsoCiMiMwFnqxFnq5JlNz/DMpmeItETiiFbD8uiw0eg1sstbXEBDr4CIkWqVi/dvhz3L4fI/gsHs7ZGJQUSn1TAh2caEZBuPzhzOqt3VLCqsYNm2Sj7YeAh/k54pw8KYnhFBTmIwOq2EZSFEFyNG4CorI8Db4+hCAvJZUhSF1KBUUoNSuSvzLqrqqygoK8BZ6uS9kvd4c8ebWPQWJkROwBHtYELkBPyN/t4ethgMAmLgliXgnAMr/wSlX6g1k+0Z3h6ZGIT0Wg2O1FAcqaE8fuVwPt+lhuUlRZW8s6GMILOBKcPCmZFhZ2xCMFqN/AROiEHv6afZ7XTSl1qFnLeArCjKS8B04Ijb7R5+muvfB/4XUIBa4B63273lfI2nt4X4hnB1ytVcnXI1DS0NrC1fi7PMSUFpAR/t/witomVU2CjyovLIj84nxi/G20MWA5lWB5Mehvhc+OBO+OckuPQxyL5T3dwnhBcYdVompYUxKS2MxuZWCkqqWFRYwX82H2LeuoPYLAYuG25neoadMXFBaCQsCyH6iPM5g/wK8Hfgta+5vg/Ic7vdXymKchnwAjD2PI7nvDHpTOTH5JMfk0+bu42t1VvVpRhlTp7a8BRPbXiKBP8E8qLVsJxhy0Crkc0r4jxIyIN7VsGCH8HSB2GvE654FnyDvD0yMcj56LVMGRbOlGHhNDS1srz4CIsKy3n3y1JeX3uAUKuRy9PtzMi0MzI6UMKyEIPJjTeSdvjw4KiD7Ha7VyiKEvcN11d3+eNa6FMz69+ZRtGQEZJBRkgGs0fN5pDrkGfd8uvbXuflrS8TaAxkYtRE8qPzyYnIwVfv6+1hi4HEbIMb3oa1z8HHv4HnxsNVL0LcBG+PTAgATAYtl6fbuTzdTt3JFj7deYRFW8p5a91BXlm9nwh/Hy5PtzM9M4LMKH/ZCC3EQFdWhrGP1UFW3G73+XtyNSAvOt0Si1PuewAY4na77/ia63cCdwKEhYWNnj9/fi+P9MxcLhcWi+WcnqOhrYEdDTsoaihie8N26tvq0aEj2SeZdN90hpuGE6gL7KURD2y98X4MBpbaPQzd/hSmhkoOxF7LgdhrcZ+nn17Ie9K39Mf3o6HFzaYjrXxR0cLW6lZa3WAzKWSH68gO1xLrp+m3Ybk/vh8DmbwffcuI++6jtbWVor/97YK/dn5+/pdutzvr1PNeD8iKouQD/wAmuN3uo2d6zqysLPeGDRt6bYxny+l04ujFqf+WthY2HdnkmV0+WHsQgLSgNBzRDvKi8xgaNLTffjE433r7/RjQTtbC4gegcD7E5Kizyf69/wMbeU/6lv7+fhxvaOa/29T6yqt2V9PS5iYu2JdpGXamZ0QwJNzar/7/2N/fj4FG3o8+xuGgpqaGgM2bL/hLK4py2oDs1SoWiqJkAP8ELjubcDyQ6DQ6xoSPYUz4GB7IeoB9J/bhLFU3+T1f+DzPbXmOUN9QHFFqWB5rH4tRa/T2sEV/ZLTC/3seEvNh8c/UJRcz/wFDpnl7ZEJ8LX+TnmuyorkmK5qv6ppYtq2SxUUVzC3Yy7PL95AQYmZ6RgQzMuwkh1m9PVwhxADjtYCsKEoM8AHwA7fbXeKtcfQFiqKQ4J9Agn8Ctw2/jWONx1hZtpKCsgIW7l3IOyXvYNKZyInIIS8qj9yoXIJNwd4etuhvMmdB1Bh471aYfwOM+aFa6ULv4+2RCfGNAs0GZmXHMCs7hqOukyzdWsniwgr+9tkunvl0FylhFqZnRDA9w05CiPzYXIh+56KLOH7w4OCog6woyjzAAdgURSkDfgvoAdxu91zgN0Aw8I/2H5O1nG6KezAK8gniiqQruCLpCk62nmR95XrPUoxPD36KgkJGSAaOaAf50fkk+Cf0qx81Ci8KToTbP4ZPfgdrn4WDa9SaySGp3h6ZEGcl2GLkxnGx3DguliO1jSwtqmRRYTl//riEP39cQprdj+kZaum42GBpmCNEvzBnDvucTmK9PY4uzmcVi+vPcP0O4LSb8kQno9bIhMgJTIicwK/G/orir4pZXrocZ6mTv278K3/d+FeiLFGebn6jwkZJNz/xzXRGmPp7tU31grvhBQdc9gcY+QOpmSz6lVCrDzfnxHFzThyVxxtZXFTB4sJynlxWzJPLikmP9Gd6hlotIzpIqgUJIc6edNLrRxRFYUjQEIYEDeGezHs4XHeYgrICCsoKeKf4Hd7Y8QZWvZUJURNwRDmYEDUBP4Oft4ct+qqUS+HuVfDvO+HDe9U21TOeBh/pACn6n3B/H26fEM/tE+Ip+6qeJUUVLC6sYM7SncxZupMR0QGesBwRYPL2cIUQXV11FcOqqmDFCm+PxEMCcj8WZg7j2tRruTb1Wuqb61lTsYaCUjUwL923FJ2iY3TYaPKi83BEO4i2Rnt7yKKv8bPDDxbA53+B5b+HQ1/C1S9D1Ghvj0yI7ywq0Jc7cxO5MzeRg0frWVRUzuLCCh5bvIPHFu8gKzaQaRl2pqXbCfWTNfhCeN3Ro+hPnPD2KLqRgDxA+Op9mRQziUkxk2hta6WouoiCsgKcpU7+uP6P/HH9H0kKSCIvSg3L6bZ06eYnVBot5D4AcRPh/dvhpUvh4ochZzZoNN4enRDnJCbYlx85kviRI4m9VS6WFFWwqLCC3y3czv8t2k52XBDTMyO4bHg4NotUChJCqCQgD0BajZYRoSMYETqCn4z6CaW1pRSUqmH51W2v8q+t/yLIJ4jcqFwc0Q4usl8k3fwExIyFu1fCh7Phk9+qbaqvfB6sYd4emRC9IiHEwv9cnMz/XJzM7iO1LNxSwaLCch5esJXf/mcrFyUGMy09gqnDwwkyG7w9XCGEF0lAHgSirdHcOPRGbhx6IyeaTrDq0CqWly7n04OfsmD3AgwaA2PtY9UGJVF5hJklEA1apkC49jX48hX46CGYO14NyUmTvD0yIXpVUqiV+ydbue+SZIoP17K4UJ1Z/uW/i3j4P1sZn2RjerqdKcPC8feVjc9CDDYSkAcZP4Mfl8VfxmXxl9Hc1symw5s8VTFWHlrJozzK0OChOKLUqhhDgoZICbnBRlEg61aIHgvv3QZv/D91ucXFD4NOZtXEwKIoCkPC/RgS7sdPJ6ewrfwEi4vUmeUH3y/kVwuKmJgcwrR0O5OHheHnI2FZiF43aRJf7ds3OOogi75Pr9GTbc8m257Ng2MeZO/xvZ56y89teY5/bPkHYb5hnhJy2eHZGLQSkAaNsKHww89g2S9h9TOw/3O1ZnJQvLdHJsR5oSgKwyP9GR7pz4NTUiksO95eOq6Cz3YewfCBhtyUEGZk2pmUFobFKF9ChegVDz/MAaeTvvTVRf51C0D9wpAYkEhiQCK3p9/O0YajrDy0Emepkw/3fMjbxW/jq/MlJyIHR7SDiVETCfIJ8vawxflm8FVLvyU4YOFsmDtR/XP61d4emRDnlaIoZEYHkBkdwC8uG8Km0hoWbalgSVEFn+w4jFGnIT81lOmZdi4eEoqvQb6cCjGQyL9ocVrBpmBmJs1kZtJMTraeZF3FOnV2uczJJwc/QaNoyAzJ9Mwux/vFy1KMgWzYTIgcBe/foVa62LscLvsjGKRTmRj4FEVhVEwgo2IC+fW0NL48+BWLtpSzuKiSj7ZVYtJruTgtlBkZdhypofjopUKQEN/KZZeRfuwYfPGFt0fiIQFZnJFRa2Ri1EQmRk3k1+5fs+PYDs9SjL98+Rf+8uVfiLHGeMLyyNCR6DTyqTXgBMTALUvAOQdW/gkOfqEuubBneHtkQlwwGo3CmLggxsQF8ZsZw1i37xiLCsv5aGsliwsrMBu0XDI0jOkZEeSm2DDqJCwLcUYNDWhPnvT2KLqRFCO+FUVRGBo8lKHBQ/nRiB9RWVeplpArczJv5zxe2/4afgY/JkZNxBHlYHzkeKwGq7eHLXqLVgeTHob4XPjgTvjnJLj0ccj+obdHJsQFp9UoXJQYzEWJwfzue8NYu7c9LG+r5D+by7EadUweFsaMjAha29zeHq4Q4luQgCzOSbg5nOuGXMd1Q66jvrme1eWrcZY6WVG2gsV7F6vd/MJHkx+dT15UHlHWKG8PWfSGhDy4ZxUsuAeW/hz2OgnwGQe1aWAJVSthCDGI6LQaJiTbmJBs49GZw1m1u5pFhRUs21bJBxsPYdJB3qEvyUsNwZEagt1f2l0L0ZdJQBa9xlfvyyWxl3BJ7CW0trVSWF3oWYrxxLoneGLdEyQFJJEfnY8j2sFw23A0inRq67fMNrjhHVj7HHz8G0a0LYYtD4OPP4QMAVuK+mtIqnr4RUlnPjEo6LUaHKmhOFJDefzK4awsqeb1zzazpayGj7ZVApAaZlXDckoIo+MCZSmGEH2MBGRxXmg1WkaGjmRk6EjuH30/B08c9Gzye2nrS7xY9CLBPsHkReeRF5XHOPs46ebXHykKXPQjyLiWLcveIDPCB6qLoaoYipfCptc779X7gi25Z3gOjFeXbggxABl16ppk3REjeXl57Driwll8hIKSKl5etY8XVuzF16AlJ9HmCczRQfL/QjHITJ/O0T17pA6yGHxi/GK4adhN3DTsJo6fPM7nhz7HWerkv/v/ywe7PsCoNTLOPs4TmEN9Q709ZPFtmG18FTQCxjm6n6872h6Yd0JVifr7/Z9D4dud92j0EJwEISndw3NwEuh9LuiHIcT5pCgKKWFWUsKs3JmbSN3JFtbsOYqz5AjO4io+2XEYgIQQM46UUPJSQxgbHyRVMcTA98ADlDqdJHp7HF1IQBYXnL/Rn2kJ05iWMI3m1ma+PPIlBaUFLC9dTkFZAQDDg4eTF51HfnQ+KYEpUkKuvzIHgzkHYnO6n288AdW7uofnyiLYsRDcbeo9igYC48CW2iU8t//eKBs/Rf9nNuq4ZGgYlwwNw+12s7e6joLiKpwlVbzxxQFeWrUPH72GcQnBOFJCyEsNJd4mpRWFuBAkIAuv0mv1jLOPY5x9HA+OeZDdNbspKFPD8j82/4NnNz+L3WwnL0oNy1nhWdLNbyDw8YOo0erRVXMjHN2thubqks7wvPsTaGvuvM8vUl2ecWp4Ngdf2I9DiF6iKAqJIRYSQyzcNiGehqZW1u47SkFxFQUlVTyycDss3E5ssG97WA7hogQbJoPMLosBwOFgRE0NbN7s7ZF4SEAWfYaiKCQHJpMcmMwd6XdQ3VDNyrKVLC9dzoLdC5hfPB+z3kxORA7hrnBGNI4gwKcvrVgS50zvA+HD1aOr1hb4ap+6ttkTnoth46vQXN95n6+tc1Ng1/BstUtlDdGvmAxa8lNDyU9Vl5sdOFpHQUkVzuIq3t5QyqtrDmDQaRgbH0ReiloZIzHEIj9tE6KXSEAWfZbNZOPK5Cu5MvlKGlsaWVe5Tl2GUVpAVUMVb77zJiNCRqgl5KLziPfvS13cRa/S6tQNfrZkSJveeb6tDU6UtQfnLuF56/vQeLzzPqNf+9rmU8JzQCxoZAZO9H2xwWZuusjMTRfF0djcyvr9x3C2zy4/tngHjy3eQWSAybPRLyfJhsUoX+KF+K7kX4/oF3x0PuRG5ZIblUvbuDbe+O8bnAg9QUFZAX/68k/86cs/EecXR15UHo5oByNCR0g3v8FAo1E7/AXEQPLkzvNuN7iOdFbUqCpWf7/7E9j8Zud9Oh8ITu4Mzh3hOSgBdLKUR/RNPnotE5NDmJgcwsNA6bF6VuxSZ5f/s+kQb31xEL1WISs2CEequhwjNcwqs8tCfAuSIES/o1E0xBhjcIx08D8j/4cKVwUFZQU4S528ufNNXt3+Kv5GfyZGTsQR7WB8xHgsBou3hy0uJEUBa5h6xOd2v9bwVWdFjY7wXLYOtr7XeY9Gp4bkU2s5ByeDQUpwib4lOsiX74+N5ftjY2lqaWPDgWMUlFRRUFzFnKU7mbN0J+F+Pp6lGOOTbfj56L09bCH6NAnIot+zW+zMGjKLWUNmUddc162b36K9i9BpdIwJG4Mj2oEj2kGEJcLbQxbeZAqEmLHq0VVTnVpZo2O2uapLPWd3a/tNCgRE96zlbEsBk6yHF95n0GnISbSRk2jjF5elUXm8kYL2MnJLiip4e0MpWo3C6JhA8lJDyEsJYajdD41GZpeFF117LUdKSqQOshDni1lvZnLsZCbHTqa1rZUtVVtwljpZXrqcOevmMGfdHFICU9SwHOVgmG2YdPMTKoMZIkaoR1ctTXBsT/elGlXFsLcAWk923mcJ71nLOSQVzCGyQVB4Tbi/D9eNieG6MTE0t7ax6WCNJzA/uayYJ5cVY7MYyWuvjJGbbCPAV5YXiQvsRz+i3Okkxdvj6EICshiwtBoto8JGMSpsFD/N+in7j+/3lJD7Z9E/eaHwBWwmm2fd8lj7WEw6k7eHLfoanQFC09Sjq7ZWqDnQZYNge3je/BY0uTrv8wloD8tdazmngn+UBGdxQem1GrLjg8iOD+LnU4ZwpLaRFSXVFJRU8enOw7y/sQyNAiOiA8hLCcWRGkJ6pL/MLovzr74eTWOjt0fRjQRkMWjE+ccR5x/HzcNupqaxhpWHVuIsdfLR/o94f9f7+Gh9GGcfhyPaQV50HjaTzdtDFn2ZRquuUw5KgNTLOs+73XCivPsyjapi2LkYNr7WeZ/e3Nl6u2t4DoyT1tviggi1+nD16CiuHh1Fa5ubLWU1nsoYT39awl8+KSHIbCA32dY+uxxCsMXo7WGLgejyy8moqYGpU709Eg/5v7AYlAJ8ApiROIMZiTNobm1m/eH1FJSqG/2cZU5YA+m2dM+65eSAZNkBLs6OooB/pHokXtz9Wl11zzXO+1ZA4fzOe7QGtc22Z5lGl9bbOgkn4vzQahRGxQQyKiaQn05O4ajrJCt3qbPLK0qqWLC5HEWB9Eh/T1e/EdEBaGV2WQxQEpDFoKfX6smJyCEnIoeHsh9iV80uNSiXOvnbpr/xt01/I8Ic4QnLWWFZ6LWyA1x8B2abesSN736+o/V21c7O8FyxBbb/B3Cr9ygaCIzv3BTYEZ5t0npb9L5gi5GZIyOZOTKStjY3W8uPe2aX/758N898tht/k56JyTbP+uVQq4+3hy1Er5GALEQXiqKQEphCSmAKd2bcSVV9FSvKVuAsdfLBrg94a+dbWPQWxkeOJy8qj9yoXPyN/t4etujvvrb1dkN76+1TNgju+viU1ttRXeo4q+FZ13ziwn4MYsDSaBQyogLIiApg9qRkauqb+Hx3tScwLyqsAGCo3U+tu5wSwqjYQPRa2QAt+i8JyEJ8gxDfEK5KuYqrUq6ioaWBLyq+wFnqpKCsgGX7l6FVtIwMHemZXY71i/X2kMVAojdBeLp6dNXaDMf29WyEsmE1tDQAMAFgc0jnpsCuM8/WcNkgKL6zAF8D0zMimJ4RgdvtZnvFCU9Yfn7FXv7h3IPVqGN8ks3TqMTuLxugRf8iAVmIs2TSmTxBuM3dxrbqbWrr67ICntrwFE9teIp4/3gcUeo9mSGZaKWNsTgftPr2tckpkDaj83xbGxwvhapidn+xhCS/FjU8b32vZ+ttT8vtLuE5IFbtTijEWVIUhWER/gyL8OfH+UmcaGxmdZfZ5Y+2VQKQGmb1tMHOigvCoJPPM9HFLbdQuXOn1EEWor/TKBrSQ9JJD0ln9qjZHHId8mzye33H67y87WUCjAHkRuXiiHaQE5GDWW/29rDFQKfRQGAsBMZSVm4gyeFQz7vd4Dp8mqUa/4XNb3Q+XmcCW1KXcnTtM85BCWooF+IM/Hz0TB1uZ+pwO263m5LDLk/d5ZdX7eOFFXvxNWjJSbR5lmNEB0l3ykHvlluodDoZ4u1xdCEBWYheEGmJ5Ia0G7gh7QZcTS5Wla/ybPT7cM+H6DV6ssOzPTPQ4eZwbw9ZDCaKoi6rsIZDQl73a/XHoLqke3g++AUUvdt5j0YHQYmn1HJOkdbb4hspikJquJXUcCt35iZSd7KF1XuOegLzJzsOA5AYYvbUXc6OD8JHLz95G3Sqq9EfP37m+y4gCchC9DKLwcKUuClMiZtCS1sLm49s9pSPe/yLx3n8i8cZEjSEvKg88qPzSQtOk25+wnt8gyBmnHp0ddIFR3d1r+V8ZAfsXHJK6+2Y0zRCSQEf2bwqujMbdUweGsbkoWG43W72VtfhLK7CWXyEN744wEur9uGj13BRQjB5KSE4UkOJs8lP3gaFq69mWE0NXHGFt0fiIQFZiPNIp9GRFZ5FVngWD4x5gH3H93lmll8sepHnC58n1BRKbnQu+dH5ZIdn46OTUkmiDzBaIGKkenTVchKO7jllg2AJ7HV2b71ttfes5WxLVcvcyQbBQU9RFBJDLCSGWLh9QjwNTa2s3XuUghI1MC8vroKF24kL9vWE5XEJwZgMMrssLgwJyEJcQPH+8cT7x3Pr8Fv5qvErTze/JXuX8F7Je5h0JsbZx5Efnc/EqInSzU/0PTojhA1Vj67aWuGr/V3WOJeodZ03v9m99bYpsD0snxKe/SIlOA9iJoOW/CGh5A8JBYaxv7qOghJ1o9/bG0p5dc0BDDoNY+ODPIE5McQsDZzEeSMBWQgvCfQJ5HuJ3+N7id+jqbWJ9ZXrPUsxlpcuR0EhPSSd/Oh8HFEOEgMS5YuB6Ls0WghOVA8u7zzvdsOJQ6dsECyBHQth46ud9xksna23PeG5vfW2VIMZdOJsZuJsZm7OiaOxuZV1+455ZpcfW7yDxxbvICrQpDYpSQkhJ8mGxSiRRvQe+WwSog8waA2MjxzP+Mjx/NL9S0q+KmF56XKcpU7+uvGv/HXjX4m0RJIfnU9edB6jw0aj10hVAdEPKAr4R6lH0qTu1+qq1VnmruF5bwFsmdd5j9aottn2LNPoaL2dKK23BwkfvZbclBByU0J4ePpQSo/Ve2aXF2w6xJtfHESvVciKDcKRqs4up4RZZEJBnBMJyEL0MYqikBqUSmpQKndn3s2R+iMUlKkl5N4teZc3dryBVW9lQuQE8qLzmBA5Qbr5if7JbAPzBIib0P184/HO1tsd4bl8E2xbQGfrbS0ExXcvR2fraL1tueAfirhwooN8uXFcLDeOi6WppY0NB45R0F53ec7SncxZuhO7v49ndnl8sg0/H5lQ6NPuuYdD27ZJHWQhxNkL9Q3lmpRruCblGuqb61lbsdYTmJfuX4pW0TIqbBSOKAf50flE+0V7e8hCnBsff4jKUo+umhvU4Fxd0j0871oGbS2d9/lHd2mE0iU8+wZd2I9DnHcGnYacRBs5iTZ+cXkaFccbPGF5cWEF89eXotUojI4JJK+97vKwCD9vD1uc6rrrqHI6vT2KbiQgC9GP+Op9uTjmYi6OuZg2dxtF1UUUlBawvHQ5T254kic3PEmCfwKOaDUsp9vSpZufGDj0JrBnqEdXHa23q3Z2r66xf5Wn9TYA5tAunQO7hGdLmGwQHCDs/iZxX5wbAAAgAElEQVRmZccwKzuG5tY2Nh2swVl8hIKSKp5cVsyTy4oJsRpJsbZSG1jOxGQbAb4Gbw9blJZiPHLE26PoRgKyEP2URtGQGZJJZkgms0fNpqy2jIIyNSy/tu01Xtr6EoHGQHKj1BJyF0VchK9emjqIAahr6+2u2trg+MHOihod4bnwXTjZtfW2f3twTmkPzu3VNfxjpPV2P6bXasiODyI7PogHpw7hyIlGVuyqVsvI7ahg1bxNaBQYER2AIzWUvJQQ0iP90Wjkm6UL7gc/IK2mBq691tsj8VDcbre3x/CtZGVluTds2NDtXHNzM2VlZTQ2Np63121sbMTHR+rT9hX95f3w8fEhKioKvf7Crn870XSC1YdWs7x0OSsPraS2qRaDxkC2PRtHlIO86Lxe7+bndDpxdLQ2Fl4n78c3cLuhtrJ7ObqOZRt1VZ336UydlTW6hueg+G/delvej77ls+XL8U8YoW72Kz5C4aHjuN0QZDaQm2zDkRrKxGQbwRbZCHpBOBzU1NQQsHnzBX9pRVG+dLvdWaeeHxAzyGVlZVitVuLi4s7brtXa2lqsVut5eW7x7fWH98PtdnP06FHKysqIj4+/oK/tZ/BjavxUpsZPpbmtmc1HNnuqYjz2xWM89sVjpAWleVpfpwWlyY5vMXgoCvjZ1SPB0f2ap/X2zs7wfHANFL3TeY9Gr1bR6FqOzpaihmm96UJ+JOI70igKo2MDGR0byE8np3DUdZKV7bPLK3ZVs2BzOYoCGZH+6ma/1FBGRAegldnlQWNABOTGxsbzGo6F+C4URSE4OJiqqqoz33we6TV6xoSPYUz4GH6e9XP2Hd/H8tLlFJQVMHfLXJ7b8hyhvqE4otSwnG3PxqiVWRMxSH1T6+3qku7h+fA22LkI3G3tNykQGNuzlrMtpcfLiL4l2GJk5shIZo6MpK3NTdGh4566y39fvptnPtuNv0nPxPbZ5dwUG6HWvv9TTPHdDYiADEg4Fn1SX/u8VBSFhIAEEgISuD39do41HmNF2QoKSgtYuHch75S8g0lnIiciB0e0g9yoXIJ8ZOe/EBgtEDlKPbpqOQlHd3e23O4Iz3s+g9Ymz23jjDaoGAsRI8CeqR7W3l3mJHqHRqOQGR1AZnQAsyclU1PfxMpd1Z7ay4sKKwAYFuHn6eo3MiYAvVbWqw8kAyYgCyG+vSCfIGYmzWRm0kxOtp70dPNbXrqcTw9+ioJCZkimZylGgn9Cnwv9QniVzghhw9Sjq9YWqDngKUd3vGg5PtUlULwETy1nS1hnWLa3B2f/KKmo0ccE+BqYkRnBjMwI2trcbK840b52uYrnV+zlH849WH10TEiytS/HCMHuL0ttvpWf/YzSoiKpgzzQHD16lEmT1A5RlZWVaLVaQkJCAFi3bh0Gw9eXkNmwYQOvvfYazzzzzDe+Rk5ODqtXrz7nsTqdTp566ikWLVp0zs8lBhaj1siEyAlMiJzAr8b+ip3HdnrC8tMbn+bpjU8TbY0mLyqP/Oh8RoaNlG5+Qnwdra6z9faQaexoHUWYwwEna6FyK1Rshoot6rH7k85lGqagztDcMdscGC+huY/QaBSGR/ozPNKfH+cncaKxmVXts8vO4iqWbq0EIDXMiiNVDctZsUEYdDK7/I1mzOBoH9tXJAG5FwQHB7O5feflI488gsVi4YEHHvBcb2lpQac7/V91VlYWWVk9Nk/20BvhWIizpSgKacFppAWncc+Ie6isq2RF2QqcpU7eKX5H7eZnULv55UfnMz5yvLeHLET/YLRC7EXq0aGpHo5sV0NzeXtwXvMstDW3P8a/vf5zl9nm4ESQGude5+ej57J0O5el23G73ZQcdnnqLr+0ah/Pr9iL2aAlp3122ZEaQlSglNvsobgY08GD3h5FNwMuIP9u4Ta2l5/o1eccGuHHTx0x3+oxt9xyCz4+PmzatInx48cza9YsfvKTn9DY2IjJZOLll18mNTW124zuI488wsGDB9m7dy8HDx7kvvvuY/bs2QBYLBZcLhdOp5NHHnkEm83G1q1bGT16NG+88QaKorBkyRJ++tOfYjabGT9+PHv37j3rmeJ58+bx+9//HrfbzbRp0/jDH/5Aa2srt99+Oxs2bEBRFG677Tbuv/9+nnnmGebOnYtOp2Po0KHMnz//W/+div4l3BzOtanXcm3qtdQ317OmfA3OMicrylawdJ/azc9X8SX438FY9BbMBjNWvRWLwYJFb/H8ajVYMeu7XOu4rrdg0plk+YYYnAy+PTsHtpyEIzvaZ5nbQ/O6F6H1pHpdb4bw9O6zzbZUdeZaeIWiKKSGW0kNt3JXXiKuky2s2XMUZ/ERnMVVfLz9MACJIWZP3eXs+CB89PKNDnfdRWpNDdx0k7dH4iH/ks6jsrIyVq9ejVar5cSJE6xcuRKdTscnn3zCL3/5S95///0ej9m5cyfLly+ntraW1NRU7rnnnh41dDdt2sS2bduIiIhg/PjxrFq1iqysLO666y5WrFhBfHw8119//VmPs7y8nP/93//lyy+/JDAwkEsvvZQFCxYQHR3NoUOH2Lp1KwA1NTUAPPHEE+zbtw+j0eg5JwYPX70vk2InMSl2Eq1trRRVF7GqfBVFe4qwBlmpba7F1eTiaMNRaptqcTW7qGuuO+PzahWtGp4N1m6humuIthgspw3eXe+RzoFiQNAZ1dAbMQK4WT3X2qxuBOxYmlG+GTa9Aeueb3+Mj7oW2jPTnAmhQ9XnEhecxahj8tAwJg8Nw+12s6eqzjO7/PraA/zr83346DVclBCMIzUUR2oIscFmbw9btBtwAfm3M4ad+abvoLa29ls/5pprrkGrVb9YHz9+nJtvvpldu3ahKArNzc2nfcy0adMwGo0YjUZCQ0M5fPgwUVFR3e7Jzs72nBsxYgT79+/HYrGQkJDgqbd7/fXX88ILL5zVONevX4/D4fCsm/7+97/PihUrePjhh9m7dy/33nsv06ZN49JLLwUgIyOD73//+8ycOZOZM2d+678XMXBoNVpGhI5gROgInDVOHHmO097X2tZKfUs9riYXtc211DXXqeG5yYWruf1oclHb1H6tPWQfrj/Mnpo9nust7pYzjslX59sZmrsE547g3XV226q3dvtzR0CXMneiT9LqOzcEjrhBPdfWCkf3dJ9pLnoPNrykXtfoITSt+/KM8OFSr/kCUxSFpFALSaEW7piYQH1TC1/sPabOLpdUsbx4GwBxwb6e2eVxCcGYDPINv7cMuIDcl5jNnd8JPvzww+Tn5/Pvf/+b/fv3f21HJaOx8wuzVqulpaVnIDibe3pDYGAgW7ZsYdmyZcydO5d33nmHl156icWLF7NixQoWLlzI448/TlFR0deusRYC1CBtNVixGqzYsX+n53C73TS2Np51uO56vtxVTl1zHa5mFw0tDWd8Lb1G71kO0m1pyGlmt63601/z1fuiUWRjjjjPNNrONtsZ16jn2tqgZn/nTHPFFti5GDa9rl5XtGp95q4zzeHp6vpocUH4GnTkDwklf0goAPur6zx1l+evP8grq/dj0GkYGx/kCcyJIWZZhnYBSaq5QI4fP05kZCQAr7zySq8/f2pqKnv37mX//v3ExcXx9ttvn/Vjs7OzmT17NtXV1QQGBjJv3jzuvfdeqqurMRgMXHXVVaSmpnLjjTfS1tZGaWkp+fn5TJgwgfnz5+NyuQgI6EvFWcRApCgKJp0Jk86EzWT7zs/T3NZMfXO9Z/mHJ1i3/7lbuG6f7XY1uShzlXmu1TXX0eZpDvE140XxzFh3ncH+ptntHsHbYJZKIeLb02ggKEE9hl2pnnO74XhZ99C85zPYMq/9QQoEJ3UPzfYMMAV67cMYTOJsZuJsZm7OiaOxuZV1+47hLK6ioOQIjy7azqNAVKDJU3c5JzEYs1Ei3Pkkf7sXyIMPPsjNN9/MY489xrRp03r9+U0mE//4xz+YOnUqZrOZMWPGfO29n376abdlG++++y5PPPEE+fn5nk16V1xxBVu2bOHWW2+lrU0NAnPmzKG1tZUbb7yR48eP43a7mT17toRj0a/oNXr8jf74G/2/83O43W7PkpGOkN11Nvt0S0nqmus42niUAycOeO5rams642v5aH16rsM+i9ntyuZKquqrMOvNsgFSqGXiAqLVI2165/nayu6hufQL2Ppe5/WA2O7NTewjwPzdv0EVZ+aj15KbEkJuSggwlNJj9Z4ycv/edIg3vziIXqswJi5ILSWXEkpKmKV//xv/9a85sGVLn6qDrLjdbm+P4VvJyspyb9iwodu5HTt2kJaWdl5ft7a2Fmsfq9F3KpfLhcViwe128+Mf/5jk5GTuv/9+bw/rvOgP70eHC/H52Rc4nc6vXTokTq+ptann0pAuS0fONLvtanJR31J/xtfRKTrPTPapmx27hesuAdyi73LNYMGsM8sGyHPQr/591FV3D80VW+CrfZ3X/SI7G5t07QrYjwJav3o/umhqaWPD/mOewFx8WN0fZff38ZSRy0my4efT/37y5K33RFGUL91ud496uzKDPIC8+OKLvPrqqzQ1NTFy5Ejuuusubw9JCPENDFoDwaZggk3B3/k5WttaqWup6xaeXc0u1m1ZR0xSzNeG68r6SmprOq+1ulvP+Fq+Ot/Tbm483ez215X5M2i/vnGS6CPMNkiapB4dGmqgsrB7aO7aFdAc2r25iT0T/KP7VWjuDww6DTlJNnKSbPzi8jTKaxpY0R6WFxdWMH99KTqNwqjYQE9gHmr36/uzy5s3Y9m9G/rQNy0SkAeQ+++/f8DOGAshTk+r0eJn8MPP4NftfNvuNhypjrN6jo4NkJ6lIU09Nzt2ndHu+P3xk8c55DrkuaextfGMr9WxAfJ0Jfy+sZRflwDuq/Pt+1/wBxpTAMTnqkeHky6oLOq5rrnjmy1TYPc22h1dATWyebW3RASYmJUdw6zsGJpb29h44CvP7PKTy4p5clkxIVajJyxPTArB37cPzi7fdx9JNTVwxx3eHomHBGQhhBjkum6ADCHkOz9Pc1vzGcP16a4drD3o2Qzpanbh5puX/nVsgDx1s+O3md2WDZC9wGjp2RWwuQEOb4eKTZ2huVtXQD8Iz+g+2xycJF0Be4Feq2FsQjBjE4J5cOoQjpxopKCkioIStUnJe1+WoVFgZEzn7PLwCH80Gvlm83QkIAshhOgVeo2eAJ8AAny++1abNncbDS0Npy/l9w2z29UN1ew/sd+zZru57fS15rvqugHya0v5dbl2utltH62PzGZ3pTdB1Gj16NDSBFU7OttoV2yBDf+ClvafOOh9u3QFbA/NIalq3WfxnYX6+XBNVjTXZEXT0trGlrIaCoqrcJZU8eePS/jzxyUEmw3kpoSQ174pMMgsS6A6SEAWQgjRZ2gUDWa9GbPeDOfQVOxk68meJfvOYnb7SP0Rz7VvuwGyRym/Lr8ecR3BVGEiwhJBuDl8cM1e6wydSyw6tLZ06QrYHpw3vQnr2htcaY3duwJGjJCugOdAp9UwOjaI0bFB/PTSVKpdJ1m5q4qCYnWG+d+bDqEokBHpT1573eUR0QFoB/HssgRkIYQQA45Ra8RoMp7zBsiOutffVCf71GsVdRW4alw9NkC+9d+3APWbgFDfUCItkURaIomwRBBhjvD8Ptwcjk4zwL88a3UQNlQ9RlyvnmtrhWN729toty/R2PoBfPmyel2j69IVcIR6hA0Dg6/3Po5+ymYxcuXIKK4cGUVrm5uth47jLK7CWXKEv3+2i2c+3UWAr56JyR2zyzZCrT7eHvYFdd7+BSqK8hIwHTjidruHn+a6AvwVuByoB25xu90bz9d4zqf8/HweeughpkyZ4jn39NNPU1xczHPPPXfaxzgcDp566imysrK4/PLLeeutt3rUE37kkUewWCw88MADX/vaCxYsICUlhaFDhwLwm9/8htzcXC655JJz+picTidPPfUUixYtOqfnEUKI/kqr0fZKzeyGlgaWOJcQMzyGQ65DHHIdotxVziHXIdZVruNw3eFu6661ipYw3zA1OFs6g3NHoA71DR2YAVqjBVuyeqRfrZ5zu+Gr/V02Am6G4qWw6Q31uqIBW2rPBifSFfCsaTUKmdEBZEYH8JNLkvmqromVu6s9s8sLt5QDMCzCz1N3eVRMADptL262/P3v2btxI6N67xnP2fn8F/YK8Hfgta+5fhmQ3H6MBZ5r/7Xfuf7665k/f363gDx//nz++Mc/ntXjlyxZ8p1fe8GCBUyfPt0TkP/v//7vOz+XEEKI3qUoCr56X2x6G9n27NPe09zaTGVdJYfqOoNzx69rK9ZSVV/VI0CHm8M7Z56t7TPR7bPQob6hA6dmtaJAULx6DJupnnO74cShztBcvhn2OqFwfufjenQFzJSugGcp0Gzge5kRfC8zgrY2N9srTqib/YqrmFuwl2eX78Hqo2NCks0TmMP9z3F2OSeHE01nbpx0IZ23gOx2u1coihL3DbdcAbzmVjuVrFUUJUBRFLvb7a44pxde+pBadqY3hafDhF997eWrr76aX//61zQ1NWEwGNi/fz/l5eVMnDiRe+65h/Xr19PQ0MDVV1/N7373ux6Pj4uLY8OGDdhsNh5//HFeffVVQkNDiY6OZvRodaPDiy++yAsvvEBTUxNJSUm8/vrrbN68mQ8//JCCggIee+wx3n//fR599FGmT5/O1VdfzaeffsoDDzxAS0sLY8aM4bnnnsNoNBIXF8fNN9/MwoULaW5u5t1332XIkCFn9Vcxb948fv/733s67v3hD3+gtbWV22+/nQ0bNqAoCrfddhv3338/zzzzDHPnzkWn0zF06FDmz59/5hcQQohBRq/VE+0XTbRf9GmvN7U2qQG6S3Du+P2a8jUcaTjS7X6doiPMHEaUJeq0s9AhppD+HaAVBfyj1GNIl860tZVQUdg501y6Dra+33k9INYTloOOasA1DCzfvWrLYKDRKAyP9Gd4pD8/zk/ieEMzq3dXt7fBrmLp1koAhoRbyUtVl2NkxQZh0H3L2eXVq/HbulXqILeLBEq7/Lms/VyPgKwoyp3AnQBhYWE4nc5u1/39/amtVbvJGJub0LS29OpA25qbaG1t9bzGqfR6PaNGjeKDDz5g2rRpvPrqq8ycOROXy8VDDz1EUFAQra2tzJgxg6lTpzJ8+HBaW1upq6ujtrYWt9uNy+Vi+/btvPXWW6xcuZKWlhYmTpzI8OHDqa2tZfLkycyaNQtQZ4mfffZZ7r77bi677DKmTp3KzJnqd9bNzc00NDRQVVXFzTffzIcffkhycjJ33nknf/nLX/jxj3+M2+3GYrFQUFDAiy++yJw5c/j73//e7WOqr6+npaWl28dcUVHBgw8+yIoVKwgICGDmzJnMmzePyMhIDh48yJo1awCoqamhtraWOXPmUFRUhNFo9JzrLd/0fvQ1jY2NPT5nByKXyzUoPs7+Qt6PvqW33o/g9v8yyABfwBea3c181fIVx1qOcbTlKEdbjnKs5RhHvjrCjqodnGg90e05tGgJ1AUSpAsiWBdMkFb9NVgXTJAuCD+tHxqlv9YqNgBjIHwMhIO+6QQW114srj1Ya/dg3bcO044PyQAo+h2NxmBclkRqrYm4LAnUWhNpMgRJg5NvYAIus8HUYA1lLhNF1S0UVdXxzxW1PF+wFx8tpAVrybBpSQ/RYjOd+XNpxH33EdvainN4jxW5XtMvFjG53e4XgBdAbTV9aivCHTt2dLYd/t6fz8sYTp6htfEPfvAD/vOf/zBr1iz+/e9/869//Qur1cqbb77JCy+8QEtLCxUVFRw4cICLLroIrVaL2WzGarWiKAoWi4WNGzdy1VVXERYWBsDMmTMxGo1YrVY2btzID37wA2pqanC5XEyZMgWr1Yper8dkMnnG1vHn8vJyEhISGDVKXdFzxx138Oyzz/LQQw+hKAo33HADVquV8ePHs2TJkh4fm6+vLzqdrtv5zz77jPz8fOLj4wG46aabWL9+PVOnTuXAgQP88pe/ZNq0aVx66aVoNBoyMzO5++67mTlzJjNnzsRisfTa+9GfWk37+PgwcuRIbw/jvOuvrVsHKnk/+hZvvh8nW09S7irvtnyj4/clrhKOuo52u1+v0WM327ute+46C20z2fpxgAYaatj80euMCFPwqdiCT8UWbPvn06MrYNcjIEZC8xm4Traos8vtyzFe3d4AQFKoxVN3eUxcED760/z0IiCAmpqaPvX/LG8G5ENA158nRbWf65euuOIK7r//fjZu3Eh9fT2jR49m3759PPXUU6xfv57AwEBuueUWGhvP3GnqdG655RYWLFhAZmYmr7zyyjnPRBiNaqkcrVZLS8u5zbgHBgayZcsWli1bxty5c3nnnXd46aWXWLx4MStWrGDhwoU8/vjjFBUVodP1i+/JhBBiwDBqjcT7xxPvH3/a6w0tDVTUVXQLzh1Bennpco41Hut2v16j77FsI8KsLuWIskYR7BPct2tDmwKoCUyHHEfnuZMuOLz1LLoCZnZW0ZCugN1YjDouHRbOpcPCcbvd7KlyeZZivL7mAP/6fB8mvZaLEoM9gTk2+BxqOZ5n3kwrHwL/oyjKfNTNecfPef2xF1ksFvLz87ntttu4/nq1ZM2JEycwm834+/tz+PBhli5d+o3fHeXm5nLLLbfwi1/8gpaWFhYuXMhdd90FqDOmdrud5uZm3nzzTSIjIwGwWq2nXWqQmprK/v372b17t2fNcl5e3jl9jNnZ2cyePZvq6moCAwOZN28e9957L9XV1RgMBq666ipSU1O58cYbaWtro7S0lPz8fCZMmMD8+fNxuVw9KnUIIYTwLpPORIJ/Agn+Cae93tDSQIWrokcFjnJXOTuP7ewRoI1aI3azvbOE3Skz0X0yQBstEDNOPTp4ugJ2aXCy9jlobd9M1rUrYMdhS5augKibU5NCrSSFWrljYgL1TS2s3XvU06jks53quvm4YF8cqaH8pL4Z9zc30LzgzmeZt3mAA7ApilIG/BbQA7jd7rnAEtQSb7tRy7zder7GcqFcf/31XHnllZ7NaJmZmYwcOZIhQ4YQHR3N+PHjv/Hxo0aN4rrrriMzM5PQ0FDGjBnjufboo48yduxYQkJCGDt2rCcUz5o1ix/+8Ic888wzvPfee577fXx8ePnll7nmmms8m/Tuvvvub/XxfPrpp0RFRXn+/O677/LEE0+Qn5/v2aR3xRVXsGXLFm699Vba2toAmDNnDq2trdx4440cP34ct9vN7NmzJRwLIUQ/ZNKZSAhIICHg9AG6vrlenX2uK+8RoLcd3UbNyZpu9xu1xs7gbI4k0hrp+X2EJYIgn6C+EaC/qStg15nmDS9BS0P7Y3whbHhnG217JoQMGfRdAX0NOi4eEsbFQ9QlpPuq6ygoPoKzpIp56w4ytfIEkRYNQV4eZ1eKu69F9jPIyspyb9iwodu5HTt2kJaWdl5ftz+teR0M+tP7cSE+P/sCWfPat8j70bcM5vejrrmuxxpoz2x0XTnHTx7vdr9JZ/Is2ThdHegAY8A5B+hefT9aW+Doru6ttCsLocmlXj+1K6A9U+0KqB9cjTe+TmNzK1uXFNBYsYcJd//wgr++oihfut3urFPPy4JQIYQQQpw3Zr2Z5MBkkgOTT3vd1eSivK7n+udyVzlbqrZwoql7FQ6TztSjA2HXWWh/o/+FnYHWtnf4C03r0hWwrb0r4ObOJRrbTukKGJIGER1dATPVmedB2BXQR68l64qLcTr71npuCchCCCGE8BqLwUKKIYWUwJTTXq9tqu0x+9yxpGPT4U3UNnffh+Or8/3aChyRlkj8DH7n/4PSaMCWpB5duwLWHOg+09yjK2BKl1bamWofBp8LMF5v++QTArdskTrIQgghhBBnw2qwkhqUSmpQ6mmvn2g60aOEXZmrjHJXORsOb6Cuua7b/Wa9GX/8ef+z97t1IIywqF0Jz1uAVhQIjFOPbl0By7tvBNy3Agrf7nxcUKIaljvWNYdngG9fWq3bCx57jNiaGvjZz7w9Eg8JyEIIIYTot/wMfvgF+TEkqGdHWLfb7QnQXYNz4YFCDrkOsa5iHfUt9d0eY9Vbv3b9c4QlAquhF/e/KAr4R6pHt66Ah9V1zOXtSzTKNqhLNDoExHRZ0zxS/VW6AvYqCchCCCGEGJAURcHf6I+/0Z+04M7N0s4GdZNeR4DuCM5dZ6JLa0tZW7GWho4KFe2sBmvnzLM1slsd6EhLJBZDLzTFsoaBdTIkT+48V3+ss412x2zzjoVdHhPRGZo7Zputdmlw8h1JQBZCCCHEoNQ1QA8LHtbjutvtpuZkzWkrcBw4cYA1FWt6BGg/g1+3GedTZ6LN+u/YHMM3CBLz1aND43GoLFLDcsfa5pKP6OwKGNKzwYl0BTwrEpB7SWVlJffddx/r168nICCAsLAwnn76aVJSTr/poDe8+uqrfPTRR8ybN89zrrq6mrS0NMrKyjzd8rp65ZVX2LBhA3//+9+ZO3cuvr6+3HTTTd3u2b9/P9OnT2fr1q1f+9r79+9n9erV3HDDDf+/vXsPi6rOHzj+/jASFzFRERWp1NbFK5c0NUuWxSfXNrO8pGumINtNMy/bzafdStt2HyvTNs3U+uWlpUXFdAu11i3NaFNDF/FGmkSGuqgYoiKB8v39McM4A4Oiwswon9fzzPMM5/udcz7nfDn64cz3nA8AGRkZLFmyhDfffPOK96tNmzZkZGQQEhJyxetSSimlLpeI0MS/CU38m9A5xHUC/dPPP1V5AsfBUwfJOZFD+sF0Ss45V9AN9gt2WYGw4n2g7yU8ycK/MbS5w/qq8PMpyN/lfLV5//rzVQH9g52T5rAYrQrogibItcAYw6BBg0hISLAXCdm+fTv5+flOCfLZs2drtdTyoEGDePLJJykuLiYw0HpCpaamcs8997hMjiu71MIhjnJzc/nggw/sCXL37t3p3r3KYwSVUi4kkWAAABkmSURBVEqpa5aI0NS/KU39m9IlpEuVdmMMx0uOV3kCx8HTB/mu8Ds25m3k53M/O32miV8T+5Xn8KBwp6vQrRq2ungC7RcEN/a0viqUlcCRXeenZhzKhM3zzlcFvK4RtKpcFfCX7qsKOH8+327eTM+L93Sbay5BfmXLK2Qfz67VdXZo2oFxHcdV275+/Xp8fX2dEs6oqCjA+jDy559/niZNmpCdnU1WVhZjx44lIyODBg0aMHPmTH7961+za9cuxowZQ2lpKeXl5axYsYKwsDCGDRtGXl4e586d4/nnn2f48OH2bVx//fX86le/4uOPP7YvT0lJ4Y9//CMff/wxL7/8MqWlpTRr1ozk5GRatGjhFPfUqVMJCgriqaeeYuvWrSQlJQHQr18/e5/c3FxGjRrF6dPWu4DnzJlD7969mTJlCnv27CE6OpqEhARiYmKYMWMGaWlpHD9+nKSkJHJycggMDGTBggVERkYydepUDhw4QE5ODgcOHGDSpElMmDChRmOQm5tLUlISx44do3nz5syePZtOnTqxfPlypk2bhsVioXHjxmzcuNHlsWzf3vXzN5VSSqm6IiI0C2hGs4BmRDaPrNJujKGgpKBqAn3qIPt+2scXP35BaXmp02ea+jd1nr7R0PlRdv4NXBQg8fWH1t2srwpnS+FotkNVwEzIWHi+KmCDAOtj5pwKnHSsm6qAERGcOXy49td7Ba65BNkTdu7cSbdu3apt37ZtGzt37qRt27a8/vrriAg7duwgOzubfv36sXfvXubNm8fEiRMZOXIkpaWlnDt3jjVr1hAWFsbq1asBOHHiRJV1jxgxguTkZIYPH86hQ4fYu3cv8fHxFBUVsWnTJkSEd999l1dffZXXX3+92hjHjBnDnDlziI2N5emnn7YvDw0NZd26dfj7+7Nv3z5GjBhBRkYG06dPtyfEYP1DoMKLL75ITEwMq1at4vPPP2f06NFkZmYCkJ2dzfr16zl58iQRERGMHTsWX9+Ln2xPPPEECQkJJCQk8N577/HMM8+QlpbGSy+9xKeffkrr1q0pLLSWM3V1LJVSSilvIyKEBIQQEhBCVPOoKu3lppyCMwVVKxCeOkT28Ww+P/A5ZeVlTp9p5t+syvznip9bNWx1PoFucJ3tqnEkMMq6rKIqoGMp7e0p8M071nbLdS6qAna+8qqAH39Msx079DnIdenZHs/WyXpPnjx58U7V6NGjB23btgUgPT2dJ554AoAOHTpw0003sXfvXm677Tb+8pe/kJeXx+DBg2nfvj1du3blySef5Nlnn2XAgAH06dOnyrrvvvtuxo0bR1FREcuWLWPIkCFYLBby8vIYPnw4hw8fprS01L59VwoLCyksLCQ2NhaAUaNGsXbtWgDKysoYP348mZmZWCwW9u7de9H9TU9PZ8WKFQDEx8dTUFBAUVGRPV4/Pz/8/PwIDQ0lPz+f8PDwi67z66+/5sMPP7THV5HE33777SQmJjJs2DAGDx4M4PJYKqWUUlcbH/GheWBzmgc2Jzo0ukp7uSnn2JljVSoQHjx1kN0Fu/n3gX9ztvys02dCAkLsV54dKxCGBYXRKqgVfhVVAaN+Z9uIY1VAW9K8axVsXWQL0lYV0DFpbtkFrruEmxFff50bCgvhuecu80jVvmsuQfaEzp07k5qaWm17w4YX/yV54IEH6NmzJ6tXr+a3v/0t8+fPJz4+nm3btrFmzRr+9Kc/0bdvX1544QWnzwUEBNC/f39WrlxJSkoKM2fOBKxXXP/whz8wcOBANmzYwNSpUy9r32bNmkWLFi3Yvn075eXl+Ptf2V+JjnOjLRYLZ8+evUDvi5s3bx6bN29m9erVdOvWja1bt1Z7LJVSSqlriY/4EBoYSmhgKDGhMVXaz5Wf4+iZo1UqEB48eZAdx3aw7od1nDXO/w83D2hepQJhWFAYrW+6lVadBnKd5brzVQEdrzTv/QQyXVUFrEiau1pvKrxKaIJcC+Lj43nuuedYsGABjzzyCABZWVkup0T06dOH5ORk4uPj2bt3LwcOHCAiIoKcnBzatWvHhAkTOHDgAFlZWXTo0IGmTZvy4IMPEhwczLvvvuty+yNGjGDKlCkUFRVx2223AdbpGK1btwasT7u4kODgYIKDg0lPT+eOO+4gOTnZ3nbixAnCw8Px8fFh8eLF9ukKjRo1qvaqesU+Pv/882zYsIGQkBCuv/7KKhP17t2blJQURo0aRXJyMr179wZg//799OzZk549e7J27Vp+/PFHTpw4UeVYaoKslFKqvrH4WGjZsCUtG7bklha3VGmvSKArkmfH50FvP7qdT3M/5Zw5P01REJoHnk+gwxqGEX5jF8I69aN1wzBanivHN3+3Q1XALytVBWx3vox2xctLqwJqglwLRISVK1cyadIkXnnlFfz9/WnTpg1vvPEGBw8edOo7btw4xo4dS9euXWnQoAGLFi3Cz8+PZcuW8f777+Pr60vLli157rnn+Oabb3j66afx8fHB19eXt99+2+X277zzTkaPHs3vf/97xPZsw6lTp3L//ffTpEkT4uPj+f777y+4DwsXLiQpKQkRcbpJb9y4cQwZMoQlS5bQv39/+9XwyMhILBYLUVFRJCYmEhNz/i/XqVOnkpSURGRkJIGBgRdN0F2JjIzEx/bImWHDhjF79mzGjBnDa6+9Zr9JD+Dpp59m3759GGPo27cvUVFRvPLKK1WOpVJKKaWcOSbQ3VpUvZfqbPlZjhYfrVpI5fQhMo9k8snpT6ok0KGBodZ5zze0I6zjHbT2bURYyWnCTvyPlkf343uwUlXAxjfCkSNYLMHu2OUaE2OMp2O4JN27dzcZGRlOy/bs2UPHjh2r+UTtOHnyJI0a1WJ5SXVFrqbxcMfvpzfYsMFamUp5Bx0P76Lj4V10PGrH2fKz5BfnuyykcujUIfKL8yk35fb+FVNCWge0oLWPH2FlZYSd/om+L6yn3K8VwTv3uX0fRGSrMabKc2r1CrJSSimllLpkDXwa2J+ScSu3VmkvKy8j/3S+y0IqW04fIP90PgbDW2Nb82CzB0l0/y5USxNkpZRSSilV63x9fAlvFE54I9dPqyo7V8b/iv/HoVOHOLb7mJujuzCtK6iUUkoppdzO1+LLDY1uoOfGXNp8sdnT4TjRBFkppZRSSnnO22/T+qOPPB2FE02QlVJKKaWUcqAJslJKKaWUUg40Qa4lFouF6Oho+2v69OmX9PmpU6cyY8aMGvfftGkTPXv2JDo6mo4dO9or5W3YsIH//Oc/l7TtmqoozlEbtmzZQmxsLBEREcTExPDQQw9RXFx8ycehOrW1no8++uiiY5mbm8sHH3xwxdtSSimllHfQp1jUkoCAADIzMy/rs5dTbjkhIYFly5YRFRXFuXPn+PbbbwFrghwUFFSryWyF2kq88/Pzuf/++0lJSbFX/ktNTa22Mp8nDRw4kIEDB16wT0WC/MADD7gpKqWUUkrVpWvzCnJcXNXX3LnWtuJi1+2LFlnbjx2r2nYFXnrpJW699Va6dOnCI488QkVhlri4OCZNmkT37t3529/+Zu+/f/9+brnlfDnIffv2Of1c4ciRI7Rq1QqwXr3u1KkTubm5zJs3j1mzZhEdHc2XX35Jbm4u8fHxREZG0rdvXw4cOABAYmIijz32GN27d+eXv/wlaWlpACxatIh7772XuLg42rdvz7Rp0+zbDAoKAs4/YH3o0KF06NCBkSNH2vdrzZo1dOjQgW7dujFhwgQGDBhQJfa33nqLhIQEe3IMMHToUFq0aAHA7t27iYuLo127drz55pv2Pn//+9/p0aMH0dHRTJw40V72+pNPPuGWW24hKiqKvn37VtneO++8w1133cWZM2eIi4tj4sSJREdH06VLF7Zs2QLA8ePHue+++4iMjKRXr15kZWXZj8f48ePtx2zChAn07t2bdu3akZqaCsCUKVP48ssviY6OZtasWVW2r5RSSqkLSE1ll0O+4Q2uzQTZA86cOeM0xWLpUmvt8fHjx/PNN9+wc+dOzpw5Y09EAUpLS8nIyODJJ5+0L7v55ptp3Lix/Wr0woULGTNmTJXtTZ48mYiICAYNGsT8+fMpKSmhTZs2PPbYY0yePJnMzEz69OnDE088QUJCAllZWYwcOZIJEybY15Gbm8uWLVtYvXo1jz32GCUlJYB1+sOKFSvIyspi+fLlVK5cCPDf//6XN954g927d5OTk8NXX31FSUkJjz76KGvXrmXr1q0cPXrU5bHauXMn3bpVLWlZITs7m08//ZQtW7Ywbdo0ysrK2LNnD0uXLuWrr74iMzMTHx8fkpOTOXr0KA8//DArVqxg+/btLF++3Gldc+bMIS0tjVWrVhEQEABAcXExmZmZzJ07l6SkJABefPFFYmJiyMrK4q9//SujR492Gdvhw4dJT08nLS2NKVOmADB9+nT69OlDZmYmkydPrna/lFJKKeVCSAhljRt7Ogon1+YUiw0bqm8LDLxwe0jIhdurUd0Ui/Xr1/Pqq69SXFzM8ePH6dy5M/fccw8Aw4cPd7muhx56iIULFzJz5kyWLl1qv8rp6IUXXmDkyJH861//4oMPPuAf//gHG1zE/fXXX/Phh9aa56NGjeKZZ56xtw0bNgwfHx/at29Pu3btyM7OBuDOO++kWbNmAAwePJj09HS6d3euwtijRw/Cw60P/o6OjiY3N5egoCDatWtH27ZtARgxYgQLFiy44HFz5e6778bPzw8/Pz9CQ0PJz8/ns88+Y+vWrdx6q7VSz+nTpwkPD2fTpk3Exsbat9m0aVP7epYsWcINN9zAqlWr8PX1tS8fMWIEALGxsRQVFVFYWEh6ejorVqwAID4+noKCAoqKiqrEdt999+Hj40OnTp3Iz8+/5H1TSimlVCWLFtEyO/uKv7WvTXoFuQ6VlJQwbtw4UlNT2bFjBw8//LD9Ki1Aw4YNXX5uyJAhrF27lrS0NLp162ZPViu7+eabGTt2LJ999hnbt2+noKDgkuITEZc/V7fckZ+fn/29xWK5pHnUnTt3ZuvWrdW2u1q3MYaEhAQyMzPJzMxk27Zt9hsTq9O1a1dyc3PJy8tzWl6T/atJbBXTSpRSSil1BRYtouUnn3g6CieaINehimQ4JCSEU6dO2eesXoy/vz+/+c1vGDt2rMvpFQCrV6+2J2j79u3DYrEQHBxMo0aNnG526927NykpKQAkJyfTp08fe9vy5cspLy9n//795OTkEBERAcC6des4fvw4Z86cYdWqVdx+++01ijsiIoKcnBxyc3MB7NNMKhs/fjyLFy9m8+bzVXM+/PDDC16R7du3L6mpqRw5cgSwzhn+4Ycf6NWrFxs3buT777+3L68QExPD/PnzGThwIIcOHbIvr4grPT2dxo0b07hxY/r06UNycjJgnWMdEhLC9ddfX6P9rnzMlVJKKXV1uzanWHhAxRzkCv3792f69Ok8/PDDdOnShZYtW9qnB9TEyJEjWblyJf369XPZ/v777zN58mQCAwNp0KABycnJWCwW7rnnHoYOHco///lPZs+ezezZsxkzZgyvvfYazZs3Z+HChfZ13HjjjfTo0YOioiLmzZuHv78/YJ0+MWTIEPLy8njwwQerTK+oTkBAAHPnzqV///40bNiw2v1t0aIFKSkpPPXUUxw5cgQfHx9iY2Pp379/tevu1KkTL7/8Mv369aO8vByLxcLbb79Nr169WLBgAYMHD6a8vJzQ0FDWrVtn/9wdd9zBjBkzuPvuu+3L/f39iYmJoaysjPfeew+wPhYuKSmJyMhIAgMDWbx4cY32GSAyMhKLxUJUVBSJiYk6D1kppZS6ysnV9jVx9+7dTeWbxvbs2UPHjh3rdLsnT56kUaNGdboNRzNmzODEiRP8+c9/rpP1JyYmMmDAAIYOHeq0fNGiRWRkZDBnzpzLWu+pU6cICgrCGMPjjz9O+/bt6yRhvNzxiIuLY8aMGTVO+muDO34/vUHF002Ud9Dx8C46Ht5Fx8PLxMVRWFhI8GU+LvdKiMhWY0yVpECvIHuhQYMGsX//fj7//HNPh3LJ3nnnHRYvXkxpaSkxMTE8+uijng5JKaWUUuqSaILshVauXFnn21hU8dznShITE0lMTLzs9U6ePNmrpxi4etKHUkoppTxozRqyNm4k1tNxOLhmbtK72qaKqPpBfy+VUkqpiwgMpNx2H5S3uCYSZH9/fwoKCjQZUV7FGENBQYH95kellFJKuTB3LmGrVnk6CifXxBSL8PBw8vLyqq3cVhtKSko00fEiV8t4+Pv72wuqKKWUUsqFZcsILSz0dBROrokE2dfX115Jra5s2LCBmJiYOt2GqjkdD6WUUkrVlWtiioVSSimllFK1RRNkpZRSSimlHGiCrJRSSimllIOrrpKeiBwFfvDApkOAYx7YrnJNx8P76Jh4Fx0P76Lj4V10PLyPp8bkJmNM88oLr7oE2VNEJMNVKULlGToe3kfHxLvoeHgXHQ/vouPhfbxtTHSKhVJKKaWUUg40QVZKKaWUUsqBJsg1t8DTASgnOh7eR8fEu+h4eBcdD++i4+F9vGpMdA6yUkoppZRSDvQKslJKKaWUUg40QVZKKaWUUsqBJsiViEh/EflWRL4TkSku2v1EZKmtfbOItHF/lPVHDcYjUUSOikim7fWQJ+KsL0TkPRE5IiI7q2kXEXnTNl5ZInKLu2OsT2owHnEicsLh/HjB3THWJyJyg4isF5HdIrJLRCa66KPniJvUcDz0HHETEfEXkS0ist02HtNc9PGaHEsTZAciYgHeAu4COgEjRKRTpW6/B34yxvwCmAW84t4o648ajgfAUmNMtO31rluDrH8WAf0v0H4X0N72egR42w0x1WeLuPB4AHzpcH685IaY6rOzwJPGmE5AL+BxF/9m6TniPjUZD9BzxF1+BuKNMVFANNBfRHpV6uM1OZYmyM56AN8ZY3KMMaVACnBvpT73Aott71OBviIiboyxPqnJeCg3MsZsBI5foMu9wBJjtQkIFpFW7omu/qnBeCg3MsYcNsZss70/CewBWlfqpueIm9RwPJSb2H7nT9l+9LW9Kj8pwmtyLE2QnbUGfnT4OY+qJ5O9jzHmLHACaOaW6OqfmowHwBDbV5WpInKDe0JT1ajpmCn3uc32leZaEens6WDqC9tXwzHA5kpNeo54wAXGA/QccRsRsYhIJnAEWGeMqfb88HSOpQmyutp9DLQxxkQC6zj/l6dSCrYBN9m+0pwNrPJwPPWCiAQBK4BJxpgiT8dT311kPPQccSNjzDljTDQQDvQQkS6ejqk6miA7Owg4XoEMty1z2UdEGgCNgQK3RFf/XHQ8jDEFxpifbT++C3RzU2zKtZqcQ8pNjDFFFV9pGmPWAL4iEuLhsK5pIuKLNRlLNsZ86KKLniNudLHx0HPEM4wxhcB6qt5D4TU5libIzr4B2otIWxG5Dvgd8FGlPh8BCbb3Q4HPjVZbqSsXHY9Kc/cGYp1jpjznI2C07U79XsAJY8xhTwdVX4lIy4r5eyLSA+u/+foHfR2xHev/A/YYY2ZW003PETepyXjoOeI+ItJcRIJt7wOAO4HsSt28Jsdq4ImNeitjzFkRGQ98CliA94wxu0TkJSDDGPMR1pPtfRH5DuvNMb/zXMTXthqOxwQRGYj1buXjQKLHAq4HROQfQBwQIiJ5wItYb7TAGDMPWAP8FvgOKAbGeCbS+qEG4zEUGCsiZ4EzwO/0D/o6dTswCthhm2cJ8BxwI+g54gE1GQ89R9ynFbDY9oQqH2CZMSbNW3MsLTWtlFJKKaWUA51ioZRSSimllANNkJVSSimllHKgCbJSSimllFIONEFWSimllFLKgSbISimllFJKOdAEWSml6hERiRORNE/HoZRS3kwTZKWUUkoppRxogqyUUl5IRB4UkS0ikiki80XEIiKnRGSWiOwSkc9EpLmtb7SIbBKRLBFZKSJNbMt/ISL/FpHtIrJNRG62rT5IRFJFJFtEkh0qiU0Xkd229czw0K4rpZTHaYKslFJeRkQ6AsOB240x0cA5YCTQEGvFqc7AF1gr5wEsAZ41xkQCOxyWJwNvGWOigN5ARUnjGGAS0AloB9wuIs2AQUBn23pertu9VEop76UJslJKeZ++QDfgG1uJ3L5YE9lyYKmtz9+BO0SkMRBsjPnCtnwxECsijYDWxpiVAMaYEmNMsa3PFmNMnjGmHMgE2gAngBLg/0RkMNYyyEopVS9pgqyUUt5HgMXGmGjbK8IYM9VFP3OZ6//Z4f05oIEx5izQA0gFBgCfXOa6lVLqqqcJslJKeZ/PgKEiEgogIk1F5Cas/2YPtfV5AEg3xpwAfhKRPrblo4AvjDEngTwRuc+2Dj8RCaxugyISBDQ2xqwBJgNRdbFjSil1NWjg6QCUUko5M8bsFpE/Af8SER+gDHgcOA30sLUdwTpPGSABmGdLgHOAMbblo4D5IvKSbR33X2CzjYB/iog/1ivYf6jl3VJKqauGGHO539AppZRyJxE5ZYwJ8nQcSil1rdMpFkoppZRSSjnQK8hKKaWUUko50CvISimllFJKOdAEWSmllFJKKQeaICullFJKKeVAE2SllFJKKaUcaIKslFJKKaWUg/8HZoltMbwpUsoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "k-fold 0:: Epoch 4: train loss 988384.9430136891 val loss 943941.3620049505\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train_loss_mean_list nan\n",
            "test_loss_mean_list nan\n",
            "val_loss_mean_list nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-34688195c97c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    328\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test_loss_mean_list\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loss_mean_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val_loss_mean_list\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loss_mean_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m   \u001b[0mvisualizeTraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_mean_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss_mean_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_mean_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/results\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-34688195c97c>\u001b[0m in \u001b[0;36mvisualizeTraining\u001b[0;34m(epoch, trn_losses, tst_losses, val_losses, save_dir)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m     \u001b[0;31m#if tst_losses:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtst_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.float64' has no len()"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x576 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3I63RgNmk73"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}