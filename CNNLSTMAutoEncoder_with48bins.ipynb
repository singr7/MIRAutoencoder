{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNLSTMAutoEncoder_with48bins.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOHemmy80KBld3Sq1tA6GGk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singr7/MIRAutoencoder/blob/master/CNNLSTMAutoEncoder_with48bins.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9JqXiXhqdkN"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORCKRmYPHk-z"
      },
      "source": [
        "#Mount the google drive\n",
        "#Create list of numpy files for western and indian dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPnD6kkyHfx8",
        "outputId": "622ef1b3-4280-4851-c03a-9e8def483502"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "western_files = []\n",
        "western_file_dir = \"/content/drive/My Drive/MusicResearchColabNB/MelFeatures/Western_mel_numpy\"\n",
        "for r,d, fileList in os.walk(western_file_dir):\n",
        "  for file in fileList:\n",
        "    western_files.append(os.path.join(r,file))\n",
        "\n",
        "indian_files = []\n",
        "indian_file_dir = \"/content/drive/My Drive/MusicResearchColabNB/MelFeatures/Indian_mel_numpy\"\n",
        "for r,d, fileList in os.walk(indian_file_dir):\n",
        "  for file in fileList:\n",
        "    indian_files.append(os.path.join(r,file))\n",
        "\n",
        "print(len(western_files))\n",
        "print(len(indian_files))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "7894\n",
            "2008\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMQbF-ylLmdK"
      },
      "source": [
        "# Balance the western dataset by taking files equal to Indian dataset files = 2008"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXHKJAOLL-iX",
        "outputId": "1d85df79-fbb6-4044-d9b9-f03869a049e5"
      },
      "source": [
        "import random \n",
        "#randomize the selection. To avoid getting a different random sample with every run, use seed\n",
        "random.seed(234)\n",
        "bal_western_files = random.sample(western_files,2008)\n",
        "len(bal_western_files)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2008"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeBwpwaTX3Jo"
      },
      "source": [
        "#Define configuration class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP4323imT53f"
      },
      "source": [
        "class Configuration:\n",
        "  seq_len = 200  # taking half of the original timesteps extracted \n",
        "  input_dim = 48  #num of mels\n",
        "  embedding_dim = 64\n",
        "  batch_size = 2\n",
        "  base_dir = \"/content/drive/My Drive/MusicResearchColabNB/vajra/westernAE_48bins\"   # need to be edited..\n",
        "  loss_function = torch.nn.MSELoss(reduction='sum')\n",
        "  lr=1e-3  # I edited it from 1e-3 to 1e-5\n",
        "  n_epochs = 4\n",
        "  model_file = \"/content/drive/My Drive/MusicResearchColabNB/vajra/westernAE_48bins/models/mel.pkl\"  #need need edits\n",
        "  results_dir = os.path.join(base_dir, \"./results\")  # may need edits\n",
        "  checkpoint_model_file = \"/content/drive/My Drive/MusicResearchColabNB/vajra/westernAE_48bins/models/mel_checkpoint.pkl\" #may need edits\n",
        "  kernel_size = 3  #why?\n",
        "  k_folds = 10 "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RALRBXgZZBA"
      },
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "  def __init__(self, seq_len, n_features, embedding_dim=64, kernel_size=3, stride=1):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.seq_len, self.n_features = seq_len, n_features\n",
        "    self.embedding_dim, self.hidden_dim = embedding_dim, 2 * embedding_dim\n",
        "\n",
        "\n",
        "    self.conv = nn.Conv1d(in_channels=seq_len,out_channels=seq_len,kernel_size=kernel_size,stride=stride, groups=seq_len)\n",
        "    conv_op_dim = int(((n_features - kernel_size)/ stride) + 1)\n",
        "\n",
        "    self.rnn1 = nn.LSTM(\n",
        "      input_size=conv_op_dim,\n",
        "      hidden_size=self.hidden_dim,\n",
        "      num_layers=1,\n",
        "      batch_first=True\n",
        "    )\n",
        "    self.rnn2 = nn.LSTM(\n",
        "      input_size=self.hidden_dim,\n",
        "      hidden_size=embedding_dim,\n",
        "      num_layers=1,\n",
        "      batch_first=True\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    #x = x.reshape((10, self.seq_len, self.n_features))\n",
        "   # print('In Encoder')\n",
        "   # print(x.shape)\n",
        "    x = self.conv(x)\n",
        "    x, (_, _) = self.rnn1(x)\n",
        "    x, (hidden_n, _) = self.rnn2(x)\n",
        "    return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkW-A8TzZdGT"
      },
      "source": [
        "class Decoder(torch.nn.Module):\n",
        "  def __init__(self, seq_len, embedding_dim=64, n_features=48):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.seq_len, self.embedding_dim = seq_len, embedding_dim\n",
        "    self.hidden_dim, self.n_features = 2 * embedding_dim, n_features\n",
        "    self.rnn1 = nn.LSTM(\n",
        "      input_size=embedding_dim,\n",
        "      hidden_size=embedding_dim,\n",
        "      num_layers=1,\n",
        "      batch_first=True\n",
        "    )\n",
        "    self.rnn2 = nn.LSTM(\n",
        "      input_size=embedding_dim,\n",
        "      hidden_size=self.hidden_dim,\n",
        "      num_layers=1,\n",
        "      batch_first=True\n",
        "    )\n",
        "    self.output_layer = nn.Linear(self.hidden_dim * self.seq_len, n_features * self.seq_len)\n",
        "  def forward(self, x):\n",
        "    x, (hidden_n, cell_n) = self.rnn1(x)\n",
        "    x, (hidden_n, cell_n) = self.rnn2(x)\n",
        "    #print(\"in decoder\", x.shape)\n",
        "    x = x.contiguous()\n",
        "    x = x.view(x.shape[0], -1)\n",
        "    x = self.output_layer(x)\n",
        "    return x.reshape(x.shape[0],self.seq_len, self.n_features)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mqrvU5MZfEA"
      },
      "source": [
        "class RecurrentAutoencoder(torch.nn.Module):\n",
        "  def __init__(self, seq_len, n_features, embedding_dim=64, device='cpu'):\n",
        "    super(RecurrentAutoencoder, self).__init__()\n",
        "    self.encoder = Encoder(seq_len, n_features, embedding_dim).to(device)\n",
        "    self.decoder = Decoder(seq_len, embedding_dim, n_features).to(device)\n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)\n",
        "    x = self.decoder(x)\n",
        "    return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRQ-t9aNZiUD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32faf639-f62e-4b5d-8392-f560c47d8c18"
      },
      "source": [
        "x = torch.randn(10, 48, 400)\n",
        "print(x.shape)\n",
        "x = x.permute(0, 2, 1)\n",
        "print(x.shape)\n",
        "\n",
        "encoder = Encoder(400, 48, embedding_dim=64, kernel_size=3, stride=1)\n",
        "encoded = encoder(x)\n",
        "print(encoded.shape)\n",
        "\n",
        "decoder = Decoder(400, 64, 48)\n",
        "decoded = decoder(encoded)\n",
        "print(decoded.shape)\n",
        "\n",
        "rae = RecurrentAutoencoder(400, 48, 64)\n",
        "output = rae(x)\n",
        "\n",
        "print(output.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 48, 400])\n",
            "torch.Size([10, 400, 48])\n",
            "torch.Size([10, 400, 64])\n",
            "torch.Size([10, 400, 48])\n",
            "torch.Size([10, 400, 48])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDpec6ICZnKN"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data.sampler import SequentialSampler\n",
        "\n",
        "class CustomDatasetMel(Dataset):\n",
        "\n",
        "    def __init__(self, dataList):\n",
        "        self.data = dataList\n",
        "        #self.labels = labelList\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        import numpy as np\n",
        "        fileName = self.data[index]\n",
        "        \n",
        "        mel_spect = np.load(fileName)\n",
        "        data = torch.tensor(mel_spect[:,:200], dtype=torch.float)\n",
        "        data = data.permute(1, 0)\n",
        "        #data = torch.unsqueeze(data, dim =0)\n",
        "\n",
        "        #label = torch.tensor(self.labels[index])\n",
        "        return data"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uAv1gLc6Zwwr",
        "outputId": "4cf7b264-cdcb-44e0-cd54-702ee8f8b7f3"
      },
      "source": [
        "import copy\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "class TrainingWrapper:\n",
        "\n",
        "  def __init__(self, config, training_loader, test_loader, device, val_loader=None, cross=10):\n",
        "    self.config = config\n",
        "    self.training_loader = training_loader\n",
        "    self.test_loader = test_loader\n",
        "    self.val_loader = val_loader\n",
        "    self.device = device\n",
        "    self.model = RecurrentAutoencoder(self.config.seq_len, self.config.input_dim, self.config.embedding_dim, device=self.device)\n",
        "    self.model = self.model.to(self.device)\n",
        "    self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.lr)\n",
        "    self.criterion = self.config.loss_function.to(self.device)\n",
        "    self.history = dict(train=[], val=[], cross_val=[])\n",
        "    self.best_model_wts = copy.deepcopy(self.model.state_dict())\n",
        "    self.best_loss = 10000.0\n",
        "    #print(self.config.base_dir + self.config.model_file)\n",
        "    torch.save(self.model.state_dict(),  self.config.model_file)\n",
        "    self.cross = cross\n",
        "    \n",
        "\n",
        "  def combine_images(self, generated_images):\n",
        "    num = generated_images.shape[0]\n",
        "    width = int(math.sqrt(num))\n",
        "    height = int(math.ceil(float(num)/width))\n",
        "    shape = generated_images.shape[1:3]\n",
        "    image = np.zeros((height*shape[0], width*shape[1]),\n",
        "                     dtype=generated_images.dtype)\n",
        "    for index, img in enumerate(generated_images):\n",
        "        i = int(index/width)\n",
        "        j = index % width\n",
        "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
        "            img[:, :, 0]\n",
        "    return image\n",
        "\n",
        "\n",
        "  def show_reconstruction(self, test_loader, n_images):\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    from PIL import Image\n",
        "    import numpy as np\n",
        "\n",
        "    self.model.eval()\n",
        "    for x, _ in self.test_loader:\n",
        "        x = x[:min(n_images, x.size(0))].to(self,device)\n",
        "        _, x_recon = self.model(x)\n",
        "        data = np.concatenate([x.data.cpu(), x_recon.data.cpu()])\n",
        "        img = self.combine_images(np.transpose(data, [0, 2, 3, 1]))\n",
        "        image = img * 255\n",
        "        Image.fromarray(image.astype(np.uint8)).save(self.config.base_dir + \"/real_and_recon.png\")\n",
        "        print()\n",
        "        print('Reconstructed images are saved to %s/real_and_recon.png' % self.config.base_dir)\n",
        "        print('-' * 70)\n",
        "        plt.imshow(plt.imread(self.config.base_dir + \"/real_and_recon.png\", ))\n",
        "        plt.show()\n",
        "        break\n",
        "\n",
        "  def visualizeTraining(self, epoch, trn_losses, tst_losses, val_losses, save_dir,cross):\n",
        "    # visualize the loss as the network trained\n",
        "    import matplotlib.pyplot as plt\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    plt.plot(range(0, len(trn_losses)), trn_losses, label='Training Loss')\n",
        "    if tst_losses:\n",
        "      plt.plot(range(0, len(tst_losses)), tst_losses, label='Validation Loss')\n",
        "    if val_losses:\n",
        "      plt.plot(range(0, len(val_losses)), val_losses, label='Cross Validation Loss')\n",
        "\n",
        "    minposs = tst_losses.index(min(tst_losses))\n",
        "    plt.axvline(minposs, linestyle='--', color='r', label='Early Stopping Checkpoint')\n",
        "\n",
        "\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('loss')\n",
        "    # plt.ylim(0, 0.5)  # consistent scale\n",
        "    # plt.xlim(0, len(trn_losses))  # consistent scale\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    fig.savefig(os.path.join(save_dir , 'loss_plot_{}.png'.format(cross)), bbox_inches='tight')\n",
        "\n",
        "  def train(self):\n",
        "    self.model.load_state_dict(torch.load(config.checkpoint_model_file))\n",
        "    for epoch in range(1, self.config.n_epochs + 1):\n",
        "      self.model = self.model.train()\n",
        "      train_losses = []\n",
        "      for i, data in enumerate(self.training_loader,0):\n",
        "        x = data\n",
        "        self.optimizer.zero_grad()\n",
        "        x = x.to(self.device)\n",
        "\n",
        "        output = self.model(x)\n",
        "        loss = self.criterion(output, x)\n",
        "\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        train_losses.append(loss.item())\n",
        "        print(\"in training loop, epoch {}, step {}, the loss is {}\".format(epoch, i, loss.item()))\n",
        "\n",
        "      val_losses = []\n",
        "      self.model = self.model.eval()\n",
        "      with torch.no_grad():\n",
        "        for i, data in enumerate(self.test_loader):\n",
        "          x = data\n",
        "          x = x.to(device)\n",
        "          output = self.model(x)\n",
        "          loss = self.criterion(output, x)\n",
        "          val_losses.append(loss.item())\n",
        "\n",
        "\n",
        "      cross_val_losses = []\n",
        "      self.model = self.model.eval()\n",
        "      with torch.no_grad():\n",
        "        for i, data in enumerate(self.val_loader):\n",
        "          x = data\n",
        "          x = x.to(device)\n",
        "          output = self.model(x)\n",
        "          loss = self.criterion(output, x)\n",
        "          cross_val_losses.append(loss.item())\n",
        "\n",
        "      train_loss = np.mean(train_losses)\n",
        "      val_loss = np.mean(val_losses)\n",
        "      cross_val_loss = np.mean(cross_val_losses)\n",
        "\n",
        "\n",
        "      self.history['train'].append(train_loss)\n",
        "      self.history['val'].append(val_loss)\n",
        "      self.history['cross_val'].append(cross_val_loss)\n",
        "\n",
        "      if val_loss < self.best_loss:\n",
        "        self.best_loss = val_loss\n",
        "        self.best_model_wts = copy.deepcopy(self.model.state_dict())\n",
        "      torch.save(self.model.state_dict(),  self.config.checkpoint_model_file)\n",
        "      if epoch % 2 == 0:\n",
        "        self.visualizeTraining(epoch, trn_losses= self.history['train'], tst_losses=self.history['val'], val_losses =self.history['cross_val'], save_dir=self.config.base_dir + \"/results\",cross=fold)\n",
        "      print(f'k-fold {fold}:: Epoch {epoch}: train loss {train_loss} val loss {val_loss}')\n",
        "    self.model.load_state_dict(self.best_model_wts)\n",
        "    torch.save(self.model.state_dict(), self.config.model_file)\n",
        "    return self.model.eval(), self.history\n",
        "\n",
        "  \n",
        "class TestingWrapper:\n",
        "  def __init__(self, config, device):\n",
        "    self.config = config\n",
        "    self.device = device\n",
        "    self.model = RecurrentAutoencoder(self.config.seq_len, self.config.input_dim, self.config.embedding_dim, device=self.device)\n",
        "    PATH =  self.config.checkpoint_model_file\n",
        "    print(PATH)\n",
        "    self.model.load_state_dict(torch.load(PATH, map_location=self.device))\n",
        "    self.model = self.model.to(self.device)\n",
        "\n",
        "  def combine_images(self, generated_images):\n",
        "    num = generated_images.shape[0]\n",
        "    width = int(math.sqrt(num))\n",
        "    height = int(math.ceil(float(num)/width))\n",
        "    shape = generated_images.shape[1:3]\n",
        "    image = np.zeros((height*shape[0], width*shape[1]),\n",
        "                     dtype=generated_images.dtype)\n",
        "    for index, img in enumerate(generated_images):\n",
        "        i = int(index/width)\n",
        "        j = index % width\n",
        "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
        "            img[:, :, 0]\n",
        "    return image\n",
        "\n",
        "\n",
        "  def show_reconstruction(self, test_loader, n_images):\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    from PIL import Image\n",
        "    import numpy as np\n",
        "\n",
        "    self.model.eval()\n",
        "    for x, _ in test_loader:\n",
        "      x = x[:min(n_images, x.size(0))].to(self.device)\n",
        "      x_recon = self.model(x)\n",
        "      data = np.concatenate([x.data.cpu(), x_recon.data.cpu()])\n",
        "      img = self.combine_images(np.transpose(data, [0, 2, 3, 1]))\n",
        "      image = img * 255\n",
        "      Image.fromarray(image.astype(np.uint8)).save(self.config.base_dir + \"/real_and_recon.png\")\n",
        "      print()\n",
        "      print('Reconstructed images are saved to %s/real_and_recon.png' % self.config.base_dir)\n",
        "      print('-' * 70)\n",
        "      plt.imshow(plt.imread(self.config.base_dir + \"/real_and_recon.png\", ))\n",
        "      plt.show()\n",
        "      break\n",
        "\n",
        "  def save_reconstruction(self, test_loader):\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    from PIL import Image\n",
        "    import numpy as np\n",
        "    import uuid\n",
        "\n",
        "\n",
        "    self.model.eval()\n",
        "    with torch.no_grad(): \n",
        "      fileCount = 0\n",
        "      for x, _ in test_loader:\n",
        "        x = x.to(self.device)\n",
        "        x_recon = self.model(x)\n",
        "        x_recon = x_recon.data.cpu().detach().numpy()\n",
        "        for mel in x_recon:\n",
        "          #print(mel.shape)\n",
        "          unique_filename = str(uuid.uuid4())\n",
        "          filename = self.config.base_dir + \"/reconstruction/\" + unique_filename + \".npy\"\n",
        "          np.save(filename, mel)\n",
        "          fileCount = fileCount + 1\n",
        "          print(\"saving file {} at index {}\".format(filename, fileCount))\n",
        "\n",
        "mode = 'train'\n",
        "data = \"mel\"\n",
        "#data = \"mnist\"\n",
        "config = Configuration()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import random\n",
        "import os\n",
        "\n",
        "def seed_everything(seed=1234):\n",
        "  \n",
        "  random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  np.random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def visualizeTraining(epoch, trn_losses, tst_losses, val_losses, save_dir):\n",
        "    # visualize the loss as the network trained\n",
        "    import matplotlib.pyplot as plt\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    plt.plot(range(0, len(trn_losses)), trn_losses, label='Training Loss')\n",
        "    #if tst_losses:\n",
        "    plt.plot(range(0, len(tst_losses)), tst_losses, label='Validation Loss')\n",
        "    #if val_losses:\n",
        "    plt.plot(range(0, len(val_losses)), val_losses, label='Cross Validation Loss')\n",
        "\n",
        "    #minposs = tst_losses.index(min(tst_losses))\n",
        "    #plt.axvline(minposs, linestyle='--', color='r', label='Early Stopping Checkpoint')\n",
        "\n",
        "\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('loss')\n",
        "    # plt.ylim(0, 0.5)  # consistent scale\n",
        "    # plt.xlim(0, len(trn_losses))  # consistent scale\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    fig.savefig(os.path.join(save_dir , 'loss_plot_{}.png'.format(\"MEAN\")), bbox_inches='tight')\n",
        "\n",
        "seed_everything()\n",
        "\n",
        "train_data = bal_western_files\n",
        "#labels = [1] * len(bal_western_files)\n",
        "\n",
        "val_data = indian_files\n",
        "#val_labels = [1] * len(indian_files)\n",
        "\n",
        "train_loss_mean_list = []\n",
        "test_loss_mean_list = []\n",
        "val_loss_mean_list = []\n",
        "\n",
        "# Cross validation runs\n",
        "# use sklearn KFolds\n",
        "from sklearn.model_selection import KFold\n",
        "kfold = KFold(n_splits=config.k_folds , shuffle=True)\n",
        "\n",
        "train_dataset = CustomDatasetMel(train_data)\n",
        "val_dataset = CustomDatasetMel(val_data)\n",
        "#Load the cross val dataset which is Full Indian dataset\n",
        "#It is identical for all K-folds\n",
        "crossval_loader = torch.utils.data.DataLoader(\n",
        "                      val_dataset,\n",
        "                      batch_size=config.batch_size, \n",
        "                      sampler=SequentialSampler(val_dataset), \n",
        "                      drop_last=False)  \n",
        "\n",
        "for fold, (train_ids, test_ids) in enumerate(kfold.split(train_dataset)):\n",
        "    # Print\n",
        "  print(f'FOLD {fold}')\n",
        "  print('--------------------------------')\n",
        "    \n",
        "    # Sample elements randomly from a given list of ids, no replacement.\n",
        "  train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "  test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
        "  \n",
        "    # Define data loaders for training and testing data in this fold\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "                      train_dataset, \n",
        "                      batch_size=config.batch_size,\n",
        "                      sampler=train_subsampler,\n",
        "                      drop_last=False)\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "                      train_dataset,\n",
        "                      batch_size=config.batch_size,\n",
        "                      sampler=test_subsampler,\n",
        "                      drop_last=False)\n",
        "    \n",
        "  print(\"length of of train_loader is {} & length of traindataset is {}\".format(len(train_loader),len(train_dataset)))\n",
        "  print(\"length of of test_loader is {}\".format(len(test_loader)))\n",
        "  print(\"length of of val_loader is {}\".format(len(crossval_loader)))\n",
        "    \n",
        "  if mode==\"train\":\n",
        "    trainingWrapper = TrainingWrapper(config=config, training_loader=train_loader, test_loader=test_loader, device=device, val_loader=crossval_loader, cross=fold)\n",
        "    model, history = trainingWrapper.train()\n",
        "    np.append(train_loss_mean_list,history['train'])\n",
        "    np.append(test_loss_mean_list,history['val'])\n",
        "    np.append(val_loss_mean_list,history['cross_val'])\n",
        "    \n",
        "\n",
        "    if data==\"mnist\":\n",
        "      #trainingWrapper.show_reconstruction(test_loader=test_loader, n_images=50)\n",
        "      pass\n",
        "\n",
        "  elif mode==\"test\":\n",
        "    testWrapper = TestingWrapper(config=config, device=device)\n",
        "    testWrapper.save_reconstruction(test_loader)\n",
        "  \n",
        "  train_loss_mean_list = np.mean(train_loss_mean_list, axis=0)\n",
        "  test_loss_mean_list = np.mean(test_loss_mean_list, axis=0)\n",
        "  val_loss_mean_list = np.mean(val_loss_mean_list, axis=0)\n",
        "  print(\"train_loss_mean_list\",train_loss_mean_list)\n",
        "  print(\"test_loss_mean_list\",test_loss_mean_list)\n",
        "  print(\"val_loss_mean_list\",val_loss_mean_list)\n",
        "  #visualizeTraining(0, train_loss_mean_list, test_loss_mean_list, val_loss_mean_list, save_dir = config.base_dir + \"/results\")\n",
        "\n",
        "  "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOLD 0\n",
            "--------------------------------\n",
            "length of of train_loader is 904 & length of traindataset is 2008\n",
            "length of of test_loader is 101\n",
            "length of of val_loader is 1004\n",
            "in training loop, epoch 1, step 0, the loss is 1127942.25\n",
            "in training loop, epoch 1, step 1, the loss is 1940171.25\n",
            "in training loop, epoch 1, step 2, the loss is 655565.4375\n",
            "in training loop, epoch 1, step 3, the loss is 1369605.75\n",
            "in training loop, epoch 1, step 4, the loss is 2766385.5\n",
            "in training loop, epoch 1, step 5, the loss is 814949.5\n",
            "in training loop, epoch 1, step 6, the loss is 1033977.0\n",
            "in training loop, epoch 1, step 7, the loss is 439433.5\n",
            "in training loop, epoch 1, step 8, the loss is 748764.4375\n",
            "in training loop, epoch 1, step 9, the loss is 1541110.625\n",
            "in training loop, epoch 1, step 10, the loss is 1354556.75\n",
            "in training loop, epoch 1, step 11, the loss is 595433.5\n",
            "in training loop, epoch 1, step 12, the loss is 454003.71875\n",
            "in training loop, epoch 1, step 13, the loss is 482447.09375\n",
            "in training loop, epoch 1, step 14, the loss is 1013819.8125\n",
            "in training loop, epoch 1, step 15, the loss is 920776.5625\n",
            "in training loop, epoch 1, step 16, the loss is 477907.03125\n",
            "in training loop, epoch 1, step 17, the loss is 863762.25\n",
            "in training loop, epoch 1, step 18, the loss is 836735.5\n",
            "in training loop, epoch 1, step 19, the loss is 1244517.625\n",
            "in training loop, epoch 1, step 20, the loss is 829329.0\n",
            "in training loop, epoch 1, step 21, the loss is 1196328.25\n",
            "in training loop, epoch 1, step 22, the loss is 963197.1875\n",
            "in training loop, epoch 1, step 23, the loss is 949882.875\n",
            "in training loop, epoch 1, step 24, the loss is 1469002.125\n",
            "in training loop, epoch 1, step 25, the loss is 572191.5\n",
            "in training loop, epoch 1, step 26, the loss is 1046271.5625\n",
            "in training loop, epoch 1, step 27, the loss is 1805021.5\n",
            "in training loop, epoch 1, step 28, the loss is 680845.75\n",
            "in training loop, epoch 1, step 29, the loss is 1042993.125\n",
            "in training loop, epoch 1, step 30, the loss is 970638.875\n",
            "in training loop, epoch 1, step 31, the loss is 2026975.25\n",
            "in training loop, epoch 1, step 32, the loss is 1264945.875\n",
            "in training loop, epoch 1, step 33, the loss is 1027443.4375\n",
            "in training loop, epoch 1, step 34, the loss is 1037493.8125\n",
            "in training loop, epoch 1, step 35, the loss is 862852.4375\n",
            "in training loop, epoch 1, step 36, the loss is 1674578.25\n",
            "in training loop, epoch 1, step 37, the loss is 552143.4375\n",
            "in training loop, epoch 1, step 38, the loss is 903434.75\n",
            "in training loop, epoch 1, step 39, the loss is 542971.0\n",
            "in training loop, epoch 1, step 40, the loss is 561137.4375\n",
            "in training loop, epoch 1, step 41, the loss is 650955.5\n",
            "in training loop, epoch 1, step 42, the loss is 1063414.25\n",
            "in training loop, epoch 1, step 43, the loss is 1848169.5\n",
            "in training loop, epoch 1, step 44, the loss is 661913.0625\n",
            "in training loop, epoch 1, step 45, the loss is 712130.125\n",
            "in training loop, epoch 1, step 46, the loss is 1256728.0\n",
            "in training loop, epoch 1, step 47, the loss is 708776.1875\n",
            "in training loop, epoch 1, step 48, the loss is 887946.0625\n",
            "in training loop, epoch 1, step 49, the loss is 1108671.25\n",
            "in training loop, epoch 1, step 50, the loss is 1658581.375\n",
            "in training loop, epoch 1, step 51, the loss is 731485.25\n",
            "in training loop, epoch 1, step 52, the loss is 1465556.75\n",
            "in training loop, epoch 1, step 53, the loss is 840438.6875\n",
            "in training loop, epoch 1, step 54, the loss is 759898.125\n",
            "in training loop, epoch 1, step 55, the loss is 746483.75\n",
            "in training loop, epoch 1, step 56, the loss is 1370674.75\n",
            "in training loop, epoch 1, step 57, the loss is 855785.4375\n",
            "in training loop, epoch 1, step 58, the loss is 721545.25\n",
            "in training loop, epoch 1, step 59, the loss is 1208320.375\n",
            "in training loop, epoch 1, step 60, the loss is 824842.125\n",
            "in training loop, epoch 1, step 61, the loss is 1787468.25\n",
            "in training loop, epoch 1, step 62, the loss is 681453.1875\n",
            "in training loop, epoch 1, step 63, the loss is 794198.75\n",
            "in training loop, epoch 1, step 64, the loss is 957645.125\n",
            "in training loop, epoch 1, step 65, the loss is 2037830.75\n",
            "in training loop, epoch 1, step 66, the loss is 969826.3125\n",
            "in training loop, epoch 1, step 67, the loss is 1029094.125\n",
            "in training loop, epoch 1, step 68, the loss is 1310090.75\n",
            "in training loop, epoch 1, step 69, the loss is 972467.9375\n",
            "in training loop, epoch 1, step 70, the loss is 715701.0\n",
            "in training loop, epoch 1, step 71, the loss is 538345.375\n",
            "in training loop, epoch 1, step 72, the loss is 612997.5\n",
            "in training loop, epoch 1, step 73, the loss is 742248.75\n",
            "in training loop, epoch 1, step 74, the loss is 1078247.75\n",
            "in training loop, epoch 1, step 75, the loss is 800418.625\n",
            "in training loop, epoch 1, step 76, the loss is 402872.90625\n",
            "in training loop, epoch 1, step 77, the loss is 653529.75\n",
            "in training loop, epoch 1, step 78, the loss is 562016.0\n",
            "in training loop, epoch 1, step 79, the loss is 1005960.1875\n",
            "in training loop, epoch 1, step 80, the loss is 1146442.75\n",
            "in training loop, epoch 1, step 81, the loss is 753565.25\n",
            "in training loop, epoch 1, step 82, the loss is 1157937.5\n",
            "in training loop, epoch 1, step 83, the loss is 700606.75\n",
            "in training loop, epoch 1, step 84, the loss is 1463041.625\n",
            "in training loop, epoch 1, step 85, the loss is 645481.9375\n",
            "in training loop, epoch 1, step 86, the loss is 704328.5\n",
            "in training loop, epoch 1, step 87, the loss is 686627.0\n",
            "in training loop, epoch 1, step 88, the loss is 3064407.25\n",
            "in training loop, epoch 1, step 89, the loss is 737002.0\n",
            "in training loop, epoch 1, step 90, the loss is 736687.0\n",
            "in training loop, epoch 1, step 91, the loss is 601676.0625\n",
            "in training loop, epoch 1, step 92, the loss is 581931.8125\n",
            "in training loop, epoch 1, step 93, the loss is 1315793.375\n",
            "in training loop, epoch 1, step 94, the loss is 465431.0\n",
            "in training loop, epoch 1, step 95, the loss is 567222.6875\n",
            "in training loop, epoch 1, step 96, the loss is 1396965.0\n",
            "in training loop, epoch 1, step 97, the loss is 824888.8125\n",
            "in training loop, epoch 1, step 98, the loss is 1194872.125\n",
            "in training loop, epoch 1, step 99, the loss is 762024.8125\n",
            "in training loop, epoch 1, step 100, the loss is 799444.625\n",
            "in training loop, epoch 1, step 101, the loss is 1472365.25\n",
            "in training loop, epoch 1, step 102, the loss is 801845.6875\n",
            "in training loop, epoch 1, step 103, the loss is 687355.0\n",
            "in training loop, epoch 1, step 104, the loss is 1372580.0\n",
            "in training loop, epoch 1, step 105, the loss is 740252.5625\n",
            "in training loop, epoch 1, step 106, the loss is 955741.4375\n",
            "in training loop, epoch 1, step 107, the loss is 635128.75\n",
            "in training loop, epoch 1, step 108, the loss is 1126559.0\n",
            "in training loop, epoch 1, step 109, the loss is 642736.6875\n",
            "in training loop, epoch 1, step 110, the loss is 982362.9375\n",
            "in training loop, epoch 1, step 111, the loss is 1127159.875\n",
            "in training loop, epoch 1, step 112, the loss is 754163.625\n",
            "in training loop, epoch 1, step 113, the loss is 1247492.75\n",
            "in training loop, epoch 1, step 114, the loss is 594987.875\n",
            "in training loop, epoch 1, step 115, the loss is 1128364.375\n",
            "in training loop, epoch 1, step 116, the loss is 1091124.625\n",
            "in training loop, epoch 1, step 117, the loss is 785104.6875\n",
            "in training loop, epoch 1, step 118, the loss is 2379713.25\n",
            "in training loop, epoch 1, step 119, the loss is 1128016.875\n",
            "in training loop, epoch 1, step 120, the loss is 1079586.875\n",
            "in training loop, epoch 1, step 121, the loss is 1571121.625\n",
            "in training loop, epoch 1, step 122, the loss is 868203.3125\n",
            "in training loop, epoch 1, step 123, the loss is 1730870.25\n",
            "in training loop, epoch 1, step 124, the loss is 813895.375\n",
            "in training loop, epoch 1, step 125, the loss is 677122.75\n",
            "in training loop, epoch 1, step 126, the loss is 574867.375\n",
            "in training loop, epoch 1, step 127, the loss is 895603.125\n",
            "in training loop, epoch 1, step 128, the loss is 848064.0625\n",
            "in training loop, epoch 1, step 129, the loss is 843432.125\n",
            "in training loop, epoch 1, step 130, the loss is 938453.3125\n",
            "in training loop, epoch 1, step 131, the loss is 694605.75\n",
            "in training loop, epoch 1, step 132, the loss is 732635.75\n",
            "in training loop, epoch 1, step 133, the loss is 1432317.125\n",
            "in training loop, epoch 1, step 134, the loss is 710108.75\n",
            "in training loop, epoch 1, step 135, the loss is 1440528.625\n",
            "in training loop, epoch 1, step 136, the loss is 732319.75\n",
            "in training loop, epoch 1, step 137, the loss is 804661.5\n",
            "in training loop, epoch 1, step 138, the loss is 883294.6875\n",
            "in training loop, epoch 1, step 139, the loss is 935803.9375\n",
            "in training loop, epoch 1, step 140, the loss is 892233.625\n",
            "in training loop, epoch 1, step 141, the loss is 661520.75\n",
            "in training loop, epoch 1, step 142, the loss is 1225744.125\n",
            "in training loop, epoch 1, step 143, the loss is 1450880.125\n",
            "in training loop, epoch 1, step 144, the loss is 832612.625\n",
            "in training loop, epoch 1, step 145, the loss is 684112.0\n",
            "in training loop, epoch 1, step 146, the loss is 910221.125\n",
            "in training loop, epoch 1, step 147, the loss is 1102939.75\n",
            "in training loop, epoch 1, step 148, the loss is 1407995.75\n",
            "in training loop, epoch 1, step 149, the loss is 1218790.5\n",
            "in training loop, epoch 1, step 150, the loss is 1037210.125\n",
            "in training loop, epoch 1, step 151, the loss is 837917.375\n",
            "in training loop, epoch 1, step 152, the loss is 1443033.5\n",
            "in training loop, epoch 1, step 153, the loss is 873676.25\n",
            "in training loop, epoch 1, step 154, the loss is 2440990.5\n",
            "in training loop, epoch 1, step 155, the loss is 1126383.5\n",
            "in training loop, epoch 1, step 156, the loss is 640003.625\n",
            "in training loop, epoch 1, step 157, the loss is 974423.25\n",
            "in training loop, epoch 1, step 158, the loss is 786623.125\n",
            "in training loop, epoch 1, step 159, the loss is 1153155.625\n",
            "in training loop, epoch 1, step 160, the loss is 955935.75\n",
            "in training loop, epoch 1, step 161, the loss is 1378219.875\n",
            "in training loop, epoch 1, step 162, the loss is 1272393.625\n",
            "in training loop, epoch 1, step 163, the loss is 922524.5\n",
            "in training loop, epoch 1, step 164, the loss is 944288.5\n",
            "in training loop, epoch 1, step 165, the loss is 1107918.625\n",
            "in training loop, epoch 1, step 166, the loss is 1131589.625\n",
            "in training loop, epoch 1, step 167, the loss is 1212181.0\n",
            "in training loop, epoch 1, step 168, the loss is 1861842.5\n",
            "in training loop, epoch 1, step 169, the loss is 630862.9375\n",
            "in training loop, epoch 1, step 170, the loss is 853240.1875\n",
            "in training loop, epoch 1, step 171, the loss is 668244.5\n",
            "in training loop, epoch 1, step 172, the loss is 950106.9375\n",
            "in training loop, epoch 1, step 173, the loss is 526260.5\n",
            "in training loop, epoch 1, step 174, the loss is 673838.3125\n",
            "in training loop, epoch 1, step 175, the loss is 1145153.625\n",
            "in training loop, epoch 1, step 176, the loss is 435006.96875\n",
            "in training loop, epoch 1, step 177, the loss is 542040.375\n",
            "in training loop, epoch 1, step 178, the loss is 873501.25\n",
            "in training loop, epoch 1, step 179, the loss is 828053.25\n",
            "in training loop, epoch 1, step 180, the loss is 1403596.5\n",
            "in training loop, epoch 1, step 181, the loss is 1157575.0\n",
            "in training loop, epoch 1, step 182, the loss is 1378471.25\n",
            "in training loop, epoch 1, step 183, the loss is 524305.75\n",
            "in training loop, epoch 1, step 184, the loss is 789524.75\n",
            "in training loop, epoch 1, step 185, the loss is 1027152.0\n",
            "in training loop, epoch 1, step 186, the loss is 1033518.3125\n",
            "in training loop, epoch 1, step 187, the loss is 1156451.375\n",
            "in training loop, epoch 1, step 188, the loss is 760401.625\n",
            "in training loop, epoch 1, step 189, the loss is 1380909.5\n",
            "in training loop, epoch 1, step 190, the loss is 870187.375\n",
            "in training loop, epoch 1, step 191, the loss is 704414.875\n",
            "in training loop, epoch 1, step 192, the loss is 900564.375\n",
            "in training loop, epoch 1, step 193, the loss is 578439.5625\n",
            "in training loop, epoch 1, step 194, the loss is 681155.625\n",
            "in training loop, epoch 1, step 195, the loss is 794219.0625\n",
            "in training loop, epoch 1, step 196, the loss is 784297.375\n",
            "in training loop, epoch 1, step 197, the loss is 660190.75\n",
            "in training loop, epoch 1, step 198, the loss is 1783231.0\n",
            "in training loop, epoch 1, step 199, the loss is 738547.75\n",
            "in training loop, epoch 1, step 200, the loss is 843364.625\n",
            "in training loop, epoch 1, step 201, the loss is 1232620.0\n",
            "in training loop, epoch 1, step 202, the loss is 1362091.75\n",
            "in training loop, epoch 1, step 203, the loss is 507115.75\n",
            "in training loop, epoch 1, step 204, the loss is 1011937.0\n",
            "in training loop, epoch 1, step 205, the loss is 992848.125\n",
            "in training loop, epoch 1, step 206, the loss is 898506.5625\n",
            "in training loop, epoch 1, step 207, the loss is 1746234.25\n",
            "in training loop, epoch 1, step 208, the loss is 749677.6875\n",
            "in training loop, epoch 1, step 209, the loss is 376440.4375\n",
            "in training loop, epoch 1, step 210, the loss is 510291.875\n",
            "in training loop, epoch 1, step 211, the loss is 858544.0\n",
            "in training loop, epoch 1, step 212, the loss is 1257306.0\n",
            "in training loop, epoch 1, step 213, the loss is 758193.75\n",
            "in training loop, epoch 1, step 214, the loss is 604535.375\n",
            "in training loop, epoch 1, step 215, the loss is 1295195.625\n",
            "in training loop, epoch 1, step 216, the loss is 1090010.75\n",
            "in training loop, epoch 1, step 217, the loss is 1228604.375\n",
            "in training loop, epoch 1, step 218, the loss is 1175336.875\n",
            "in training loop, epoch 1, step 219, the loss is 1079469.625\n",
            "in training loop, epoch 1, step 220, the loss is 1279482.0\n",
            "in training loop, epoch 1, step 221, the loss is 1300434.125\n",
            "in training loop, epoch 1, step 222, the loss is 706940.75\n",
            "in training loop, epoch 1, step 223, the loss is 504535.5625\n",
            "in training loop, epoch 1, step 224, the loss is 1123055.25\n",
            "in training loop, epoch 1, step 225, the loss is 806300.5625\n",
            "in training loop, epoch 1, step 226, the loss is 722606.25\n",
            "in training loop, epoch 1, step 227, the loss is 566800.875\n",
            "in training loop, epoch 1, step 228, the loss is 1108642.0\n",
            "in training loop, epoch 1, step 229, the loss is 1196190.875\n",
            "in training loop, epoch 1, step 230, the loss is 755766.9375\n",
            "in training loop, epoch 1, step 231, the loss is 1364908.5\n",
            "in training loop, epoch 1, step 232, the loss is 910311.75\n",
            "in training loop, epoch 1, step 233, the loss is 865119.625\n",
            "in training loop, epoch 1, step 234, the loss is 1424060.0\n",
            "in training loop, epoch 1, step 235, the loss is 593046.9375\n",
            "in training loop, epoch 1, step 236, the loss is 966777.8125\n",
            "in training loop, epoch 1, step 237, the loss is 1329557.625\n",
            "in training loop, epoch 1, step 238, the loss is 712544.875\n",
            "in training loop, epoch 1, step 239, the loss is 618551.8125\n",
            "in training loop, epoch 1, step 240, the loss is 705543.6875\n",
            "in training loop, epoch 1, step 241, the loss is 1339930.875\n",
            "in training loop, epoch 1, step 242, the loss is 688369.5625\n",
            "in training loop, epoch 1, step 243, the loss is 703406.625\n",
            "in training loop, epoch 1, step 244, the loss is 781964.0625\n",
            "in training loop, epoch 1, step 245, the loss is 377276.84375\n",
            "in training loop, epoch 1, step 246, the loss is 662014.1875\n",
            "in training loop, epoch 1, step 247, the loss is 1011321.625\n",
            "in training loop, epoch 1, step 248, the loss is 791299.5\n",
            "in training loop, epoch 1, step 249, the loss is 682764.25\n",
            "in training loop, epoch 1, step 250, the loss is 1358293.5\n",
            "in training loop, epoch 1, step 251, the loss is 1220540.5\n",
            "in training loop, epoch 1, step 252, the loss is 995254.9375\n",
            "in training loop, epoch 1, step 253, the loss is 1385051.125\n",
            "in training loop, epoch 1, step 254, the loss is 1157341.625\n",
            "in training loop, epoch 1, step 255, the loss is 658715.4375\n",
            "in training loop, epoch 1, step 256, the loss is 557508.375\n",
            "in training loop, epoch 1, step 257, the loss is 1583295.625\n",
            "in training loop, epoch 1, step 258, the loss is 855507.125\n",
            "in training loop, epoch 1, step 259, the loss is 967195.875\n",
            "in training loop, epoch 1, step 260, the loss is 563909.9375\n",
            "in training loop, epoch 1, step 261, the loss is 605189.25\n",
            "in training loop, epoch 1, step 262, the loss is 1166385.25\n",
            "in training loop, epoch 1, step 263, the loss is 1004135.875\n",
            "in training loop, epoch 1, step 264, the loss is 529833.25\n",
            "in training loop, epoch 1, step 265, the loss is 756407.0\n",
            "in training loop, epoch 1, step 266, the loss is 527266.6875\n",
            "in training loop, epoch 1, step 267, the loss is 427713.8125\n",
            "in training loop, epoch 1, step 268, the loss is 861416.5\n",
            "in training loop, epoch 1, step 269, the loss is 412890.5625\n",
            "in training loop, epoch 1, step 270, the loss is 1022729.5\n",
            "in training loop, epoch 1, step 271, the loss is 1056806.75\n",
            "in training loop, epoch 1, step 272, the loss is 694783.6875\n",
            "in training loop, epoch 1, step 273, the loss is 810462.5625\n",
            "in training loop, epoch 1, step 274, the loss is 1133497.75\n",
            "in training loop, epoch 1, step 275, the loss is 658158.4375\n",
            "in training loop, epoch 1, step 276, the loss is 994931.1875\n",
            "in training loop, epoch 1, step 277, the loss is 1079201.5\n",
            "in training loop, epoch 1, step 278, the loss is 591931.75\n",
            "in training loop, epoch 1, step 279, the loss is 1013368.25\n",
            "in training loop, epoch 1, step 280, the loss is 869406.1875\n",
            "in training loop, epoch 1, step 281, the loss is 483848.28125\n",
            "in training loop, epoch 1, step 282, the loss is 1039516.6875\n",
            "in training loop, epoch 1, step 283, the loss is 1016779.125\n",
            "in training loop, epoch 1, step 284, the loss is 1101680.125\n",
            "in training loop, epoch 1, step 285, the loss is 670593.1875\n",
            "in training loop, epoch 1, step 286, the loss is 413309.15625\n",
            "in training loop, epoch 1, step 287, the loss is 744422.8125\n",
            "in training loop, epoch 1, step 288, the loss is 1221300.0\n",
            "in training loop, epoch 1, step 289, the loss is 514338.0625\n",
            "in training loop, epoch 1, step 290, the loss is 560097.5625\n",
            "in training loop, epoch 1, step 291, the loss is 736224.0625\n",
            "in training loop, epoch 1, step 292, the loss is 1094350.5\n",
            "in training loop, epoch 1, step 293, the loss is 959912.25\n",
            "in training loop, epoch 1, step 294, the loss is 546791.1875\n",
            "in training loop, epoch 1, step 295, the loss is 630065.5625\n",
            "in training loop, epoch 1, step 296, the loss is 838888.0\n",
            "in training loop, epoch 1, step 297, the loss is 483961.375\n",
            "in training loop, epoch 1, step 298, the loss is 760479.375\n",
            "in training loop, epoch 1, step 299, the loss is 937888.5\n",
            "in training loop, epoch 1, step 300, the loss is 565274.3125\n",
            "in training loop, epoch 1, step 301, the loss is 621320.3125\n",
            "in training loop, epoch 1, step 302, the loss is 696043.0625\n",
            "in training loop, epoch 1, step 303, the loss is 587883.0\n",
            "in training loop, epoch 1, step 304, the loss is 928473.0\n",
            "in training loop, epoch 1, step 305, the loss is 444236.78125\n",
            "in training loop, epoch 1, step 306, the loss is 767911.375\n",
            "in training loop, epoch 1, step 307, the loss is 1234868.0\n",
            "in training loop, epoch 1, step 308, the loss is 949337.125\n",
            "in training loop, epoch 1, step 309, the loss is 1348338.75\n",
            "in training loop, epoch 1, step 310, the loss is 1358049.0\n",
            "in training loop, epoch 1, step 311, the loss is 340189.09375\n",
            "in training loop, epoch 1, step 312, the loss is 822171.4375\n",
            "in training loop, epoch 1, step 313, the loss is 782747.375\n",
            "in training loop, epoch 1, step 314, the loss is 746422.0\n",
            "in training loop, epoch 1, step 315, the loss is 580460.5625\n",
            "in training loop, epoch 1, step 316, the loss is 975075.5\n",
            "in training loop, epoch 1, step 317, the loss is 886387.375\n",
            "in training loop, epoch 1, step 318, the loss is 1797690.0\n",
            "in training loop, epoch 1, step 319, the loss is 873109.875\n",
            "in training loop, epoch 1, step 320, the loss is 1026395.0\n",
            "in training loop, epoch 1, step 321, the loss is 672929.625\n",
            "in training loop, epoch 1, step 322, the loss is 932953.125\n",
            "in training loop, epoch 1, step 323, the loss is 485741.625\n",
            "in training loop, epoch 1, step 324, the loss is 1598155.25\n",
            "in training loop, epoch 1, step 325, the loss is 760130.9375\n",
            "in training loop, epoch 1, step 326, the loss is 654481.9375\n",
            "in training loop, epoch 1, step 327, the loss is 593393.0625\n",
            "in training loop, epoch 1, step 328, the loss is 741514.875\n",
            "in training loop, epoch 1, step 329, the loss is 1130988.875\n",
            "in training loop, epoch 1, step 330, the loss is 762084.5\n",
            "in training loop, epoch 1, step 331, the loss is 709808.0625\n",
            "in training loop, epoch 1, step 332, the loss is 572467.125\n",
            "in training loop, epoch 1, step 333, the loss is 1027847.75\n",
            "in training loop, epoch 1, step 334, the loss is 1332149.0\n",
            "in training loop, epoch 1, step 335, the loss is 369847.5\n",
            "in training loop, epoch 1, step 336, the loss is 592503.0625\n",
            "in training loop, epoch 1, step 337, the loss is 533640.25\n",
            "in training loop, epoch 1, step 338, the loss is 861285.0625\n",
            "in training loop, epoch 1, step 339, the loss is 1478228.5\n",
            "in training loop, epoch 1, step 340, the loss is 974942.25\n",
            "in training loop, epoch 1, step 341, the loss is 853475.6875\n",
            "in training loop, epoch 1, step 342, the loss is 813859.4375\n",
            "in training loop, epoch 1, step 343, the loss is 814229.625\n",
            "in training loop, epoch 1, step 344, the loss is 1052111.125\n",
            "in training loop, epoch 1, step 345, the loss is 771285.6875\n",
            "in training loop, epoch 1, step 346, the loss is 926248.1875\n",
            "in training loop, epoch 1, step 347, the loss is 900179.0625\n",
            "in training loop, epoch 1, step 348, the loss is 789259.4375\n",
            "in training loop, epoch 1, step 349, the loss is 723626.5625\n",
            "in training loop, epoch 1, step 350, the loss is 1047053.0625\n",
            "in training loop, epoch 1, step 351, the loss is 1085217.75\n",
            "in training loop, epoch 1, step 352, the loss is 607829.5625\n",
            "in training loop, epoch 1, step 353, the loss is 740183.5\n",
            "in training loop, epoch 1, step 354, the loss is 871322.125\n",
            "in training loop, epoch 1, step 355, the loss is 891147.75\n",
            "in training loop, epoch 1, step 356, the loss is 992760.5625\n",
            "in training loop, epoch 1, step 357, the loss is 794251.0\n",
            "in training loop, epoch 1, step 358, the loss is 1021310.5625\n",
            "in training loop, epoch 1, step 359, the loss is 779825.3125\n",
            "in training loop, epoch 1, step 360, the loss is 494083.65625\n",
            "in training loop, epoch 1, step 361, the loss is 948830.125\n",
            "in training loop, epoch 1, step 362, the loss is 777885.5\n",
            "in training loop, epoch 1, step 363, the loss is 759401.5\n",
            "in training loop, epoch 1, step 364, the loss is 595489.8125\n",
            "in training loop, epoch 1, step 365, the loss is 656699.125\n",
            "in training loop, epoch 1, step 366, the loss is 787059.0\n",
            "in training loop, epoch 1, step 367, the loss is 1172066.5\n",
            "in training loop, epoch 1, step 368, the loss is 807663.625\n",
            "in training loop, epoch 1, step 369, the loss is 721162.875\n",
            "in training loop, epoch 1, step 370, the loss is 829087.5625\n",
            "in training loop, epoch 1, step 371, the loss is 823072.25\n",
            "in training loop, epoch 1, step 372, the loss is 432331.75\n",
            "in training loop, epoch 1, step 373, the loss is 457337.375\n",
            "in training loop, epoch 1, step 374, the loss is 853108.75\n",
            "in training loop, epoch 1, step 375, the loss is 712435.75\n",
            "in training loop, epoch 1, step 376, the loss is 712586.75\n",
            "in training loop, epoch 1, step 377, the loss is 1112731.75\n",
            "in training loop, epoch 1, step 378, the loss is 737491.3125\n",
            "in training loop, epoch 1, step 379, the loss is 713476.1875\n",
            "in training loop, epoch 1, step 380, the loss is 891270.0\n",
            "in training loop, epoch 1, step 381, the loss is 1012979.0625\n",
            "in training loop, epoch 1, step 382, the loss is 1496427.75\n",
            "in training loop, epoch 1, step 383, the loss is 506565.25\n",
            "in training loop, epoch 1, step 384, the loss is 1012474.5625\n",
            "in training loop, epoch 1, step 385, the loss is 1015366.3125\n",
            "in training loop, epoch 1, step 386, the loss is 742413.875\n",
            "in training loop, epoch 1, step 387, the loss is 1222397.5\n",
            "in training loop, epoch 1, step 388, the loss is 1475463.125\n",
            "in training loop, epoch 1, step 389, the loss is 1257942.625\n",
            "in training loop, epoch 1, step 390, the loss is 1143480.875\n",
            "in training loop, epoch 1, step 391, the loss is 800128.0\n",
            "in training loop, epoch 1, step 392, the loss is 719293.6875\n",
            "in training loop, epoch 1, step 393, the loss is 1036433.5\n",
            "in training loop, epoch 1, step 394, the loss is 877478.125\n",
            "in training loop, epoch 1, step 395, the loss is 771912.375\n",
            "in training loop, epoch 1, step 396, the loss is 608418.0\n",
            "in training loop, epoch 1, step 397, the loss is 631898.875\n",
            "in training loop, epoch 1, step 398, the loss is 553585.0\n",
            "in training loop, epoch 1, step 399, the loss is 809187.75\n",
            "in training loop, epoch 1, step 400, the loss is 1059897.0\n",
            "in training loop, epoch 1, step 401, the loss is 650389.375\n",
            "in training loop, epoch 1, step 402, the loss is 546309.875\n",
            "in training loop, epoch 1, step 403, the loss is 687793.625\n",
            "in training loop, epoch 1, step 404, the loss is 1256650.0\n",
            "in training loop, epoch 1, step 405, the loss is 1186657.375\n",
            "in training loop, epoch 1, step 406, the loss is 537580.0\n",
            "in training loop, epoch 1, step 407, the loss is 785965.875\n",
            "in training loop, epoch 1, step 408, the loss is 760600.125\n",
            "in training loop, epoch 1, step 409, the loss is 1136678.5\n",
            "in training loop, epoch 1, step 410, the loss is 1806008.375\n",
            "in training loop, epoch 1, step 411, the loss is 978531.25\n",
            "in training loop, epoch 1, step 412, the loss is 850904.625\n",
            "in training loop, epoch 1, step 413, the loss is 981883.0\n",
            "in training loop, epoch 1, step 414, the loss is 1158626.0\n",
            "in training loop, epoch 1, step 415, the loss is 420157.1875\n",
            "in training loop, epoch 1, step 416, the loss is 756338.9375\n",
            "in training loop, epoch 1, step 417, the loss is 759176.5\n",
            "in training loop, epoch 1, step 418, the loss is 758573.1875\n",
            "in training loop, epoch 1, step 419, the loss is 639832.375\n",
            "in training loop, epoch 1, step 420, the loss is 459976.40625\n",
            "in training loop, epoch 1, step 421, the loss is 876140.0\n",
            "in training loop, epoch 1, step 422, the loss is 1057101.75\n",
            "in training loop, epoch 1, step 423, the loss is 872759.9375\n",
            "in training loop, epoch 1, step 424, the loss is 695441.5625\n",
            "in training loop, epoch 1, step 425, the loss is 1111041.375\n",
            "in training loop, epoch 1, step 426, the loss is 885277.8125\n",
            "in training loop, epoch 1, step 427, the loss is 946555.9375\n",
            "in training loop, epoch 1, step 428, the loss is 684646.5625\n",
            "in training loop, epoch 1, step 429, the loss is 1070929.25\n",
            "in training loop, epoch 1, step 430, the loss is 1241336.25\n",
            "in training loop, epoch 1, step 431, the loss is 871302.0\n",
            "in training loop, epoch 1, step 432, the loss is 814784.125\n",
            "in training loop, epoch 1, step 433, the loss is 857125.3125\n",
            "in training loop, epoch 1, step 434, the loss is 1011022.75\n",
            "in training loop, epoch 1, step 435, the loss is 1095686.5\n",
            "in training loop, epoch 1, step 436, the loss is 1242827.875\n",
            "in training loop, epoch 1, step 437, the loss is 670573.6875\n",
            "in training loop, epoch 1, step 438, the loss is 1379432.75\n",
            "in training loop, epoch 1, step 439, the loss is 450734.125\n",
            "in training loop, epoch 1, step 440, the loss is 582236.375\n",
            "in training loop, epoch 1, step 441, the loss is 1182180.25\n",
            "in training loop, epoch 1, step 442, the loss is 480352.71875\n",
            "in training loop, epoch 1, step 443, the loss is 872930.75\n",
            "in training loop, epoch 1, step 444, the loss is 827915.25\n",
            "in training loop, epoch 1, step 445, the loss is 306429.0\n",
            "in training loop, epoch 1, step 446, the loss is 1108638.0\n",
            "in training loop, epoch 1, step 447, the loss is 403765.625\n",
            "in training loop, epoch 1, step 448, the loss is 462865.125\n",
            "in training loop, epoch 1, step 449, the loss is 1150373.5\n",
            "in training loop, epoch 1, step 450, the loss is 856008.625\n",
            "in training loop, epoch 1, step 451, the loss is 1103801.25\n",
            "in training loop, epoch 1, step 452, the loss is 1435660.25\n",
            "in training loop, epoch 1, step 453, the loss is 606472.5\n",
            "in training loop, epoch 1, step 454, the loss is 1479316.5\n",
            "in training loop, epoch 1, step 455, the loss is 661296.625\n",
            "in training loop, epoch 1, step 456, the loss is 524895.625\n",
            "in training loop, epoch 1, step 457, the loss is 875857.8125\n",
            "in training loop, epoch 1, step 458, the loss is 911098.375\n",
            "in training loop, epoch 1, step 459, the loss is 684198.625\n",
            "in training loop, epoch 1, step 460, the loss is 610330.9375\n",
            "in training loop, epoch 1, step 461, the loss is 660305.9375\n",
            "in training loop, epoch 1, step 462, the loss is 1103363.75\n",
            "in training loop, epoch 1, step 463, the loss is 822578.4375\n",
            "in training loop, epoch 1, step 464, the loss is 499179.78125\n",
            "in training loop, epoch 1, step 465, the loss is 1140195.625\n",
            "in training loop, epoch 1, step 466, the loss is 1146055.25\n",
            "in training loop, epoch 1, step 467, the loss is 923171.5625\n",
            "in training loop, epoch 1, step 468, the loss is 789639.75\n",
            "in training loop, epoch 1, step 469, the loss is 843516.0\n",
            "in training loop, epoch 1, step 470, the loss is 453196.875\n",
            "in training loop, epoch 1, step 471, the loss is 827998.3125\n",
            "in training loop, epoch 1, step 472, the loss is 1640401.5\n",
            "in training loop, epoch 1, step 473, the loss is 486956.75\n",
            "in training loop, epoch 1, step 474, the loss is 1590877.875\n",
            "in training loop, epoch 1, step 475, the loss is 732639.375\n",
            "in training loop, epoch 1, step 476, the loss is 986157.9375\n",
            "in training loop, epoch 1, step 477, the loss is 419750.125\n",
            "in training loop, epoch 1, step 478, the loss is 1043586.0\n",
            "in training loop, epoch 1, step 479, the loss is 1021804.75\n",
            "in training loop, epoch 1, step 480, the loss is 1265275.75\n",
            "in training loop, epoch 1, step 481, the loss is 452293.5625\n",
            "in training loop, epoch 1, step 482, the loss is 531008.5\n",
            "in training loop, epoch 1, step 483, the loss is 487967.6875\n",
            "in training loop, epoch 1, step 484, the loss is 597108.75\n",
            "in training loop, epoch 1, step 485, the loss is 1167664.375\n",
            "in training loop, epoch 1, step 486, the loss is 501443.0\n",
            "in training loop, epoch 1, step 487, the loss is 675637.375\n",
            "in training loop, epoch 1, step 488, the loss is 383070.75\n",
            "in training loop, epoch 1, step 489, the loss is 1057246.625\n",
            "in training loop, epoch 1, step 490, the loss is 1118563.25\n",
            "in training loop, epoch 1, step 491, the loss is 942798.4375\n",
            "in training loop, epoch 1, step 492, the loss is 1281914.0\n",
            "in training loop, epoch 1, step 493, the loss is 1169329.625\n",
            "in training loop, epoch 1, step 494, the loss is 862276.0\n",
            "in training loop, epoch 1, step 495, the loss is 1046552.875\n",
            "in training loop, epoch 1, step 496, the loss is 454485.4375\n",
            "in training loop, epoch 1, step 497, the loss is 855509.6875\n",
            "in training loop, epoch 1, step 498, the loss is 791229.375\n",
            "in training loop, epoch 1, step 499, the loss is 808632.6875\n",
            "in training loop, epoch 1, step 500, the loss is 1011657.125\n",
            "in training loop, epoch 1, step 501, the loss is 470159.09375\n",
            "in training loop, epoch 1, step 502, the loss is 556044.1875\n",
            "in training loop, epoch 1, step 503, the loss is 1220502.25\n",
            "in training loop, epoch 1, step 504, the loss is 1081623.25\n",
            "in training loop, epoch 1, step 505, the loss is 497991.78125\n",
            "in training loop, epoch 1, step 506, the loss is 846758.625\n",
            "in training loop, epoch 1, step 507, the loss is 597372.6875\n",
            "in training loop, epoch 1, step 508, the loss is 848000.125\n",
            "in training loop, epoch 1, step 509, the loss is 712366.5\n",
            "in training loop, epoch 1, step 510, the loss is 1230386.25\n",
            "in training loop, epoch 1, step 511, the loss is 868664.625\n",
            "in training loop, epoch 1, step 512, the loss is 371501.6875\n",
            "in training loop, epoch 1, step 513, the loss is 1287647.5\n",
            "in training loop, epoch 1, step 514, the loss is 1166347.0\n",
            "in training loop, epoch 1, step 515, the loss is 842586.1875\n",
            "in training loop, epoch 1, step 516, the loss is 872175.8125\n",
            "in training loop, epoch 1, step 517, the loss is 834041.875\n",
            "in training loop, epoch 1, step 518, the loss is 1622166.5\n",
            "in training loop, epoch 1, step 519, the loss is 935209.375\n",
            "in training loop, epoch 1, step 520, the loss is 722005.1875\n",
            "in training loop, epoch 1, step 521, the loss is 748453.1875\n",
            "in training loop, epoch 1, step 522, the loss is 922278.25\n",
            "in training loop, epoch 1, step 523, the loss is 1013304.875\n",
            "in training loop, epoch 1, step 524, the loss is 1176233.125\n",
            "in training loop, epoch 1, step 525, the loss is 460122.90625\n",
            "in training loop, epoch 1, step 526, the loss is 1004158.9375\n",
            "in training loop, epoch 1, step 527, the loss is 1238231.625\n",
            "in training loop, epoch 1, step 528, the loss is 568491.4375\n",
            "in training loop, epoch 1, step 529, the loss is 1264681.875\n",
            "in training loop, epoch 1, step 530, the loss is 848711.8125\n",
            "in training loop, epoch 1, step 531, the loss is 1382885.0\n",
            "in training loop, epoch 1, step 532, the loss is 753015.125\n",
            "in training loop, epoch 1, step 533, the loss is 506136.21875\n",
            "in training loop, epoch 1, step 534, the loss is 850316.8125\n",
            "in training loop, epoch 1, step 535, the loss is 1195950.5\n",
            "in training loop, epoch 1, step 536, the loss is 677786.0625\n",
            "in training loop, epoch 1, step 537, the loss is 893265.0625\n",
            "in training loop, epoch 1, step 538, the loss is 1110498.375\n",
            "in training loop, epoch 1, step 539, the loss is 888882.6875\n",
            "in training loop, epoch 1, step 540, the loss is 1318451.875\n",
            "in training loop, epoch 1, step 541, the loss is 638427.25\n",
            "in training loop, epoch 1, step 542, the loss is 461753.25\n",
            "in training loop, epoch 1, step 543, the loss is 1002190.8125\n",
            "in training loop, epoch 1, step 544, the loss is 787645.0\n",
            "in training loop, epoch 1, step 545, the loss is 1102281.875\n",
            "in training loop, epoch 1, step 546, the loss is 1137461.875\n",
            "in training loop, epoch 1, step 547, the loss is 1198088.5\n",
            "in training loop, epoch 1, step 548, the loss is 575623.5\n",
            "in training loop, epoch 1, step 549, the loss is 699198.3125\n",
            "in training loop, epoch 1, step 550, the loss is 1967965.75\n",
            "in training loop, epoch 1, step 551, the loss is 955819.5\n",
            "in training loop, epoch 1, step 552, the loss is 811010.8125\n",
            "in training loop, epoch 1, step 553, the loss is 675786.5\n",
            "in training loop, epoch 1, step 554, the loss is 749661.25\n",
            "in training loop, epoch 1, step 555, the loss is 750290.5\n",
            "in training loop, epoch 1, step 556, the loss is 937977.0\n",
            "in training loop, epoch 1, step 557, the loss is 905559.3125\n",
            "in training loop, epoch 1, step 558, the loss is 1094663.625\n",
            "in training loop, epoch 1, step 559, the loss is 628536.75\n",
            "in training loop, epoch 1, step 560, the loss is 968838.0\n",
            "in training loop, epoch 1, step 561, the loss is 926364.875\n",
            "in training loop, epoch 1, step 562, the loss is 2097086.875\n",
            "in training loop, epoch 1, step 563, the loss is 474865.375\n",
            "in training loop, epoch 1, step 564, the loss is 679832.875\n",
            "in training loop, epoch 1, step 565, the loss is 654722.375\n",
            "in training loop, epoch 1, step 566, the loss is 760287.5625\n",
            "in training loop, epoch 1, step 567, the loss is 1177847.375\n",
            "in training loop, epoch 1, step 568, the loss is 963541.75\n",
            "in training loop, epoch 1, step 569, the loss is 449185.34375\n",
            "in training loop, epoch 1, step 570, the loss is 943136.6875\n",
            "in training loop, epoch 1, step 571, the loss is 964651.9375\n",
            "in training loop, epoch 1, step 572, the loss is 1498529.125\n",
            "in training loop, epoch 1, step 573, the loss is 1110497.75\n",
            "in training loop, epoch 1, step 574, the loss is 783384.8125\n",
            "in training loop, epoch 1, step 575, the loss is 1437602.375\n",
            "in training loop, epoch 1, step 576, the loss is 896934.5625\n",
            "in training loop, epoch 1, step 577, the loss is 1665739.25\n",
            "in training loop, epoch 1, step 578, the loss is 1336632.875\n",
            "in training loop, epoch 1, step 579, the loss is 1177350.125\n",
            "in training loop, epoch 1, step 580, the loss is 386026.375\n",
            "in training loop, epoch 1, step 581, the loss is 695306.625\n",
            "in training loop, epoch 1, step 582, the loss is 753302.4375\n",
            "in training loop, epoch 1, step 583, the loss is 345088.09375\n",
            "in training loop, epoch 1, step 584, the loss is 750463.4375\n",
            "in training loop, epoch 1, step 585, the loss is 433173.875\n",
            "in training loop, epoch 1, step 586, the loss is 1076436.5\n",
            "in training loop, epoch 1, step 587, the loss is 491729.625\n",
            "in training loop, epoch 1, step 588, the loss is 835704.9375\n",
            "in training loop, epoch 1, step 589, the loss is 814962.625\n",
            "in training loop, epoch 1, step 590, the loss is 1415669.75\n",
            "in training loop, epoch 1, step 591, the loss is 1213266.0\n",
            "in training loop, epoch 1, step 592, the loss is 1017899.75\n",
            "in training loop, epoch 1, step 593, the loss is 801636.625\n",
            "in training loop, epoch 1, step 594, the loss is 933104.3125\n",
            "in training loop, epoch 1, step 595, the loss is 1285422.5\n",
            "in training loop, epoch 1, step 596, the loss is 829851.75\n",
            "in training loop, epoch 1, step 597, the loss is 777819.25\n",
            "in training loop, epoch 1, step 598, the loss is 776718.8125\n",
            "in training loop, epoch 1, step 599, the loss is 1043026.125\n",
            "in training loop, epoch 1, step 600, the loss is 740716.25\n",
            "in training loop, epoch 1, step 601, the loss is 677204.3125\n",
            "in training loop, epoch 1, step 602, the loss is 573391.8125\n",
            "in training loop, epoch 1, step 603, the loss is 544134.125\n",
            "in training loop, epoch 1, step 604, the loss is 550562.5625\n",
            "in training loop, epoch 1, step 605, the loss is 1041295.625\n",
            "in training loop, epoch 1, step 606, the loss is 569202.25\n",
            "in training loop, epoch 1, step 607, the loss is 882127.9375\n",
            "in training loop, epoch 1, step 608, the loss is 1135943.5\n",
            "in training loop, epoch 1, step 609, the loss is 865450.8125\n",
            "in training loop, epoch 1, step 610, the loss is 822507.8125\n",
            "in training loop, epoch 1, step 611, the loss is 818670.5625\n",
            "in training loop, epoch 1, step 612, the loss is 1177850.625\n",
            "in training loop, epoch 1, step 613, the loss is 476751.21875\n",
            "in training loop, epoch 1, step 614, the loss is 738135.1875\n",
            "in training loop, epoch 1, step 615, the loss is 1165342.125\n",
            "in training loop, epoch 1, step 616, the loss is 480558.875\n",
            "in training loop, epoch 1, step 617, the loss is 345158.875\n",
            "in training loop, epoch 1, step 618, the loss is 1088594.0\n",
            "in training loop, epoch 1, step 619, the loss is 757890.0625\n",
            "in training loop, epoch 1, step 620, the loss is 904402.5\n",
            "in training loop, epoch 1, step 621, the loss is 1174708.5\n",
            "in training loop, epoch 1, step 622, the loss is 1219354.125\n",
            "in training loop, epoch 1, step 623, the loss is 673150.625\n",
            "in training loop, epoch 1, step 624, the loss is 1278250.875\n",
            "in training loop, epoch 1, step 625, the loss is 1008430.4375\n",
            "in training loop, epoch 1, step 626, the loss is 1005697.4375\n",
            "in training loop, epoch 1, step 627, the loss is 1032551.5\n",
            "in training loop, epoch 1, step 628, the loss is 559106.4375\n",
            "in training loop, epoch 1, step 629, the loss is 576502.9375\n",
            "in training loop, epoch 1, step 630, the loss is 568396.875\n",
            "in training loop, epoch 1, step 631, the loss is 1305617.5\n",
            "in training loop, epoch 1, step 632, the loss is 794639.0625\n",
            "in training loop, epoch 1, step 633, the loss is 714056.75\n",
            "in training loop, epoch 1, step 634, the loss is 968594.8125\n",
            "in training loop, epoch 1, step 635, the loss is 729281.875\n",
            "in training loop, epoch 1, step 636, the loss is 715153.875\n",
            "in training loop, epoch 1, step 637, the loss is 1154061.875\n",
            "in training loop, epoch 1, step 638, the loss is 940083.0\n",
            "in training loop, epoch 1, step 639, the loss is 967582.5\n",
            "in training loop, epoch 1, step 640, the loss is 759824.4375\n",
            "in training loop, epoch 1, step 641, the loss is 770575.0\n",
            "in training loop, epoch 1, step 642, the loss is 531531.25\n",
            "in training loop, epoch 1, step 643, the loss is 818067.9375\n",
            "in training loop, epoch 1, step 644, the loss is 699680.625\n",
            "in training loop, epoch 1, step 645, the loss is 553191.625\n",
            "in training loop, epoch 1, step 646, the loss is 692908.125\n",
            "in training loop, epoch 1, step 647, the loss is 796022.0625\n",
            "in training loop, epoch 1, step 648, the loss is 528754.1875\n",
            "in training loop, epoch 1, step 649, the loss is 1101942.5\n",
            "in training loop, epoch 1, step 650, the loss is 952055.25\n",
            "in training loop, epoch 1, step 651, the loss is 652145.6875\n",
            "in training loop, epoch 1, step 652, the loss is 1515543.5\n",
            "in training loop, epoch 1, step 653, the loss is 595080.3125\n",
            "in training loop, epoch 1, step 654, the loss is 1038846.375\n",
            "in training loop, epoch 1, step 655, the loss is 461995.28125\n",
            "in training loop, epoch 1, step 656, the loss is 1533706.0\n",
            "in training loop, epoch 1, step 657, the loss is 1186917.625\n",
            "in training loop, epoch 1, step 658, the loss is 666590.0\n",
            "in training loop, epoch 1, step 659, the loss is 1068076.625\n",
            "in training loop, epoch 1, step 660, the loss is 1414600.875\n",
            "in training loop, epoch 1, step 661, the loss is 1043684.625\n",
            "in training loop, epoch 1, step 662, the loss is 534317.5625\n",
            "in training loop, epoch 1, step 663, the loss is 1301962.5\n",
            "in training loop, epoch 1, step 664, the loss is 1155056.625\n",
            "in training loop, epoch 1, step 665, the loss is 1454094.75\n",
            "in training loop, epoch 1, step 666, the loss is 1087256.5\n",
            "in training loop, epoch 1, step 667, the loss is 988446.1875\n",
            "in training loop, epoch 1, step 668, the loss is 925228.125\n",
            "in training loop, epoch 1, step 669, the loss is 1759704.375\n",
            "in training loop, epoch 1, step 670, the loss is 954502.5\n",
            "in training loop, epoch 1, step 671, the loss is 1330934.375\n",
            "in training loop, epoch 1, step 672, the loss is 577724.75\n",
            "in training loop, epoch 1, step 673, the loss is 900172.125\n",
            "in training loop, epoch 1, step 674, the loss is 710133.8125\n",
            "in training loop, epoch 1, step 675, the loss is 983513.0625\n",
            "in training loop, epoch 1, step 676, the loss is 835129.3125\n",
            "in training loop, epoch 1, step 677, the loss is 415828.1875\n",
            "in training loop, epoch 1, step 678, the loss is 1178150.0\n",
            "in training loop, epoch 1, step 679, the loss is 892249.625\n",
            "in training loop, epoch 1, step 680, the loss is 678765.8125\n",
            "in training loop, epoch 1, step 681, the loss is 577140.125\n",
            "in training loop, epoch 1, step 682, the loss is 589898.4375\n",
            "in training loop, epoch 1, step 683, the loss is 1166154.25\n",
            "in training loop, epoch 1, step 684, the loss is 874479.75\n",
            "in training loop, epoch 1, step 685, the loss is 1147349.5\n",
            "in training loop, epoch 1, step 686, the loss is 842339.0\n",
            "in training loop, epoch 1, step 687, the loss is 909823.75\n",
            "in training loop, epoch 1, step 688, the loss is 844941.5625\n",
            "in training loop, epoch 1, step 689, the loss is 1052760.25\n",
            "in training loop, epoch 1, step 690, the loss is 574218.875\n",
            "in training loop, epoch 1, step 691, the loss is 874559.4375\n",
            "in training loop, epoch 1, step 692, the loss is 635909.5\n",
            "in training loop, epoch 1, step 693, the loss is 749164.25\n",
            "in training loop, epoch 1, step 694, the loss is 1394914.5\n",
            "in training loop, epoch 1, step 695, the loss is 891753.625\n",
            "in training loop, epoch 1, step 696, the loss is 672205.375\n",
            "in training loop, epoch 1, step 697, the loss is 1280890.25\n",
            "in training loop, epoch 1, step 698, the loss is 1452055.125\n",
            "in training loop, epoch 1, step 699, the loss is 694895.4375\n",
            "in training loop, epoch 1, step 700, the loss is 725182.75\n",
            "in training loop, epoch 1, step 701, the loss is 931331.125\n",
            "in training loop, epoch 1, step 702, the loss is 919724.4375\n",
            "in training loop, epoch 1, step 703, the loss is 922901.25\n",
            "in training loop, epoch 1, step 704, the loss is 740977.0625\n",
            "in training loop, epoch 1, step 705, the loss is 1230651.75\n",
            "in training loop, epoch 1, step 706, the loss is 667031.3125\n",
            "in training loop, epoch 1, step 707, the loss is 780129.4375\n",
            "in training loop, epoch 1, step 708, the loss is 510898.15625\n",
            "in training loop, epoch 1, step 709, the loss is 1498304.125\n",
            "in training loop, epoch 1, step 710, the loss is 591230.3125\n",
            "in training loop, epoch 1, step 711, the loss is 821072.6875\n",
            "in training loop, epoch 1, step 712, the loss is 1090477.125\n",
            "in training loop, epoch 1, step 713, the loss is 424614.875\n",
            "in training loop, epoch 1, step 714, the loss is 1363601.125\n",
            "in training loop, epoch 1, step 715, the loss is 1066834.0\n",
            "in training loop, epoch 1, step 716, the loss is 583860.375\n",
            "in training loop, epoch 1, step 717, the loss is 1167473.5\n",
            "in training loop, epoch 1, step 718, the loss is 935505.5625\n",
            "in training loop, epoch 1, step 719, the loss is 1018659.0625\n",
            "in training loop, epoch 1, step 720, the loss is 866288.625\n",
            "in training loop, epoch 1, step 721, the loss is 715534.9375\n",
            "in training loop, epoch 1, step 722, the loss is 517863.46875\n",
            "in training loop, epoch 1, step 723, the loss is 611873.5625\n",
            "in training loop, epoch 1, step 724, the loss is 1463411.875\n",
            "in training loop, epoch 1, step 725, the loss is 891558.0\n",
            "in training loop, epoch 1, step 726, the loss is 1098497.125\n",
            "in training loop, epoch 1, step 727, the loss is 668148.75\n",
            "in training loop, epoch 1, step 728, the loss is 764398.125\n",
            "in training loop, epoch 1, step 729, the loss is 746709.8125\n",
            "in training loop, epoch 1, step 730, the loss is 1310422.25\n",
            "in training loop, epoch 1, step 731, the loss is 742748.125\n",
            "in training loop, epoch 1, step 732, the loss is 1085980.0\n",
            "in training loop, epoch 1, step 733, the loss is 722125.25\n",
            "in training loop, epoch 1, step 734, the loss is 782815.75\n",
            "in training loop, epoch 1, step 735, the loss is 951530.5625\n",
            "in training loop, epoch 1, step 736, the loss is 520592.59375\n",
            "in training loop, epoch 1, step 737, the loss is 835702.1875\n",
            "in training loop, epoch 1, step 738, the loss is 1206576.75\n",
            "in training loop, epoch 1, step 739, the loss is 800066.0\n",
            "in training loop, epoch 1, step 740, the loss is 970728.375\n",
            "in training loop, epoch 1, step 741, the loss is 785137.9375\n",
            "in training loop, epoch 1, step 742, the loss is 1103337.25\n",
            "in training loop, epoch 1, step 743, the loss is 1207395.5\n",
            "in training loop, epoch 1, step 744, the loss is 590743.625\n",
            "in training loop, epoch 1, step 745, the loss is 783014.0\n",
            "in training loop, epoch 1, step 746, the loss is 1056784.5\n",
            "in training loop, epoch 1, step 747, the loss is 1102541.375\n",
            "in training loop, epoch 1, step 748, the loss is 1202112.5\n",
            "in training loop, epoch 1, step 749, the loss is 632395.375\n",
            "in training loop, epoch 1, step 750, the loss is 607320.4375\n",
            "in training loop, epoch 1, step 751, the loss is 668809.375\n",
            "in training loop, epoch 1, step 752, the loss is 722777.125\n",
            "in training loop, epoch 1, step 753, the loss is 1518149.125\n",
            "in training loop, epoch 1, step 754, the loss is 820416.125\n",
            "in training loop, epoch 1, step 755, the loss is 1221964.5\n",
            "in training loop, epoch 1, step 756, the loss is 904723.4375\n",
            "in training loop, epoch 1, step 757, the loss is 850604.25\n",
            "in training loop, epoch 1, step 758, the loss is 706213.4375\n",
            "in training loop, epoch 1, step 759, the loss is 939580.0625\n",
            "in training loop, epoch 1, step 760, the loss is 1791491.5\n",
            "in training loop, epoch 1, step 761, the loss is 796492.875\n",
            "in training loop, epoch 1, step 762, the loss is 901389.0\n",
            "in training loop, epoch 1, step 763, the loss is 849600.25\n",
            "in training loop, epoch 1, step 764, the loss is 912710.125\n",
            "in training loop, epoch 1, step 765, the loss is 698421.9375\n",
            "in training loop, epoch 1, step 766, the loss is 853013.3125\n",
            "in training loop, epoch 1, step 767, the loss is 1145484.875\n",
            "in training loop, epoch 1, step 768, the loss is 1300871.125\n",
            "in training loop, epoch 1, step 769, the loss is 780814.875\n",
            "in training loop, epoch 1, step 770, the loss is 421851.21875\n",
            "in training loop, epoch 1, step 771, the loss is 859830.3125\n",
            "in training loop, epoch 1, step 772, the loss is 910999.8125\n",
            "in training loop, epoch 1, step 773, the loss is 644026.375\n",
            "in training loop, epoch 1, step 774, the loss is 722212.9375\n",
            "in training loop, epoch 1, step 775, the loss is 1801066.5\n",
            "in training loop, epoch 1, step 776, the loss is 1119301.375\n",
            "in training loop, epoch 1, step 777, the loss is 1653432.625\n",
            "in training loop, epoch 1, step 778, the loss is 443894.125\n",
            "in training loop, epoch 1, step 779, the loss is 596006.875\n",
            "in training loop, epoch 1, step 780, the loss is 1149900.625\n",
            "in training loop, epoch 1, step 781, the loss is 1037365.875\n",
            "in training loop, epoch 1, step 782, the loss is 690550.875\n",
            "in training loop, epoch 1, step 783, the loss is 783354.875\n",
            "in training loop, epoch 1, step 784, the loss is 1169537.875\n",
            "in training loop, epoch 1, step 785, the loss is 1036661.0625\n",
            "in training loop, epoch 1, step 786, the loss is 882226.5625\n",
            "in training loop, epoch 1, step 787, the loss is 1038570.375\n",
            "in training loop, epoch 1, step 788, the loss is 702262.6875\n",
            "in training loop, epoch 1, step 789, the loss is 934686.375\n",
            "in training loop, epoch 1, step 790, the loss is 929491.5\n",
            "in training loop, epoch 1, step 791, the loss is 595987.6875\n",
            "in training loop, epoch 1, step 792, the loss is 615823.1875\n",
            "in training loop, epoch 1, step 793, the loss is 868767.875\n",
            "in training loop, epoch 1, step 794, the loss is 1407549.5\n",
            "in training loop, epoch 1, step 795, the loss is 887453.375\n",
            "in training loop, epoch 1, step 796, the loss is 1514100.0\n",
            "in training loop, epoch 1, step 797, the loss is 551062.875\n",
            "in training loop, epoch 1, step 798, the loss is 664320.625\n",
            "in training loop, epoch 1, step 799, the loss is 743732.8125\n",
            "in training loop, epoch 1, step 800, the loss is 907441.0\n",
            "in training loop, epoch 1, step 801, the loss is 1126398.375\n",
            "in training loop, epoch 1, step 802, the loss is 678428.8125\n",
            "in training loop, epoch 1, step 803, the loss is 549885.75\n",
            "in training loop, epoch 1, step 804, the loss is 596928.75\n",
            "in training loop, epoch 1, step 805, the loss is 745544.625\n",
            "in training loop, epoch 1, step 806, the loss is 733935.25\n",
            "in training loop, epoch 1, step 807, the loss is 1114300.5\n",
            "in training loop, epoch 1, step 808, the loss is 1000562.5\n",
            "in training loop, epoch 1, step 809, the loss is 860804.0625\n",
            "in training loop, epoch 1, step 810, the loss is 833818.0625\n",
            "in training loop, epoch 1, step 811, the loss is 582031.4375\n",
            "in training loop, epoch 1, step 812, the loss is 813623.9375\n",
            "in training loop, epoch 1, step 813, the loss is 567337.5\n",
            "in training loop, epoch 1, step 814, the loss is 643502.375\n",
            "in training loop, epoch 1, step 815, the loss is 669706.5\n",
            "in training loop, epoch 1, step 816, the loss is 1041570.6875\n",
            "in training loop, epoch 1, step 817, the loss is 711731.375\n",
            "in training loop, epoch 1, step 818, the loss is 589936.125\n",
            "in training loop, epoch 1, step 819, the loss is 909784.625\n",
            "in training loop, epoch 1, step 820, the loss is 734863.125\n",
            "in training loop, epoch 1, step 821, the loss is 775416.375\n",
            "in training loop, epoch 1, step 822, the loss is 972090.6875\n",
            "in training loop, epoch 1, step 823, the loss is 591248.8125\n",
            "in training loop, epoch 1, step 824, the loss is 1048158.25\n",
            "in training loop, epoch 1, step 825, the loss is 1870580.75\n",
            "in training loop, epoch 1, step 826, the loss is 949509.625\n",
            "in training loop, epoch 1, step 827, the loss is 439198.75\n",
            "in training loop, epoch 1, step 828, the loss is 1276865.75\n",
            "in training loop, epoch 1, step 829, the loss is 783238.625\n",
            "in training loop, epoch 1, step 830, the loss is 670895.125\n",
            "in training loop, epoch 1, step 831, the loss is 786831.3125\n",
            "in training loop, epoch 1, step 832, the loss is 1804745.0\n",
            "in training loop, epoch 1, step 833, the loss is 995483.25\n",
            "in training loop, epoch 1, step 834, the loss is 990756.375\n",
            "in training loop, epoch 1, step 835, the loss is 885732.8125\n",
            "in training loop, epoch 1, step 836, the loss is 1700895.75\n",
            "in training loop, epoch 1, step 837, the loss is 805485.375\n",
            "in training loop, epoch 1, step 838, the loss is 971637.9375\n",
            "in training loop, epoch 1, step 839, the loss is 626539.0625\n",
            "in training loop, epoch 1, step 840, the loss is 1060411.625\n",
            "in training loop, epoch 1, step 841, the loss is 1255849.0\n",
            "in training loop, epoch 1, step 842, the loss is 920619.9375\n",
            "in training loop, epoch 1, step 843, the loss is 636602.3125\n",
            "in training loop, epoch 1, step 844, the loss is 1324968.25\n",
            "in training loop, epoch 1, step 845, the loss is 802522.75\n",
            "in training loop, epoch 1, step 846, the loss is 787939.25\n",
            "in training loop, epoch 1, step 847, the loss is 469354.4375\n",
            "in training loop, epoch 1, step 848, the loss is 954386.875\n",
            "in training loop, epoch 1, step 849, the loss is 773589.9375\n",
            "in training loop, epoch 1, step 850, the loss is 1167590.5\n",
            "in training loop, epoch 1, step 851, the loss is 890858.8125\n",
            "in training loop, epoch 1, step 852, the loss is 858060.375\n",
            "in training loop, epoch 1, step 853, the loss is 970186.0\n",
            "in training loop, epoch 1, step 854, the loss is 928709.5\n",
            "in training loop, epoch 1, step 855, the loss is 1159627.25\n",
            "in training loop, epoch 1, step 856, the loss is 582691.0\n",
            "in training loop, epoch 1, step 857, the loss is 933567.4375\n",
            "in training loop, epoch 1, step 858, the loss is 721223.25\n",
            "in training loop, epoch 1, step 859, the loss is 1055606.125\n",
            "in training loop, epoch 1, step 860, the loss is 1104587.25\n",
            "in training loop, epoch 1, step 861, the loss is 333112.75\n",
            "in training loop, epoch 1, step 862, the loss is 513649.4375\n",
            "in training loop, epoch 1, step 863, the loss is 867474.75\n",
            "in training loop, epoch 1, step 864, the loss is 1138744.5\n",
            "in training loop, epoch 1, step 865, the loss is 704041.625\n",
            "in training loop, epoch 1, step 866, the loss is 591639.75\n",
            "in training loop, epoch 1, step 867, the loss is 199555.703125\n",
            "in training loop, epoch 1, step 868, the loss is 1922934.25\n",
            "in training loop, epoch 1, step 869, the loss is 863024.9375\n",
            "in training loop, epoch 1, step 870, the loss is 763879.4375\n",
            "in training loop, epoch 1, step 871, the loss is 701012.1875\n",
            "in training loop, epoch 1, step 872, the loss is 496972.78125\n",
            "in training loop, epoch 1, step 873, the loss is 832701.625\n",
            "in training loop, epoch 1, step 874, the loss is 678820.8125\n",
            "in training loop, epoch 1, step 875, the loss is 550835.5625\n",
            "in training loop, epoch 1, step 876, the loss is 848467.0625\n",
            "in training loop, epoch 1, step 877, the loss is 1434456.5\n",
            "in training loop, epoch 1, step 878, the loss is 510933.8125\n",
            "in training loop, epoch 1, step 879, the loss is 564501.125\n",
            "in training loop, epoch 1, step 880, the loss is 898944.875\n",
            "in training loop, epoch 1, step 881, the loss is 688487.1875\n",
            "in training loop, epoch 1, step 882, the loss is 878639.875\n",
            "in training loop, epoch 1, step 883, the loss is 782006.6875\n",
            "in training loop, epoch 1, step 884, the loss is 1315272.25\n",
            "in training loop, epoch 1, step 885, the loss is 1483753.0\n",
            "in training loop, epoch 1, step 886, the loss is 890470.625\n",
            "in training loop, epoch 1, step 887, the loss is 732224.625\n",
            "in training loop, epoch 1, step 888, the loss is 1981887.25\n",
            "in training loop, epoch 1, step 889, the loss is 783454.5\n",
            "in training loop, epoch 1, step 890, the loss is 519711.84375\n",
            "in training loop, epoch 1, step 891, the loss is 1049455.25\n",
            "in training loop, epoch 1, step 892, the loss is 1232898.75\n",
            "in training loop, epoch 1, step 893, the loss is 1279774.125\n",
            "in training loop, epoch 1, step 894, the loss is 819397.5625\n",
            "in training loop, epoch 1, step 895, the loss is 1015255.625\n",
            "in training loop, epoch 1, step 896, the loss is 772641.6875\n",
            "in training loop, epoch 1, step 897, the loss is 1009180.5625\n",
            "in training loop, epoch 1, step 898, the loss is 991874.8125\n",
            "in training loop, epoch 1, step 899, the loss is 1763001.25\n",
            "in training loop, epoch 1, step 900, the loss is 1448983.75\n",
            "in training loop, epoch 1, step 901, the loss is 425861.625\n",
            "in training loop, epoch 1, step 902, the loss is 1098389.5\n",
            "in training loop, epoch 1, step 903, the loss is 207785.546875\n",
            "k-fold 0:: Epoch 1: train loss 912928.6227876106 val loss 918259.4220297029\n",
            "in training loop, epoch 2, step 0, the loss is 998572.875\n",
            "in training loop, epoch 2, step 1, the loss is 979258.5\n",
            "in training loop, epoch 2, step 2, the loss is 668845.5\n",
            "in training loop, epoch 2, step 3, the loss is 715853.0\n",
            "in training loop, epoch 2, step 4, the loss is 895836.4375\n",
            "in training loop, epoch 2, step 5, the loss is 1612138.0\n",
            "in training loop, epoch 2, step 6, the loss is 978830.1875\n",
            "in training loop, epoch 2, step 7, the loss is 1660917.0\n",
            "in training loop, epoch 2, step 8, the loss is 1063002.5\n",
            "in training loop, epoch 2, step 9, the loss is 1566801.375\n",
            "in training loop, epoch 2, step 10, the loss is 681622.8125\n",
            "in training loop, epoch 2, step 11, the loss is 1057353.25\n",
            "in training loop, epoch 2, step 12, the loss is 1049583.125\n",
            "in training loop, epoch 2, step 13, the loss is 1191942.625\n",
            "in training loop, epoch 2, step 14, the loss is 711999.75\n",
            "in training loop, epoch 2, step 15, the loss is 946577.3125\n",
            "in training loop, epoch 2, step 16, the loss is 1322844.125\n",
            "in training loop, epoch 2, step 17, the loss is 1438739.75\n",
            "in training loop, epoch 2, step 18, the loss is 601678.75\n",
            "in training loop, epoch 2, step 19, the loss is 924086.75\n",
            "in training loop, epoch 2, step 20, the loss is 706925.0\n",
            "in training loop, epoch 2, step 21, the loss is 930335.25\n",
            "in training loop, epoch 2, step 22, the loss is 765405.9375\n",
            "in training loop, epoch 2, step 23, the loss is 700756.625\n",
            "in training loop, epoch 2, step 24, the loss is 740885.125\n",
            "in training loop, epoch 2, step 25, the loss is 1246343.125\n",
            "in training loop, epoch 2, step 26, the loss is 666694.0625\n",
            "in training loop, epoch 2, step 27, the loss is 988160.75\n",
            "in training loop, epoch 2, step 28, the loss is 1075453.25\n",
            "in training loop, epoch 2, step 29, the loss is 1269671.625\n",
            "in training loop, epoch 2, step 30, the loss is 704979.125\n",
            "in training loop, epoch 2, step 31, the loss is 680616.6875\n",
            "in training loop, epoch 2, step 32, the loss is 959910.875\n",
            "in training loop, epoch 2, step 33, the loss is 382661.5\n",
            "in training loop, epoch 2, step 34, the loss is 1208735.0\n",
            "in training loop, epoch 2, step 35, the loss is 753186.625\n",
            "in training loop, epoch 2, step 36, the loss is 493677.0625\n",
            "in training loop, epoch 2, step 37, the loss is 873135.875\n",
            "in training loop, epoch 2, step 38, the loss is 876698.0625\n",
            "in training loop, epoch 2, step 39, the loss is 1205648.5\n",
            "in training loop, epoch 2, step 40, the loss is 1088782.75\n",
            "in training loop, epoch 2, step 41, the loss is 645225.375\n",
            "in training loop, epoch 2, step 42, the loss is 886877.6875\n",
            "in training loop, epoch 2, step 43, the loss is 1182040.75\n",
            "in training loop, epoch 2, step 44, the loss is 813529.5625\n",
            "in training loop, epoch 2, step 45, the loss is 1001295.875\n",
            "in training loop, epoch 2, step 46, the loss is 1185377.125\n",
            "in training loop, epoch 2, step 47, the loss is 1136022.625\n",
            "in training loop, epoch 2, step 48, the loss is 1055294.75\n",
            "in training loop, epoch 2, step 49, the loss is 1001149.9375\n",
            "in training loop, epoch 2, step 50, the loss is 846678.9375\n",
            "in training loop, epoch 2, step 51, the loss is 983120.75\n",
            "in training loop, epoch 2, step 52, the loss is 670920.6875\n",
            "in training loop, epoch 2, step 53, the loss is 929935.5\n",
            "in training loop, epoch 2, step 54, the loss is 994812.75\n",
            "in training loop, epoch 2, step 55, the loss is 872057.8125\n",
            "in training loop, epoch 2, step 56, the loss is 779845.25\n",
            "in training loop, epoch 2, step 57, the loss is 1143109.125\n",
            "in training loop, epoch 2, step 58, the loss is 591600.1875\n",
            "in training loop, epoch 2, step 59, the loss is 1042316.375\n",
            "in training loop, epoch 2, step 60, the loss is 583000.875\n",
            "in training loop, epoch 2, step 61, the loss is 714546.375\n",
            "in training loop, epoch 2, step 62, the loss is 1348879.875\n",
            "in training loop, epoch 2, step 63, the loss is 447564.375\n",
            "in training loop, epoch 2, step 64, the loss is 687363.375\n",
            "in training loop, epoch 2, step 65, the loss is 787484.375\n",
            "in training loop, epoch 2, step 66, the loss is 1057371.25\n",
            "in training loop, epoch 2, step 67, the loss is 661372.5\n",
            "in training loop, epoch 2, step 68, the loss is 866700.375\n",
            "in training loop, epoch 2, step 69, the loss is 1180989.75\n",
            "in training loop, epoch 2, step 70, the loss is 1928629.0\n",
            "in training loop, epoch 2, step 71, the loss is 1048192.9375\n",
            "in training loop, epoch 2, step 72, the loss is 796169.375\n",
            "in training loop, epoch 2, step 73, the loss is 419335.65625\n",
            "in training loop, epoch 2, step 74, the loss is 702482.25\n",
            "in training loop, epoch 2, step 75, the loss is 941187.5\n",
            "in training loop, epoch 2, step 76, the loss is 517043.15625\n",
            "in training loop, epoch 2, step 77, the loss is 764670.0\n",
            "in training loop, epoch 2, step 78, the loss is 724839.5\n",
            "in training loop, epoch 2, step 79, the loss is 535608.3125\n",
            "in training loop, epoch 2, step 80, the loss is 964668.4375\n",
            "in training loop, epoch 2, step 81, the loss is 613994.4375\n",
            "in training loop, epoch 2, step 82, the loss is 674881.1875\n",
            "in training loop, epoch 2, step 83, the loss is 726092.875\n",
            "in training loop, epoch 2, step 84, the loss is 972931.5\n",
            "in training loop, epoch 2, step 85, the loss is 785452.5\n",
            "in training loop, epoch 2, step 86, the loss is 948646.875\n",
            "in training loop, epoch 2, step 87, the loss is 965566.125\n",
            "in training loop, epoch 2, step 88, the loss is 810504.375\n",
            "in training loop, epoch 2, step 89, the loss is 2035706.125\n",
            "in training loop, epoch 2, step 90, the loss is 460756.90625\n",
            "in training loop, epoch 2, step 91, the loss is 850652.125\n",
            "in training loop, epoch 2, step 92, the loss is 632193.375\n",
            "in training loop, epoch 2, step 93, the loss is 412124.3125\n",
            "in training loop, epoch 2, step 94, the loss is 714178.6875\n",
            "in training loop, epoch 2, step 95, the loss is 823307.25\n",
            "in training loop, epoch 2, step 96, the loss is 1425636.0\n",
            "in training loop, epoch 2, step 97, the loss is 748662.4375\n",
            "in training loop, epoch 2, step 98, the loss is 562863.8125\n",
            "in training loop, epoch 2, step 99, the loss is 1122049.625\n",
            "in training loop, epoch 2, step 100, the loss is 942036.0\n",
            "in training loop, epoch 2, step 101, the loss is 833356.3125\n",
            "in training loop, epoch 2, step 102, the loss is 1186982.125\n",
            "in training loop, epoch 2, step 103, the loss is 732852.125\n",
            "in training loop, epoch 2, step 104, the loss is 508786.5625\n",
            "in training loop, epoch 2, step 105, the loss is 1592768.25\n",
            "in training loop, epoch 2, step 106, the loss is 1162177.375\n",
            "in training loop, epoch 2, step 107, the loss is 839622.125\n",
            "in training loop, epoch 2, step 108, the loss is 756685.3125\n",
            "in training loop, epoch 2, step 109, the loss is 1040826.8125\n",
            "in training loop, epoch 2, step 110, the loss is 744930.125\n",
            "in training loop, epoch 2, step 111, the loss is 1541004.25\n",
            "in training loop, epoch 2, step 112, the loss is 848138.0\n",
            "in training loop, epoch 2, step 113, the loss is 1685941.875\n",
            "in training loop, epoch 2, step 114, the loss is 875357.875\n",
            "in training loop, epoch 2, step 115, the loss is 656322.1875\n",
            "in training loop, epoch 2, step 116, the loss is 605577.5625\n",
            "in training loop, epoch 2, step 117, the loss is 1109067.75\n",
            "in training loop, epoch 2, step 118, the loss is 715267.9375\n",
            "in training loop, epoch 2, step 119, the loss is 666691.3125\n",
            "in training loop, epoch 2, step 120, the loss is 1249849.25\n",
            "in training loop, epoch 2, step 121, the loss is 959663.8125\n",
            "in training loop, epoch 2, step 122, the loss is 548572.9375\n",
            "in training loop, epoch 2, step 123, the loss is 442630.8125\n",
            "in training loop, epoch 2, step 124, the loss is 1005158.375\n",
            "in training loop, epoch 2, step 125, the loss is 689308.3125\n",
            "in training loop, epoch 2, step 126, the loss is 754197.8125\n",
            "in training loop, epoch 2, step 127, the loss is 700374.1875\n",
            "in training loop, epoch 2, step 128, the loss is 829725.3125\n",
            "in training loop, epoch 2, step 129, the loss is 1020593.375\n",
            "in training loop, epoch 2, step 130, the loss is 937993.875\n",
            "in training loop, epoch 2, step 131, the loss is 604776.625\n",
            "in training loop, epoch 2, step 132, the loss is 852799.0\n",
            "in training loop, epoch 2, step 133, the loss is 793779.375\n",
            "in training loop, epoch 2, step 134, the loss is 1128566.625\n",
            "in training loop, epoch 2, step 135, the loss is 619150.3125\n",
            "in training loop, epoch 2, step 136, the loss is 682250.625\n",
            "in training loop, epoch 2, step 137, the loss is 742267.25\n",
            "in training loop, epoch 2, step 138, the loss is 1101553.0\n",
            "in training loop, epoch 2, step 139, the loss is 831354.875\n",
            "in training loop, epoch 2, step 140, the loss is 997074.625\n",
            "in training loop, epoch 2, step 141, the loss is 871679.5\n",
            "in training loop, epoch 2, step 142, the loss is 602419.6875\n",
            "in training loop, epoch 2, step 143, the loss is 1089555.0\n",
            "in training loop, epoch 2, step 144, the loss is 1256960.0\n",
            "in training loop, epoch 2, step 145, the loss is 434092.09375\n",
            "in training loop, epoch 2, step 146, the loss is 594761.375\n",
            "in training loop, epoch 2, step 147, the loss is 904317.4375\n",
            "in training loop, epoch 2, step 148, the loss is 527030.5\n",
            "in training loop, epoch 2, step 149, the loss is 1454570.75\n",
            "in training loop, epoch 2, step 150, the loss is 799690.625\n",
            "in training loop, epoch 2, step 151, the loss is 955179.0\n",
            "in training loop, epoch 2, step 152, the loss is 667510.375\n",
            "in training loop, epoch 2, step 153, the loss is 1250385.5\n",
            "in training loop, epoch 2, step 154, the loss is 752588.5\n",
            "in training loop, epoch 2, step 155, the loss is 864396.6875\n",
            "in training loop, epoch 2, step 156, the loss is 476741.25\n",
            "in training loop, epoch 2, step 157, the loss is 1228499.625\n",
            "in training loop, epoch 2, step 158, the loss is 767635.125\n",
            "in training loop, epoch 2, step 159, the loss is 834258.0\n",
            "in training loop, epoch 2, step 160, the loss is 1284252.0\n",
            "in training loop, epoch 2, step 161, the loss is 1094517.0\n",
            "in training loop, epoch 2, step 162, the loss is 553416.125\n",
            "in training loop, epoch 2, step 163, the loss is 579585.9375\n",
            "in training loop, epoch 2, step 164, the loss is 353355.1875\n",
            "in training loop, epoch 2, step 165, the loss is 733217.375\n",
            "in training loop, epoch 2, step 166, the loss is 483312.5\n",
            "in training loop, epoch 2, step 167, the loss is 866376.0\n",
            "in training loop, epoch 2, step 168, the loss is 1072466.5\n",
            "in training loop, epoch 2, step 169, the loss is 903659.8125\n",
            "in training loop, epoch 2, step 170, the loss is 559600.25\n",
            "in training loop, epoch 2, step 171, the loss is 938667.1875\n",
            "in training loop, epoch 2, step 172, the loss is 1041041.1875\n",
            "in training loop, epoch 2, step 173, the loss is 878713.1875\n",
            "in training loop, epoch 2, step 174, the loss is 862785.8125\n",
            "in training loop, epoch 2, step 175, the loss is 1122716.875\n",
            "in training loop, epoch 2, step 176, the loss is 392168.4375\n",
            "in training loop, epoch 2, step 177, the loss is 656016.75\n",
            "in training loop, epoch 2, step 178, the loss is 911212.5625\n",
            "in training loop, epoch 2, step 179, the loss is 1063314.625\n",
            "in training loop, epoch 2, step 180, the loss is 864004.1875\n",
            "in training loop, epoch 2, step 181, the loss is 1247962.5\n",
            "in training loop, epoch 2, step 182, the loss is 951842.0\n",
            "in training loop, epoch 2, step 183, the loss is 1308664.25\n",
            "in training loop, epoch 2, step 184, the loss is 714576.625\n",
            "in training loop, epoch 2, step 185, the loss is 1224271.375\n",
            "in training loop, epoch 2, step 186, the loss is 927781.9375\n",
            "in training loop, epoch 2, step 187, the loss is 510844.125\n",
            "in training loop, epoch 2, step 188, the loss is 968442.5\n",
            "in training loop, epoch 2, step 189, the loss is 745826.5625\n",
            "in training loop, epoch 2, step 190, the loss is 1060361.875\n",
            "in training loop, epoch 2, step 191, the loss is 936827.5625\n",
            "in training loop, epoch 2, step 192, the loss is 608101.1875\n",
            "in training loop, epoch 2, step 193, the loss is 438478.84375\n",
            "in training loop, epoch 2, step 194, the loss is 732129.1875\n",
            "in training loop, epoch 2, step 195, the loss is 576357.4375\n",
            "in training loop, epoch 2, step 196, the loss is 643468.8125\n",
            "in training loop, epoch 2, step 197, the loss is 1084173.125\n",
            "in training loop, epoch 2, step 198, the loss is 790228.3125\n",
            "in training loop, epoch 2, step 199, the loss is 785967.3125\n",
            "in training loop, epoch 2, step 200, the loss is 469715.0\n",
            "in training loop, epoch 2, step 201, the loss is 639148.6875\n",
            "in training loop, epoch 2, step 202, the loss is 994002.5\n",
            "in training loop, epoch 2, step 203, the loss is 524251.6875\n",
            "in training loop, epoch 2, step 204, the loss is 847717.25\n",
            "in training loop, epoch 2, step 205, the loss is 644829.875\n",
            "in training loop, epoch 2, step 206, the loss is 947241.25\n",
            "in training loop, epoch 2, step 207, the loss is 793111.25\n",
            "in training loop, epoch 2, step 208, the loss is 916902.125\n",
            "in training loop, epoch 2, step 209, the loss is 1053026.625\n",
            "in training loop, epoch 2, step 210, the loss is 741350.3125\n",
            "in training loop, epoch 2, step 211, the loss is 894380.9375\n",
            "in training loop, epoch 2, step 212, the loss is 834177.75\n",
            "in training loop, epoch 2, step 213, the loss is 1107655.25\n",
            "in training loop, epoch 2, step 214, the loss is 495610.5\n",
            "in training loop, epoch 2, step 215, the loss is 860046.625\n",
            "in training loop, epoch 2, step 216, the loss is 388731.46875\n",
            "in training loop, epoch 2, step 217, the loss is 1359075.625\n",
            "in training loop, epoch 2, step 218, the loss is 587716.0625\n",
            "in training loop, epoch 2, step 219, the loss is 1004007.125\n",
            "in training loop, epoch 2, step 220, the loss is 713187.875\n",
            "in training loop, epoch 2, step 221, the loss is 588023.0\n",
            "in training loop, epoch 2, step 222, the loss is 648804.1875\n",
            "in training loop, epoch 2, step 223, the loss is 739020.3125\n",
            "in training loop, epoch 2, step 224, the loss is 1032847.9375\n",
            "in training loop, epoch 2, step 225, the loss is 703568.3125\n",
            "in training loop, epoch 2, step 226, the loss is 545991.4375\n",
            "in training loop, epoch 2, step 227, the loss is 943191.1875\n",
            "in training loop, epoch 2, step 228, the loss is 918540.6875\n",
            "in training loop, epoch 2, step 229, the loss is 960874.75\n",
            "in training loop, epoch 2, step 230, the loss is 1174758.5\n",
            "in training loop, epoch 2, step 231, the loss is 481108.8125\n",
            "in training loop, epoch 2, step 232, the loss is 864651.5\n",
            "in training loop, epoch 2, step 233, the loss is 2077682.0\n",
            "in training loop, epoch 2, step 234, the loss is 881411.625\n",
            "in training loop, epoch 2, step 235, the loss is 825085.375\n",
            "in training loop, epoch 2, step 236, the loss is 663455.25\n",
            "in training loop, epoch 2, step 237, the loss is 577027.3125\n",
            "in training loop, epoch 2, step 238, the loss is 1504484.875\n",
            "in training loop, epoch 2, step 239, the loss is 738701.4375\n",
            "in training loop, epoch 2, step 240, the loss is 1346517.75\n",
            "in training loop, epoch 2, step 241, the loss is 987602.75\n",
            "in training loop, epoch 2, step 242, the loss is 782038.9375\n",
            "in training loop, epoch 2, step 243, the loss is 734845.875\n",
            "in training loop, epoch 2, step 244, the loss is 741223.5625\n",
            "in training loop, epoch 2, step 245, the loss is 780763.375\n",
            "in training loop, epoch 2, step 246, the loss is 997574.6875\n",
            "in training loop, epoch 2, step 247, the loss is 660806.6875\n",
            "in training loop, epoch 2, step 248, the loss is 775603.25\n",
            "in training loop, epoch 2, step 249, the loss is 571551.375\n",
            "in training loop, epoch 2, step 250, the loss is 1060392.625\n",
            "in training loop, epoch 2, step 251, the loss is 442348.40625\n",
            "in training loop, epoch 2, step 252, the loss is 1016832.5625\n",
            "in training loop, epoch 2, step 253, the loss is 994401.375\n",
            "in training loop, epoch 2, step 254, the loss is 1384429.5\n",
            "in training loop, epoch 2, step 255, the loss is 571079.9375\n",
            "in training loop, epoch 2, step 256, the loss is 861959.5625\n",
            "in training loop, epoch 2, step 257, the loss is 688686.5625\n",
            "in training loop, epoch 2, step 258, the loss is 1014809.625\n",
            "in training loop, epoch 2, step 259, the loss is 728986.5625\n",
            "in training loop, epoch 2, step 260, the loss is 1081304.625\n",
            "in training loop, epoch 2, step 261, the loss is 690239.375\n",
            "in training loop, epoch 2, step 262, the loss is 981173.5625\n",
            "in training loop, epoch 2, step 263, the loss is 814753.125\n",
            "in training loop, epoch 2, step 264, the loss is 669930.375\n",
            "in training loop, epoch 2, step 265, the loss is 503110.03125\n",
            "in training loop, epoch 2, step 266, the loss is 561968.125\n",
            "in training loop, epoch 2, step 267, the loss is 518567.0\n",
            "in training loop, epoch 2, step 268, the loss is 495519.78125\n",
            "in training loop, epoch 2, step 269, the loss is 548597.5\n",
            "in training loop, epoch 2, step 270, the loss is 415770.625\n",
            "in training loop, epoch 2, step 271, the loss is 1739273.375\n",
            "in training loop, epoch 2, step 272, the loss is 850865.5625\n",
            "in training loop, epoch 2, step 273, the loss is 1324121.375\n",
            "in training loop, epoch 2, step 274, the loss is 580118.4375\n",
            "in training loop, epoch 2, step 275, the loss is 2880515.0\n",
            "in training loop, epoch 2, step 276, the loss is 1165147.75\n",
            "in training loop, epoch 2, step 277, the loss is 715514.75\n",
            "in training loop, epoch 2, step 278, the loss is 872359.625\n",
            "in training loop, epoch 2, step 279, the loss is 359920.65625\n",
            "in training loop, epoch 2, step 280, the loss is 617299.1875\n",
            "in training loop, epoch 2, step 281, the loss is 728707.5\n",
            "in training loop, epoch 2, step 282, the loss is 630198.4375\n",
            "in training loop, epoch 2, step 283, the loss is 513987.03125\n",
            "in training loop, epoch 2, step 284, the loss is 511916.59375\n",
            "in training loop, epoch 2, step 285, the loss is 1985419.25\n",
            "in training loop, epoch 2, step 286, the loss is 1150744.25\n",
            "in training loop, epoch 2, step 287, the loss is 902063.5625\n",
            "in training loop, epoch 2, step 288, the loss is 414992.0625\n",
            "in training loop, epoch 2, step 289, the loss is 952603.9375\n",
            "in training loop, epoch 2, step 290, the loss is 883508.875\n",
            "in training loop, epoch 2, step 291, the loss is 452764.40625\n",
            "in training loop, epoch 2, step 292, the loss is 689752.5\n",
            "in training loop, epoch 2, step 293, the loss is 609950.875\n",
            "in training loop, epoch 2, step 294, the loss is 907600.125\n",
            "in training loop, epoch 2, step 295, the loss is 991798.4375\n",
            "in training loop, epoch 2, step 296, the loss is 767743.75\n",
            "in training loop, epoch 2, step 297, the loss is 758998.25\n",
            "in training loop, epoch 2, step 298, the loss is 860539.875\n",
            "in training loop, epoch 2, step 299, the loss is 1588414.125\n",
            "in training loop, epoch 2, step 300, the loss is 1436418.0\n",
            "in training loop, epoch 2, step 301, the loss is 531234.375\n",
            "in training loop, epoch 2, step 302, the loss is 515135.8125\n",
            "in training loop, epoch 2, step 303, the loss is 802007.5\n",
            "in training loop, epoch 2, step 304, the loss is 1051272.0\n",
            "in training loop, epoch 2, step 305, the loss is 856406.3125\n",
            "in training loop, epoch 2, step 306, the loss is 994970.75\n",
            "in training loop, epoch 2, step 307, the loss is 523806.40625\n",
            "in training loop, epoch 2, step 308, the loss is 688172.625\n",
            "in training loop, epoch 2, step 309, the loss is 1609678.25\n",
            "in training loop, epoch 2, step 310, the loss is 961078.8125\n",
            "in training loop, epoch 2, step 311, the loss is 832323.1875\n",
            "in training loop, epoch 2, step 312, the loss is 852366.0\n",
            "in training loop, epoch 2, step 313, the loss is 487383.0\n",
            "in training loop, epoch 2, step 314, the loss is 806902.75\n",
            "in training loop, epoch 2, step 315, the loss is 794097.375\n",
            "in training loop, epoch 2, step 316, the loss is 537874.375\n",
            "in training loop, epoch 2, step 317, the loss is 902816.125\n",
            "in training loop, epoch 2, step 318, the loss is 774475.1875\n",
            "in training loop, epoch 2, step 319, the loss is 856735.375\n",
            "in training loop, epoch 2, step 320, the loss is 678624.3125\n",
            "in training loop, epoch 2, step 321, the loss is 596940.0625\n",
            "in training loop, epoch 2, step 322, the loss is 1115892.5\n",
            "in training loop, epoch 2, step 323, the loss is 915274.625\n",
            "in training loop, epoch 2, step 324, the loss is 444724.625\n",
            "in training loop, epoch 2, step 325, the loss is 660647.375\n",
            "in training loop, epoch 2, step 326, the loss is 731516.8125\n",
            "in training loop, epoch 2, step 327, the loss is 1305815.625\n",
            "in training loop, epoch 2, step 328, the loss is 501353.21875\n",
            "in training loop, epoch 2, step 329, the loss is 388011.34375\n",
            "in training loop, epoch 2, step 330, the loss is 796606.6875\n",
            "in training loop, epoch 2, step 331, the loss is 882963.375\n",
            "in training loop, epoch 2, step 332, the loss is 588915.375\n",
            "in training loop, epoch 2, step 333, the loss is 1079832.0\n",
            "in training loop, epoch 2, step 334, the loss is 458295.34375\n",
            "in training loop, epoch 2, step 335, the loss is 429472.1875\n",
            "in training loop, epoch 2, step 336, the loss is 662164.75\n",
            "in training loop, epoch 2, step 337, the loss is 394869.03125\n",
            "in training loop, epoch 2, step 338, the loss is 677461.375\n",
            "in training loop, epoch 2, step 339, the loss is 1035200.4375\n",
            "in training loop, epoch 2, step 340, the loss is 835341.125\n",
            "in training loop, epoch 2, step 341, the loss is 909990.5625\n",
            "in training loop, epoch 2, step 342, the loss is 1492899.625\n",
            "in training loop, epoch 2, step 343, the loss is 615000.3125\n",
            "in training loop, epoch 2, step 344, the loss is 737280.5\n",
            "in training loop, epoch 2, step 345, the loss is 813201.0\n",
            "in training loop, epoch 2, step 346, the loss is 761343.9375\n",
            "in training loop, epoch 2, step 347, the loss is 512535.65625\n",
            "in training loop, epoch 2, step 348, the loss is 815561.5625\n",
            "in training loop, epoch 2, step 349, the loss is 578704.625\n",
            "in training loop, epoch 2, step 350, the loss is 866492.0625\n",
            "in training loop, epoch 2, step 351, the loss is 835549.9375\n",
            "in training loop, epoch 2, step 352, the loss is 572576.25\n",
            "in training loop, epoch 2, step 353, the loss is 709924.375\n",
            "in training loop, epoch 2, step 354, the loss is 745675.75\n",
            "in training loop, epoch 2, step 355, the loss is 504385.375\n",
            "in training loop, epoch 2, step 356, the loss is 1397996.875\n",
            "in training loop, epoch 2, step 357, the loss is 773704.4375\n",
            "in training loop, epoch 2, step 358, the loss is 831070.5\n",
            "in training loop, epoch 2, step 359, the loss is 1209026.125\n",
            "in training loop, epoch 2, step 360, the loss is 733936.625\n",
            "in training loop, epoch 2, step 361, the loss is 789030.875\n",
            "in training loop, epoch 2, step 362, the loss is 860490.5\n",
            "in training loop, epoch 2, step 363, the loss is 1155922.625\n",
            "in training loop, epoch 2, step 364, the loss is 1033302.5\n",
            "in training loop, epoch 2, step 365, the loss is 801335.125\n",
            "in training loop, epoch 2, step 366, the loss is 830151.5\n",
            "in training loop, epoch 2, step 367, the loss is 886297.5\n",
            "in training loop, epoch 2, step 368, the loss is 1179772.375\n",
            "in training loop, epoch 2, step 369, the loss is 733153.9375\n",
            "in training loop, epoch 2, step 370, the loss is 739911.125\n",
            "in training loop, epoch 2, step 371, the loss is 1090930.75\n",
            "in training loop, epoch 2, step 372, the loss is 374521.375\n",
            "in training loop, epoch 2, step 373, the loss is 579716.3125\n",
            "in training loop, epoch 2, step 374, the loss is 844356.0625\n",
            "in training loop, epoch 2, step 375, the loss is 643413.25\n",
            "in training loop, epoch 2, step 376, the loss is 614347.5\n",
            "in training loop, epoch 2, step 377, the loss is 671988.0\n",
            "in training loop, epoch 2, step 378, the loss is 1164962.375\n",
            "in training loop, epoch 2, step 379, the loss is 637884.0\n",
            "in training loop, epoch 2, step 380, the loss is 551544.75\n",
            "in training loop, epoch 2, step 381, the loss is 750405.3125\n",
            "in training loop, epoch 2, step 382, the loss is 846667.5\n",
            "in training loop, epoch 2, step 383, the loss is 1394544.25\n",
            "in training loop, epoch 2, step 384, the loss is 734874.6875\n",
            "in training loop, epoch 2, step 385, the loss is 432572.5\n",
            "in training loop, epoch 2, step 386, the loss is 599956.375\n",
            "in training loop, epoch 2, step 387, the loss is 911483.9375\n",
            "in training loop, epoch 2, step 388, the loss is 563595.0\n",
            "in training loop, epoch 2, step 389, the loss is 373435.0\n",
            "in training loop, epoch 2, step 390, the loss is 1356904.0\n",
            "in training loop, epoch 2, step 391, the loss is 675297.375\n",
            "in training loop, epoch 2, step 392, the loss is 1184902.0\n",
            "in training loop, epoch 2, step 393, the loss is 749786.375\n",
            "in training loop, epoch 2, step 394, the loss is 992871.3125\n",
            "in training loop, epoch 2, step 395, the loss is 945637.75\n",
            "in training loop, epoch 2, step 396, the loss is 627890.75\n",
            "in training loop, epoch 2, step 397, the loss is 659182.125\n",
            "in training loop, epoch 2, step 398, the loss is 1045795.0625\n",
            "in training loop, epoch 2, step 399, the loss is 923656.375\n",
            "in training loop, epoch 2, step 400, the loss is 570320.25\n",
            "in training loop, epoch 2, step 401, the loss is 509108.6875\n",
            "in training loop, epoch 2, step 402, the loss is 709223.6875\n",
            "in training loop, epoch 2, step 403, the loss is 431893.09375\n",
            "in training loop, epoch 2, step 404, the loss is 720619.125\n",
            "in training loop, epoch 2, step 405, the loss is 727969.9375\n",
            "in training loop, epoch 2, step 406, the loss is 526932.0\n",
            "in training loop, epoch 2, step 407, the loss is 663745.4375\n",
            "in training loop, epoch 2, step 408, the loss is 708890.8125\n",
            "in training loop, epoch 2, step 409, the loss is 673243.0625\n",
            "in training loop, epoch 2, step 410, the loss is 444118.53125\n",
            "in training loop, epoch 2, step 411, the loss is 1093642.375\n",
            "in training loop, epoch 2, step 412, the loss is 872878.9375\n",
            "in training loop, epoch 2, step 413, the loss is 817103.5\n",
            "in training loop, epoch 2, step 414, the loss is 873537.0\n",
            "in training loop, epoch 2, step 415, the loss is 837857.3125\n",
            "in training loop, epoch 2, step 416, the loss is 572567.3125\n",
            "in training loop, epoch 2, step 417, the loss is 865996.25\n",
            "in training loop, epoch 2, step 418, the loss is 841267.0\n",
            "in training loop, epoch 2, step 419, the loss is 595090.0625\n",
            "in training loop, epoch 2, step 420, the loss is 893394.75\n",
            "in training loop, epoch 2, step 421, the loss is 571267.6875\n",
            "in training loop, epoch 2, step 422, the loss is 1370381.25\n",
            "in training loop, epoch 2, step 423, the loss is 795272.5\n",
            "in training loop, epoch 2, step 424, the loss is 1147113.5\n",
            "in training loop, epoch 2, step 425, the loss is 747909.1875\n",
            "in training loop, epoch 2, step 426, the loss is 1034334.5625\n",
            "in training loop, epoch 2, step 427, the loss is 903049.1875\n",
            "in training loop, epoch 2, step 428, the loss is 934528.0\n",
            "in training loop, epoch 2, step 429, the loss is 841804.8125\n",
            "in training loop, epoch 2, step 430, the loss is 622839.125\n",
            "in training loop, epoch 2, step 431, the loss is 814301.0625\n",
            "in training loop, epoch 2, step 432, the loss is 1022712.25\n",
            "in training loop, epoch 2, step 433, the loss is 1142951.5\n",
            "in training loop, epoch 2, step 434, the loss is 507535.5\n",
            "in training loop, epoch 2, step 435, the loss is 599085.6875\n",
            "in training loop, epoch 2, step 436, the loss is 828514.0625\n",
            "in training loop, epoch 2, step 437, the loss is 832181.0\n",
            "in training loop, epoch 2, step 438, the loss is 1359928.5\n",
            "in training loop, epoch 2, step 439, the loss is 916756.9375\n",
            "in training loop, epoch 2, step 440, the loss is 1235725.75\n",
            "in training loop, epoch 2, step 441, the loss is 1105043.25\n",
            "in training loop, epoch 2, step 442, the loss is 966491.4375\n",
            "in training loop, epoch 2, step 443, the loss is 666579.4375\n",
            "in training loop, epoch 2, step 444, the loss is 1205143.875\n",
            "in training loop, epoch 2, step 445, the loss is 809647.5\n",
            "in training loop, epoch 2, step 446, the loss is 1098960.25\n",
            "in training loop, epoch 2, step 447, the loss is 1533759.75\n",
            "in training loop, epoch 2, step 448, the loss is 629114.875\n",
            "in training loop, epoch 2, step 449, the loss is 1285593.5\n",
            "in training loop, epoch 2, step 450, the loss is 1175920.875\n",
            "in training loop, epoch 2, step 451, the loss is 842638.8125\n",
            "in training loop, epoch 2, step 452, the loss is 957640.0\n",
            "in training loop, epoch 2, step 453, the loss is 706211.4375\n",
            "in training loop, epoch 2, step 454, the loss is 944104.125\n",
            "in training loop, epoch 2, step 455, the loss is 760337.4375\n",
            "in training loop, epoch 2, step 456, the loss is 633265.875\n",
            "in training loop, epoch 2, step 457, the loss is 795534.8125\n",
            "in training loop, epoch 2, step 458, the loss is 1257132.0\n",
            "in training loop, epoch 2, step 459, the loss is 722536.625\n",
            "in training loop, epoch 2, step 460, the loss is 637369.25\n",
            "in training loop, epoch 2, step 461, the loss is 828924.6875\n",
            "in training loop, epoch 2, step 462, the loss is 1071985.375\n",
            "in training loop, epoch 2, step 463, the loss is 1213234.5\n",
            "in training loop, epoch 2, step 464, the loss is 977964.125\n",
            "in training loop, epoch 2, step 465, the loss is 574860.6875\n",
            "in training loop, epoch 2, step 466, the loss is 531711.3125\n",
            "in training loop, epoch 2, step 467, the loss is 1034221.625\n",
            "in training loop, epoch 2, step 468, the loss is 775902.5\n",
            "in training loop, epoch 2, step 469, the loss is 899433.5\n",
            "in training loop, epoch 2, step 470, the loss is 522774.0\n",
            "in training loop, epoch 2, step 471, the loss is 674015.25\n",
            "in training loop, epoch 2, step 472, the loss is 1141217.25\n",
            "in training loop, epoch 2, step 473, the loss is 1044137.6875\n",
            "in training loop, epoch 2, step 474, the loss is 925655.4375\n",
            "in training loop, epoch 2, step 475, the loss is 1088401.875\n",
            "in training loop, epoch 2, step 476, the loss is 1136269.75\n",
            "in training loop, epoch 2, step 477, the loss is 1145058.125\n",
            "in training loop, epoch 2, step 478, the loss is 312857.0\n",
            "in training loop, epoch 2, step 479, the loss is 621157.625\n",
            "in training loop, epoch 2, step 480, the loss is 420884.71875\n",
            "in training loop, epoch 2, step 481, the loss is 1042825.75\n",
            "in training loop, epoch 2, step 482, the loss is 777290.0\n",
            "in training loop, epoch 2, step 483, the loss is 975655.3125\n",
            "in training loop, epoch 2, step 484, the loss is 827858.75\n",
            "in training loop, epoch 2, step 485, the loss is 897212.8125\n",
            "in training loop, epoch 2, step 486, the loss is 674768.375\n",
            "in training loop, epoch 2, step 487, the loss is 749502.9375\n",
            "in training loop, epoch 2, step 488, the loss is 576258.4375\n",
            "in training loop, epoch 2, step 489, the loss is 577825.5625\n",
            "in training loop, epoch 2, step 490, the loss is 915898.8125\n",
            "in training loop, epoch 2, step 491, the loss is 794179.375\n",
            "in training loop, epoch 2, step 492, the loss is 845926.8125\n",
            "in training loop, epoch 2, step 493, the loss is 752611.75\n",
            "in training loop, epoch 2, step 494, the loss is 1280555.0\n",
            "in training loop, epoch 2, step 495, the loss is 741213.9375\n",
            "in training loop, epoch 2, step 496, the loss is 1436781.75\n",
            "in training loop, epoch 2, step 497, the loss is 916982.375\n",
            "in training loop, epoch 2, step 498, the loss is 1142787.5\n",
            "in training loop, epoch 2, step 499, the loss is 319854.5625\n",
            "in training loop, epoch 2, step 500, the loss is 557284.0625\n",
            "in training loop, epoch 2, step 501, the loss is 1052264.75\n",
            "in training loop, epoch 2, step 502, the loss is 1571425.0\n",
            "in training loop, epoch 2, step 503, the loss is 895918.875\n",
            "in training loop, epoch 2, step 504, the loss is 837832.3125\n",
            "in training loop, epoch 2, step 505, the loss is 636726.0\n",
            "in training loop, epoch 2, step 506, the loss is 717788.875\n",
            "in training loop, epoch 2, step 507, the loss is 786181.25\n",
            "in training loop, epoch 2, step 508, the loss is 959778.5\n",
            "in training loop, epoch 2, step 509, the loss is 1681739.5\n",
            "in training loop, epoch 2, step 510, the loss is 1091789.75\n",
            "in training loop, epoch 2, step 511, the loss is 1251543.375\n",
            "in training loop, epoch 2, step 512, the loss is 675687.9375\n",
            "in training loop, epoch 2, step 513, the loss is 1296599.75\n",
            "in training loop, epoch 2, step 514, the loss is 1137323.0\n",
            "in training loop, epoch 2, step 515, the loss is 1060345.125\n",
            "in training loop, epoch 2, step 516, the loss is 2094457.625\n",
            "in training loop, epoch 2, step 517, the loss is 752122.125\n",
            "in training loop, epoch 2, step 518, the loss is 842775.8125\n",
            "in training loop, epoch 2, step 519, the loss is 820338.625\n",
            "in training loop, epoch 2, step 520, the loss is 720506.25\n",
            "in training loop, epoch 2, step 521, the loss is 712416.375\n",
            "in training loop, epoch 2, step 522, the loss is 1268959.0\n",
            "in training loop, epoch 2, step 523, the loss is 776866.0625\n",
            "in training loop, epoch 2, step 524, the loss is 1892879.0\n",
            "in training loop, epoch 2, step 525, the loss is 714390.375\n",
            "in training loop, epoch 2, step 526, the loss is 962051.5625\n",
            "in training loop, epoch 2, step 527, the loss is 932139.4375\n",
            "in training loop, epoch 2, step 528, the loss is 573031.0625\n",
            "in training loop, epoch 2, step 529, the loss is 569446.375\n",
            "in training loop, epoch 2, step 530, the loss is 1005827.75\n",
            "in training loop, epoch 2, step 531, the loss is 1191761.875\n",
            "in training loop, epoch 2, step 532, the loss is 1312871.625\n",
            "in training loop, epoch 2, step 533, the loss is 527735.9375\n",
            "in training loop, epoch 2, step 534, the loss is 1080485.5\n",
            "in training loop, epoch 2, step 535, the loss is 1460948.75\n",
            "in training loop, epoch 2, step 536, the loss is 759850.5\n",
            "in training loop, epoch 2, step 537, the loss is 1176102.75\n",
            "in training loop, epoch 2, step 538, the loss is 835065.875\n",
            "in training loop, epoch 2, step 539, the loss is 944429.5625\n",
            "in training loop, epoch 2, step 540, the loss is 1006809.9375\n",
            "in training loop, epoch 2, step 541, the loss is 833104.125\n",
            "in training loop, epoch 2, step 542, the loss is 1512879.5\n",
            "in training loop, epoch 2, step 543, the loss is 819969.6875\n",
            "in training loop, epoch 2, step 544, the loss is 811235.25\n",
            "in training loop, epoch 2, step 545, the loss is 625189.125\n",
            "in training loop, epoch 2, step 546, the loss is 744683.4375\n",
            "in training loop, epoch 2, step 547, the loss is 1176920.5\n",
            "in training loop, epoch 2, step 548, the loss is 809800.75\n",
            "in training loop, epoch 2, step 549, the loss is 1016188.125\n",
            "in training loop, epoch 2, step 550, the loss is 902591.25\n",
            "in training loop, epoch 2, step 551, the loss is 979557.75\n",
            "in training loop, epoch 2, step 552, the loss is 724765.375\n",
            "in training loop, epoch 2, step 553, the loss is 519862.8125\n",
            "in training loop, epoch 2, step 554, the loss is 565726.5\n",
            "in training loop, epoch 2, step 555, the loss is 1195882.25\n",
            "in training loop, epoch 2, step 556, the loss is 406862.90625\n",
            "in training loop, epoch 2, step 557, the loss is 1318893.5\n",
            "in training loop, epoch 2, step 558, the loss is 1031936.5625\n",
            "in training loop, epoch 2, step 559, the loss is 1133542.75\n",
            "in training loop, epoch 2, step 560, the loss is 729070.875\n",
            "in training loop, epoch 2, step 561, the loss is 721403.375\n",
            "in training loop, epoch 2, step 562, the loss is 530486.625\n",
            "in training loop, epoch 2, step 563, the loss is 1001061.25\n",
            "in training loop, epoch 2, step 564, the loss is 583095.8125\n",
            "in training loop, epoch 2, step 565, the loss is 699079.375\n",
            "in training loop, epoch 2, step 566, the loss is 546361.75\n",
            "in training loop, epoch 2, step 567, the loss is 1051851.0\n",
            "in training loop, epoch 2, step 568, the loss is 645125.0\n",
            "in training loop, epoch 2, step 569, the loss is 560325.5625\n",
            "in training loop, epoch 2, step 570, the loss is 543158.75\n",
            "in training loop, epoch 2, step 571, the loss is 1192095.0\n",
            "in training loop, epoch 2, step 572, the loss is 559630.75\n",
            "in training loop, epoch 2, step 573, the loss is 947324.875\n",
            "in training loop, epoch 2, step 574, the loss is 732761.3125\n",
            "in training loop, epoch 2, step 575, the loss is 691625.875\n",
            "in training loop, epoch 2, step 576, the loss is 535529.9375\n",
            "in training loop, epoch 2, step 577, the loss is 873814.75\n",
            "in training loop, epoch 2, step 578, the loss is 685626.5\n",
            "in training loop, epoch 2, step 579, the loss is 499291.71875\n",
            "in training loop, epoch 2, step 580, the loss is 3791669.75\n",
            "in training loop, epoch 2, step 581, the loss is 775932.3125\n",
            "in training loop, epoch 2, step 582, the loss is 856111.5625\n",
            "in training loop, epoch 2, step 583, the loss is 772441.75\n",
            "in training loop, epoch 2, step 584, the loss is 1454914.625\n",
            "in training loop, epoch 2, step 585, the loss is 1074097.5\n",
            "in training loop, epoch 2, step 586, the loss is 1238912.5\n",
            "in training loop, epoch 2, step 587, the loss is 1190481.875\n",
            "in training loop, epoch 2, step 588, the loss is 723092.8125\n",
            "in training loop, epoch 2, step 589, the loss is 1408866.25\n",
            "in training loop, epoch 2, step 590, the loss is 827264.0\n",
            "in training loop, epoch 2, step 591, the loss is 1303554.25\n",
            "in training loop, epoch 2, step 592, the loss is 847647.9375\n",
            "in training loop, epoch 2, step 593, the loss is 893529.875\n",
            "in training loop, epoch 2, step 594, the loss is 1210928.125\n",
            "in training loop, epoch 2, step 595, the loss is 959909.5\n",
            "in training loop, epoch 2, step 596, the loss is 1563064.0\n",
            "in training loop, epoch 2, step 597, the loss is 775416.4375\n",
            "in training loop, epoch 2, step 598, the loss is 799310.75\n",
            "in training loop, epoch 2, step 599, the loss is 626666.1875\n",
            "in training loop, epoch 2, step 600, the loss is 974866.125\n",
            "in training loop, epoch 2, step 601, the loss is 759195.875\n",
            "in training loop, epoch 2, step 602, the loss is 563374.1875\n",
            "in training loop, epoch 2, step 603, the loss is 774968.1875\n",
            "in training loop, epoch 2, step 604, the loss is 1030143.5625\n",
            "in training loop, epoch 2, step 605, the loss is 713130.875\n",
            "in training loop, epoch 2, step 606, the loss is 933102.375\n",
            "in training loop, epoch 2, step 607, the loss is 1170069.5\n",
            "in training loop, epoch 2, step 608, the loss is 838704.0625\n",
            "in training loop, epoch 2, step 609, the loss is 507916.625\n",
            "in training loop, epoch 2, step 610, the loss is 1064540.0\n",
            "in training loop, epoch 2, step 611, the loss is 1022021.0\n",
            "in training loop, epoch 2, step 612, the loss is 1252288.0\n",
            "in training loop, epoch 2, step 613, the loss is 754164.3125\n",
            "in training loop, epoch 2, step 614, the loss is 846311.25\n",
            "in training loop, epoch 2, step 615, the loss is 958363.5\n",
            "in training loop, epoch 2, step 616, the loss is 563054.875\n",
            "in training loop, epoch 2, step 617, the loss is 992916.1875\n",
            "in training loop, epoch 2, step 618, the loss is 1265763.75\n",
            "in training loop, epoch 2, step 619, the loss is 1043466.8125\n",
            "in training loop, epoch 2, step 620, the loss is 612723.5625\n",
            "in training loop, epoch 2, step 621, the loss is 320170.96875\n",
            "in training loop, epoch 2, step 622, the loss is 782890.1875\n",
            "in training loop, epoch 2, step 623, the loss is 689174.875\n",
            "in training loop, epoch 2, step 624, the loss is 1396506.0\n",
            "in training loop, epoch 2, step 625, the loss is 1225218.125\n",
            "in training loop, epoch 2, step 626, the loss is 718601.75\n",
            "in training loop, epoch 2, step 627, the loss is 445880.1875\n",
            "in training loop, epoch 2, step 628, the loss is 966471.6875\n",
            "in training loop, epoch 2, step 629, the loss is 1419620.75\n",
            "in training loop, epoch 2, step 630, the loss is 218919.875\n",
            "in training loop, epoch 2, step 631, the loss is 1032460.1875\n",
            "in training loop, epoch 2, step 632, the loss is 736911.6875\n",
            "in training loop, epoch 2, step 633, the loss is 763363.375\n",
            "in training loop, epoch 2, step 634, the loss is 563560.75\n",
            "in training loop, epoch 2, step 635, the loss is 846509.75\n",
            "in training loop, epoch 2, step 636, the loss is 610359.75\n",
            "in training loop, epoch 2, step 637, the loss is 781448.75\n",
            "in training loop, epoch 2, step 638, the loss is 822479.125\n",
            "in training loop, epoch 2, step 639, the loss is 543367.5\n",
            "in training loop, epoch 2, step 640, the loss is 416683.125\n",
            "in training loop, epoch 2, step 641, the loss is 448062.5\n",
            "in training loop, epoch 2, step 642, the loss is 1222112.25\n",
            "in training loop, epoch 2, step 643, the loss is 622572.375\n",
            "in training loop, epoch 2, step 644, the loss is 649043.3125\n",
            "in training loop, epoch 2, step 645, the loss is 1036975.8125\n",
            "in training loop, epoch 2, step 646, the loss is 1358699.125\n",
            "in training loop, epoch 2, step 647, the loss is 687158.0\n",
            "in training loop, epoch 2, step 648, the loss is 1094418.875\n",
            "in training loop, epoch 2, step 649, the loss is 1201269.5\n",
            "in training loop, epoch 2, step 650, the loss is 782815.5625\n",
            "in training loop, epoch 2, step 651, the loss is 751804.0\n",
            "in training loop, epoch 2, step 652, the loss is 474568.71875\n",
            "in training loop, epoch 2, step 653, the loss is 1049832.25\n",
            "in training loop, epoch 2, step 654, the loss is 665734.6875\n",
            "in training loop, epoch 2, step 655, the loss is 1081440.0\n",
            "in training loop, epoch 2, step 656, the loss is 578404.75\n",
            "in training loop, epoch 2, step 657, the loss is 765810.875\n",
            "in training loop, epoch 2, step 658, the loss is 1228706.625\n",
            "in training loop, epoch 2, step 659, the loss is 835946.3125\n",
            "in training loop, epoch 2, step 660, the loss is 800288.375\n",
            "in training loop, epoch 2, step 661, the loss is 756202.6875\n",
            "in training loop, epoch 2, step 662, the loss is 882530.3125\n",
            "in training loop, epoch 2, step 663, the loss is 914024.9375\n",
            "in training loop, epoch 2, step 664, the loss is 665479.9375\n",
            "in training loop, epoch 2, step 665, the loss is 912672.75\n",
            "in training loop, epoch 2, step 666, the loss is 636995.3125\n",
            "in training loop, epoch 2, step 667, the loss is 612811.5\n",
            "in training loop, epoch 2, step 668, the loss is 528924.375\n",
            "in training loop, epoch 2, step 669, the loss is 1035577.1875\n",
            "in training loop, epoch 2, step 670, the loss is 1147155.375\n",
            "in training loop, epoch 2, step 671, the loss is 606990.625\n",
            "in training loop, epoch 2, step 672, the loss is 655267.3125\n",
            "in training loop, epoch 2, step 673, the loss is 459206.40625\n",
            "in training loop, epoch 2, step 674, the loss is 772700.375\n",
            "in training loop, epoch 2, step 675, the loss is 553738.625\n",
            "in training loop, epoch 2, step 676, the loss is 1007146.5\n",
            "in training loop, epoch 2, step 677, the loss is 1446815.5\n",
            "in training loop, epoch 2, step 678, the loss is 933375.4375\n",
            "in training loop, epoch 2, step 679, the loss is 1108834.75\n",
            "in training loop, epoch 2, step 680, the loss is 776129.1875\n",
            "in training loop, epoch 2, step 681, the loss is 699992.5\n",
            "in training loop, epoch 2, step 682, the loss is 721825.5\n",
            "in training loop, epoch 2, step 683, the loss is 893257.375\n",
            "in training loop, epoch 2, step 684, the loss is 353780.96875\n",
            "in training loop, epoch 2, step 685, the loss is 480294.6875\n",
            "in training loop, epoch 2, step 686, the loss is 487139.21875\n",
            "in training loop, epoch 2, step 687, the loss is 910891.625\n",
            "in training loop, epoch 2, step 688, the loss is 529213.9375\n",
            "in training loop, epoch 2, step 689, the loss is 654582.5\n",
            "in training loop, epoch 2, step 690, the loss is 601417.375\n",
            "in training loop, epoch 2, step 691, the loss is 670862.0\n",
            "in training loop, epoch 2, step 692, the loss is 809566.8125\n",
            "in training loop, epoch 2, step 693, the loss is 544627.6875\n",
            "in training loop, epoch 2, step 694, the loss is 914203.5\n",
            "in training loop, epoch 2, step 695, the loss is 1049376.25\n",
            "in training loop, epoch 2, step 696, the loss is 1234775.25\n",
            "in training loop, epoch 2, step 697, the loss is 1026929.0625\n",
            "in training loop, epoch 2, step 698, the loss is 779129.125\n",
            "in training loop, epoch 2, step 699, the loss is 876960.0625\n",
            "in training loop, epoch 2, step 700, the loss is 614338.375\n",
            "in training loop, epoch 2, step 701, the loss is 364111.5\n",
            "in training loop, epoch 2, step 702, the loss is 1056178.0\n",
            "in training loop, epoch 2, step 703, the loss is 726956.1875\n",
            "in training loop, epoch 2, step 704, the loss is 377663.71875\n",
            "in training loop, epoch 2, step 705, the loss is 825991.875\n",
            "in training loop, epoch 2, step 706, the loss is 501569.59375\n",
            "in training loop, epoch 2, step 707, the loss is 679409.75\n",
            "in training loop, epoch 2, step 708, the loss is 768463.5625\n",
            "in training loop, epoch 2, step 709, the loss is 1255804.5\n",
            "in training loop, epoch 2, step 710, the loss is 1314012.75\n",
            "in training loop, epoch 2, step 711, the loss is 757459.0625\n",
            "in training loop, epoch 2, step 712, the loss is 868530.5625\n",
            "in training loop, epoch 2, step 713, the loss is 661745.1875\n",
            "in training loop, epoch 2, step 714, the loss is 930187.5625\n",
            "in training loop, epoch 2, step 715, the loss is 910168.375\n",
            "in training loop, epoch 2, step 716, the loss is 555775.375\n",
            "in training loop, epoch 2, step 717, the loss is 1195847.125\n",
            "in training loop, epoch 2, step 718, the loss is 556269.75\n",
            "in training loop, epoch 2, step 719, the loss is 1390906.875\n",
            "in training loop, epoch 2, step 720, the loss is 532472.0625\n",
            "in training loop, epoch 2, step 721, the loss is 616855.0\n",
            "in training loop, epoch 2, step 722, the loss is 948973.6875\n",
            "in training loop, epoch 2, step 723, the loss is 642045.25\n",
            "in training loop, epoch 2, step 724, the loss is 671177.3125\n",
            "in training loop, epoch 2, step 725, the loss is 972297.25\n",
            "in training loop, epoch 2, step 726, the loss is 1079957.875\n",
            "in training loop, epoch 2, step 727, the loss is 855910.3125\n",
            "in training loop, epoch 2, step 728, the loss is 650476.25\n",
            "in training loop, epoch 2, step 729, the loss is 1195573.25\n",
            "in training loop, epoch 2, step 730, the loss is 312581.625\n",
            "in training loop, epoch 2, step 731, the loss is 733906.75\n",
            "in training loop, epoch 2, step 732, the loss is 447061.96875\n",
            "in training loop, epoch 2, step 733, the loss is 697194.25\n",
            "in training loop, epoch 2, step 734, the loss is 675295.9375\n",
            "in training loop, epoch 2, step 735, the loss is 793765.1875\n",
            "in training loop, epoch 2, step 736, the loss is 1151843.375\n",
            "in training loop, epoch 2, step 737, the loss is 550171.6875\n",
            "in training loop, epoch 2, step 738, the loss is 673432.0625\n",
            "in training loop, epoch 2, step 739, the loss is 842731.8125\n",
            "in training loop, epoch 2, step 740, the loss is 585192.4375\n",
            "in training loop, epoch 2, step 741, the loss is 880129.125\n",
            "in training loop, epoch 2, step 742, the loss is 969443.875\n",
            "in training loop, epoch 2, step 743, the loss is 937222.5625\n",
            "in training loop, epoch 2, step 744, the loss is 783946.0\n",
            "in training loop, epoch 2, step 745, the loss is 850030.6875\n",
            "in training loop, epoch 2, step 746, the loss is 559604.375\n",
            "in training loop, epoch 2, step 747, the loss is 511230.21875\n",
            "in training loop, epoch 2, step 748, the loss is 721205.25\n",
            "in training loop, epoch 2, step 749, the loss is 807246.6875\n",
            "in training loop, epoch 2, step 750, the loss is 1155390.0\n",
            "in training loop, epoch 2, step 751, the loss is 290713.625\n",
            "in training loop, epoch 2, step 752, the loss is 443007.34375\n",
            "in training loop, epoch 2, step 753, the loss is 946275.25\n",
            "in training loop, epoch 2, step 754, the loss is 633527.125\n",
            "in training loop, epoch 2, step 755, the loss is 824294.3125\n",
            "in training loop, epoch 2, step 756, the loss is 1271210.25\n",
            "in training loop, epoch 2, step 757, the loss is 1400609.875\n",
            "in training loop, epoch 2, step 758, the loss is 1085382.625\n",
            "in training loop, epoch 2, step 759, the loss is 637379.4375\n",
            "in training loop, epoch 2, step 760, the loss is 529012.5\n",
            "in training loop, epoch 2, step 761, the loss is 358746.5\n",
            "in training loop, epoch 2, step 762, the loss is 613955.875\n",
            "in training loop, epoch 2, step 763, the loss is 800135.1875\n",
            "in training loop, epoch 2, step 764, the loss is 713452.8125\n",
            "in training loop, epoch 2, step 765, the loss is 606236.6875\n",
            "in training loop, epoch 2, step 766, the loss is 610690.375\n",
            "in training loop, epoch 2, step 767, the loss is 673779.75\n",
            "in training loop, epoch 2, step 768, the loss is 614527.9375\n",
            "in training loop, epoch 2, step 769, the loss is 1184293.0\n",
            "in training loop, epoch 2, step 770, the loss is 1317109.75\n",
            "in training loop, epoch 2, step 771, the loss is 852662.75\n",
            "in training loop, epoch 2, step 772, the loss is 584319.125\n",
            "in training loop, epoch 2, step 773, the loss is 721980.125\n",
            "in training loop, epoch 2, step 774, the loss is 781357.375\n",
            "in training loop, epoch 2, step 775, the loss is 1227703.375\n",
            "in training loop, epoch 2, step 776, the loss is 688833.8125\n",
            "in training loop, epoch 2, step 777, the loss is 550731.25\n",
            "in training loop, epoch 2, step 778, the loss is 744430.5625\n",
            "in training loop, epoch 2, step 779, the loss is 669396.25\n",
            "in training loop, epoch 2, step 780, the loss is 1123155.875\n",
            "in training loop, epoch 2, step 781, the loss is 771511.4375\n",
            "in training loop, epoch 2, step 782, the loss is 820280.25\n",
            "in training loop, epoch 2, step 783, the loss is 1579387.625\n",
            "in training loop, epoch 2, step 784, the loss is 822357.9375\n",
            "in training loop, epoch 2, step 785, the loss is 873557.1875\n",
            "in training loop, epoch 2, step 786, the loss is 765201.75\n",
            "in training loop, epoch 2, step 787, the loss is 451782.59375\n",
            "in training loop, epoch 2, step 788, the loss is 661830.375\n",
            "in training loop, epoch 2, step 789, the loss is 585173.1875\n",
            "in training loop, epoch 2, step 790, the loss is 1180069.625\n",
            "in training loop, epoch 2, step 791, the loss is 686962.6875\n",
            "in training loop, epoch 2, step 792, the loss is 574193.3125\n",
            "in training loop, epoch 2, step 793, the loss is 844234.5\n",
            "in training loop, epoch 2, step 794, the loss is 918569.1875\n",
            "in training loop, epoch 2, step 795, the loss is 1269488.75\n",
            "in training loop, epoch 2, step 796, the loss is 599858.75\n",
            "in training loop, epoch 2, step 797, the loss is 1124597.875\n",
            "in training loop, epoch 2, step 798, the loss is 886343.6875\n",
            "in training loop, epoch 2, step 799, the loss is 998921.5625\n",
            "in training loop, epoch 2, step 800, the loss is 596896.75\n",
            "in training loop, epoch 2, step 801, the loss is 1458633.5\n",
            "in training loop, epoch 2, step 802, the loss is 636325.75\n",
            "in training loop, epoch 2, step 803, the loss is 1088257.75\n",
            "in training loop, epoch 2, step 804, the loss is 643824.0\n",
            "in training loop, epoch 2, step 805, the loss is 678146.125\n",
            "in training loop, epoch 2, step 806, the loss is 1141989.0\n",
            "in training loop, epoch 2, step 807, the loss is 568556.75\n",
            "in training loop, epoch 2, step 808, the loss is 1287561.25\n",
            "in training loop, epoch 2, step 809, the loss is 776576.125\n",
            "in training loop, epoch 2, step 810, the loss is 1015059.75\n",
            "in training loop, epoch 2, step 811, the loss is 859455.5\n",
            "in training loop, epoch 2, step 812, the loss is 746068.25\n",
            "in training loop, epoch 2, step 813, the loss is 1099274.375\n",
            "in training loop, epoch 2, step 814, the loss is 1081303.25\n",
            "in training loop, epoch 2, step 815, the loss is 1059977.125\n",
            "in training loop, epoch 2, step 816, the loss is 362007.46875\n",
            "in training loop, epoch 2, step 817, the loss is 1448423.0\n",
            "in training loop, epoch 2, step 818, the loss is 839585.375\n",
            "in training loop, epoch 2, step 819, the loss is 341452.15625\n",
            "in training loop, epoch 2, step 820, the loss is 645115.4375\n",
            "in training loop, epoch 2, step 821, the loss is 934153.375\n",
            "in training loop, epoch 2, step 822, the loss is 483445.5\n",
            "in training loop, epoch 2, step 823, the loss is 963516.3125\n",
            "in training loop, epoch 2, step 824, the loss is 499010.4375\n",
            "in training loop, epoch 2, step 825, the loss is 530215.5625\n",
            "in training loop, epoch 2, step 826, the loss is 1124889.125\n",
            "in training loop, epoch 2, step 827, the loss is 623885.0625\n",
            "in training loop, epoch 2, step 828, the loss is 1303534.0\n",
            "in training loop, epoch 2, step 829, the loss is 577643.875\n",
            "in training loop, epoch 2, step 830, the loss is 1254177.875\n",
            "in training loop, epoch 2, step 831, the loss is 540088.0\n",
            "in training loop, epoch 2, step 832, the loss is 606803.6875\n",
            "in training loop, epoch 2, step 833, the loss is 580469.1875\n",
            "in training loop, epoch 2, step 834, the loss is 857727.1875\n",
            "in training loop, epoch 2, step 835, the loss is 1034163.25\n",
            "in training loop, epoch 2, step 836, the loss is 1348839.375\n",
            "in training loop, epoch 2, step 837, the loss is 1059693.75\n",
            "in training loop, epoch 2, step 838, the loss is 959396.4375\n",
            "in training loop, epoch 2, step 839, the loss is 950762.375\n",
            "in training loop, epoch 2, step 840, the loss is 701632.625\n",
            "in training loop, epoch 2, step 841, the loss is 599336.9375\n",
            "in training loop, epoch 2, step 842, the loss is 1160819.0\n",
            "in training loop, epoch 2, step 843, the loss is 627369.3125\n",
            "in training loop, epoch 2, step 844, the loss is 607991.4375\n",
            "in training loop, epoch 2, step 845, the loss is 1081396.5\n",
            "in training loop, epoch 2, step 846, the loss is 1244768.25\n",
            "in training loop, epoch 2, step 847, the loss is 565955.5\n",
            "in training loop, epoch 2, step 848, the loss is 491134.5625\n",
            "in training loop, epoch 2, step 849, the loss is 562442.6875\n",
            "in training loop, epoch 2, step 850, the loss is 727146.75\n",
            "in training loop, epoch 2, step 851, the loss is 722603.9375\n",
            "in training loop, epoch 2, step 852, the loss is 630497.875\n",
            "in training loop, epoch 2, step 853, the loss is 1353768.625\n",
            "in training loop, epoch 2, step 854, the loss is 487305.65625\n",
            "in training loop, epoch 2, step 855, the loss is 796096.9375\n",
            "in training loop, epoch 2, step 856, the loss is 457261.5\n",
            "in training loop, epoch 2, step 857, the loss is 630013.0625\n",
            "in training loop, epoch 2, step 858, the loss is 886106.75\n",
            "in training loop, epoch 2, step 859, the loss is 925777.3125\n",
            "in training loop, epoch 2, step 860, the loss is 989036.25\n",
            "in training loop, epoch 2, step 861, the loss is 979815.6875\n",
            "in training loop, epoch 2, step 862, the loss is 1077688.75\n",
            "in training loop, epoch 2, step 863, the loss is 610141.375\n",
            "in training loop, epoch 2, step 864, the loss is 868458.1875\n",
            "in training loop, epoch 2, step 865, the loss is 1030552.9375\n",
            "in training loop, epoch 2, step 866, the loss is 441255.9375\n",
            "in training loop, epoch 2, step 867, the loss is 907141.125\n",
            "in training loop, epoch 2, step 868, the loss is 800809.6875\n",
            "in training loop, epoch 2, step 869, the loss is 937470.4375\n",
            "in training loop, epoch 2, step 870, the loss is 570069.625\n",
            "in training loop, epoch 2, step 871, the loss is 895778.5625\n",
            "in training loop, epoch 2, step 872, the loss is 989246.6875\n",
            "in training loop, epoch 2, step 873, the loss is 651007.75\n",
            "in training loop, epoch 2, step 874, the loss is 844193.6875\n",
            "in training loop, epoch 2, step 875, the loss is 944344.125\n",
            "in training loop, epoch 2, step 876, the loss is 989448.25\n",
            "in training loop, epoch 2, step 877, the loss is 1237369.75\n",
            "in training loop, epoch 2, step 878, the loss is 429004.375\n",
            "in training loop, epoch 2, step 879, the loss is 803251.6875\n",
            "in training loop, epoch 2, step 880, the loss is 1154412.125\n",
            "in training loop, epoch 2, step 881, the loss is 525385.8125\n",
            "in training loop, epoch 2, step 882, the loss is 998590.5625\n",
            "in training loop, epoch 2, step 883, the loss is 619227.4375\n",
            "in training loop, epoch 2, step 884, the loss is 830614.25\n",
            "in training loop, epoch 2, step 885, the loss is 963615.25\n",
            "in training loop, epoch 2, step 886, the loss is 998164.375\n",
            "in training loop, epoch 2, step 887, the loss is 662427.9375\n",
            "in training loop, epoch 2, step 888, the loss is 560216.9375\n",
            "in training loop, epoch 2, step 889, the loss is 1012312.1875\n",
            "in training loop, epoch 2, step 890, the loss is 574381.125\n",
            "in training loop, epoch 2, step 891, the loss is 999767.8125\n",
            "in training loop, epoch 2, step 892, the loss is 950611.125\n",
            "in training loop, epoch 2, step 893, the loss is 942884.125\n",
            "in training loop, epoch 2, step 894, the loss is 765390.3125\n",
            "in training loop, epoch 2, step 895, the loss is 954930.3125\n",
            "in training loop, epoch 2, step 896, the loss is 1633670.5\n",
            "in training loop, epoch 2, step 897, the loss is 792413.1875\n",
            "in training loop, epoch 2, step 898, the loss is 1339864.25\n",
            "in training loop, epoch 2, step 899, the loss is 660847.8125\n",
            "in training loop, epoch 2, step 900, the loss is 857540.5\n",
            "in training loop, epoch 2, step 901, the loss is 560645.0\n",
            "in training loop, epoch 2, step 902, the loss is 687748.625\n",
            "in training loop, epoch 2, step 903, the loss is 734613.625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "k-fold 0:: Epoch 2: train loss 855889.1073008849 val loss 922907.3384900991\n",
            "in training loop, epoch 3, step 0, the loss is 648843.75\n",
            "in training loop, epoch 3, step 1, the loss is 646910.4375\n",
            "in training loop, epoch 3, step 2, the loss is 620880.125\n",
            "in training loop, epoch 3, step 3, the loss is 546082.5\n",
            "in training loop, epoch 3, step 4, the loss is 461157.6875\n",
            "in training loop, epoch 3, step 5, the loss is 1116210.75\n",
            "in training loop, epoch 3, step 6, the loss is 691350.75\n",
            "in training loop, epoch 3, step 7, the loss is 1317974.0\n",
            "in training loop, epoch 3, step 8, the loss is 710751.5\n",
            "in training loop, epoch 3, step 9, the loss is 861571.5\n",
            "in training loop, epoch 3, step 10, the loss is 1076415.5\n",
            "in training loop, epoch 3, step 11, the loss is 1141832.25\n",
            "in training loop, epoch 3, step 12, the loss is 929942.0\n",
            "in training loop, epoch 3, step 13, the loss is 646186.0625\n",
            "in training loop, epoch 3, step 14, the loss is 764967.5625\n",
            "in training loop, epoch 3, step 15, the loss is 740230.5\n",
            "in training loop, epoch 3, step 16, the loss is 1107598.75\n",
            "in training loop, epoch 3, step 17, the loss is 1074201.75\n",
            "in training loop, epoch 3, step 18, the loss is 774676.0625\n",
            "in training loop, epoch 3, step 19, the loss is 797854.9375\n",
            "in training loop, epoch 3, step 20, the loss is 681985.75\n",
            "in training loop, epoch 3, step 21, the loss is 557127.4375\n",
            "in training loop, epoch 3, step 22, the loss is 485113.03125\n",
            "in training loop, epoch 3, step 23, the loss is 982994.0\n",
            "in training loop, epoch 3, step 24, the loss is 710383.125\n",
            "in training loop, epoch 3, step 25, the loss is 540571.6875\n",
            "in training loop, epoch 3, step 26, the loss is 696723.625\n",
            "in training loop, epoch 3, step 27, the loss is 463650.90625\n",
            "in training loop, epoch 3, step 28, the loss is 931794.375\n",
            "in training loop, epoch 3, step 29, the loss is 921676.4375\n",
            "in training loop, epoch 3, step 30, the loss is 635081.125\n",
            "in training loop, epoch 3, step 31, the loss is 728881.4375\n",
            "in training loop, epoch 3, step 32, the loss is 368771.125\n",
            "in training loop, epoch 3, step 33, the loss is 601286.125\n",
            "in training loop, epoch 3, step 34, the loss is 753626.625\n",
            "in training loop, epoch 3, step 35, the loss is 1296624.875\n",
            "in training loop, epoch 3, step 36, the loss is 598218.6875\n",
            "in training loop, epoch 3, step 37, the loss is 649506.625\n",
            "in training loop, epoch 3, step 38, the loss is 661753.1875\n",
            "in training loop, epoch 3, step 39, the loss is 665028.4375\n",
            "in training loop, epoch 3, step 40, the loss is 828273.0625\n",
            "in training loop, epoch 3, step 41, the loss is 666538.5625\n",
            "in training loop, epoch 3, step 42, the loss is 735136.5\n",
            "in training loop, epoch 3, step 43, the loss is 1040194.3125\n",
            "in training loop, epoch 3, step 44, the loss is 1495552.0\n",
            "in training loop, epoch 3, step 45, the loss is 1094997.75\n",
            "in training loop, epoch 3, step 46, the loss is 693809.5625\n",
            "in training loop, epoch 3, step 47, the loss is 1026338.0\n",
            "in training loop, epoch 3, step 48, the loss is 484421.40625\n",
            "in training loop, epoch 3, step 49, the loss is 625559.375\n",
            "in training loop, epoch 3, step 50, the loss is 997402.5\n",
            "in training loop, epoch 3, step 51, the loss is 658464.125\n",
            "in training loop, epoch 3, step 52, the loss is 706941.75\n",
            "in training loop, epoch 3, step 53, the loss is 470143.15625\n",
            "in training loop, epoch 3, step 54, the loss is 418033.71875\n",
            "in training loop, epoch 3, step 55, the loss is 758990.0\n",
            "in training loop, epoch 3, step 56, the loss is 531635.5625\n",
            "in training loop, epoch 3, step 57, the loss is 516093.0625\n",
            "in training loop, epoch 3, step 58, the loss is 344605.28125\n",
            "in training loop, epoch 3, step 59, the loss is 509419.09375\n",
            "in training loop, epoch 3, step 60, the loss is 1160234.375\n",
            "in training loop, epoch 3, step 61, the loss is 815295.125\n",
            "in training loop, epoch 3, step 62, the loss is 929845.0625\n",
            "in training loop, epoch 3, step 63, the loss is 876078.6875\n",
            "in training loop, epoch 3, step 64, the loss is 547596.3125\n",
            "in training loop, epoch 3, step 65, the loss is 675434.5625\n",
            "in training loop, epoch 3, step 66, the loss is 1342941.125\n",
            "in training loop, epoch 3, step 67, the loss is 953679.75\n",
            "in training loop, epoch 3, step 68, the loss is 1382980.25\n",
            "in training loop, epoch 3, step 69, the loss is 1189302.25\n",
            "in training loop, epoch 3, step 70, the loss is 1059058.5\n",
            "in training loop, epoch 3, step 71, the loss is 531547.9375\n",
            "in training loop, epoch 3, step 72, the loss is 429887.34375\n",
            "in training loop, epoch 3, step 73, the loss is 969992.625\n",
            "in training loop, epoch 3, step 74, the loss is 813761.375\n",
            "in training loop, epoch 3, step 75, the loss is 1261238.5\n",
            "in training loop, epoch 3, step 76, the loss is 777812.8125\n",
            "in training loop, epoch 3, step 77, the loss is 604043.9375\n",
            "in training loop, epoch 3, step 78, the loss is 532743.375\n",
            "in training loop, epoch 3, step 79, the loss is 641013.375\n",
            "in training loop, epoch 3, step 80, the loss is 618410.5\n",
            "in training loop, epoch 3, step 81, the loss is 578116.1875\n",
            "in training loop, epoch 3, step 82, the loss is 745742.375\n",
            "in training loop, epoch 3, step 83, the loss is 664125.0625\n",
            "in training loop, epoch 3, step 84, the loss is 499883.5\n",
            "in training loop, epoch 3, step 85, the loss is 569144.125\n",
            "in training loop, epoch 3, step 86, the loss is 918446.625\n",
            "in training loop, epoch 3, step 87, the loss is 500467.25\n",
            "in training loop, epoch 3, step 88, the loss is 942430.875\n",
            "in training loop, epoch 3, step 89, the loss is 976931.5\n",
            "in training loop, epoch 3, step 90, the loss is 543699.25\n",
            "in training loop, epoch 3, step 91, the loss is 700810.1875\n",
            "in training loop, epoch 3, step 92, the loss is 752773.75\n",
            "in training loop, epoch 3, step 93, the loss is 975291.1875\n",
            "in training loop, epoch 3, step 94, the loss is 713940.9375\n",
            "in training loop, epoch 3, step 95, the loss is 929862.5625\n",
            "in training loop, epoch 3, step 96, the loss is 863587.75\n",
            "in training loop, epoch 3, step 97, the loss is 781276.875\n",
            "in training loop, epoch 3, step 98, the loss is 836550.25\n",
            "in training loop, epoch 3, step 99, the loss is 865134.5\n",
            "in training loop, epoch 3, step 100, the loss is 860059.5\n",
            "in training loop, epoch 3, step 101, the loss is 1186160.125\n",
            "in training loop, epoch 3, step 102, the loss is 839115.75\n",
            "in training loop, epoch 3, step 103, the loss is 799727.375\n",
            "in training loop, epoch 3, step 104, the loss is 531718.0\n",
            "in training loop, epoch 3, step 105, the loss is 923519.625\n",
            "in training loop, epoch 3, step 106, the loss is 545704.125\n",
            "in training loop, epoch 3, step 107, the loss is 915858.125\n",
            "in training loop, epoch 3, step 108, the loss is 630192.875\n",
            "in training loop, epoch 3, step 109, the loss is 731198.25\n",
            "in training loop, epoch 3, step 110, the loss is 1014495.875\n",
            "in training loop, epoch 3, step 111, the loss is 1041777.625\n",
            "in training loop, epoch 3, step 112, the loss is 791532.1875\n",
            "in training loop, epoch 3, step 113, the loss is 685094.875\n",
            "in training loop, epoch 3, step 114, the loss is 1250921.875\n",
            "in training loop, epoch 3, step 115, the loss is 720906.5\n",
            "in training loop, epoch 3, step 116, the loss is 487524.5\n",
            "in training loop, epoch 3, step 117, the loss is 790654.5\n",
            "in training loop, epoch 3, step 118, the loss is 490402.03125\n",
            "in training loop, epoch 3, step 119, the loss is 975610.0\n",
            "in training loop, epoch 3, step 120, the loss is 1252799.5\n",
            "in training loop, epoch 3, step 121, the loss is 249707.375\n",
            "in training loop, epoch 3, step 122, the loss is 467608.8125\n",
            "in training loop, epoch 3, step 123, the loss is 603987.3125\n",
            "in training loop, epoch 3, step 124, the loss is 708432.125\n",
            "in training loop, epoch 3, step 125, the loss is 496550.375\n",
            "in training loop, epoch 3, step 126, the loss is 577422.75\n",
            "in training loop, epoch 3, step 127, the loss is 477826.09375\n",
            "in training loop, epoch 3, step 128, the loss is 940585.1875\n",
            "in training loop, epoch 3, step 129, the loss is 676379.9375\n",
            "in training loop, epoch 3, step 130, the loss is 734351.75\n",
            "in training loop, epoch 3, step 131, the loss is 765895.9375\n",
            "in training loop, epoch 3, step 132, the loss is 848774.0\n",
            "in training loop, epoch 3, step 133, the loss is 484988.53125\n",
            "in training loop, epoch 3, step 134, the loss is 271776.09375\n",
            "in training loop, epoch 3, step 135, the loss is 356272.8125\n",
            "in training loop, epoch 3, step 136, the loss is 1720925.375\n",
            "in training loop, epoch 3, step 137, the loss is 869852.4375\n",
            "in training loop, epoch 3, step 138, the loss is 764627.25\n",
            "in training loop, epoch 3, step 139, the loss is 419971.0625\n",
            "in training loop, epoch 3, step 140, the loss is 829189.0\n",
            "in training loop, epoch 3, step 141, the loss is 489768.5625\n",
            "in training loop, epoch 3, step 142, the loss is 828533.4375\n",
            "in training loop, epoch 3, step 143, the loss is 563607.1875\n",
            "in training loop, epoch 3, step 144, the loss is 1049600.375\n",
            "in training loop, epoch 3, step 145, the loss is 613021.375\n",
            "in training loop, epoch 3, step 146, the loss is 610356.3125\n",
            "in training loop, epoch 3, step 147, the loss is 360658.09375\n",
            "in training loop, epoch 3, step 148, the loss is 1142952.875\n",
            "in training loop, epoch 3, step 149, the loss is 409058.125\n",
            "in training loop, epoch 3, step 150, the loss is 856652.25\n",
            "in training loop, epoch 3, step 151, the loss is 694952.875\n",
            "in training loop, epoch 3, step 152, the loss is 503646.375\n",
            "in training loop, epoch 3, step 153, the loss is 472480.5625\n",
            "in training loop, epoch 3, step 154, the loss is 828977.125\n",
            "in training loop, epoch 3, step 155, the loss is 191201.75\n",
            "in training loop, epoch 3, step 156, the loss is 695480.1875\n",
            "in training loop, epoch 3, step 157, the loss is 446952.5625\n",
            "in training loop, epoch 3, step 158, the loss is 430393.21875\n",
            "in training loop, epoch 3, step 159, the loss is 731626.4375\n",
            "in training loop, epoch 3, step 160, the loss is 503270.3125\n",
            "in training loop, epoch 3, step 161, the loss is 1108534.125\n",
            "in training loop, epoch 3, step 162, the loss is 545734.875\n",
            "in training loop, epoch 3, step 163, the loss is 758362.4375\n",
            "in training loop, epoch 3, step 164, the loss is 822935.6875\n",
            "in training loop, epoch 3, step 165, the loss is 789416.125\n",
            "in training loop, epoch 3, step 166, the loss is 535220.5625\n",
            "in training loop, epoch 3, step 167, the loss is 777531.0625\n",
            "in training loop, epoch 3, step 168, the loss is 542021.9375\n",
            "in training loop, epoch 3, step 169, the loss is 526018.4375\n",
            "in training loop, epoch 3, step 170, the loss is 583055.0\n",
            "in training loop, epoch 3, step 171, the loss is 819618.375\n",
            "in training loop, epoch 3, step 172, the loss is 484188.40625\n",
            "in training loop, epoch 3, step 173, the loss is 541115.875\n",
            "in training loop, epoch 3, step 174, the loss is 520782.0625\n",
            "in training loop, epoch 3, step 175, the loss is 763943.375\n",
            "in training loop, epoch 3, step 176, the loss is 1137953.25\n",
            "in training loop, epoch 3, step 177, the loss is 353258.90625\n",
            "in training loop, epoch 3, step 178, the loss is 492985.84375\n",
            "in training loop, epoch 3, step 179, the loss is 668689.5\n",
            "in training loop, epoch 3, step 180, the loss is 726040.0625\n",
            "in training loop, epoch 3, step 181, the loss is 801141.6875\n",
            "in training loop, epoch 3, step 182, the loss is 938164.25\n",
            "in training loop, epoch 3, step 183, the loss is 686289.8125\n",
            "in training loop, epoch 3, step 184, the loss is 385253.25\n",
            "in training loop, epoch 3, step 185, the loss is 654902.1875\n",
            "in training loop, epoch 3, step 186, the loss is 1068711.5\n",
            "in training loop, epoch 3, step 187, the loss is 707085.6875\n",
            "in training loop, epoch 3, step 188, the loss is 808027.8125\n",
            "in training loop, epoch 3, step 189, the loss is 686083.25\n",
            "in training loop, epoch 3, step 190, the loss is 507703.53125\n",
            "in training loop, epoch 3, step 191, the loss is 764285.9375\n",
            "in training loop, epoch 3, step 192, the loss is 1030851.75\n",
            "in training loop, epoch 3, step 193, the loss is 704594.1875\n",
            "in training loop, epoch 3, step 194, the loss is 665584.0625\n",
            "in training loop, epoch 3, step 195, the loss is 732584.9375\n",
            "in training loop, epoch 3, step 196, the loss is 400514.875\n",
            "in training loop, epoch 3, step 197, the loss is 783411.5\n",
            "in training loop, epoch 3, step 198, the loss is 550829.3125\n",
            "in training loop, epoch 3, step 199, the loss is 610829.625\n",
            "in training loop, epoch 3, step 200, the loss is 792298.4375\n",
            "in training loop, epoch 3, step 201, the loss is 337955.65625\n",
            "in training loop, epoch 3, step 202, the loss is 1056745.375\n",
            "in training loop, epoch 3, step 203, the loss is 653710.6875\n",
            "in training loop, epoch 3, step 204, the loss is 593198.25\n",
            "in training loop, epoch 3, step 205, the loss is 915103.0625\n",
            "in training loop, epoch 3, step 206, the loss is 1096107.875\n",
            "in training loop, epoch 3, step 207, the loss is 593185.6875\n",
            "in training loop, epoch 3, step 208, the loss is 613481.25\n",
            "in training loop, epoch 3, step 209, the loss is 647865.0\n",
            "in training loop, epoch 3, step 210, the loss is 892397.875\n",
            "in training loop, epoch 3, step 211, the loss is 703083.125\n",
            "in training loop, epoch 3, step 212, the loss is 805820.0\n",
            "in training loop, epoch 3, step 213, the loss is 746817.1875\n",
            "in training loop, epoch 3, step 214, the loss is 995371.5\n",
            "in training loop, epoch 3, step 215, the loss is 804737.1875\n",
            "in training loop, epoch 3, step 216, the loss is 957687.0\n",
            "in training loop, epoch 3, step 217, the loss is 587520.3125\n",
            "in training loop, epoch 3, step 218, the loss is 671171.8125\n",
            "in training loop, epoch 3, step 219, the loss is 904586.375\n",
            "in training loop, epoch 3, step 220, the loss is 928752.875\n",
            "in training loop, epoch 3, step 221, the loss is 641452.875\n",
            "in training loop, epoch 3, step 222, the loss is 1043738.125\n",
            "in training loop, epoch 3, step 223, the loss is 816032.9375\n",
            "in training loop, epoch 3, step 224, the loss is 694054.875\n",
            "in training loop, epoch 3, step 225, the loss is 658843.5\n",
            "in training loop, epoch 3, step 226, the loss is 993283.0625\n",
            "in training loop, epoch 3, step 227, the loss is 767884.3125\n",
            "in training loop, epoch 3, step 228, the loss is 1298807.125\n",
            "in training loop, epoch 3, step 229, the loss is 754413.75\n",
            "in training loop, epoch 3, step 230, the loss is 874827.375\n",
            "in training loop, epoch 3, step 231, the loss is 832415.125\n",
            "in training loop, epoch 3, step 232, the loss is 754379.125\n",
            "in training loop, epoch 3, step 233, the loss is 369668.71875\n",
            "in training loop, epoch 3, step 234, the loss is 916526.5625\n",
            "in training loop, epoch 3, step 235, the loss is 479301.625\n",
            "in training loop, epoch 3, step 236, the loss is 884186.3125\n",
            "in training loop, epoch 3, step 237, the loss is 526683.1875\n",
            "in training loop, epoch 3, step 238, the loss is 572208.875\n",
            "in training loop, epoch 3, step 239, the loss is 227075.5625\n",
            "in training loop, epoch 3, step 240, the loss is 455341.53125\n",
            "in training loop, epoch 3, step 241, the loss is 1163459.875\n",
            "in training loop, epoch 3, step 242, the loss is 511793.15625\n",
            "in training loop, epoch 3, step 243, the loss is 1335396.0\n",
            "in training loop, epoch 3, step 244, the loss is 607359.0\n",
            "in training loop, epoch 3, step 245, the loss is 414199.28125\n",
            "in training loop, epoch 3, step 246, the loss is 425726.375\n",
            "in training loop, epoch 3, step 247, the loss is 299222.15625\n",
            "in training loop, epoch 3, step 248, the loss is 601788.75\n",
            "in training loop, epoch 3, step 249, the loss is 971811.0\n",
            "in training loop, epoch 3, step 250, the loss is 830620.0\n",
            "in training loop, epoch 3, step 251, the loss is 650704.3125\n",
            "in training loop, epoch 3, step 252, the loss is 1119372.25\n",
            "in training loop, epoch 3, step 253, the loss is 966514.125\n",
            "in training loop, epoch 3, step 254, the loss is 856113.25\n",
            "in training loop, epoch 3, step 255, the loss is 540630.25\n",
            "in training loop, epoch 3, step 256, the loss is 482932.84375\n",
            "in training loop, epoch 3, step 257, the loss is 633154.3125\n",
            "in training loop, epoch 3, step 258, the loss is 796011.1875\n",
            "in training loop, epoch 3, step 259, the loss is 771412.625\n",
            "in training loop, epoch 3, step 260, the loss is 635770.4375\n",
            "in training loop, epoch 3, step 261, the loss is 820415.875\n",
            "in training loop, epoch 3, step 262, the loss is 773611.75\n",
            "in training loop, epoch 3, step 263, the loss is 1440849.375\n",
            "in training loop, epoch 3, step 264, the loss is 542248.125\n",
            "in training loop, epoch 3, step 265, the loss is 1061540.25\n",
            "in training loop, epoch 3, step 266, the loss is 674547.1875\n",
            "in training loop, epoch 3, step 267, the loss is 625419.0\n",
            "in training loop, epoch 3, step 268, the loss is 754097.4375\n",
            "in training loop, epoch 3, step 269, the loss is 734361.5\n",
            "in training loop, epoch 3, step 270, the loss is 990282.625\n",
            "in training loop, epoch 3, step 271, the loss is 701291.1875\n",
            "in training loop, epoch 3, step 272, the loss is 856131.0\n",
            "in training loop, epoch 3, step 273, the loss is 636111.6875\n",
            "in training loop, epoch 3, step 274, the loss is 504246.65625\n",
            "in training loop, epoch 3, step 275, the loss is 476486.375\n",
            "in training loop, epoch 3, step 276, the loss is 669784.875\n",
            "in training loop, epoch 3, step 277, the loss is 707265.6875\n",
            "in training loop, epoch 3, step 278, the loss is 1161402.75\n",
            "in training loop, epoch 3, step 279, the loss is 687596.125\n",
            "in training loop, epoch 3, step 280, the loss is 588146.1875\n",
            "in training loop, epoch 3, step 281, the loss is 367052.5\n",
            "in training loop, epoch 3, step 282, the loss is 1636644.5\n",
            "in training loop, epoch 3, step 283, the loss is 937103.3125\n",
            "in training loop, epoch 3, step 284, the loss is 523417.125\n",
            "in training loop, epoch 3, step 285, the loss is 602941.0\n",
            "in training loop, epoch 3, step 286, the loss is 603537.5\n",
            "in training loop, epoch 3, step 287, the loss is 387967.875\n",
            "in training loop, epoch 3, step 288, the loss is 1116410.0\n",
            "in training loop, epoch 3, step 289, the loss is 1269083.75\n",
            "in training loop, epoch 3, step 290, the loss is 651179.6875\n",
            "in training loop, epoch 3, step 291, the loss is 853802.625\n",
            "in training loop, epoch 3, step 292, the loss is 628396.625\n",
            "in training loop, epoch 3, step 293, the loss is 1170143.5\n",
            "in training loop, epoch 3, step 294, the loss is 1069692.5\n",
            "in training loop, epoch 3, step 295, the loss is 404639.25\n",
            "in training loop, epoch 3, step 296, the loss is 708433.9375\n",
            "in training loop, epoch 3, step 297, the loss is 551278.0\n",
            "in training loop, epoch 3, step 298, the loss is 1016523.5625\n",
            "in training loop, epoch 3, step 299, the loss is 484513.96875\n",
            "in training loop, epoch 3, step 300, the loss is 535136.0625\n",
            "in training loop, epoch 3, step 301, the loss is 593495.75\n",
            "in training loop, epoch 3, step 302, the loss is 699272.0\n",
            "in training loop, epoch 3, step 303, the loss is 704469.25\n",
            "in training loop, epoch 3, step 304, the loss is 540140.125\n",
            "in training loop, epoch 3, step 305, the loss is 754928.0625\n",
            "in training loop, epoch 3, step 306, the loss is 1614420.125\n",
            "in training loop, epoch 3, step 307, the loss is 1516222.125\n",
            "in training loop, epoch 3, step 308, the loss is 909543.5\n",
            "in training loop, epoch 3, step 309, the loss is 1342968.75\n",
            "in training loop, epoch 3, step 310, the loss is 1098123.875\n",
            "in training loop, epoch 3, step 311, the loss is 699996.75\n",
            "in training loop, epoch 3, step 312, the loss is 787061.25\n",
            "in training loop, epoch 3, step 313, the loss is 788877.5625\n",
            "in training loop, epoch 3, step 314, the loss is 1650385.875\n",
            "in training loop, epoch 3, step 315, the loss is 1250102.125\n",
            "in training loop, epoch 3, step 316, the loss is 833123.125\n",
            "in training loop, epoch 3, step 317, the loss is 871319.25\n",
            "in training loop, epoch 3, step 318, the loss is 907142.9375\n",
            "in training loop, epoch 3, step 319, the loss is 1075093.75\n",
            "in training loop, epoch 3, step 320, the loss is 731723.4375\n",
            "in training loop, epoch 3, step 321, the loss is 1108527.625\n",
            "in training loop, epoch 3, step 322, the loss is 384040.5\n",
            "in training loop, epoch 3, step 323, the loss is 806305.3125\n",
            "in training loop, epoch 3, step 324, the loss is 807729.6875\n",
            "in training loop, epoch 3, step 325, the loss is 718565.3125\n",
            "in training loop, epoch 3, step 326, the loss is 641048.3125\n",
            "in training loop, epoch 3, step 327, the loss is 612235.25\n",
            "in training loop, epoch 3, step 328, the loss is 430905.71875\n",
            "in training loop, epoch 3, step 329, the loss is 730706.125\n",
            "in training loop, epoch 3, step 330, the loss is 807172.6875\n",
            "in training loop, epoch 3, step 331, the loss is 984069.375\n",
            "in training loop, epoch 3, step 332, the loss is 609554.6875\n",
            "in training loop, epoch 3, step 333, the loss is 838879.75\n",
            "in training loop, epoch 3, step 334, the loss is 770423.3125\n",
            "in training loop, epoch 3, step 335, the loss is 1272068.25\n",
            "in training loop, epoch 3, step 336, the loss is 743746.875\n",
            "in training loop, epoch 3, step 337, the loss is 1438849.0\n",
            "in training loop, epoch 3, step 338, the loss is 456941.0\n",
            "in training loop, epoch 3, step 339, the loss is 756504.9375\n",
            "in training loop, epoch 3, step 340, the loss is 774801.125\n",
            "in training loop, epoch 3, step 341, the loss is 801851.625\n",
            "in training loop, epoch 3, step 342, the loss is 601799.0625\n",
            "in training loop, epoch 3, step 343, the loss is 1025229.875\n",
            "in training loop, epoch 3, step 344, the loss is 1365898.625\n",
            "in training loop, epoch 3, step 345, the loss is 1697894.75\n",
            "in training loop, epoch 3, step 346, the loss is 573132.1875\n",
            "in training loop, epoch 3, step 347, the loss is 832613.75\n",
            "in training loop, epoch 3, step 348, the loss is 870238.625\n",
            "in training loop, epoch 3, step 349, the loss is 612244.0\n",
            "in training loop, epoch 3, step 350, the loss is 799656.3125\n",
            "in training loop, epoch 3, step 351, the loss is 660081.0\n",
            "in training loop, epoch 3, step 352, the loss is 939502.875\n",
            "in training loop, epoch 3, step 353, the loss is 1070775.125\n",
            "in training loop, epoch 3, step 354, the loss is 461058.5625\n",
            "in training loop, epoch 3, step 355, the loss is 790311.375\n",
            "in training loop, epoch 3, step 356, the loss is 589389.625\n",
            "in training loop, epoch 3, step 357, the loss is 917542.6875\n",
            "in training loop, epoch 3, step 358, the loss is 518954.84375\n",
            "in training loop, epoch 3, step 359, the loss is 337341.25\n",
            "in training loop, epoch 3, step 360, the loss is 699807.9375\n",
            "in training loop, epoch 3, step 361, the loss is 500709.4375\n",
            "in training loop, epoch 3, step 362, the loss is 531629.25\n",
            "in training loop, epoch 3, step 363, the loss is 781226.125\n",
            "in training loop, epoch 3, step 364, the loss is 937608.75\n",
            "in training loop, epoch 3, step 365, the loss is 985440.625\n",
            "in training loop, epoch 3, step 366, the loss is 502594.46875\n",
            "in training loop, epoch 3, step 367, the loss is 978525.125\n",
            "in training loop, epoch 3, step 368, the loss is 421740.6875\n",
            "in training loop, epoch 3, step 369, the loss is 777039.25\n",
            "in training loop, epoch 3, step 370, the loss is 522010.46875\n",
            "in training loop, epoch 3, step 371, the loss is 630488.6875\n",
            "in training loop, epoch 3, step 372, the loss is 483347.15625\n",
            "in training loop, epoch 3, step 373, the loss is 569276.5625\n",
            "in training loop, epoch 3, step 374, the loss is 497143.25\n",
            "in training loop, epoch 3, step 375, the loss is 657394.0625\n",
            "in training loop, epoch 3, step 376, the loss is 661296.625\n",
            "in training loop, epoch 3, step 377, the loss is 966640.1875\n",
            "in training loop, epoch 3, step 378, the loss is 1073468.75\n",
            "in training loop, epoch 3, step 379, the loss is 537011.5625\n",
            "in training loop, epoch 3, step 380, the loss is 977545.625\n",
            "in training loop, epoch 3, step 381, the loss is 1120483.75\n",
            "in training loop, epoch 3, step 382, the loss is 630069.6875\n",
            "in training loop, epoch 3, step 383, the loss is 846313.875\n",
            "in training loop, epoch 3, step 384, the loss is 785586.5\n",
            "in training loop, epoch 3, step 385, the loss is 698015.25\n",
            "in training loop, epoch 3, step 386, the loss is 621483.75\n",
            "in training loop, epoch 3, step 387, the loss is 877858.125\n",
            "in training loop, epoch 3, step 388, the loss is 986861.3125\n",
            "in training loop, epoch 3, step 389, the loss is 787463.375\n",
            "in training loop, epoch 3, step 390, the loss is 1087049.25\n",
            "in training loop, epoch 3, step 391, the loss is 696764.3125\n",
            "in training loop, epoch 3, step 392, the loss is 1012974.875\n",
            "in training loop, epoch 3, step 393, the loss is 543199.5\n",
            "in training loop, epoch 3, step 394, the loss is 819273.8125\n",
            "in training loop, epoch 3, step 395, the loss is 750392.125\n",
            "in training loop, epoch 3, step 396, the loss is 732522.4375\n",
            "in training loop, epoch 3, step 397, the loss is 911432.375\n",
            "in training loop, epoch 3, step 398, the loss is 799717.6875\n",
            "in training loop, epoch 3, step 399, the loss is 1169666.875\n",
            "in training loop, epoch 3, step 400, the loss is 1140095.5\n",
            "in training loop, epoch 3, step 401, the loss is 939404.3125\n",
            "in training loop, epoch 3, step 402, the loss is 775565.0625\n",
            "in training loop, epoch 3, step 403, the loss is 668623.0\n",
            "in training loop, epoch 3, step 404, the loss is 963314.625\n",
            "in training loop, epoch 3, step 405, the loss is 768961.0\n",
            "in training loop, epoch 3, step 406, the loss is 345158.4375\n",
            "in training loop, epoch 3, step 407, the loss is 562406.0\n",
            "in training loop, epoch 3, step 408, the loss is 895469.5625\n",
            "in training loop, epoch 3, step 409, the loss is 1034558.625\n",
            "in training loop, epoch 3, step 410, the loss is 552335.75\n",
            "in training loop, epoch 3, step 411, the loss is 465197.96875\n",
            "in training loop, epoch 3, step 412, the loss is 729303.375\n",
            "in training loop, epoch 3, step 413, the loss is 859535.5625\n",
            "in training loop, epoch 3, step 414, the loss is 981859.875\n",
            "in training loop, epoch 3, step 415, the loss is 328918.0\n",
            "in training loop, epoch 3, step 416, the loss is 1175132.5\n",
            "in training loop, epoch 3, step 417, the loss is 839379.375\n",
            "in training loop, epoch 3, step 418, the loss is 1057893.875\n",
            "in training loop, epoch 3, step 419, the loss is 757060.5\n",
            "in training loop, epoch 3, step 420, the loss is 456896.25\n",
            "in training loop, epoch 3, step 421, the loss is 864797.125\n",
            "in training loop, epoch 3, step 422, the loss is 850782.875\n",
            "in training loop, epoch 3, step 423, the loss is 721930.375\n",
            "in training loop, epoch 3, step 424, the loss is 653712.5\n",
            "in training loop, epoch 3, step 425, the loss is 738469.625\n",
            "in training loop, epoch 3, step 426, the loss is 387510.4375\n",
            "in training loop, epoch 3, step 427, the loss is 713537.5\n",
            "in training loop, epoch 3, step 428, the loss is 808207.6875\n",
            "in training loop, epoch 3, step 429, the loss is 754514.25\n",
            "in training loop, epoch 3, step 430, the loss is 929850.125\n",
            "in training loop, epoch 3, step 431, the loss is 678346.4375\n",
            "in training loop, epoch 3, step 432, the loss is 671669.3125\n",
            "in training loop, epoch 3, step 433, the loss is 753968.375\n",
            "in training loop, epoch 3, step 434, the loss is 486068.25\n",
            "in training loop, epoch 3, step 435, the loss is 661389.875\n",
            "in training loop, epoch 3, step 436, the loss is 620220.8125\n",
            "in training loop, epoch 3, step 437, the loss is 760473.875\n",
            "in training loop, epoch 3, step 438, the loss is 481597.90625\n",
            "in training loop, epoch 3, step 439, the loss is 825104.1875\n",
            "in training loop, epoch 3, step 440, the loss is 986830.0625\n",
            "in training loop, epoch 3, step 441, the loss is 411294.28125\n",
            "in training loop, epoch 3, step 442, the loss is 2247729.0\n",
            "in training loop, epoch 3, step 443, the loss is 821835.875\n",
            "in training loop, epoch 3, step 444, the loss is 678604.8125\n",
            "in training loop, epoch 3, step 445, the loss is 597448.0625\n",
            "in training loop, epoch 3, step 446, the loss is 631452.0625\n",
            "in training loop, epoch 3, step 447, the loss is 595704.0\n",
            "in training loop, epoch 3, step 448, the loss is 223356.859375\n",
            "in training loop, epoch 3, step 449, the loss is 877060.5\n",
            "in training loop, epoch 3, step 450, the loss is 682196.25\n",
            "in training loop, epoch 3, step 451, the loss is 762326.75\n",
            "in training loop, epoch 3, step 452, the loss is 658000.0\n",
            "in training loop, epoch 3, step 453, the loss is 328190.8125\n",
            "in training loop, epoch 3, step 454, the loss is 439445.1875\n",
            "in training loop, epoch 3, step 455, the loss is 602146.5625\n",
            "in training loop, epoch 3, step 456, the loss is 1314578.625\n",
            "in training loop, epoch 3, step 457, the loss is 826660.6875\n",
            "in training loop, epoch 3, step 458, the loss is 593041.9375\n",
            "in training loop, epoch 3, step 459, the loss is 773980.125\n",
            "in training loop, epoch 3, step 460, the loss is 757873.375\n",
            "in training loop, epoch 3, step 461, the loss is 475897.0\n",
            "in training loop, epoch 3, step 462, the loss is 881444.0625\n",
            "in training loop, epoch 3, step 463, the loss is 1132570.875\n",
            "in training loop, epoch 3, step 464, the loss is 601287.1875\n",
            "in training loop, epoch 3, step 465, the loss is 469162.625\n",
            "in training loop, epoch 3, step 466, the loss is 579660.8125\n",
            "in training loop, epoch 3, step 467, the loss is 1006358.125\n",
            "in training loop, epoch 3, step 468, the loss is 994960.6875\n",
            "in training loop, epoch 3, step 469, the loss is 548287.5\n",
            "in training loop, epoch 3, step 470, the loss is 667721.8125\n",
            "in training loop, epoch 3, step 471, the loss is 783794.875\n",
            "in training loop, epoch 3, step 472, the loss is 769947.0625\n",
            "in training loop, epoch 3, step 473, the loss is 924431.875\n",
            "in training loop, epoch 3, step 474, the loss is 425197.5\n",
            "in training loop, epoch 3, step 475, the loss is 598235.9375\n",
            "in training loop, epoch 3, step 476, the loss is 1332633.125\n",
            "in training loop, epoch 3, step 477, the loss is 453566.3125\n",
            "in training loop, epoch 3, step 478, the loss is 734209.625\n",
            "in training loop, epoch 3, step 479, the loss is 1278555.25\n",
            "in training loop, epoch 3, step 480, the loss is 475385.21875\n",
            "in training loop, epoch 3, step 481, the loss is 798694.125\n",
            "in training loop, epoch 3, step 482, the loss is 925903.25\n",
            "in training loop, epoch 3, step 483, the loss is 513779.625\n",
            "in training loop, epoch 3, step 484, the loss is 637229.875\n",
            "in training loop, epoch 3, step 485, the loss is 303891.46875\n",
            "in training loop, epoch 3, step 486, the loss is 471615.40625\n",
            "in training loop, epoch 3, step 487, the loss is 550451.5\n",
            "in training loop, epoch 3, step 488, the loss is 471521.1875\n",
            "in training loop, epoch 3, step 489, the loss is 517191.0\n",
            "in training loop, epoch 3, step 490, the loss is 843290.625\n",
            "in training loop, epoch 3, step 491, the loss is 716314.0\n",
            "in training loop, epoch 3, step 492, the loss is 1087338.0\n",
            "in training loop, epoch 3, step 493, the loss is 696304.3125\n",
            "in training loop, epoch 3, step 494, the loss is 798411.875\n",
            "in training loop, epoch 3, step 495, the loss is 612915.375\n",
            "in training loop, epoch 3, step 496, the loss is 1095927.375\n",
            "in training loop, epoch 3, step 497, the loss is 633517.5625\n",
            "in training loop, epoch 3, step 498, the loss is 509742.96875\n",
            "in training loop, epoch 3, step 499, the loss is 673057.125\n",
            "in training loop, epoch 3, step 500, the loss is 743527.25\n",
            "in training loop, epoch 3, step 501, the loss is 407360.4375\n",
            "in training loop, epoch 3, step 502, the loss is 549981.5625\n",
            "in training loop, epoch 3, step 503, the loss is 929267.6875\n",
            "in training loop, epoch 3, step 504, the loss is 1117400.5\n",
            "in training loop, epoch 3, step 505, the loss is 557632.5625\n",
            "in training loop, epoch 3, step 506, the loss is 877039.0625\n",
            "in training loop, epoch 3, step 507, the loss is 707154.4375\n",
            "in training loop, epoch 3, step 508, the loss is 895369.375\n",
            "in training loop, epoch 3, step 509, the loss is 664785.25\n",
            "in training loop, epoch 3, step 510, the loss is 766431.3125\n",
            "in training loop, epoch 3, step 511, the loss is 547152.3125\n",
            "in training loop, epoch 3, step 512, the loss is 455199.46875\n",
            "in training loop, epoch 3, step 513, the loss is 646942.6875\n",
            "in training loop, epoch 3, step 514, the loss is 668753.375\n",
            "in training loop, epoch 3, step 515, the loss is 580020.3125\n",
            "in training loop, epoch 3, step 516, the loss is 444944.40625\n",
            "in training loop, epoch 3, step 517, the loss is 686374.5625\n",
            "in training loop, epoch 3, step 518, the loss is 505861.25\n",
            "in training loop, epoch 3, step 519, the loss is 689250.125\n",
            "in training loop, epoch 3, step 520, the loss is 391932.40625\n",
            "in training loop, epoch 3, step 521, the loss is 798182.9375\n",
            "in training loop, epoch 3, step 522, the loss is 776822.0625\n",
            "in training loop, epoch 3, step 523, the loss is 405606.125\n",
            "in training loop, epoch 3, step 524, the loss is 717048.125\n",
            "in training loop, epoch 3, step 525, the loss is 667129.1875\n",
            "in training loop, epoch 3, step 526, the loss is 561742.3125\n",
            "in training loop, epoch 3, step 527, the loss is 816205.875\n",
            "in training loop, epoch 3, step 528, the loss is 716627.0625\n",
            "in training loop, epoch 3, step 529, the loss is 925860.5625\n",
            "in training loop, epoch 3, step 530, the loss is 719961.75\n",
            "in training loop, epoch 3, step 531, the loss is 506057.46875\n",
            "in training loop, epoch 3, step 532, the loss is 842293.0625\n",
            "in training loop, epoch 3, step 533, the loss is 550311.3125\n",
            "in training loop, epoch 3, step 534, the loss is 544011.875\n",
            "in training loop, epoch 3, step 535, the loss is 600798.5\n",
            "in training loop, epoch 3, step 536, the loss is 806284.125\n",
            "in training loop, epoch 3, step 537, the loss is 1299768.875\n",
            "in training loop, epoch 3, step 538, the loss is 883701.375\n",
            "in training loop, epoch 3, step 539, the loss is 1024050.625\n",
            "in training loop, epoch 3, step 540, the loss is 616977.75\n",
            "in training loop, epoch 3, step 541, the loss is 1234511.25\n",
            "in training loop, epoch 3, step 542, the loss is 1332822.25\n",
            "in training loop, epoch 3, step 543, the loss is 1082869.75\n",
            "in training loop, epoch 3, step 544, the loss is 810494.75\n",
            "in training loop, epoch 3, step 545, the loss is 834905.125\n",
            "in training loop, epoch 3, step 546, the loss is 1300355.5\n",
            "in training loop, epoch 3, step 547, the loss is 871149.4375\n",
            "in training loop, epoch 3, step 548, the loss is 1461100.0\n",
            "in training loop, epoch 3, step 549, the loss is 981779.25\n",
            "in training loop, epoch 3, step 550, the loss is 1026994.5625\n",
            "in training loop, epoch 3, step 551, the loss is 1025178.3125\n",
            "in training loop, epoch 3, step 552, the loss is 1093498.875\n",
            "in training loop, epoch 3, step 553, the loss is 1000341.1875\n",
            "in training loop, epoch 3, step 554, the loss is 1208231.375\n",
            "in training loop, epoch 3, step 555, the loss is 654309.9375\n",
            "in training loop, epoch 3, step 556, the loss is 533746.4375\n",
            "in training loop, epoch 3, step 557, the loss is 1232196.0\n",
            "in training loop, epoch 3, step 558, the loss is 922160.5625\n",
            "in training loop, epoch 3, step 559, the loss is 781272.375\n",
            "in training loop, epoch 3, step 560, the loss is 780271.5\n",
            "in training loop, epoch 3, step 561, the loss is 800065.125\n",
            "in training loop, epoch 3, step 562, the loss is 858166.5\n",
            "in training loop, epoch 3, step 563, the loss is 774651.75\n",
            "in training loop, epoch 3, step 564, the loss is 830889.25\n",
            "in training loop, epoch 3, step 565, the loss is 1107410.5\n",
            "in training loop, epoch 3, step 566, the loss is 750499.125\n",
            "in training loop, epoch 3, step 567, the loss is 1070751.25\n",
            "in training loop, epoch 3, step 568, the loss is 691772.5625\n",
            "in training loop, epoch 3, step 569, the loss is 1053486.625\n",
            "in training loop, epoch 3, step 570, the loss is 580919.3125\n",
            "in training loop, epoch 3, step 571, the loss is 584997.9375\n",
            "in training loop, epoch 3, step 572, the loss is 596091.25\n",
            "in training loop, epoch 3, step 573, the loss is 791242.9375\n",
            "in training loop, epoch 3, step 574, the loss is 592955.1875\n",
            "in training loop, epoch 3, step 575, the loss is 586025.0\n",
            "in training loop, epoch 3, step 576, the loss is 742565.625\n",
            "in training loop, epoch 3, step 577, the loss is 482948.28125\n",
            "in training loop, epoch 3, step 578, the loss is 704656.625\n",
            "in training loop, epoch 3, step 579, the loss is 521543.125\n",
            "in training loop, epoch 3, step 580, the loss is 1599881.5\n",
            "in training loop, epoch 3, step 581, the loss is 459997.96875\n",
            "in training loop, epoch 3, step 582, the loss is 998919.3125\n",
            "in training loop, epoch 3, step 583, the loss is 690653.5625\n",
            "in training loop, epoch 3, step 584, the loss is 1786156.375\n",
            "in training loop, epoch 3, step 585, the loss is 569024.5\n",
            "in training loop, epoch 3, step 586, the loss is 751979.75\n",
            "in training loop, epoch 3, step 587, the loss is 1164240.5\n",
            "in training loop, epoch 3, step 588, the loss is 497039.25\n",
            "in training loop, epoch 3, step 589, the loss is 929429.25\n",
            "in training loop, epoch 3, step 590, the loss is 630112.5625\n",
            "in training loop, epoch 3, step 591, the loss is 737018.4375\n",
            "in training loop, epoch 3, step 592, the loss is 1176510.5\n",
            "in training loop, epoch 3, step 593, the loss is 631918.9375\n",
            "in training loop, epoch 3, step 594, the loss is 894929.75\n",
            "in training loop, epoch 3, step 595, the loss is 716006.875\n",
            "in training loop, epoch 3, step 596, the loss is 923027.4375\n",
            "in training loop, epoch 3, step 597, the loss is 923081.5\n",
            "in training loop, epoch 3, step 598, the loss is 452262.65625\n",
            "in training loop, epoch 3, step 599, the loss is 923202.5\n",
            "in training loop, epoch 3, step 600, the loss is 625294.5\n",
            "in training loop, epoch 3, step 601, the loss is 719967.4375\n",
            "in training loop, epoch 3, step 602, the loss is 492985.03125\n",
            "in training loop, epoch 3, step 603, the loss is 741180.1875\n",
            "in training loop, epoch 3, step 604, the loss is 601149.4375\n",
            "in training loop, epoch 3, step 605, the loss is 968624.5\n",
            "in training loop, epoch 3, step 606, the loss is 565325.125\n",
            "in training loop, epoch 3, step 607, the loss is 751441.125\n",
            "in training loop, epoch 3, step 608, the loss is 869707.9375\n",
            "in training loop, epoch 3, step 609, the loss is 566266.5\n",
            "in training loop, epoch 3, step 610, the loss is 478093.4375\n",
            "in training loop, epoch 3, step 611, the loss is 635352.75\n",
            "in training loop, epoch 3, step 612, the loss is 929071.125\n",
            "in training loop, epoch 3, step 613, the loss is 473734.25\n",
            "in training loop, epoch 3, step 614, the loss is 725754.6875\n",
            "in training loop, epoch 3, step 615, the loss is 790400.8125\n",
            "in training loop, epoch 3, step 616, the loss is 1431270.0\n",
            "in training loop, epoch 3, step 617, the loss is 753788.875\n",
            "in training loop, epoch 3, step 618, the loss is 779127.75\n",
            "in training loop, epoch 3, step 619, the loss is 907155.375\n",
            "in training loop, epoch 3, step 620, the loss is 658816.25\n",
            "in training loop, epoch 3, step 621, the loss is 755329.875\n",
            "in training loop, epoch 3, step 622, the loss is 1145286.25\n",
            "in training loop, epoch 3, step 623, the loss is 1079858.75\n",
            "in training loop, epoch 3, step 624, the loss is 634410.3125\n",
            "in training loop, epoch 3, step 625, the loss is 1096619.125\n",
            "in training loop, epoch 3, step 626, the loss is 939967.8125\n",
            "in training loop, epoch 3, step 627, the loss is 541331.1875\n",
            "in training loop, epoch 3, step 628, the loss is 851461.25\n",
            "in training loop, epoch 3, step 629, the loss is 766741.0\n",
            "in training loop, epoch 3, step 630, the loss is 680533.8125\n",
            "in training loop, epoch 3, step 631, the loss is 662449.4375\n",
            "in training loop, epoch 3, step 632, the loss is 462409.21875\n",
            "in training loop, epoch 3, step 633, the loss is 848235.4375\n",
            "in training loop, epoch 3, step 634, the loss is 1063416.25\n",
            "in training loop, epoch 3, step 635, the loss is 900064.125\n",
            "in training loop, epoch 3, step 636, the loss is 763649.8125\n",
            "in training loop, epoch 3, step 637, the loss is 689678.75\n",
            "in training loop, epoch 3, step 638, the loss is 737227.5\n",
            "in training loop, epoch 3, step 639, the loss is 626066.8125\n",
            "in training loop, epoch 3, step 640, the loss is 907218.5\n",
            "in training loop, epoch 3, step 641, the loss is 798251.0625\n",
            "in training loop, epoch 3, step 642, the loss is 576198.8125\n",
            "in training loop, epoch 3, step 643, the loss is 695664.4375\n",
            "in training loop, epoch 3, step 644, the loss is 709194.875\n",
            "in training loop, epoch 3, step 645, the loss is 787333.375\n",
            "in training loop, epoch 3, step 646, the loss is 706339.5\n",
            "in training loop, epoch 3, step 647, the loss is 1397442.125\n",
            "in training loop, epoch 3, step 648, the loss is 2052311.375\n",
            "in training loop, epoch 3, step 649, the loss is 776629.6875\n",
            "in training loop, epoch 3, step 650, the loss is 931629.9375\n",
            "in training loop, epoch 3, step 651, the loss is 1293829.75\n",
            "in training loop, epoch 3, step 652, the loss is 654002.0625\n",
            "in training loop, epoch 3, step 653, the loss is 970135.9375\n",
            "in training loop, epoch 3, step 654, the loss is 377250.84375\n",
            "in training loop, epoch 3, step 655, the loss is 1283076.0\n",
            "in training loop, epoch 3, step 656, the loss is 614349.5\n",
            "in training loop, epoch 3, step 657, the loss is 797755.875\n",
            "in training loop, epoch 3, step 658, the loss is 588137.125\n",
            "in training loop, epoch 3, step 659, the loss is 1072742.0\n",
            "in training loop, epoch 3, step 660, the loss is 569338.4375\n",
            "in training loop, epoch 3, step 661, the loss is 660702.3125\n",
            "in training loop, epoch 3, step 662, the loss is 665447.5\n",
            "in training loop, epoch 3, step 663, the loss is 667057.875\n",
            "in training loop, epoch 3, step 664, the loss is 747201.125\n",
            "in training loop, epoch 3, step 665, the loss is 927767.0\n",
            "in training loop, epoch 3, step 666, the loss is 566246.75\n",
            "in training loop, epoch 3, step 667, the loss is 721262.0625\n",
            "in training loop, epoch 3, step 668, the loss is 623673.75\n",
            "in training loop, epoch 3, step 669, the loss is 560786.625\n",
            "in training loop, epoch 3, step 670, the loss is 635780.875\n",
            "in training loop, epoch 3, step 671, the loss is 756998.3125\n",
            "in training loop, epoch 3, step 672, the loss is 539995.6875\n",
            "in training loop, epoch 3, step 673, the loss is 741551.75\n",
            "in training loop, epoch 3, step 674, the loss is 319536.875\n",
            "in training loop, epoch 3, step 675, the loss is 350042.15625\n",
            "in training loop, epoch 3, step 676, the loss is 842555.5\n",
            "in training loop, epoch 3, step 677, the loss is 751958.75\n",
            "in training loop, epoch 3, step 678, the loss is 971119.25\n",
            "in training loop, epoch 3, step 679, the loss is 516048.0625\n",
            "in training loop, epoch 3, step 680, the loss is 850885.375\n",
            "in training loop, epoch 3, step 681, the loss is 514075.875\n",
            "in training loop, epoch 3, step 682, the loss is 799771.0\n",
            "in training loop, epoch 3, step 683, the loss is 935357.875\n",
            "in training loop, epoch 3, step 684, the loss is 737953.8125\n",
            "in training loop, epoch 3, step 685, the loss is 437632.6875\n",
            "in training loop, epoch 3, step 686, the loss is 531765.75\n",
            "in training loop, epoch 3, step 687, the loss is 888447.75\n",
            "in training loop, epoch 3, step 688, the loss is 694881.375\n",
            "in training loop, epoch 3, step 689, the loss is 852022.8125\n",
            "in training loop, epoch 3, step 690, the loss is 1040358.0625\n",
            "in training loop, epoch 3, step 691, the loss is 498044.34375\n",
            "in training loop, epoch 3, step 692, the loss is 1070483.5\n",
            "in training loop, epoch 3, step 693, the loss is 578337.0625\n",
            "in training loop, epoch 3, step 694, the loss is 486261.09375\n",
            "in training loop, epoch 3, step 695, the loss is 583004.9375\n",
            "in training loop, epoch 3, step 696, the loss is 662167.0625\n",
            "in training loop, epoch 3, step 697, the loss is 1000542.0625\n",
            "in training loop, epoch 3, step 698, the loss is 729954.625\n",
            "in training loop, epoch 3, step 699, the loss is 495503.96875\n",
            "in training loop, epoch 3, step 700, the loss is 392981.59375\n",
            "in training loop, epoch 3, step 701, the loss is 1175765.125\n",
            "in training loop, epoch 3, step 702, the loss is 1035128.1875\n",
            "in training loop, epoch 3, step 703, the loss is 682957.8125\n",
            "in training loop, epoch 3, step 704, the loss is 1139385.75\n",
            "in training loop, epoch 3, step 705, the loss is 513339.5625\n",
            "in training loop, epoch 3, step 706, the loss is 701345.875\n",
            "in training loop, epoch 3, step 707, the loss is 551111.8125\n",
            "in training loop, epoch 3, step 708, the loss is 511111.3125\n",
            "in training loop, epoch 3, step 709, the loss is 927879.375\n",
            "in training loop, epoch 3, step 710, the loss is 497194.28125\n",
            "in training loop, epoch 3, step 711, the loss is 948270.125\n",
            "in training loop, epoch 3, step 712, the loss is 765154.9375\n",
            "in training loop, epoch 3, step 713, the loss is 988197.125\n",
            "in training loop, epoch 3, step 714, the loss is 1369960.375\n",
            "in training loop, epoch 3, step 715, the loss is 1013650.25\n",
            "in training loop, epoch 3, step 716, the loss is 603110.5\n",
            "in training loop, epoch 3, step 717, the loss is 509992.75\n",
            "in training loop, epoch 3, step 718, the loss is 615671.9375\n",
            "in training loop, epoch 3, step 719, the loss is 1406138.0\n",
            "in training loop, epoch 3, step 720, the loss is 668512.5\n",
            "in training loop, epoch 3, step 721, the loss is 739424.375\n",
            "in training loop, epoch 3, step 722, the loss is 640626.25\n",
            "in training loop, epoch 3, step 723, the loss is 724520.75\n",
            "in training loop, epoch 3, step 724, the loss is 454843.6875\n",
            "in training loop, epoch 3, step 725, the loss is 395674.46875\n",
            "in training loop, epoch 3, step 726, the loss is 282589.59375\n",
            "in training loop, epoch 3, step 727, the loss is 893874.0\n",
            "in training loop, epoch 3, step 728, the loss is 720166.1875\n",
            "in training loop, epoch 3, step 729, the loss is 1505144.75\n",
            "in training loop, epoch 3, step 730, the loss is 843095.0625\n",
            "in training loop, epoch 3, step 731, the loss is 720488.3125\n",
            "in training loop, epoch 3, step 732, the loss is 686699.125\n",
            "in training loop, epoch 3, step 733, the loss is 787688.5\n",
            "in training loop, epoch 3, step 734, the loss is 820995.25\n",
            "in training loop, epoch 3, step 735, the loss is 995499.9375\n",
            "in training loop, epoch 3, step 736, the loss is 1431897.75\n",
            "in training loop, epoch 3, step 737, the loss is 853603.1875\n",
            "in training loop, epoch 3, step 738, the loss is 861778.625\n",
            "in training loop, epoch 3, step 739, the loss is 1300803.75\n",
            "in training loop, epoch 3, step 740, the loss is 695182.875\n",
            "in training loop, epoch 3, step 741, the loss is 677617.3125\n",
            "in training loop, epoch 3, step 742, the loss is 964766.125\n",
            "in training loop, epoch 3, step 743, the loss is 1178288.25\n",
            "in training loop, epoch 3, step 744, the loss is 439911.34375\n",
            "in training loop, epoch 3, step 745, the loss is 946876.125\n",
            "in training loop, epoch 3, step 746, the loss is 1145758.125\n",
            "in training loop, epoch 3, step 747, the loss is 637254.375\n",
            "in training loop, epoch 3, step 748, the loss is 815899.0625\n",
            "in training loop, epoch 3, step 749, the loss is 505913.09375\n",
            "in training loop, epoch 3, step 750, the loss is 1394986.0\n",
            "in training loop, epoch 3, step 751, the loss is 630472.5\n",
            "in training loop, epoch 3, step 752, the loss is 595504.375\n",
            "in training loop, epoch 3, step 753, the loss is 929669.75\n",
            "in training loop, epoch 3, step 754, the loss is 1537360.375\n",
            "in training loop, epoch 3, step 755, the loss is 686629.3125\n",
            "in training loop, epoch 3, step 756, the loss is 745612.0625\n",
            "in training loop, epoch 3, step 757, the loss is 853471.6875\n",
            "in training loop, epoch 3, step 758, the loss is 1014201.5625\n",
            "in training loop, epoch 3, step 759, the loss is 885499.375\n",
            "in training loop, epoch 3, step 760, the loss is 1100128.0\n",
            "in training loop, epoch 3, step 761, the loss is 1141656.25\n",
            "in training loop, epoch 3, step 762, the loss is 304190.5\n",
            "in training loop, epoch 3, step 763, the loss is 456784.53125\n",
            "in training loop, epoch 3, step 764, the loss is 1025684.9375\n",
            "in training loop, epoch 3, step 765, the loss is 966601.5625\n",
            "in training loop, epoch 3, step 766, the loss is 1388408.25\n",
            "in training loop, epoch 3, step 767, the loss is 914668.6875\n",
            "in training loop, epoch 3, step 768, the loss is 1524270.875\n",
            "in training loop, epoch 3, step 769, the loss is 772748.5625\n",
            "in training loop, epoch 3, step 770, the loss is 743491.0625\n",
            "in training loop, epoch 3, step 771, the loss is 1314116.75\n",
            "in training loop, epoch 3, step 772, the loss is 715940.25\n",
            "in training loop, epoch 3, step 773, the loss is 507943.96875\n",
            "in training loop, epoch 3, step 774, the loss is 569650.875\n",
            "in training loop, epoch 3, step 775, the loss is 983501.125\n",
            "in training loop, epoch 3, step 776, the loss is 1060101.0\n",
            "in training loop, epoch 3, step 777, the loss is 997220.0\n",
            "in training loop, epoch 3, step 778, the loss is 752627.25\n",
            "in training loop, epoch 3, step 779, the loss is 563038.0\n",
            "in training loop, epoch 3, step 780, the loss is 776662.25\n",
            "in training loop, epoch 3, step 781, the loss is 1082114.125\n",
            "in training loop, epoch 3, step 782, the loss is 1045034.4375\n",
            "in training loop, epoch 3, step 783, the loss is 497430.59375\n",
            "in training loop, epoch 3, step 784, the loss is 1054411.375\n",
            "in training loop, epoch 3, step 785, the loss is 442315.125\n",
            "in training loop, epoch 3, step 786, the loss is 872324.0625\n",
            "in training loop, epoch 3, step 787, the loss is 736327.1875\n",
            "in training loop, epoch 3, step 788, the loss is 1208371.5\n",
            "in training loop, epoch 3, step 789, the loss is 628524.0625\n",
            "in training loop, epoch 3, step 790, the loss is 968644.4375\n",
            "in training loop, epoch 3, step 791, the loss is 1815174.375\n",
            "in training loop, epoch 3, step 792, the loss is 1179730.375\n",
            "in training loop, epoch 3, step 793, the loss is 789742.4375\n",
            "in training loop, epoch 3, step 794, the loss is 661693.0\n",
            "in training loop, epoch 3, step 795, the loss is 1049876.625\n",
            "in training loop, epoch 3, step 796, the loss is 483831.78125\n",
            "in training loop, epoch 3, step 797, the loss is 930769.8125\n",
            "in training loop, epoch 3, step 798, the loss is 820149.9375\n",
            "in training loop, epoch 3, step 799, the loss is 814908.375\n",
            "in training loop, epoch 3, step 800, the loss is 796654.8125\n",
            "in training loop, epoch 3, step 801, the loss is 732053.75\n",
            "in training loop, epoch 3, step 802, the loss is 1071435.5\n",
            "in training loop, epoch 3, step 803, the loss is 972956.25\n",
            "in training loop, epoch 3, step 804, the loss is 794185.5\n",
            "in training loop, epoch 3, step 805, the loss is 575218.4375\n",
            "in training loop, epoch 3, step 806, the loss is 1004569.5\n",
            "in training loop, epoch 3, step 807, the loss is 772796.125\n",
            "in training loop, epoch 3, step 808, the loss is 326767.34375\n",
            "in training loop, epoch 3, step 809, the loss is 1574981.0\n",
            "in training loop, epoch 3, step 810, the loss is 542819.0625\n",
            "in training loop, epoch 3, step 811, the loss is 975118.625\n",
            "in training loop, epoch 3, step 812, the loss is 915504.75\n",
            "in training loop, epoch 3, step 813, the loss is 841464.9375\n",
            "in training loop, epoch 3, step 814, the loss is 832936.9375\n",
            "in training loop, epoch 3, step 815, the loss is 719747.4375\n",
            "in training loop, epoch 3, step 816, the loss is 753379.5625\n",
            "in training loop, epoch 3, step 817, the loss is 536784.875\n",
            "in training loop, epoch 3, step 818, the loss is 1916606.625\n",
            "in training loop, epoch 3, step 819, the loss is 660046.4375\n",
            "in training loop, epoch 3, step 820, the loss is 663477.5625\n",
            "in training loop, epoch 3, step 821, the loss is 389511.96875\n",
            "in training loop, epoch 3, step 822, the loss is 841546.375\n",
            "in training loop, epoch 3, step 823, the loss is 510483.0625\n",
            "in training loop, epoch 3, step 824, the loss is 623504.75\n",
            "in training loop, epoch 3, step 825, the loss is 630883.6875\n",
            "in training loop, epoch 3, step 826, the loss is 869503.6875\n",
            "in training loop, epoch 3, step 827, the loss is 896152.625\n",
            "in training loop, epoch 3, step 828, the loss is 534876.8125\n",
            "in training loop, epoch 3, step 829, the loss is 602279.0625\n",
            "in training loop, epoch 3, step 830, the loss is 883466.875\n",
            "in training loop, epoch 3, step 831, the loss is 612774.1875\n",
            "in training loop, epoch 3, step 832, the loss is 768214.9375\n",
            "in training loop, epoch 3, step 833, the loss is 584889.375\n",
            "in training loop, epoch 3, step 834, the loss is 705522.625\n",
            "in training loop, epoch 3, step 835, the loss is 515836.90625\n",
            "in training loop, epoch 3, step 836, the loss is 1044484.0625\n",
            "in training loop, epoch 3, step 837, the loss is 548109.75\n",
            "in training loop, epoch 3, step 838, the loss is 1126386.25\n",
            "in training loop, epoch 3, step 839, the loss is 531396.5625\n",
            "in training loop, epoch 3, step 840, the loss is 449939.5625\n",
            "in training loop, epoch 3, step 841, the loss is 823258.875\n",
            "in training loop, epoch 3, step 842, the loss is 914835.4375\n",
            "in training loop, epoch 3, step 843, the loss is 1450208.375\n",
            "in training loop, epoch 3, step 844, the loss is 748629.375\n",
            "in training loop, epoch 3, step 845, the loss is 658124.0\n",
            "in training loop, epoch 3, step 846, the loss is 865228.0625\n",
            "in training loop, epoch 3, step 847, the loss is 291522.96875\n",
            "in training loop, epoch 3, step 848, the loss is 376206.90625\n",
            "in training loop, epoch 3, step 849, the loss is 805989.375\n",
            "in training loop, epoch 3, step 850, the loss is 509766.09375\n",
            "in training loop, epoch 3, step 851, the loss is 1149136.5\n",
            "in training loop, epoch 3, step 852, the loss is 458496.75\n",
            "in training loop, epoch 3, step 853, the loss is 860369.125\n",
            "in training loop, epoch 3, step 854, the loss is 779213.875\n",
            "in training loop, epoch 3, step 855, the loss is 903691.375\n",
            "in training loop, epoch 3, step 856, the loss is 453408.65625\n",
            "in training loop, epoch 3, step 857, the loss is 771486.625\n",
            "in training loop, epoch 3, step 858, the loss is 659512.8125\n",
            "in training loop, epoch 3, step 859, the loss is 533366.0625\n",
            "in training loop, epoch 3, step 860, the loss is 490052.40625\n",
            "in training loop, epoch 3, step 861, the loss is 860601.625\n",
            "in training loop, epoch 3, step 862, the loss is 455145.71875\n",
            "in training loop, epoch 3, step 863, the loss is 518946.46875\n",
            "in training loop, epoch 3, step 864, the loss is 636772.75\n",
            "in training loop, epoch 3, step 865, the loss is 969985.0625\n",
            "in training loop, epoch 3, step 866, the loss is 823075.125\n",
            "in training loop, epoch 3, step 867, the loss is 596389.25\n",
            "in training loop, epoch 3, step 868, the loss is 950320.25\n",
            "in training loop, epoch 3, step 869, the loss is 686793.5\n",
            "in training loop, epoch 3, step 870, the loss is 686507.375\n",
            "in training loop, epoch 3, step 871, the loss is 753145.1875\n",
            "in training loop, epoch 3, step 872, the loss is 728386.3125\n",
            "in training loop, epoch 3, step 873, the loss is 1251452.875\n",
            "in training loop, epoch 3, step 874, the loss is 882757.0\n",
            "in training loop, epoch 3, step 875, the loss is 891904.5625\n",
            "in training loop, epoch 3, step 876, the loss is 1007980.5\n",
            "in training loop, epoch 3, step 877, the loss is 777870.125\n",
            "in training loop, epoch 3, step 878, the loss is 1006346.25\n",
            "in training loop, epoch 3, step 879, the loss is 617963.625\n",
            "in training loop, epoch 3, step 880, the loss is 774473.875\n",
            "in training loop, epoch 3, step 881, the loss is 606977.875\n",
            "in training loop, epoch 3, step 882, the loss is 1388460.25\n",
            "in training loop, epoch 3, step 883, the loss is 654161.375\n",
            "in training loop, epoch 3, step 884, the loss is 574840.5625\n",
            "in training loop, epoch 3, step 885, the loss is 1043941.5\n",
            "in training loop, epoch 3, step 886, the loss is 968107.1875\n",
            "in training loop, epoch 3, step 887, the loss is 1001508.125\n",
            "in training loop, epoch 3, step 888, the loss is 510256.28125\n",
            "in training loop, epoch 3, step 889, the loss is 1223663.125\n",
            "in training loop, epoch 3, step 890, the loss is 700299.625\n",
            "in training loop, epoch 3, step 891, the loss is 875556.0625\n",
            "in training loop, epoch 3, step 892, the loss is 819433.0625\n",
            "in training loop, epoch 3, step 893, the loss is 965707.625\n",
            "in training loop, epoch 3, step 894, the loss is 620669.6875\n",
            "in training loop, epoch 3, step 895, the loss is 789361.125\n",
            "in training loop, epoch 3, step 896, the loss is 917074.4375\n",
            "in training loop, epoch 3, step 897, the loss is 659496.125\n",
            "in training loop, epoch 3, step 898, the loss is 866998.875\n",
            "in training loop, epoch 3, step 899, the loss is 961955.0625\n",
            "in training loop, epoch 3, step 900, the loss is 345207.9375\n",
            "in training loop, epoch 3, step 901, the loss is 567843.0625\n",
            "in training loop, epoch 3, step 902, the loss is 859292.625\n",
            "in training loop, epoch 3, step 903, the loss is 485553.4375\n",
            "k-fold 0:: Epoch 3: train loss 775885.7842056139 val loss 814365.823019802\n",
            "in training loop, epoch 4, step 0, the loss is 699585.1875\n",
            "in training loop, epoch 4, step 1, the loss is 560651.625\n",
            "in training loop, epoch 4, step 2, the loss is 469656.03125\n",
            "in training loop, epoch 4, step 3, the loss is 657645.5\n",
            "in training loop, epoch 4, step 4, the loss is 693285.5\n",
            "in training loop, epoch 4, step 5, the loss is 832663.75\n",
            "in training loop, epoch 4, step 6, the loss is 710918.75\n",
            "in training loop, epoch 4, step 7, the loss is 716617.1875\n",
            "in training loop, epoch 4, step 8, the loss is 390784.125\n",
            "in training loop, epoch 4, step 9, the loss is 542751.3125\n",
            "in training loop, epoch 4, step 10, the loss is 868786.875\n",
            "in training loop, epoch 4, step 11, the loss is 553002.75\n",
            "in training loop, epoch 4, step 12, the loss is 576490.3125\n",
            "in training loop, epoch 4, step 13, the loss is 913428.4375\n",
            "in training loop, epoch 4, step 14, the loss is 512161.25\n",
            "in training loop, epoch 4, step 15, the loss is 854506.1875\n",
            "in training loop, epoch 4, step 16, the loss is 700908.875\n",
            "in training loop, epoch 4, step 17, the loss is 713254.375\n",
            "in training loop, epoch 4, step 18, the loss is 708998.8125\n",
            "in training loop, epoch 4, step 19, the loss is 558354.1875\n",
            "in training loop, epoch 4, step 20, the loss is 872157.25\n",
            "in training loop, epoch 4, step 21, the loss is 646744.0\n",
            "in training loop, epoch 4, step 22, the loss is 902850.375\n",
            "in training loop, epoch 4, step 23, the loss is 486643.0\n",
            "in training loop, epoch 4, step 24, the loss is 804826.4375\n",
            "in training loop, epoch 4, step 25, the loss is 351064.875\n",
            "in training loop, epoch 4, step 26, the loss is 697088.125\n",
            "in training loop, epoch 4, step 27, the loss is 528445.5\n",
            "in training loop, epoch 4, step 28, the loss is 567751.0\n",
            "in training loop, epoch 4, step 29, the loss is 669413.375\n",
            "in training loop, epoch 4, step 30, the loss is 514435.34375\n",
            "in training loop, epoch 4, step 31, the loss is 603416.625\n",
            "in training loop, epoch 4, step 32, the loss is 682619.5\n",
            "in training loop, epoch 4, step 33, the loss is 461622.90625\n",
            "in training loop, epoch 4, step 34, the loss is 418408.21875\n",
            "in training loop, epoch 4, step 35, the loss is 660698.1875\n",
            "in training loop, epoch 4, step 36, the loss is 528718.3125\n",
            "in training loop, epoch 4, step 37, the loss is 474056.53125\n",
            "in training loop, epoch 4, step 38, the loss is 1048661.625\n",
            "in training loop, epoch 4, step 39, the loss is 670700.75\n",
            "in training loop, epoch 4, step 40, the loss is 429622.625\n",
            "in training loop, epoch 4, step 41, the loss is 879632.625\n",
            "in training loop, epoch 4, step 42, the loss is 785055.125\n",
            "in training loop, epoch 4, step 43, the loss is 733667.1875\n",
            "in training loop, epoch 4, step 44, the loss is 444194.375\n",
            "in training loop, epoch 4, step 45, the loss is 556780.3125\n",
            "in training loop, epoch 4, step 46, the loss is 596794.125\n",
            "in training loop, epoch 4, step 47, the loss is 767522.0\n",
            "in training loop, epoch 4, step 48, the loss is 812866.125\n",
            "in training loop, epoch 4, step 49, the loss is 772467.0\n",
            "in training loop, epoch 4, step 50, the loss is 799847.375\n",
            "in training loop, epoch 4, step 51, the loss is 854311.75\n",
            "in training loop, epoch 4, step 52, the loss is 550810.375\n",
            "in training loop, epoch 4, step 53, the loss is 875684.8125\n",
            "in training loop, epoch 4, step 54, the loss is 684440.1875\n",
            "in training loop, epoch 4, step 55, the loss is 690355.3125\n",
            "in training loop, epoch 4, step 56, the loss is 561977.8125\n",
            "in training loop, epoch 4, step 57, the loss is 543798.3125\n",
            "in training loop, epoch 4, step 58, the loss is 575917.5625\n",
            "in training loop, epoch 4, step 59, the loss is 414009.09375\n",
            "in training loop, epoch 4, step 60, the loss is 968578.25\n",
            "in training loop, epoch 4, step 61, the loss is 538637.375\n",
            "in training loop, epoch 4, step 62, the loss is 768570.9375\n",
            "in training loop, epoch 4, step 63, the loss is 745010.6875\n",
            "in training loop, epoch 4, step 64, the loss is 867996.5625\n",
            "in training loop, epoch 4, step 65, the loss is 467602.0\n",
            "in training loop, epoch 4, step 66, the loss is 719710.75\n",
            "in training loop, epoch 4, step 67, the loss is 763531.125\n",
            "in training loop, epoch 4, step 68, the loss is 669767.3125\n",
            "in training loop, epoch 4, step 69, the loss is 838896.625\n",
            "in training loop, epoch 4, step 70, the loss is 673933.9375\n",
            "in training loop, epoch 4, step 71, the loss is 568763.125\n",
            "in training loop, epoch 4, step 72, the loss is 547817.9375\n",
            "in training loop, epoch 4, step 73, the loss is 1034600.4375\n",
            "in training loop, epoch 4, step 74, the loss is 675121.0\n",
            "in training loop, epoch 4, step 75, the loss is 756956.6875\n",
            "in training loop, epoch 4, step 76, the loss is 411390.375\n",
            "in training loop, epoch 4, step 77, the loss is 717111.75\n",
            "in training loop, epoch 4, step 78, the loss is 775555.375\n",
            "in training loop, epoch 4, step 79, the loss is 973775.9375\n",
            "in training loop, epoch 4, step 80, the loss is 1088248.875\n",
            "in training loop, epoch 4, step 81, the loss is 690167.0625\n",
            "in training loop, epoch 4, step 82, the loss is 614791.3125\n",
            "in training loop, epoch 4, step 83, the loss is 461782.46875\n",
            "in training loop, epoch 4, step 84, the loss is 263332.03125\n",
            "in training loop, epoch 4, step 85, the loss is 705770.875\n",
            "in training loop, epoch 4, step 86, the loss is 494997.875\n",
            "in training loop, epoch 4, step 87, the loss is 808438.25\n",
            "in training loop, epoch 4, step 88, the loss is 517186.21875\n",
            "in training loop, epoch 4, step 89, the loss is 379662.53125\n",
            "in training loop, epoch 4, step 90, the loss is 999014.0625\n",
            "in training loop, epoch 4, step 91, the loss is 554295.0\n",
            "in training loop, epoch 4, step 92, the loss is 271889.5\n",
            "in training loop, epoch 4, step 93, the loss is 860922.4375\n",
            "in training loop, epoch 4, step 94, the loss is 636625.8125\n",
            "in training loop, epoch 4, step 95, the loss is 633988.4375\n",
            "in training loop, epoch 4, step 96, the loss is 610018.6875\n",
            "in training loop, epoch 4, step 97, the loss is 409578.125\n",
            "in training loop, epoch 4, step 98, the loss is 632150.9375\n",
            "in training loop, epoch 4, step 99, the loss is 1009205.5625\n",
            "in training loop, epoch 4, step 100, the loss is 492086.0625\n",
            "in training loop, epoch 4, step 101, the loss is 1003746.9375\n",
            "in training loop, epoch 4, step 102, the loss is 433960.9375\n",
            "in training loop, epoch 4, step 103, the loss is 628556.1875\n",
            "in training loop, epoch 4, step 104, the loss is 730010.0\n",
            "in training loop, epoch 4, step 105, the loss is 738722.5625\n",
            "in training loop, epoch 4, step 106, the loss is 807028.375\n",
            "in training loop, epoch 4, step 107, the loss is 362280.5625\n",
            "in training loop, epoch 4, step 108, the loss is 676213.25\n",
            "in training loop, epoch 4, step 109, the loss is 405911.9375\n",
            "in training loop, epoch 4, step 110, the loss is 452172.5\n",
            "in training loop, epoch 4, step 111, the loss is 479281.9375\n",
            "in training loop, epoch 4, step 112, the loss is 597103.3125\n",
            "in training loop, epoch 4, step 113, the loss is 804443.125\n",
            "in training loop, epoch 4, step 114, the loss is 676549.1875\n",
            "in training loop, epoch 4, step 115, the loss is 1046850.6875\n",
            "in training loop, epoch 4, step 116, the loss is 809230.375\n",
            "in training loop, epoch 4, step 117, the loss is 599491.3125\n",
            "in training loop, epoch 4, step 118, the loss is 643586.3125\n",
            "in training loop, epoch 4, step 119, the loss is 542551.3125\n",
            "in training loop, epoch 4, step 120, the loss is 550459.1875\n",
            "in training loop, epoch 4, step 121, the loss is 504144.21875\n",
            "in training loop, epoch 4, step 122, the loss is 505061.75\n",
            "in training loop, epoch 4, step 123, the loss is 690019.25\n",
            "in training loop, epoch 4, step 124, the loss is 706693.75\n",
            "in training loop, epoch 4, step 125, the loss is 891840.75\n",
            "in training loop, epoch 4, step 126, the loss is 672461.3125\n",
            "in training loop, epoch 4, step 127, the loss is 533227.875\n",
            "in training loop, epoch 4, step 128, the loss is 380041.9375\n",
            "in training loop, epoch 4, step 129, the loss is 797746.875\n",
            "in training loop, epoch 4, step 130, the loss is 725922.5\n",
            "in training loop, epoch 4, step 131, the loss is 758739.3125\n",
            "in training loop, epoch 4, step 132, the loss is 484983.96875\n",
            "in training loop, epoch 4, step 133, the loss is 675897.6875\n",
            "in training loop, epoch 4, step 134, the loss is 932455.75\n",
            "in training loop, epoch 4, step 135, the loss is 1105437.5\n",
            "in training loop, epoch 4, step 136, the loss is 554191.875\n",
            "in training loop, epoch 4, step 137, the loss is 710529.25\n",
            "in training loop, epoch 4, step 138, the loss is 564685.375\n",
            "in training loop, epoch 4, step 139, the loss is 301135.3125\n",
            "in training loop, epoch 4, step 140, the loss is 1137136.25\n",
            "in training loop, epoch 4, step 141, the loss is 700431.0\n",
            "in training loop, epoch 4, step 142, the loss is 1001928.25\n",
            "in training loop, epoch 4, step 143, the loss is 819546.8125\n",
            "in training loop, epoch 4, step 144, the loss is 502588.375\n",
            "in training loop, epoch 4, step 145, the loss is 627172.5625\n",
            "in training loop, epoch 4, step 146, the loss is 646936.0\n",
            "in training loop, epoch 4, step 147, the loss is 920972.5\n",
            "in training loop, epoch 4, step 148, the loss is 298736.3125\n",
            "in training loop, epoch 4, step 149, the loss is 1033042.6875\n",
            "in training loop, epoch 4, step 150, the loss is 849526.9375\n",
            "in training loop, epoch 4, step 151, the loss is 521098.15625\n",
            "in training loop, epoch 4, step 152, the loss is 719544.75\n",
            "in training loop, epoch 4, step 153, the loss is 775983.25\n",
            "in training loop, epoch 4, step 154, the loss is 533774.1875\n",
            "in training loop, epoch 4, step 155, the loss is 510022.0\n",
            "in training loop, epoch 4, step 156, the loss is 692890.6875\n",
            "in training loop, epoch 4, step 157, the loss is 631036.0\n",
            "in training loop, epoch 4, step 158, the loss is 1536380.625\n",
            "in training loop, epoch 4, step 159, the loss is 528708.9375\n",
            "in training loop, epoch 4, step 160, the loss is 439881.03125\n",
            "in training loop, epoch 4, step 161, the loss is 847844.625\n",
            "in training loop, epoch 4, step 162, the loss is 823960.4375\n",
            "in training loop, epoch 4, step 163, the loss is 674837.6875\n",
            "in training loop, epoch 4, step 164, the loss is 540228.625\n",
            "in training loop, epoch 4, step 165, the loss is 450840.4375\n",
            "in training loop, epoch 4, step 166, the loss is 856477.75\n",
            "in training loop, epoch 4, step 167, the loss is 948956.0625\n",
            "in training loop, epoch 4, step 168, the loss is 638990.1875\n",
            "in training loop, epoch 4, step 169, the loss is 442696.46875\n",
            "in training loop, epoch 4, step 170, the loss is 513445.84375\n",
            "in training loop, epoch 4, step 171, the loss is 854560.0625\n",
            "in training loop, epoch 4, step 172, the loss is 820334.0625\n",
            "in training loop, epoch 4, step 173, the loss is 657128.25\n",
            "in training loop, epoch 4, step 174, the loss is 365459.25\n",
            "in training loop, epoch 4, step 175, the loss is 563339.3125\n",
            "in training loop, epoch 4, step 176, the loss is 782936.625\n",
            "in training loop, epoch 4, step 177, the loss is 792360.125\n",
            "in training loop, epoch 4, step 178, the loss is 511033.375\n",
            "in training loop, epoch 4, step 179, the loss is 420694.09375\n",
            "in training loop, epoch 4, step 180, the loss is 596022.5625\n",
            "in training loop, epoch 4, step 181, the loss is 653096.6875\n",
            "in training loop, epoch 4, step 182, the loss is 908165.5625\n",
            "in training loop, epoch 4, step 183, the loss is 626853.375\n",
            "in training loop, epoch 4, step 184, the loss is 744572.875\n",
            "in training loop, epoch 4, step 185, the loss is 458675.34375\n",
            "in training loop, epoch 4, step 186, the loss is 481403.21875\n",
            "in training loop, epoch 4, step 187, the loss is 621566.9375\n",
            "in training loop, epoch 4, step 188, the loss is 697637.5625\n",
            "in training loop, epoch 4, step 189, the loss is 716794.8125\n",
            "in training loop, epoch 4, step 190, the loss is 872989.875\n",
            "in training loop, epoch 4, step 191, the loss is 801084.8125\n",
            "in training loop, epoch 4, step 192, the loss is 532416.625\n",
            "in training loop, epoch 4, step 193, the loss is 970452.125\n",
            "in training loop, epoch 4, step 194, the loss is 423848.40625\n",
            "in training loop, epoch 4, step 195, the loss is 439158.4375\n",
            "in training loop, epoch 4, step 196, the loss is 587024.5625\n",
            "in training loop, epoch 4, step 197, the loss is 598084.125\n",
            "in training loop, epoch 4, step 198, the loss is 647113.125\n",
            "in training loop, epoch 4, step 199, the loss is 814408.875\n",
            "in training loop, epoch 4, step 200, the loss is 761104.9375\n",
            "in training loop, epoch 4, step 201, the loss is 1705946.5\n",
            "in training loop, epoch 4, step 202, the loss is 722657.5625\n",
            "in training loop, epoch 4, step 203, the loss is 566624.5\n",
            "in training loop, epoch 4, step 204, the loss is 980239.75\n",
            "in training loop, epoch 4, step 205, the loss is 641188.8125\n",
            "in training loop, epoch 4, step 206, the loss is 1720845.75\n",
            "in training loop, epoch 4, step 207, the loss is 765908.6875\n",
            "in training loop, epoch 4, step 208, the loss is 368420.40625\n",
            "in training loop, epoch 4, step 209, the loss is 627329.5625\n",
            "in training loop, epoch 4, step 210, the loss is 1356953.625\n",
            "in training loop, epoch 4, step 211, the loss is 500237.25\n",
            "in training loop, epoch 4, step 212, the loss is 476494.84375\n",
            "in training loop, epoch 4, step 213, the loss is 425971.1875\n",
            "in training loop, epoch 4, step 214, the loss is 891469.0625\n",
            "in training loop, epoch 4, step 215, the loss is 518671.625\n",
            "in training loop, epoch 4, step 216, the loss is 464775.09375\n",
            "in training loop, epoch 4, step 217, the loss is 992534.5\n",
            "in training loop, epoch 4, step 218, the loss is 832366.0\n",
            "in training loop, epoch 4, step 219, the loss is 864530.5625\n",
            "in training loop, epoch 4, step 220, the loss is 913186.625\n",
            "in training loop, epoch 4, step 221, the loss is 468207.9375\n",
            "in training loop, epoch 4, step 222, the loss is 345521.9375\n",
            "in training loop, epoch 4, step 223, the loss is 576325.9375\n",
            "in training loop, epoch 4, step 224, the loss is 1215802.75\n",
            "in training loop, epoch 4, step 225, the loss is 895640.4375\n",
            "in training loop, epoch 4, step 226, the loss is 478900.5625\n",
            "in training loop, epoch 4, step 227, the loss is 778649.0625\n",
            "in training loop, epoch 4, step 228, the loss is 636264.375\n",
            "in training loop, epoch 4, step 229, the loss is 968296.6875\n",
            "in training loop, epoch 4, step 230, the loss is 921448.75\n",
            "in training loop, epoch 4, step 231, the loss is 743043.8125\n",
            "in training loop, epoch 4, step 232, the loss is 792035.375\n",
            "in training loop, epoch 4, step 233, the loss is 602274.75\n",
            "in training loop, epoch 4, step 234, the loss is 835423.125\n",
            "in training loop, epoch 4, step 235, the loss is 876077.25\n",
            "in training loop, epoch 4, step 236, the loss is 687916.375\n",
            "in training loop, epoch 4, step 237, the loss is 677597.0625\n",
            "in training loop, epoch 4, step 238, the loss is 1087105.75\n",
            "in training loop, epoch 4, step 239, the loss is 1072832.375\n",
            "in training loop, epoch 4, step 240, the loss is 967169.3125\n",
            "in training loop, epoch 4, step 241, the loss is 830732.9375\n",
            "in training loop, epoch 4, step 242, the loss is 711220.125\n",
            "in training loop, epoch 4, step 243, the loss is 430489.78125\n",
            "in training loop, epoch 4, step 244, the loss is 882244.375\n",
            "in training loop, epoch 4, step 245, the loss is 535286.125\n",
            "in training loop, epoch 4, step 246, the loss is 847503.6875\n",
            "in training loop, epoch 4, step 247, the loss is 425174.96875\n",
            "in training loop, epoch 4, step 248, the loss is 854472.75\n",
            "in training loop, epoch 4, step 249, the loss is 382542.78125\n",
            "in training loop, epoch 4, step 250, the loss is 888186.25\n",
            "in training loop, epoch 4, step 251, the loss is 802209.0\n",
            "in training loop, epoch 4, step 252, the loss is 308505.125\n",
            "in training loop, epoch 4, step 253, the loss is 464142.875\n",
            "in training loop, epoch 4, step 254, the loss is 539973.25\n",
            "in training loop, epoch 4, step 255, the loss is 409681.9375\n",
            "in training loop, epoch 4, step 256, the loss is 934172.5625\n",
            "in training loop, epoch 4, step 257, the loss is 736918.0625\n",
            "in training loop, epoch 4, step 258, the loss is 524861.8125\n",
            "in training loop, epoch 4, step 259, the loss is 709908.5625\n",
            "in training loop, epoch 4, step 260, the loss is 736846.5\n",
            "in training loop, epoch 4, step 261, the loss is 512011.21875\n",
            "in training loop, epoch 4, step 262, the loss is 582558.1875\n",
            "in training loop, epoch 4, step 263, the loss is 724208.75\n",
            "in training loop, epoch 4, step 264, the loss is 1285488.125\n",
            "in training loop, epoch 4, step 265, the loss is 720166.375\n",
            "in training loop, epoch 4, step 266, the loss is 678435.5625\n",
            "in training loop, epoch 4, step 267, the loss is 455954.46875\n",
            "in training loop, epoch 4, step 268, the loss is 1084964.0\n",
            "in training loop, epoch 4, step 269, the loss is 887726.875\n",
            "in training loop, epoch 4, step 270, the loss is 778060.0\n",
            "in training loop, epoch 4, step 271, the loss is 521967.15625\n",
            "in training loop, epoch 4, step 272, the loss is 511882.03125\n",
            "in training loop, epoch 4, step 273, the loss is 1057660.25\n",
            "in training loop, epoch 4, step 274, the loss is 732982.6875\n",
            "in training loop, epoch 4, step 275, the loss is 988141.5\n",
            "in training loop, epoch 4, step 276, the loss is 810550.9375\n",
            "in training loop, epoch 4, step 277, the loss is 547104.625\n",
            "in training loop, epoch 4, step 278, the loss is 449361.5\n",
            "in training loop, epoch 4, step 279, the loss is 719977.75\n",
            "in training loop, epoch 4, step 280, the loss is 1229812.375\n",
            "in training loop, epoch 4, step 281, the loss is 1046564.5\n",
            "in training loop, epoch 4, step 282, the loss is 890477.0\n",
            "in training loop, epoch 4, step 283, the loss is 882174.5625\n",
            "in training loop, epoch 4, step 284, the loss is 647710.75\n",
            "in training loop, epoch 4, step 285, the loss is 906301.9375\n",
            "in training loop, epoch 4, step 286, the loss is 897195.4375\n",
            "in training loop, epoch 4, step 287, the loss is 1218791.25\n",
            "in training loop, epoch 4, step 288, the loss is 612265.5625\n",
            "in training loop, epoch 4, step 289, the loss is 833234.375\n",
            "in training loop, epoch 4, step 290, the loss is 960291.4375\n",
            "in training loop, epoch 4, step 291, the loss is 648177.1875\n",
            "in training loop, epoch 4, step 292, the loss is 732411.3125\n",
            "in training loop, epoch 4, step 293, the loss is 318632.71875\n",
            "in training loop, epoch 4, step 294, the loss is 779454.875\n",
            "in training loop, epoch 4, step 295, the loss is 780640.3125\n",
            "in training loop, epoch 4, step 296, the loss is 749012.125\n",
            "in training loop, epoch 4, step 297, the loss is 630959.875\n",
            "in training loop, epoch 4, step 298, the loss is 783814.1875\n",
            "in training loop, epoch 4, step 299, the loss is 811593.125\n",
            "in training loop, epoch 4, step 300, the loss is 759116.125\n",
            "in training loop, epoch 4, step 301, the loss is 1073964.625\n",
            "in training loop, epoch 4, step 302, the loss is 585013.375\n",
            "in training loop, epoch 4, step 303, the loss is 453995.09375\n",
            "in training loop, epoch 4, step 304, the loss is 446008.3125\n",
            "in training loop, epoch 4, step 305, the loss is 627131.1875\n",
            "in training loop, epoch 4, step 306, the loss is 540320.9375\n",
            "in training loop, epoch 4, step 307, the loss is 1205705.5\n",
            "in training loop, epoch 4, step 308, the loss is 529559.75\n",
            "in training loop, epoch 4, step 309, the loss is 459250.84375\n",
            "in training loop, epoch 4, step 310, the loss is 514674.84375\n",
            "in training loop, epoch 4, step 311, the loss is 873879.9375\n",
            "in training loop, epoch 4, step 312, the loss is 810064.5\n",
            "in training loop, epoch 4, step 313, the loss is 631315.625\n",
            "in training loop, epoch 4, step 314, the loss is 780550.3125\n",
            "in training loop, epoch 4, step 315, the loss is 861231.1875\n",
            "in training loop, epoch 4, step 316, the loss is 561645.3125\n",
            "in training loop, epoch 4, step 317, the loss is 742848.25\n",
            "in training loop, epoch 4, step 318, the loss is 707183.6875\n",
            "in training loop, epoch 4, step 319, the loss is 410172.28125\n",
            "in training loop, epoch 4, step 320, the loss is 534434.0\n",
            "in training loop, epoch 4, step 321, the loss is 627303.5\n",
            "in training loop, epoch 4, step 322, the loss is 658864.25\n",
            "in training loop, epoch 4, step 323, the loss is 706986.25\n",
            "in training loop, epoch 4, step 324, the loss is 729946.4375\n",
            "in training loop, epoch 4, step 325, the loss is 761427.25\n",
            "in training loop, epoch 4, step 326, the loss is 549576.5\n",
            "in training loop, epoch 4, step 327, the loss is 653539.0\n",
            "in training loop, epoch 4, step 328, the loss is 842640.6875\n",
            "in training loop, epoch 4, step 329, the loss is 605692.1875\n",
            "in training loop, epoch 4, step 330, the loss is 504284.6875\n",
            "in training loop, epoch 4, step 331, the loss is 662036.8125\n",
            "in training loop, epoch 4, step 332, the loss is 754748.0\n",
            "in training loop, epoch 4, step 333, the loss is 665747.375\n",
            "in training loop, epoch 4, step 334, the loss is 1346076.25\n",
            "in training loop, epoch 4, step 335, the loss is 630599.5\n",
            "in training loop, epoch 4, step 336, the loss is 501636.96875\n",
            "in training loop, epoch 4, step 337, the loss is 470721.5\n",
            "in training loop, epoch 4, step 338, the loss is 750696.3125\n",
            "in training loop, epoch 4, step 339, the loss is 618603.5\n",
            "in training loop, epoch 4, step 340, the loss is 1096127.75\n",
            "in training loop, epoch 4, step 341, the loss is 966026.0\n",
            "in training loop, epoch 4, step 342, the loss is 671313.9375\n",
            "in training loop, epoch 4, step 343, the loss is 979983.375\n",
            "in training loop, epoch 4, step 344, the loss is 790009.9375\n",
            "in training loop, epoch 4, step 345, the loss is 878332.75\n",
            "in training loop, epoch 4, step 346, the loss is 437615.125\n",
            "in training loop, epoch 4, step 347, the loss is 976264.875\n",
            "in training loop, epoch 4, step 348, the loss is 807623.5625\n",
            "in training loop, epoch 4, step 349, the loss is 534735.125\n",
            "in training loop, epoch 4, step 350, the loss is 640603.375\n",
            "in training loop, epoch 4, step 351, the loss is 738554.625\n",
            "in training loop, epoch 4, step 352, the loss is 654051.75\n",
            "in training loop, epoch 4, step 353, the loss is 640792.5\n",
            "in training loop, epoch 4, step 354, the loss is 654539.3125\n",
            "in training loop, epoch 4, step 355, the loss is 830164.0\n",
            "in training loop, epoch 4, step 356, the loss is 455938.5\n",
            "in training loop, epoch 4, step 357, the loss is 893220.25\n",
            "in training loop, epoch 4, step 358, the loss is 743407.8125\n",
            "in training loop, epoch 4, step 359, the loss is 321073.3125\n",
            "in training loop, epoch 4, step 360, the loss is 971997.875\n",
            "in training loop, epoch 4, step 361, the loss is 369775.0\n",
            "in training loop, epoch 4, step 362, the loss is 1022744.0\n",
            "in training loop, epoch 4, step 363, the loss is 692138.875\n",
            "in training loop, epoch 4, step 364, the loss is 907852.9375\n",
            "in training loop, epoch 4, step 365, the loss is 430073.21875\n",
            "in training loop, epoch 4, step 366, the loss is 960145.375\n",
            "in training loop, epoch 4, step 367, the loss is 503982.75\n",
            "in training loop, epoch 4, step 368, the loss is 510248.5625\n",
            "in training loop, epoch 4, step 369, the loss is 535718.0\n",
            "in training loop, epoch 4, step 370, the loss is 850560.25\n",
            "in training loop, epoch 4, step 371, the loss is 863275.375\n",
            "in training loop, epoch 4, step 372, the loss is 312104.59375\n",
            "in training loop, epoch 4, step 373, the loss is 740718.375\n",
            "in training loop, epoch 4, step 374, the loss is 483939.625\n",
            "in training loop, epoch 4, step 375, the loss is 475790.75\n",
            "in training loop, epoch 4, step 376, the loss is 715596.75\n",
            "in training loop, epoch 4, step 377, the loss is 640140.25\n",
            "in training loop, epoch 4, step 378, the loss is 830601.9375\n",
            "in training loop, epoch 4, step 379, the loss is 639687.9375\n",
            "in training loop, epoch 4, step 380, the loss is 657176.3125\n",
            "in training loop, epoch 4, step 381, the loss is 793532.3125\n",
            "in training loop, epoch 4, step 382, the loss is 960032.3125\n",
            "in training loop, epoch 4, step 383, the loss is 656797.0\n",
            "in training loop, epoch 4, step 384, the loss is 731632.25\n",
            "in training loop, epoch 4, step 385, the loss is 922809.4375\n",
            "in training loop, epoch 4, step 386, the loss is 774611.375\n",
            "in training loop, epoch 4, step 387, the loss is 1244664.375\n",
            "in training loop, epoch 4, step 388, the loss is 459230.0625\n",
            "in training loop, epoch 4, step 389, the loss is 564390.875\n",
            "in training loop, epoch 4, step 390, the loss is 461786.34375\n",
            "in training loop, epoch 4, step 391, the loss is 1079660.0\n",
            "in training loop, epoch 4, step 392, the loss is 1239489.0\n",
            "in training loop, epoch 4, step 393, the loss is 572601.1875\n",
            "in training loop, epoch 4, step 394, the loss is 672375.6875\n",
            "in training loop, epoch 4, step 395, the loss is 600432.5\n",
            "in training loop, epoch 4, step 396, the loss is 404742.375\n",
            "in training loop, epoch 4, step 397, the loss is 597786.375\n",
            "in training loop, epoch 4, step 398, the loss is 504008.0625\n",
            "in training loop, epoch 4, step 399, the loss is 1025459.8125\n",
            "in training loop, epoch 4, step 400, the loss is 397653.25\n",
            "in training loop, epoch 4, step 401, the loss is 898375.5625\n",
            "in training loop, epoch 4, step 402, the loss is 1070235.125\n",
            "in training loop, epoch 4, step 403, the loss is 609034.125\n",
            "in training loop, epoch 4, step 404, the loss is 512436.90625\n",
            "in training loop, epoch 4, step 405, the loss is 450894.75\n",
            "in training loop, epoch 4, step 406, the loss is 621373.4375\n",
            "in training loop, epoch 4, step 407, the loss is 714104.375\n",
            "in training loop, epoch 4, step 408, the loss is 622018.3125\n",
            "in training loop, epoch 4, step 409, the loss is 705029.875\n",
            "in training loop, epoch 4, step 410, the loss is 718243.5\n",
            "in training loop, epoch 4, step 411, the loss is 435527.71875\n",
            "in training loop, epoch 4, step 412, the loss is 607238.5625\n",
            "in training loop, epoch 4, step 413, the loss is 802314.75\n",
            "in training loop, epoch 4, step 414, the loss is 496340.0625\n",
            "in training loop, epoch 4, step 415, the loss is 1045700.125\n",
            "in training loop, epoch 4, step 416, the loss is 796043.8125\n",
            "in training loop, epoch 4, step 417, the loss is 777004.75\n",
            "in training loop, epoch 4, step 418, the loss is 711065.875\n",
            "in training loop, epoch 4, step 419, the loss is 742500.3125\n",
            "in training loop, epoch 4, step 420, the loss is 1080296.625\n",
            "in training loop, epoch 4, step 421, the loss is 609355.625\n",
            "in training loop, epoch 4, step 422, the loss is 639885.5625\n",
            "in training loop, epoch 4, step 423, the loss is 893656.9375\n",
            "in training loop, epoch 4, step 424, the loss is 396568.625\n",
            "in training loop, epoch 4, step 425, the loss is 631484.5625\n",
            "in training loop, epoch 4, step 426, the loss is 884827.0\n",
            "in training loop, epoch 4, step 427, the loss is 1017028.75\n",
            "in training loop, epoch 4, step 428, the loss is 996286.0\n",
            "in training loop, epoch 4, step 429, the loss is 597669.5\n",
            "in training loop, epoch 4, step 430, the loss is 551629.75\n",
            "in training loop, epoch 4, step 431, the loss is 494342.5625\n",
            "in training loop, epoch 4, step 432, the loss is 272691.84375\n",
            "in training loop, epoch 4, step 433, the loss is 727285.3125\n",
            "in training loop, epoch 4, step 434, the loss is 517001.75\n",
            "in training loop, epoch 4, step 435, the loss is 510642.625\n",
            "in training loop, epoch 4, step 436, the loss is 562650.5625\n",
            "in training loop, epoch 4, step 437, the loss is 502313.28125\n",
            "in training loop, epoch 4, step 438, the loss is 782648.75\n",
            "in training loop, epoch 4, step 439, the loss is 484914.3125\n",
            "in training loop, epoch 4, step 440, the loss is 769310.9375\n",
            "in training loop, epoch 4, step 441, the loss is 620719.75\n",
            "in training loop, epoch 4, step 442, the loss is 319841.40625\n",
            "in training loop, epoch 4, step 443, the loss is 534828.6875\n",
            "in training loop, epoch 4, step 444, the loss is 748311.0\n",
            "in training loop, epoch 4, step 445, the loss is 666990.25\n",
            "in training loop, epoch 4, step 446, the loss is 733443.125\n",
            "in training loop, epoch 4, step 447, the loss is 703582.0625\n",
            "in training loop, epoch 4, step 448, the loss is 740644.6875\n",
            "in training loop, epoch 4, step 449, the loss is 808884.75\n",
            "in training loop, epoch 4, step 450, the loss is 758102.3125\n",
            "in training loop, epoch 4, step 451, the loss is 695369.875\n",
            "in training loop, epoch 4, step 452, the loss is 730856.0625\n",
            "in training loop, epoch 4, step 453, the loss is 959200.625\n",
            "in training loop, epoch 4, step 454, the loss is 748084.875\n",
            "in training loop, epoch 4, step 455, the loss is 704770.875\n",
            "in training loop, epoch 4, step 456, the loss is 456692.75\n",
            "in training loop, epoch 4, step 457, the loss is 547597.375\n",
            "in training loop, epoch 4, step 458, the loss is 719467.1875\n",
            "in training loop, epoch 4, step 459, the loss is 850096.0\n",
            "in training loop, epoch 4, step 460, the loss is 604113.6875\n",
            "in training loop, epoch 4, step 461, the loss is 1102704.125\n",
            "in training loop, epoch 4, step 462, the loss is 624943.4375\n",
            "in training loop, epoch 4, step 463, the loss is 801624.0625\n",
            "in training loop, epoch 4, step 464, the loss is 509050.6875\n",
            "in training loop, epoch 4, step 465, the loss is 338664.90625\n",
            "in training loop, epoch 4, step 466, the loss is 788716.5\n",
            "in training loop, epoch 4, step 467, the loss is 626477.875\n",
            "in training loop, epoch 4, step 468, the loss is 494258.1875\n",
            "in training loop, epoch 4, step 469, the loss is 657591.5625\n",
            "in training loop, epoch 4, step 470, the loss is 715166.875\n",
            "in training loop, epoch 4, step 471, the loss is 615319.9375\n",
            "in training loop, epoch 4, step 472, the loss is 482582.3125\n",
            "in training loop, epoch 4, step 473, the loss is 681608.9375\n",
            "in training loop, epoch 4, step 474, the loss is 1001971.5\n",
            "in training loop, epoch 4, step 475, the loss is 594925.9375\n",
            "in training loop, epoch 4, step 476, the loss is 655887.625\n",
            "in training loop, epoch 4, step 477, the loss is 634795.25\n",
            "in training loop, epoch 4, step 478, the loss is 663441.875\n",
            "in training loop, epoch 4, step 479, the loss is 616151.875\n",
            "in training loop, epoch 4, step 480, the loss is 647314.875\n",
            "in training loop, epoch 4, step 481, the loss is 409853.25\n",
            "in training loop, epoch 4, step 482, the loss is 654685.75\n",
            "in training loop, epoch 4, step 483, the loss is 760004.25\n",
            "in training loop, epoch 4, step 484, the loss is 500290.375\n",
            "in training loop, epoch 4, step 485, the loss is 1084554.125\n",
            "in training loop, epoch 4, step 486, the loss is 751956.25\n",
            "in training loop, epoch 4, step 487, the loss is 651380.4375\n",
            "in training loop, epoch 4, step 488, the loss is 845252.875\n",
            "in training loop, epoch 4, step 489, the loss is 737487.5\n",
            "in training loop, epoch 4, step 490, the loss is 696612.75\n",
            "in training loop, epoch 4, step 491, the loss is 693951.1875\n",
            "in training loop, epoch 4, step 492, the loss is 829106.5\n",
            "in training loop, epoch 4, step 493, the loss is 761753.5625\n",
            "in training loop, epoch 4, step 494, the loss is 641834.375\n",
            "in training loop, epoch 4, step 495, the loss is 565950.6875\n",
            "in training loop, epoch 4, step 496, the loss is 664773.25\n",
            "in training loop, epoch 4, step 497, the loss is 894423.125\n",
            "in training loop, epoch 4, step 498, the loss is 633222.75\n",
            "in training loop, epoch 4, step 499, the loss is 951436.625\n",
            "in training loop, epoch 4, step 500, the loss is 675250.25\n",
            "in training loop, epoch 4, step 501, the loss is 750045.6875\n",
            "in training loop, epoch 4, step 502, the loss is 637380.375\n",
            "in training loop, epoch 4, step 503, the loss is 878218.375\n",
            "in training loop, epoch 4, step 504, the loss is 895266.1875\n",
            "in training loop, epoch 4, step 505, the loss is 691561.8125\n",
            "in training loop, epoch 4, step 506, the loss is 637504.0625\n",
            "in training loop, epoch 4, step 507, the loss is 725907.75\n",
            "in training loop, epoch 4, step 508, the loss is 615463.25\n",
            "in training loop, epoch 4, step 509, the loss is 648075.25\n",
            "in training loop, epoch 4, step 510, the loss is 577687.625\n",
            "in training loop, epoch 4, step 511, the loss is 496827.875\n",
            "in training loop, epoch 4, step 512, the loss is 613207.8125\n",
            "in training loop, epoch 4, step 513, the loss is 469284.9375\n",
            "in training loop, epoch 4, step 514, the loss is 371808.03125\n",
            "in training loop, epoch 4, step 515, the loss is 908524.6875\n",
            "in training loop, epoch 4, step 516, the loss is 463433.25\n",
            "in training loop, epoch 4, step 517, the loss is 1027139.875\n",
            "in training loop, epoch 4, step 518, the loss is 1086868.25\n",
            "in training loop, epoch 4, step 519, the loss is 827811.125\n",
            "in training loop, epoch 4, step 520, the loss is 502291.875\n",
            "in training loop, epoch 4, step 521, the loss is 486791.84375\n",
            "in training loop, epoch 4, step 522, the loss is 171338.953125\n",
            "in training loop, epoch 4, step 523, the loss is 777272.1875\n",
            "in training loop, epoch 4, step 524, the loss is 892995.8125\n",
            "in training loop, epoch 4, step 525, the loss is 644373.4375\n",
            "in training loop, epoch 4, step 526, the loss is 640024.0625\n",
            "in training loop, epoch 4, step 527, the loss is 591999.375\n",
            "in training loop, epoch 4, step 528, the loss is 639735.1875\n",
            "in training loop, epoch 4, step 529, the loss is 1210815.125\n",
            "in training loop, epoch 4, step 530, the loss is 474702.75\n",
            "in training loop, epoch 4, step 531, the loss is 802786.125\n",
            "in training loop, epoch 4, step 532, the loss is 951087.375\n",
            "in training loop, epoch 4, step 533, the loss is 825977.5\n",
            "in training loop, epoch 4, step 534, the loss is 611427.4375\n",
            "in training loop, epoch 4, step 535, the loss is 1237715.5\n",
            "in training loop, epoch 4, step 536, the loss is 726232.125\n",
            "in training loop, epoch 4, step 537, the loss is 1144648.0\n",
            "in training loop, epoch 4, step 538, the loss is 647180.9375\n",
            "in training loop, epoch 4, step 539, the loss is 727755.875\n",
            "in training loop, epoch 4, step 540, the loss is 881523.875\n",
            "in training loop, epoch 4, step 541, the loss is 1827382.25\n",
            "in training loop, epoch 4, step 542, the loss is 840308.8125\n",
            "in training loop, epoch 4, step 543, the loss is 553304.5\n",
            "in training loop, epoch 4, step 544, the loss is 1147348.5\n",
            "in training loop, epoch 4, step 545, the loss is 1160175.0\n",
            "in training loop, epoch 4, step 546, the loss is 853824.0\n",
            "in training loop, epoch 4, step 547, the loss is 1287792.125\n",
            "in training loop, epoch 4, step 548, the loss is 1152743.625\n",
            "in training loop, epoch 4, step 549, the loss is 2470697.5\n",
            "in training loop, epoch 4, step 550, the loss is 1341429.625\n",
            "in training loop, epoch 4, step 551, the loss is 655691.125\n",
            "in training loop, epoch 4, step 552, the loss is 548158.3125\n",
            "in training loop, epoch 4, step 553, the loss is 728401.4375\n",
            "in training loop, epoch 4, step 554, the loss is 767297.75\n",
            "in training loop, epoch 4, step 555, the loss is 834098.5\n",
            "in training loop, epoch 4, step 556, the loss is 1309816.25\n",
            "in training loop, epoch 4, step 557, the loss is 774335.875\n",
            "in training loop, epoch 4, step 558, the loss is 929813.8125\n",
            "in training loop, epoch 4, step 559, the loss is 1189152.125\n",
            "in training loop, epoch 4, step 560, the loss is 525536.625\n",
            "in training loop, epoch 4, step 561, the loss is 723125.5\n",
            "in training loop, epoch 4, step 562, the loss is 538150.875\n",
            "in training loop, epoch 4, step 563, the loss is 716088.5\n",
            "in training loop, epoch 4, step 564, the loss is 634412.5\n",
            "in training loop, epoch 4, step 565, the loss is 750101.375\n",
            "in training loop, epoch 4, step 566, the loss is 602416.0625\n",
            "in training loop, epoch 4, step 567, the loss is 637304.25\n",
            "in training loop, epoch 4, step 568, the loss is 1085816.5\n",
            "in training loop, epoch 4, step 569, the loss is 907092.25\n",
            "in training loop, epoch 4, step 570, the loss is 661164.0\n",
            "in training loop, epoch 4, step 571, the loss is 1212539.0\n",
            "in training loop, epoch 4, step 572, the loss is 461336.28125\n",
            "in training loop, epoch 4, step 573, the loss is 1070919.0\n",
            "in training loop, epoch 4, step 574, the loss is 572220.0625\n",
            "in training loop, epoch 4, step 575, the loss is 424894.46875\n",
            "in training loop, epoch 4, step 576, the loss is 1582384.25\n",
            "in training loop, epoch 4, step 577, the loss is 658670.0625\n",
            "in training loop, epoch 4, step 578, the loss is 768353.125\n",
            "in training loop, epoch 4, step 579, the loss is 1010615.5\n",
            "in training loop, epoch 4, step 580, the loss is 891010.4375\n",
            "in training loop, epoch 4, step 581, the loss is 915187.875\n",
            "in training loop, epoch 4, step 582, the loss is 551344.875\n",
            "in training loop, epoch 4, step 583, the loss is 1132534.75\n",
            "in training loop, epoch 4, step 584, the loss is 930766.375\n",
            "in training loop, epoch 4, step 585, the loss is 786221.1875\n",
            "in training loop, epoch 4, step 586, the loss is 712596.5\n",
            "in training loop, epoch 4, step 587, the loss is 639104.125\n",
            "in training loop, epoch 4, step 588, the loss is 833010.875\n",
            "in training loop, epoch 4, step 589, the loss is 736123.0625\n",
            "in training loop, epoch 4, step 590, the loss is 660357.3125\n",
            "in training loop, epoch 4, step 591, the loss is 711530.625\n",
            "in training loop, epoch 4, step 592, the loss is 535910.5625\n",
            "in training loop, epoch 4, step 593, the loss is 1160384.25\n",
            "in training loop, epoch 4, step 594, the loss is 685420.5\n",
            "in training loop, epoch 4, step 595, the loss is 455705.0\n",
            "in training loop, epoch 4, step 596, the loss is 512319.5\n",
            "in training loop, epoch 4, step 597, the loss is 544067.75\n",
            "in training loop, epoch 4, step 598, the loss is 544671.8125\n",
            "in training loop, epoch 4, step 599, the loss is 1142419.0\n",
            "in training loop, epoch 4, step 600, the loss is 719455.3125\n",
            "in training loop, epoch 4, step 601, the loss is 458244.25\n",
            "in training loop, epoch 4, step 602, the loss is 1755766.375\n",
            "in training loop, epoch 4, step 603, the loss is 732178.0\n",
            "in training loop, epoch 4, step 604, the loss is 1231603.25\n",
            "in training loop, epoch 4, step 605, the loss is 1131269.75\n",
            "in training loop, epoch 4, step 606, the loss is 371267.5625\n",
            "in training loop, epoch 4, step 607, the loss is 626349.0625\n",
            "in training loop, epoch 4, step 608, the loss is 482799.03125\n",
            "in training loop, epoch 4, step 609, the loss is 1014952.75\n",
            "in training loop, epoch 4, step 610, the loss is 525751.5\n",
            "in training loop, epoch 4, step 611, the loss is 867940.75\n",
            "in training loop, epoch 4, step 612, the loss is 452271.875\n",
            "in training loop, epoch 4, step 613, the loss is 600575.9375\n",
            "in training loop, epoch 4, step 614, the loss is 846127.8125\n",
            "in training loop, epoch 4, step 615, the loss is 558659.5\n",
            "in training loop, epoch 4, step 616, the loss is 711513.25\n",
            "in training loop, epoch 4, step 617, the loss is 823773.75\n",
            "in training loop, epoch 4, step 618, the loss is 665918.0625\n",
            "in training loop, epoch 4, step 619, the loss is 582916.4375\n",
            "in training loop, epoch 4, step 620, the loss is 573842.125\n",
            "in training loop, epoch 4, step 621, the loss is 717940.1875\n",
            "in training loop, epoch 4, step 622, the loss is 236067.828125\n",
            "in training loop, epoch 4, step 623, the loss is 377193.25\n",
            "in training loop, epoch 4, step 624, the loss is 556093.5625\n",
            "in training loop, epoch 4, step 625, the loss is 1112371.125\n",
            "in training loop, epoch 4, step 626, the loss is 1256952.375\n",
            "in training loop, epoch 4, step 627, the loss is 656762.25\n",
            "in training loop, epoch 4, step 628, the loss is 687775.5625\n",
            "in training loop, epoch 4, step 629, the loss is 305298.4375\n",
            "in training loop, epoch 4, step 630, the loss is 769168.3125\n",
            "in training loop, epoch 4, step 631, the loss is 706350.375\n",
            "in training loop, epoch 4, step 632, the loss is 573136.625\n",
            "in training loop, epoch 4, step 633, the loss is 884311.1875\n",
            "in training loop, epoch 4, step 634, the loss is 642263.1875\n",
            "in training loop, epoch 4, step 635, the loss is 509166.4375\n",
            "in training loop, epoch 4, step 636, the loss is 657662.75\n",
            "in training loop, epoch 4, step 637, the loss is 665666.125\n",
            "in training loop, epoch 4, step 638, the loss is 572325.4375\n",
            "in training loop, epoch 4, step 639, the loss is 490434.625\n",
            "in training loop, epoch 4, step 640, the loss is 738511.8125\n",
            "in training loop, epoch 4, step 641, the loss is 825843.625\n",
            "in training loop, epoch 4, step 642, the loss is 676442.5\n",
            "in training loop, epoch 4, step 643, the loss is 554200.375\n",
            "in training loop, epoch 4, step 644, the loss is 512445.625\n",
            "in training loop, epoch 4, step 645, the loss is 1052863.0\n",
            "in training loop, epoch 4, step 646, the loss is 642601.625\n",
            "in training loop, epoch 4, step 647, the loss is 322994.28125\n",
            "in training loop, epoch 4, step 648, the loss is 362909.34375\n",
            "in training loop, epoch 4, step 649, the loss is 664342.125\n",
            "in training loop, epoch 4, step 650, the loss is 865764.625\n",
            "in training loop, epoch 4, step 651, the loss is 943902.375\n",
            "in training loop, epoch 4, step 652, the loss is 561267.0\n",
            "in training loop, epoch 4, step 653, the loss is 699993.25\n",
            "in training loop, epoch 4, step 654, the loss is 568170.125\n",
            "in training loop, epoch 4, step 655, the loss is 461030.21875\n",
            "in training loop, epoch 4, step 656, the loss is 644306.1875\n",
            "in training loop, epoch 4, step 657, the loss is 761777.0\n",
            "in training loop, epoch 4, step 658, the loss is 757699.75\n",
            "in training loop, epoch 4, step 659, the loss is 828066.125\n",
            "in training loop, epoch 4, step 660, the loss is 798627.375\n",
            "in training loop, epoch 4, step 661, the loss is 659787.6875\n",
            "in training loop, epoch 4, step 662, the loss is 984175.5\n",
            "in training loop, epoch 4, step 663, the loss is 1242665.75\n",
            "in training loop, epoch 4, step 664, the loss is 904898.0625\n",
            "in training loop, epoch 4, step 665, the loss is 969593.625\n",
            "in training loop, epoch 4, step 666, the loss is 1571329.375\n",
            "in training loop, epoch 4, step 667, the loss is 663899.125\n",
            "in training loop, epoch 4, step 668, the loss is 349830.5625\n",
            "in training loop, epoch 4, step 669, the loss is 1208309.5\n",
            "in training loop, epoch 4, step 670, the loss is 788115.625\n",
            "in training loop, epoch 4, step 671, the loss is 676187.125\n",
            "in training loop, epoch 4, step 672, the loss is 848842.8125\n",
            "in training loop, epoch 4, step 673, the loss is 505708.875\n",
            "in training loop, epoch 4, step 674, the loss is 867365.25\n",
            "in training loop, epoch 4, step 675, the loss is 761981.625\n",
            "in training loop, epoch 4, step 676, the loss is 607746.9375\n",
            "in training loop, epoch 4, step 677, the loss is 585110.25\n",
            "in training loop, epoch 4, step 678, the loss is 753092.0\n",
            "in training loop, epoch 4, step 679, the loss is 1068446.25\n",
            "in training loop, epoch 4, step 680, the loss is 472522.46875\n",
            "in training loop, epoch 4, step 681, the loss is 952834.0\n",
            "in training loop, epoch 4, step 682, the loss is 438459.375\n",
            "in training loop, epoch 4, step 683, the loss is 505464.3125\n",
            "in training loop, epoch 4, step 684, the loss is 962758.125\n",
            "in training loop, epoch 4, step 685, the loss is 644619.625\n",
            "in training loop, epoch 4, step 686, the loss is 711828.125\n",
            "in training loop, epoch 4, step 687, the loss is 588548.25\n",
            "in training loop, epoch 4, step 688, the loss is 330558.9375\n",
            "in training loop, epoch 4, step 689, the loss is 735574.375\n",
            "in training loop, epoch 4, step 690, the loss is 647089.0\n",
            "in training loop, epoch 4, step 691, the loss is 761456.0\n",
            "in training loop, epoch 4, step 692, the loss is 686573.625\n",
            "in training loop, epoch 4, step 693, the loss is 527078.9375\n",
            "in training loop, epoch 4, step 694, the loss is 481890.4375\n",
            "in training loop, epoch 4, step 695, the loss is 792998.25\n",
            "in training loop, epoch 4, step 696, the loss is 486749.1875\n",
            "in training loop, epoch 4, step 697, the loss is 764047.8125\n",
            "in training loop, epoch 4, step 698, the loss is 841351.625\n",
            "in training loop, epoch 4, step 699, the loss is 511473.375\n",
            "in training loop, epoch 4, step 700, the loss is 425489.375\n",
            "in training loop, epoch 4, step 701, the loss is 776159.1875\n",
            "in training loop, epoch 4, step 702, the loss is 508296.34375\n",
            "in training loop, epoch 4, step 703, the loss is 592457.625\n",
            "in training loop, epoch 4, step 704, the loss is 1134635.375\n",
            "in training loop, epoch 4, step 705, the loss is 831965.9375\n",
            "in training loop, epoch 4, step 706, the loss is 699775.625\n",
            "in training loop, epoch 4, step 707, the loss is 453800.65625\n",
            "in training loop, epoch 4, step 708, the loss is 564245.3125\n",
            "in training loop, epoch 4, step 709, the loss is 508085.84375\n",
            "in training loop, epoch 4, step 710, the loss is 599746.8125\n",
            "in training loop, epoch 4, step 711, the loss is 811098.375\n",
            "in training loop, epoch 4, step 712, the loss is 420062.3125\n",
            "in training loop, epoch 4, step 713, the loss is 809117.0\n",
            "in training loop, epoch 4, step 714, the loss is 457157.09375\n",
            "in training loop, epoch 4, step 715, the loss is 663917.1875\n",
            "in training loop, epoch 4, step 716, the loss is 371575.21875\n",
            "in training loop, epoch 4, step 717, the loss is 1012985.6875\n",
            "in training loop, epoch 4, step 718, the loss is 524099.09375\n",
            "in training loop, epoch 4, step 719, the loss is 368516.9375\n",
            "in training loop, epoch 4, step 720, the loss is 260179.296875\n",
            "in training loop, epoch 4, step 721, the loss is 638589.625\n",
            "in training loop, epoch 4, step 722, the loss is 582297.3125\n",
            "in training loop, epoch 4, step 723, the loss is 897663.75\n",
            "in training loop, epoch 4, step 724, the loss is 514697.0625\n",
            "in training loop, epoch 4, step 725, the loss is 1032724.5\n",
            "in training loop, epoch 4, step 726, the loss is 537076.25\n",
            "in training loop, epoch 4, step 727, the loss is 608450.3125\n",
            "in training loop, epoch 4, step 728, the loss is 455886.875\n",
            "in training loop, epoch 4, step 729, the loss is 653566.875\n",
            "in training loop, epoch 4, step 730, the loss is 817653.375\n",
            "in training loop, epoch 4, step 731, the loss is 573709.5\n",
            "in training loop, epoch 4, step 732, the loss is 630589.625\n",
            "in training loop, epoch 4, step 733, the loss is 497885.5625\n",
            "in training loop, epoch 4, step 734, the loss is 513560.375\n",
            "in training loop, epoch 4, step 735, the loss is 757892.875\n",
            "in training loop, epoch 4, step 736, the loss is 636952.4375\n",
            "in training loop, epoch 4, step 737, the loss is 671938.875\n",
            "in training loop, epoch 4, step 738, the loss is 613473.0\n",
            "in training loop, epoch 4, step 739, the loss is 537449.75\n",
            "in training loop, epoch 4, step 740, the loss is 837149.75\n",
            "in training loop, epoch 4, step 741, the loss is 746000.375\n",
            "in training loop, epoch 4, step 742, the loss is 764124.125\n",
            "in training loop, epoch 4, step 743, the loss is 958299.0\n",
            "in training loop, epoch 4, step 744, the loss is 728286.6875\n",
            "in training loop, epoch 4, step 745, the loss is 918740.875\n",
            "in training loop, epoch 4, step 746, the loss is 445512.40625\n",
            "in training loop, epoch 4, step 747, the loss is 557096.875\n",
            "in training loop, epoch 4, step 748, the loss is 453114.34375\n",
            "in training loop, epoch 4, step 749, the loss is 476288.53125\n",
            "in training loop, epoch 4, step 750, the loss is 584971.625\n",
            "in training loop, epoch 4, step 751, the loss is 611954.625\n",
            "in training loop, epoch 4, step 752, the loss is 540627.8125\n",
            "in training loop, epoch 4, step 753, the loss is 784470.25\n",
            "in training loop, epoch 4, step 754, the loss is 616938.4375\n",
            "in training loop, epoch 4, step 755, the loss is 936990.4375\n",
            "in training loop, epoch 4, step 756, the loss is 385710.4375\n",
            "in training loop, epoch 4, step 757, the loss is 502967.9375\n",
            "in training loop, epoch 4, step 758, the loss is 801993.75\n",
            "in training loop, epoch 4, step 759, the loss is 383404.46875\n",
            "in training loop, epoch 4, step 760, the loss is 941754.8125\n",
            "in training loop, epoch 4, step 761, the loss is 592913.5625\n",
            "in training loop, epoch 4, step 762, the loss is 974489.4375\n",
            "in training loop, epoch 4, step 763, the loss is 580979.375\n",
            "in training loop, epoch 4, step 764, the loss is 479104.375\n",
            "in training loop, epoch 4, step 765, the loss is 614639.0\n",
            "in training loop, epoch 4, step 766, the loss is 345047.3125\n",
            "in training loop, epoch 4, step 767, the loss is 410753.5\n",
            "in training loop, epoch 4, step 768, the loss is 553424.0625\n",
            "in training loop, epoch 4, step 769, the loss is 674152.5625\n",
            "in training loop, epoch 4, step 770, the loss is 746248.3125\n",
            "in training loop, epoch 4, step 771, the loss is 605942.3125\n",
            "in training loop, epoch 4, step 772, the loss is 669495.75\n",
            "in training loop, epoch 4, step 773, the loss is 360358.625\n",
            "in training loop, epoch 4, step 774, the loss is 934973.6875\n",
            "in training loop, epoch 4, step 775, the loss is 962514.0625\n",
            "in training loop, epoch 4, step 776, the loss is 522442.0625\n",
            "in training loop, epoch 4, step 777, the loss is 786517.875\n",
            "in training loop, epoch 4, step 778, the loss is 626481.125\n",
            "in training loop, epoch 4, step 779, the loss is 1416531.875\n",
            "in training loop, epoch 4, step 780, the loss is 458940.0\n",
            "in training loop, epoch 4, step 781, the loss is 699595.25\n",
            "in training loop, epoch 4, step 782, the loss is 889808.3125\n",
            "in training loop, epoch 4, step 783, the loss is 794726.0625\n",
            "in training loop, epoch 4, step 784, the loss is 656239.5625\n",
            "in training loop, epoch 4, step 785, the loss is 668482.25\n",
            "in training loop, epoch 4, step 786, the loss is 430971.625\n",
            "in training loop, epoch 4, step 787, the loss is 794617.0\n",
            "in training loop, epoch 4, step 788, the loss is 652434.375\n",
            "in training loop, epoch 4, step 789, the loss is 477584.0\n",
            "in training loop, epoch 4, step 790, the loss is 440740.46875\n",
            "in training loop, epoch 4, step 791, the loss is 484357.3125\n",
            "in training loop, epoch 4, step 792, the loss is 927840.0625\n",
            "in training loop, epoch 4, step 793, the loss is 632422.875\n",
            "in training loop, epoch 4, step 794, the loss is 896387.3125\n",
            "in training loop, epoch 4, step 795, the loss is 732422.75\n",
            "in training loop, epoch 4, step 796, the loss is 502600.5\n",
            "in training loop, epoch 4, step 797, the loss is 855654.5\n",
            "in training loop, epoch 4, step 798, the loss is 674340.5\n",
            "in training loop, epoch 4, step 799, the loss is 243987.625\n",
            "in training loop, epoch 4, step 800, the loss is 602880.0625\n",
            "in training loop, epoch 4, step 801, the loss is 440224.59375\n",
            "in training loop, epoch 4, step 802, the loss is 636144.0\n",
            "in training loop, epoch 4, step 803, the loss is 629903.125\n",
            "in training loop, epoch 4, step 804, the loss is 441210.75\n",
            "in training loop, epoch 4, step 805, the loss is 452873.0625\n",
            "in training loop, epoch 4, step 806, the loss is 747339.25\n",
            "in training loop, epoch 4, step 807, the loss is 323243.28125\n",
            "in training loop, epoch 4, step 808, the loss is 398322.3125\n",
            "in training loop, epoch 4, step 809, the loss is 819126.5625\n",
            "in training loop, epoch 4, step 810, the loss is 416828.625\n",
            "in training loop, epoch 4, step 811, the loss is 509194.4375\n",
            "in training loop, epoch 4, step 812, the loss is 589930.25\n",
            "in training loop, epoch 4, step 813, the loss is 841651.9375\n",
            "in training loop, epoch 4, step 814, the loss is 1120790.125\n",
            "in training loop, epoch 4, step 815, the loss is 657964.75\n",
            "in training loop, epoch 4, step 816, the loss is 613491.625\n",
            "in training loop, epoch 4, step 817, the loss is 562131.0\n",
            "in training loop, epoch 4, step 818, the loss is 937574.25\n",
            "in training loop, epoch 4, step 819, the loss is 676855.8125\n",
            "in training loop, epoch 4, step 820, the loss is 892507.0\n",
            "in training loop, epoch 4, step 821, the loss is 453368.1875\n",
            "in training loop, epoch 4, step 822, the loss is 746046.9375\n",
            "in training loop, epoch 4, step 823, the loss is 476488.03125\n",
            "in training loop, epoch 4, step 824, the loss is 520847.6875\n",
            "in training loop, epoch 4, step 825, the loss is 635700.5625\n",
            "in training loop, epoch 4, step 826, the loss is 709095.8125\n",
            "in training loop, epoch 4, step 827, the loss is 465922.0625\n",
            "in training loop, epoch 4, step 828, the loss is 566527.1875\n",
            "in training loop, epoch 4, step 829, the loss is 832426.5\n",
            "in training loop, epoch 4, step 830, the loss is 845923.375\n",
            "in training loop, epoch 4, step 831, the loss is 1104898.25\n",
            "in training loop, epoch 4, step 832, the loss is 1266385.0\n",
            "in training loop, epoch 4, step 833, the loss is 582344.0625\n",
            "in training loop, epoch 4, step 834, the loss is 571659.75\n",
            "in training loop, epoch 4, step 835, the loss is 323346.0625\n",
            "in training loop, epoch 4, step 836, the loss is 675316.1875\n",
            "in training loop, epoch 4, step 837, the loss is 837317.5\n",
            "in training loop, epoch 4, step 838, the loss is 877943.5\n",
            "in training loop, epoch 4, step 839, the loss is 795429.625\n",
            "in training loop, epoch 4, step 840, the loss is 528952.5\n",
            "in training loop, epoch 4, step 841, the loss is 627992.5625\n",
            "in training loop, epoch 4, step 842, the loss is 539072.375\n",
            "in training loop, epoch 4, step 843, the loss is 981722.9375\n",
            "in training loop, epoch 4, step 844, the loss is 779237.375\n",
            "in training loop, epoch 4, step 845, the loss is 684461.5625\n",
            "in training loop, epoch 4, step 846, the loss is 800378.4375\n",
            "in training loop, epoch 4, step 847, the loss is 971235.5625\n",
            "in training loop, epoch 4, step 848, the loss is 610844.0625\n",
            "in training loop, epoch 4, step 849, the loss is 814429.4375\n",
            "in training loop, epoch 4, step 850, the loss is 659606.125\n",
            "in training loop, epoch 4, step 851, the loss is 584795.25\n",
            "in training loop, epoch 4, step 852, the loss is 460793.3125\n",
            "in training loop, epoch 4, step 853, the loss is 775131.5625\n",
            "in training loop, epoch 4, step 854, the loss is 779003.75\n",
            "in training loop, epoch 4, step 855, the loss is 920564.5\n",
            "in training loop, epoch 4, step 856, the loss is 469614.5625\n",
            "in training loop, epoch 4, step 857, the loss is 570702.375\n",
            "in training loop, epoch 4, step 858, the loss is 635752.4375\n",
            "in training loop, epoch 4, step 859, the loss is 615997.25\n",
            "in training loop, epoch 4, step 860, the loss is 766668.9375\n",
            "in training loop, epoch 4, step 861, the loss is 507785.21875\n",
            "in training loop, epoch 4, step 862, the loss is 605732.5625\n",
            "in training loop, epoch 4, step 863, the loss is 786728.125\n",
            "in training loop, epoch 4, step 864, the loss is 574723.0\n",
            "in training loop, epoch 4, step 865, the loss is 1188387.875\n",
            "in training loop, epoch 4, step 866, the loss is 1220278.75\n",
            "in training loop, epoch 4, step 867, the loss is 727590.9375\n",
            "in training loop, epoch 4, step 868, the loss is 805796.375\n",
            "in training loop, epoch 4, step 869, the loss is 495759.0\n",
            "in training loop, epoch 4, step 870, the loss is 541746.5\n",
            "in training loop, epoch 4, step 871, the loss is 586864.5\n",
            "in training loop, epoch 4, step 872, the loss is 575168.875\n",
            "in training loop, epoch 4, step 873, the loss is 860109.5\n",
            "in training loop, epoch 4, step 874, the loss is 1400423.25\n",
            "in training loop, epoch 4, step 875, the loss is 618067.875\n",
            "in training loop, epoch 4, step 876, the loss is 781898.5625\n",
            "in training loop, epoch 4, step 877, the loss is 792403.5\n",
            "in training loop, epoch 4, step 878, the loss is 424706.34375\n",
            "in training loop, epoch 4, step 879, the loss is 302967.21875\n",
            "in training loop, epoch 4, step 880, the loss is 381665.78125\n",
            "in training loop, epoch 4, step 881, the loss is 289957.125\n",
            "in training loop, epoch 4, step 882, the loss is 505344.96875\n",
            "in training loop, epoch 4, step 883, the loss is 842229.25\n",
            "in training loop, epoch 4, step 884, the loss is 967203.75\n",
            "in training loop, epoch 4, step 885, the loss is 564748.125\n",
            "in training loop, epoch 4, step 886, the loss is 329169.65625\n",
            "in training loop, epoch 4, step 887, the loss is 532697.375\n",
            "in training loop, epoch 4, step 888, the loss is 454060.15625\n",
            "in training loop, epoch 4, step 889, the loss is 624950.25\n",
            "in training loop, epoch 4, step 890, the loss is 757274.8125\n",
            "in training loop, epoch 4, step 891, the loss is 584045.875\n",
            "in training loop, epoch 4, step 892, the loss is 811643.6875\n",
            "in training loop, epoch 4, step 893, the loss is 495749.25\n",
            "in training loop, epoch 4, step 894, the loss is 377865.8125\n",
            "in training loop, epoch 4, step 895, the loss is 620601.8125\n",
            "in training loop, epoch 4, step 896, the loss is 504174.75\n",
            "in training loop, epoch 4, step 897, the loss is 428506.21875\n",
            "in training loop, epoch 4, step 898, the loss is 590107.375\n",
            "in training loop, epoch 4, step 899, the loss is 454334.46875\n",
            "in training loop, epoch 4, step 900, the loss is 630614.0625\n",
            "in training loop, epoch 4, step 901, the loss is 761508.8125\n",
            "in training loop, epoch 4, step 902, the loss is 663576.0\n",
            "in training loop, epoch 4, step 903, the loss is 288554.8125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "k-fold 0:: Epoch 4: train loss 700152.685546875 val loss 771119.8264232674\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train_loss_mean_list nan\n",
            "test_loss_mean_list nan\n",
            "val_loss_mean_list nan\n",
            "FOLD 1\n",
            "--------------------------------\n",
            "length of of train_loader is 904 & length of traindataset is 2008\n",
            "length of of test_loader is 101\n",
            "length of of val_loader is 1004\n",
            "in training loop, epoch 1, step 0, the loss is 350883.0625\n",
            "in training loop, epoch 1, step 1, the loss is 799415.75\n",
            "in training loop, epoch 1, step 2, the loss is 656104.125\n",
            "in training loop, epoch 1, step 3, the loss is 883751.5625\n",
            "in training loop, epoch 1, step 4, the loss is 429102.5625\n",
            "in training loop, epoch 1, step 5, the loss is 589378.5\n",
            "in training loop, epoch 1, step 6, the loss is 645642.125\n",
            "in training loop, epoch 1, step 7, the loss is 652300.1875\n",
            "in training loop, epoch 1, step 8, the loss is 707501.0\n",
            "in training loop, epoch 1, step 9, the loss is 572348.625\n",
            "in training loop, epoch 1, step 10, the loss is 347989.71875\n",
            "in training loop, epoch 1, step 11, the loss is 547670.6875\n",
            "in training loop, epoch 1, step 12, the loss is 668144.0\n",
            "in training loop, epoch 1, step 13, the loss is 511547.625\n",
            "in training loop, epoch 1, step 14, the loss is 575879.125\n",
            "in training loop, epoch 1, step 15, the loss is 404892.09375\n",
            "in training loop, epoch 1, step 16, the loss is 620557.875\n",
            "in training loop, epoch 1, step 17, the loss is 579296.6875\n",
            "in training loop, epoch 1, step 18, the loss is 599320.4375\n",
            "in training loop, epoch 1, step 19, the loss is 659852.6875\n",
            "in training loop, epoch 1, step 20, the loss is 623318.5625\n",
            "in training loop, epoch 1, step 21, the loss is 865394.625\n",
            "in training loop, epoch 1, step 22, the loss is 556673.4375\n",
            "in training loop, epoch 1, step 23, the loss is 428795.5\n",
            "in training loop, epoch 1, step 24, the loss is 571199.4375\n",
            "in training loop, epoch 1, step 25, the loss is 631239.875\n",
            "in training loop, epoch 1, step 26, the loss is 394302.625\n",
            "in training loop, epoch 1, step 27, the loss is 632126.875\n",
            "in training loop, epoch 1, step 28, the loss is 470885.34375\n",
            "in training loop, epoch 1, step 29, the loss is 420109.3125\n",
            "in training loop, epoch 1, step 30, the loss is 500994.6875\n",
            "in training loop, epoch 1, step 31, the loss is 1099313.5\n",
            "in training loop, epoch 1, step 32, the loss is 994838.75\n",
            "in training loop, epoch 1, step 33, the loss is 811116.6875\n",
            "in training loop, epoch 1, step 34, the loss is 461315.9375\n",
            "in training loop, epoch 1, step 35, the loss is 668403.875\n",
            "in training loop, epoch 1, step 36, the loss is 509246.0625\n",
            "in training loop, epoch 1, step 37, the loss is 595199.625\n",
            "in training loop, epoch 1, step 38, the loss is 579544.3125\n",
            "in training loop, epoch 1, step 39, the loss is 799507.75\n",
            "in training loop, epoch 1, step 40, the loss is 732067.6875\n",
            "in training loop, epoch 1, step 41, the loss is 612790.875\n",
            "in training loop, epoch 1, step 42, the loss is 942139.5\n",
            "in training loop, epoch 1, step 43, the loss is 889345.0\n",
            "in training loop, epoch 1, step 44, the loss is 481712.3125\n",
            "in training loop, epoch 1, step 45, the loss is 608868.0\n",
            "in training loop, epoch 1, step 46, the loss is 613819.1875\n",
            "in training loop, epoch 1, step 47, the loss is 683896.375\n",
            "in training loop, epoch 1, step 48, the loss is 694873.625\n",
            "in training loop, epoch 1, step 49, the loss is 528501.1875\n",
            "in training loop, epoch 1, step 50, the loss is 733446.9375\n",
            "in training loop, epoch 1, step 51, the loss is 775653.0\n",
            "in training loop, epoch 1, step 52, the loss is 696383.5625\n",
            "in training loop, epoch 1, step 53, the loss is 549235.875\n",
            "in training loop, epoch 1, step 54, the loss is 629718.25\n",
            "in training loop, epoch 1, step 55, the loss is 1144727.75\n",
            "in training loop, epoch 1, step 56, the loss is 653307.0\n",
            "in training loop, epoch 1, step 57, the loss is 623087.5\n",
            "in training loop, epoch 1, step 58, the loss is 743500.4375\n",
            "in training loop, epoch 1, step 59, the loss is 784791.3125\n",
            "in training loop, epoch 1, step 60, the loss is 709994.5625\n",
            "in training loop, epoch 1, step 61, the loss is 533866.5\n",
            "in training loop, epoch 1, step 62, the loss is 1008133.75\n",
            "in training loop, epoch 1, step 63, the loss is 868097.25\n",
            "in training loop, epoch 1, step 64, the loss is 532915.8125\n",
            "in training loop, epoch 1, step 65, the loss is 453665.0\n",
            "in training loop, epoch 1, step 66, the loss is 1106483.375\n",
            "in training loop, epoch 1, step 67, the loss is 581036.25\n",
            "in training loop, epoch 1, step 68, the loss is 821407.75\n",
            "in training loop, epoch 1, step 69, the loss is 518890.0\n",
            "in training loop, epoch 1, step 70, the loss is 927992.6875\n",
            "in training loop, epoch 1, step 71, the loss is 740426.9375\n",
            "in training loop, epoch 1, step 72, the loss is 493950.375\n",
            "in training loop, epoch 1, step 73, the loss is 353752.96875\n",
            "in training loop, epoch 1, step 74, the loss is 469799.59375\n",
            "in training loop, epoch 1, step 75, the loss is 677447.625\n",
            "in training loop, epoch 1, step 76, the loss is 588075.25\n",
            "in training loop, epoch 1, step 77, the loss is 927731.3125\n",
            "in training loop, epoch 1, step 78, the loss is 694671.9375\n",
            "in training loop, epoch 1, step 79, the loss is 660254.125\n",
            "in training loop, epoch 1, step 80, the loss is 933771.5\n",
            "in training loop, epoch 1, step 81, the loss is 632064.75\n",
            "in training loop, epoch 1, step 82, the loss is 1011648.125\n",
            "in training loop, epoch 1, step 83, the loss is 575829.5625\n",
            "in training loop, epoch 1, step 84, the loss is 473892.15625\n",
            "in training loop, epoch 1, step 85, the loss is 521599.875\n",
            "in training loop, epoch 1, step 86, the loss is 549489.3125\n",
            "in training loop, epoch 1, step 87, the loss is 511730.1875\n",
            "in training loop, epoch 1, step 88, the loss is 703674.875\n",
            "in training loop, epoch 1, step 89, the loss is 729087.125\n",
            "in training loop, epoch 1, step 90, the loss is 955836.75\n",
            "in training loop, epoch 1, step 91, the loss is 859441.625\n",
            "in training loop, epoch 1, step 92, the loss is 602971.9375\n",
            "in training loop, epoch 1, step 93, the loss is 901204.875\n",
            "in training loop, epoch 1, step 94, the loss is 462689.875\n",
            "in training loop, epoch 1, step 95, the loss is 503365.78125\n",
            "in training loop, epoch 1, step 96, the loss is 810866.1875\n",
            "in training loop, epoch 1, step 97, the loss is 384998.40625\n",
            "in training loop, epoch 1, step 98, the loss is 731795.5\n",
            "in training loop, epoch 1, step 99, the loss is 851203.9375\n",
            "in training loop, epoch 1, step 100, the loss is 625570.625\n",
            "in training loop, epoch 1, step 101, the loss is 648705.0625\n",
            "in training loop, epoch 1, step 102, the loss is 273216.4375\n",
            "in training loop, epoch 1, step 103, the loss is 679649.3125\n",
            "in training loop, epoch 1, step 104, the loss is 618171.0\n",
            "in training loop, epoch 1, step 105, the loss is 400351.625\n",
            "in training loop, epoch 1, step 106, the loss is 751905.25\n",
            "in training loop, epoch 1, step 107, the loss is 1204940.0\n",
            "in training loop, epoch 1, step 108, the loss is 754961.125\n",
            "in training loop, epoch 1, step 109, the loss is 579754.8125\n",
            "in training loop, epoch 1, step 110, the loss is 712260.875\n",
            "in training loop, epoch 1, step 111, the loss is 395101.3125\n",
            "in training loop, epoch 1, step 112, the loss is 551612.8125\n",
            "in training loop, epoch 1, step 113, the loss is 450484.0\n",
            "in training loop, epoch 1, step 114, the loss is 499802.375\n",
            "in training loop, epoch 1, step 115, the loss is 541341.0\n",
            "in training loop, epoch 1, step 116, the loss is 602751.8125\n",
            "in training loop, epoch 1, step 117, the loss is 641162.75\n",
            "in training loop, epoch 1, step 118, the loss is 434327.71875\n",
            "in training loop, epoch 1, step 119, the loss is 597507.8125\n",
            "in training loop, epoch 1, step 120, the loss is 393915.59375\n",
            "in training loop, epoch 1, step 121, the loss is 674243.4375\n",
            "in training loop, epoch 1, step 122, the loss is 470579.65625\n",
            "in training loop, epoch 1, step 123, the loss is 676347.75\n",
            "in training loop, epoch 1, step 124, the loss is 657149.0\n",
            "in training loop, epoch 1, step 125, the loss is 361048.6875\n",
            "in training loop, epoch 1, step 126, the loss is 478631.28125\n",
            "in training loop, epoch 1, step 127, the loss is 683990.875\n",
            "in training loop, epoch 1, step 128, the loss is 682979.0\n",
            "in training loop, epoch 1, step 129, the loss is 797843.9375\n",
            "in training loop, epoch 1, step 130, the loss is 493060.75\n",
            "in training loop, epoch 1, step 131, the loss is 619071.0\n",
            "in training loop, epoch 1, step 132, the loss is 646773.5625\n",
            "in training loop, epoch 1, step 133, the loss is 1070323.875\n",
            "in training loop, epoch 1, step 134, the loss is 477068.0625\n",
            "in training loop, epoch 1, step 135, the loss is 394219.90625\n",
            "in training loop, epoch 1, step 136, the loss is 655766.0\n",
            "in training loop, epoch 1, step 137, the loss is 485880.78125\n",
            "in training loop, epoch 1, step 138, the loss is 485797.25\n",
            "in training loop, epoch 1, step 139, the loss is 682573.0\n",
            "in training loop, epoch 1, step 140, the loss is 438737.90625\n",
            "in training loop, epoch 1, step 141, the loss is 640615.625\n",
            "in training loop, epoch 1, step 142, the loss is 769065.0625\n",
            "in training loop, epoch 1, step 143, the loss is 498428.53125\n",
            "in training loop, epoch 1, step 144, the loss is 554752.875\n",
            "in training loop, epoch 1, step 145, the loss is 685038.6875\n",
            "in training loop, epoch 1, step 146, the loss is 429783.09375\n",
            "in training loop, epoch 1, step 147, the loss is 376616.46875\n",
            "in training loop, epoch 1, step 148, the loss is 929451.625\n",
            "in training loop, epoch 1, step 149, the loss is 457885.0\n",
            "in training loop, epoch 1, step 150, the loss is 494825.46875\n",
            "in training loop, epoch 1, step 151, the loss is 545510.3125\n",
            "in training loop, epoch 1, step 152, the loss is 540681.3125\n",
            "in training loop, epoch 1, step 153, the loss is 770706.0\n",
            "in training loop, epoch 1, step 154, the loss is 644234.0625\n",
            "in training loop, epoch 1, step 155, the loss is 447550.0\n",
            "in training loop, epoch 1, step 156, the loss is 769991.25\n",
            "in training loop, epoch 1, step 157, the loss is 719265.75\n",
            "in training loop, epoch 1, step 158, the loss is 668500.3125\n",
            "in training loop, epoch 1, step 159, the loss is 617141.875\n",
            "in training loop, epoch 1, step 160, the loss is 1042904.0625\n",
            "in training loop, epoch 1, step 161, the loss is 449090.1875\n",
            "in training loop, epoch 1, step 162, the loss is 610837.75\n",
            "in training loop, epoch 1, step 163, the loss is 414977.90625\n",
            "in training loop, epoch 1, step 164, the loss is 556968.875\n",
            "in training loop, epoch 1, step 165, the loss is 458250.34375\n",
            "in training loop, epoch 1, step 166, the loss is 958422.5625\n",
            "in training loop, epoch 1, step 167, the loss is 670352.4375\n",
            "in training loop, epoch 1, step 168, the loss is 572042.5\n",
            "in training loop, epoch 1, step 169, the loss is 585398.4375\n",
            "in training loop, epoch 1, step 170, the loss is 891232.0\n",
            "in training loop, epoch 1, step 171, the loss is 664679.125\n",
            "in training loop, epoch 1, step 172, the loss is 723946.9375\n",
            "in training loop, epoch 1, step 173, the loss is 644821.4375\n",
            "in training loop, epoch 1, step 174, the loss is 515621.90625\n",
            "in training loop, epoch 1, step 175, the loss is 635050.375\n",
            "in training loop, epoch 1, step 176, the loss is 540042.4375\n",
            "in training loop, epoch 1, step 177, the loss is 553953.1875\n",
            "in training loop, epoch 1, step 178, the loss is 780531.75\n",
            "in training loop, epoch 1, step 179, the loss is 549578.0625\n",
            "in training loop, epoch 1, step 180, the loss is 570344.9375\n",
            "in training loop, epoch 1, step 181, the loss is 569455.75\n",
            "in training loop, epoch 1, step 182, the loss is 544301.1875\n",
            "in training loop, epoch 1, step 183, the loss is 457257.34375\n",
            "in training loop, epoch 1, step 184, the loss is 641862.8125\n",
            "in training loop, epoch 1, step 185, the loss is 656166.0\n",
            "in training loop, epoch 1, step 186, the loss is 593764.875\n",
            "in training loop, epoch 1, step 187, the loss is 612421.9375\n",
            "in training loop, epoch 1, step 188, the loss is 422827.09375\n",
            "in training loop, epoch 1, step 189, the loss is 673361.6875\n",
            "in training loop, epoch 1, step 190, the loss is 807138.5625\n",
            "in training loop, epoch 1, step 191, the loss is 519109.34375\n",
            "in training loop, epoch 1, step 192, the loss is 388859.65625\n",
            "in training loop, epoch 1, step 193, the loss is 810943.75\n",
            "in training loop, epoch 1, step 194, the loss is 1055769.125\n",
            "in training loop, epoch 1, step 195, the loss is 857987.9375\n",
            "in training loop, epoch 1, step 196, the loss is 785632.3125\n",
            "in training loop, epoch 1, step 197, the loss is 821386.25\n",
            "in training loop, epoch 1, step 198, the loss is 324792.3125\n",
            "in training loop, epoch 1, step 199, the loss is 900223.25\n",
            "in training loop, epoch 1, step 200, the loss is 722094.375\n",
            "in training loop, epoch 1, step 201, the loss is 568575.0625\n",
            "in training loop, epoch 1, step 202, the loss is 452562.03125\n",
            "in training loop, epoch 1, step 203, the loss is 644038.625\n",
            "in training loop, epoch 1, step 204, the loss is 525594.8125\n",
            "in training loop, epoch 1, step 205, the loss is 367095.1875\n",
            "in training loop, epoch 1, step 206, the loss is 808786.625\n",
            "in training loop, epoch 1, step 207, the loss is 466503.8125\n",
            "in training loop, epoch 1, step 208, the loss is 464475.3125\n",
            "in training loop, epoch 1, step 209, the loss is 531540.25\n",
            "in training loop, epoch 1, step 210, the loss is 547888.75\n",
            "in training loop, epoch 1, step 211, the loss is 380270.59375\n",
            "in training loop, epoch 1, step 212, the loss is 405304.375\n",
            "in training loop, epoch 1, step 213, the loss is 846384.5625\n",
            "in training loop, epoch 1, step 214, the loss is 1270436.0\n",
            "in training loop, epoch 1, step 215, the loss is 504775.5625\n",
            "in training loop, epoch 1, step 216, the loss is 537518.4375\n",
            "in training loop, epoch 1, step 217, the loss is 574935.375\n",
            "in training loop, epoch 1, step 218, the loss is 821188.25\n",
            "in training loop, epoch 1, step 219, the loss is 413959.0\n",
            "in training loop, epoch 1, step 220, the loss is 910493.25\n",
            "in training loop, epoch 1, step 221, the loss is 393970.875\n",
            "in training loop, epoch 1, step 222, the loss is 335336.40625\n",
            "in training loop, epoch 1, step 223, the loss is 502131.21875\n",
            "in training loop, epoch 1, step 224, the loss is 456174.625\n",
            "in training loop, epoch 1, step 225, the loss is 574787.0625\n",
            "in training loop, epoch 1, step 226, the loss is 359232.28125\n",
            "in training loop, epoch 1, step 227, the loss is 265514.6875\n",
            "in training loop, epoch 1, step 228, the loss is 631628.3125\n",
            "in training loop, epoch 1, step 229, the loss is 685090.375\n",
            "in training loop, epoch 1, step 230, the loss is 549775.75\n",
            "in training loop, epoch 1, step 231, the loss is 695848.75\n",
            "in training loop, epoch 1, step 232, the loss is 696629.0\n",
            "in training loop, epoch 1, step 233, the loss is 356765.65625\n",
            "in training loop, epoch 1, step 234, the loss is 477699.0625\n",
            "in training loop, epoch 1, step 235, the loss is 678904.125\n",
            "in training loop, epoch 1, step 236, the loss is 555944.5\n",
            "in training loop, epoch 1, step 237, the loss is 398912.96875\n",
            "in training loop, epoch 1, step 238, the loss is 911835.625\n",
            "in training loop, epoch 1, step 239, the loss is 754973.75\n",
            "in training loop, epoch 1, step 240, the loss is 572882.0\n",
            "in training loop, epoch 1, step 241, the loss is 639751.0625\n",
            "in training loop, epoch 1, step 242, the loss is 567344.125\n",
            "in training loop, epoch 1, step 243, the loss is 756700.5625\n",
            "in training loop, epoch 1, step 244, the loss is 536979.875\n",
            "in training loop, epoch 1, step 245, the loss is 735576.8125\n",
            "in training loop, epoch 1, step 246, the loss is 676322.125\n",
            "in training loop, epoch 1, step 247, the loss is 667790.25\n",
            "in training loop, epoch 1, step 248, the loss is 472783.96875\n",
            "in training loop, epoch 1, step 249, the loss is 880186.0\n",
            "in training loop, epoch 1, step 250, the loss is 676712.6875\n",
            "in training loop, epoch 1, step 251, the loss is 461987.1875\n",
            "in training loop, epoch 1, step 252, the loss is 252119.984375\n",
            "in training loop, epoch 1, step 253, the loss is 888430.5\n",
            "in training loop, epoch 1, step 254, the loss is 550466.8125\n",
            "in training loop, epoch 1, step 255, the loss is 518153.40625\n",
            "in training loop, epoch 1, step 256, the loss is 1640937.5\n",
            "in training loop, epoch 1, step 257, the loss is 740774.4375\n",
            "in training loop, epoch 1, step 258, the loss is 630669.1875\n",
            "in training loop, epoch 1, step 259, the loss is 1287964.625\n",
            "in training loop, epoch 1, step 260, the loss is 853992.6875\n",
            "in training loop, epoch 1, step 261, the loss is 505411.125\n",
            "in training loop, epoch 1, step 262, the loss is 1293663.75\n",
            "in training loop, epoch 1, step 263, the loss is 490697.75\n",
            "in training loop, epoch 1, step 264, the loss is 671464.125\n",
            "in training loop, epoch 1, step 265, the loss is 785676.0\n",
            "in training loop, epoch 1, step 266, the loss is 722429.5625\n",
            "in training loop, epoch 1, step 267, the loss is 911071.1875\n",
            "in training loop, epoch 1, step 268, the loss is 690381.875\n",
            "in training loop, epoch 1, step 269, the loss is 767464.75\n",
            "in training loop, epoch 1, step 270, the loss is 665309.0\n",
            "in training loop, epoch 1, step 271, the loss is 589009.4375\n",
            "in training loop, epoch 1, step 272, the loss is 414612.375\n",
            "in training loop, epoch 1, step 273, the loss is 741632.8125\n",
            "in training loop, epoch 1, step 274, the loss is 1784809.75\n",
            "in training loop, epoch 1, step 275, the loss is 463964.21875\n",
            "in training loop, epoch 1, step 276, the loss is 1055331.5\n",
            "in training loop, epoch 1, step 277, the loss is 1154090.75\n",
            "in training loop, epoch 1, step 278, the loss is 511335.25\n",
            "in training loop, epoch 1, step 279, the loss is 584765.6875\n",
            "in training loop, epoch 1, step 280, the loss is 837785.375\n",
            "in training loop, epoch 1, step 281, the loss is 872263.3125\n",
            "in training loop, epoch 1, step 282, the loss is 1101150.5\n",
            "in training loop, epoch 1, step 283, the loss is 976294.4375\n",
            "in training loop, epoch 1, step 284, the loss is 762091.8125\n",
            "in training loop, epoch 1, step 285, the loss is 1023690.1875\n",
            "in training loop, epoch 1, step 286, the loss is 770946.5\n",
            "in training loop, epoch 1, step 287, the loss is 862072.5\n",
            "in training loop, epoch 1, step 288, the loss is 849717.75\n",
            "in training loop, epoch 1, step 289, the loss is 620413.125\n",
            "in training loop, epoch 1, step 290, the loss is 899421.0\n",
            "in training loop, epoch 1, step 291, the loss is 738992.0\n",
            "in training loop, epoch 1, step 292, the loss is 501731.09375\n",
            "in training loop, epoch 1, step 293, the loss is 481328.65625\n",
            "in training loop, epoch 1, step 294, the loss is 674811.125\n",
            "in training loop, epoch 1, step 295, the loss is 1081620.0\n",
            "in training loop, epoch 1, step 296, the loss is 469693.625\n",
            "in training loop, epoch 1, step 297, the loss is 1409593.875\n",
            "in training loop, epoch 1, step 298, the loss is 1172561.625\n",
            "in training loop, epoch 1, step 299, the loss is 805911.4375\n",
            "in training loop, epoch 1, step 300, the loss is 875539.3125\n",
            "in training loop, epoch 1, step 301, the loss is 626467.5\n",
            "in training loop, epoch 1, step 302, the loss is 645862.625\n",
            "in training loop, epoch 1, step 303, the loss is 822266.75\n",
            "in training loop, epoch 1, step 304, the loss is 625482.5\n",
            "in training loop, epoch 1, step 305, the loss is 639610.0\n",
            "in training loop, epoch 1, step 306, the loss is 722446.9375\n",
            "in training loop, epoch 1, step 307, the loss is 670136.1875\n",
            "in training loop, epoch 1, step 308, the loss is 372147.4375\n",
            "in training loop, epoch 1, step 309, the loss is 877286.3125\n",
            "in training loop, epoch 1, step 310, the loss is 1035465.875\n",
            "in training loop, epoch 1, step 311, the loss is 555964.25\n",
            "in training loop, epoch 1, step 312, the loss is 570389.0625\n",
            "in training loop, epoch 1, step 313, the loss is 847522.625\n",
            "in training loop, epoch 1, step 314, the loss is 1924342.25\n",
            "in training loop, epoch 1, step 315, the loss is 763867.5625\n",
            "in training loop, epoch 1, step 316, the loss is 1025738.875\n",
            "in training loop, epoch 1, step 317, the loss is 420160.0\n",
            "in training loop, epoch 1, step 318, the loss is 1160577.625\n",
            "in training loop, epoch 1, step 319, the loss is 688002.875\n",
            "in training loop, epoch 1, step 320, the loss is 667322.25\n",
            "in training loop, epoch 1, step 321, the loss is 708766.0\n",
            "in training loop, epoch 1, step 322, the loss is 682661.625\n",
            "in training loop, epoch 1, step 323, the loss is 620200.5625\n",
            "in training loop, epoch 1, step 324, the loss is 230722.640625\n",
            "in training loop, epoch 1, step 325, the loss is 805936.75\n",
            "in training loop, epoch 1, step 326, the loss is 698807.5\n",
            "in training loop, epoch 1, step 327, the loss is 354498.125\n",
            "in training loop, epoch 1, step 328, the loss is 583513.875\n",
            "in training loop, epoch 1, step 329, the loss is 664543.6875\n",
            "in training loop, epoch 1, step 330, the loss is 766866.75\n",
            "in training loop, epoch 1, step 331, the loss is 631571.75\n",
            "in training loop, epoch 1, step 332, the loss is 687180.75\n",
            "in training loop, epoch 1, step 333, the loss is 418817.8125\n",
            "in training loop, epoch 1, step 334, the loss is 473267.25\n",
            "in training loop, epoch 1, step 335, the loss is 459999.71875\n",
            "in training loop, epoch 1, step 336, the loss is 915323.25\n",
            "in training loop, epoch 1, step 337, the loss is 1083648.5\n",
            "in training loop, epoch 1, step 338, the loss is 861342.75\n",
            "in training loop, epoch 1, step 339, the loss is 737459.5\n",
            "in training loop, epoch 1, step 340, the loss is 706769.5\n",
            "in training loop, epoch 1, step 341, the loss is 460568.25\n",
            "in training loop, epoch 1, step 342, the loss is 1053360.75\n",
            "in training loop, epoch 1, step 343, the loss is 879089.875\n",
            "in training loop, epoch 1, step 344, the loss is 770756.0625\n",
            "in training loop, epoch 1, step 345, the loss is 659798.8125\n",
            "in training loop, epoch 1, step 346, the loss is 997201.3125\n",
            "in training loop, epoch 1, step 347, the loss is 1233309.625\n",
            "in training loop, epoch 1, step 348, the loss is 907842.75\n",
            "in training loop, epoch 1, step 349, the loss is 853179.625\n",
            "in training loop, epoch 1, step 350, the loss is 700834.0\n",
            "in training loop, epoch 1, step 351, the loss is 773196.75\n",
            "in training loop, epoch 1, step 352, the loss is 789109.9375\n",
            "in training loop, epoch 1, step 353, the loss is 467996.59375\n",
            "in training loop, epoch 1, step 354, the loss is 637168.0\n",
            "in training loop, epoch 1, step 355, the loss is 977016.125\n",
            "in training loop, epoch 1, step 356, the loss is 1078149.0\n",
            "in training loop, epoch 1, step 357, the loss is 507616.5625\n",
            "in training loop, epoch 1, step 358, the loss is 839861.4375\n",
            "in training loop, epoch 1, step 359, the loss is 328442.96875\n",
            "in training loop, epoch 1, step 360, the loss is 440082.1875\n",
            "in training loop, epoch 1, step 361, the loss is 521252.21875\n",
            "in training loop, epoch 1, step 362, the loss is 820039.75\n",
            "in training loop, epoch 1, step 363, the loss is 545367.125\n",
            "in training loop, epoch 1, step 364, the loss is 1626585.5\n",
            "in training loop, epoch 1, step 365, the loss is 687141.0\n",
            "in training loop, epoch 1, step 366, the loss is 653050.75\n",
            "in training loop, epoch 1, step 367, the loss is 379698.5\n",
            "in training loop, epoch 1, step 368, the loss is 968934.625\n",
            "in training loop, epoch 1, step 369, the loss is 1091173.25\n",
            "in training loop, epoch 1, step 370, the loss is 541732.3125\n",
            "in training loop, epoch 1, step 371, the loss is 700621.5\n",
            "in training loop, epoch 1, step 372, the loss is 1113651.75\n",
            "in training loop, epoch 1, step 373, the loss is 881841.3125\n",
            "in training loop, epoch 1, step 374, the loss is 469415.5625\n",
            "in training loop, epoch 1, step 375, the loss is 708074.625\n",
            "in training loop, epoch 1, step 376, the loss is 570634.5\n",
            "in training loop, epoch 1, step 377, the loss is 1066054.5\n",
            "in training loop, epoch 1, step 378, the loss is 774389.75\n",
            "in training loop, epoch 1, step 379, the loss is 1589686.375\n",
            "in training loop, epoch 1, step 380, the loss is 456362.875\n",
            "in training loop, epoch 1, step 381, the loss is 670841.125\n",
            "in training loop, epoch 1, step 382, the loss is 795646.625\n",
            "in training loop, epoch 1, step 383, the loss is 741324.25\n",
            "in training loop, epoch 1, step 384, the loss is 1479367.125\n",
            "in training loop, epoch 1, step 385, the loss is 763939.75\n",
            "in training loop, epoch 1, step 386, the loss is 407022.625\n",
            "in training loop, epoch 1, step 387, the loss is 651951.5\n",
            "in training loop, epoch 1, step 388, the loss is 706223.6875\n",
            "in training loop, epoch 1, step 389, the loss is 518203.90625\n",
            "in training loop, epoch 1, step 390, the loss is 729863.375\n",
            "in training loop, epoch 1, step 391, the loss is 388472.78125\n",
            "in training loop, epoch 1, step 392, the loss is 594030.5625\n",
            "in training loop, epoch 1, step 393, the loss is 627097.3125\n",
            "in training loop, epoch 1, step 394, the loss is 807341.6875\n",
            "in training loop, epoch 1, step 395, the loss is 971535.4375\n",
            "in training loop, epoch 1, step 396, the loss is 491744.75\n",
            "in training loop, epoch 1, step 397, the loss is 713431.125\n",
            "in training loop, epoch 1, step 398, the loss is 1015088.0\n",
            "in training loop, epoch 1, step 399, the loss is 494818.25\n",
            "in training loop, epoch 1, step 400, the loss is 755282.6875\n",
            "in training loop, epoch 1, step 401, the loss is 697862.25\n",
            "in training loop, epoch 1, step 402, the loss is 632498.5\n",
            "in training loop, epoch 1, step 403, the loss is 605110.875\n",
            "in training loop, epoch 1, step 404, the loss is 899479.75\n",
            "in training loop, epoch 1, step 405, the loss is 615963.0625\n",
            "in training loop, epoch 1, step 406, the loss is 987870.3125\n",
            "in training loop, epoch 1, step 407, the loss is 717534.25\n",
            "in training loop, epoch 1, step 408, the loss is 762398.5\n",
            "in training loop, epoch 1, step 409, the loss is 414775.625\n",
            "in training loop, epoch 1, step 410, the loss is 716938.125\n",
            "in training loop, epoch 1, step 411, the loss is 892934.375\n",
            "in training loop, epoch 1, step 412, the loss is 468617.75\n",
            "in training loop, epoch 1, step 413, the loss is 526316.4375\n",
            "in training loop, epoch 1, step 414, the loss is 886458.0\n",
            "in training loop, epoch 1, step 415, the loss is 540448.75\n",
            "in training loop, epoch 1, step 416, the loss is 382159.625\n",
            "in training loop, epoch 1, step 417, the loss is 624259.875\n",
            "in training loop, epoch 1, step 418, the loss is 513812.3125\n",
            "in training loop, epoch 1, step 419, the loss is 603276.5\n",
            "in training loop, epoch 1, step 420, the loss is 791221.375\n",
            "in training loop, epoch 1, step 421, the loss is 778903.5625\n",
            "in training loop, epoch 1, step 422, the loss is 416131.3125\n",
            "in training loop, epoch 1, step 423, the loss is 1061159.75\n",
            "in training loop, epoch 1, step 424, the loss is 582856.25\n",
            "in training loop, epoch 1, step 425, the loss is 458504.8125\n",
            "in training loop, epoch 1, step 426, the loss is 594779.5625\n",
            "in training loop, epoch 1, step 427, the loss is 908161.6875\n",
            "in training loop, epoch 1, step 428, the loss is 539712.125\n",
            "in training loop, epoch 1, step 429, the loss is 410113.71875\n",
            "in training loop, epoch 1, step 430, the loss is 651127.1875\n",
            "in training loop, epoch 1, step 431, the loss is 597347.25\n",
            "in training loop, epoch 1, step 432, the loss is 1437991.25\n",
            "in training loop, epoch 1, step 433, the loss is 401852.8125\n",
            "in training loop, epoch 1, step 434, the loss is 782913.9375\n",
            "in training loop, epoch 1, step 435, the loss is 726199.25\n",
            "in training loop, epoch 1, step 436, the loss is 685448.0625\n",
            "in training loop, epoch 1, step 437, the loss is 381394.28125\n",
            "in training loop, epoch 1, step 438, the loss is 430806.5625\n",
            "in training loop, epoch 1, step 439, the loss is 616274.9375\n",
            "in training loop, epoch 1, step 440, the loss is 399900.4375\n",
            "in training loop, epoch 1, step 441, the loss is 759599.0625\n",
            "in training loop, epoch 1, step 442, the loss is 366347.96875\n",
            "in training loop, epoch 1, step 443, the loss is 867235.125\n",
            "in training loop, epoch 1, step 444, the loss is 516164.625\n",
            "in training loop, epoch 1, step 445, the loss is 698429.0625\n",
            "in training loop, epoch 1, step 446, the loss is 631007.375\n",
            "in training loop, epoch 1, step 447, the loss is 477094.75\n",
            "in training loop, epoch 1, step 448, the loss is 743339.5625\n",
            "in training loop, epoch 1, step 449, the loss is 512726.40625\n",
            "in training loop, epoch 1, step 450, the loss is 648114.125\n",
            "in training loop, epoch 1, step 451, the loss is 419005.375\n",
            "in training loop, epoch 1, step 452, the loss is 918584.9375\n",
            "in training loop, epoch 1, step 453, the loss is 510529.1875\n",
            "in training loop, epoch 1, step 454, the loss is 922734.125\n",
            "in training loop, epoch 1, step 455, the loss is 383656.5625\n",
            "in training loop, epoch 1, step 456, the loss is 1607850.625\n",
            "in training loop, epoch 1, step 457, the loss is 490647.125\n",
            "in training loop, epoch 1, step 458, the loss is 325428.84375\n",
            "in training loop, epoch 1, step 459, the loss is 747058.875\n",
            "in training loop, epoch 1, step 460, the loss is 821148.75\n",
            "in training loop, epoch 1, step 461, the loss is 608180.9375\n",
            "in training loop, epoch 1, step 462, the loss is 628758.9375\n",
            "in training loop, epoch 1, step 463, the loss is 618505.625\n",
            "in training loop, epoch 1, step 464, the loss is 766996.75\n",
            "in training loop, epoch 1, step 465, the loss is 759028.9375\n",
            "in training loop, epoch 1, step 466, the loss is 485439.8125\n",
            "in training loop, epoch 1, step 467, the loss is 311592.25\n",
            "in training loop, epoch 1, step 468, the loss is 1027824.125\n",
            "in training loop, epoch 1, step 469, the loss is 408539.46875\n",
            "in training loop, epoch 1, step 470, the loss is 507669.25\n",
            "in training loop, epoch 1, step 471, the loss is 723646.875\n",
            "in training loop, epoch 1, step 472, the loss is 772434.625\n",
            "in training loop, epoch 1, step 473, the loss is 827181.1875\n",
            "in training loop, epoch 1, step 474, the loss is 574489.5625\n",
            "in training loop, epoch 1, step 475, the loss is 369831.625\n",
            "in training loop, epoch 1, step 476, the loss is 576959.25\n",
            "in training loop, epoch 1, step 477, the loss is 768442.0\n",
            "in training loop, epoch 1, step 478, the loss is 474450.5\n",
            "in training loop, epoch 1, step 479, the loss is 512129.25\n",
            "in training loop, epoch 1, step 480, the loss is 915792.1875\n",
            "in training loop, epoch 1, step 481, the loss is 803121.5\n",
            "in training loop, epoch 1, step 482, the loss is 712258.8125\n",
            "in training loop, epoch 1, step 483, the loss is 594311.1875\n",
            "in training loop, epoch 1, step 484, the loss is 510160.4375\n",
            "in training loop, epoch 1, step 485, the loss is 697196.125\n",
            "in training loop, epoch 1, step 486, the loss is 723828.5625\n",
            "in training loop, epoch 1, step 487, the loss is 501912.1875\n",
            "in training loop, epoch 1, step 488, the loss is 599995.625\n",
            "in training loop, epoch 1, step 489, the loss is 582716.5\n",
            "in training loop, epoch 1, step 490, the loss is 549829.375\n",
            "in training loop, epoch 1, step 491, the loss is 579943.3125\n",
            "in training loop, epoch 1, step 492, the loss is 788906.875\n",
            "in training loop, epoch 1, step 493, the loss is 575919.625\n",
            "in training loop, epoch 1, step 494, the loss is 463550.40625\n",
            "in training loop, epoch 1, step 495, the loss is 428280.5625\n",
            "in training loop, epoch 1, step 496, the loss is 1087108.25\n",
            "in training loop, epoch 1, step 497, the loss is 879037.5\n",
            "in training loop, epoch 1, step 498, the loss is 824430.375\n",
            "in training loop, epoch 1, step 499, the loss is 537402.875\n",
            "in training loop, epoch 1, step 500, the loss is 775262.8125\n",
            "in training loop, epoch 1, step 501, the loss is 529237.75\n",
            "in training loop, epoch 1, step 502, the loss is 908180.9375\n",
            "in training loop, epoch 1, step 503, the loss is 549166.8125\n",
            "in training loop, epoch 1, step 504, the loss is 545393.0625\n",
            "in training loop, epoch 1, step 505, the loss is 703484.375\n",
            "in training loop, epoch 1, step 506, the loss is 360603.3125\n",
            "in training loop, epoch 1, step 507, the loss is 576900.125\n",
            "in training loop, epoch 1, step 508, the loss is 625272.0\n",
            "in training loop, epoch 1, step 509, the loss is 482501.75\n",
            "in training loop, epoch 1, step 510, the loss is 268137.40625\n",
            "in training loop, epoch 1, step 511, the loss is 378393.09375\n",
            "in training loop, epoch 1, step 512, the loss is 666769.0625\n",
            "in training loop, epoch 1, step 513, the loss is 882234.0625\n",
            "in training loop, epoch 1, step 514, the loss is 986857.5\n",
            "in training loop, epoch 1, step 515, the loss is 479188.5\n",
            "in training loop, epoch 1, step 516, the loss is 363553.90625\n",
            "in training loop, epoch 1, step 517, the loss is 628482.9375\n",
            "in training loop, epoch 1, step 518, the loss is 610905.8125\n",
            "in training loop, epoch 1, step 519, the loss is 909262.4375\n",
            "in training loop, epoch 1, step 520, the loss is 661699.375\n",
            "in training loop, epoch 1, step 521, the loss is 375644.71875\n",
            "in training loop, epoch 1, step 522, the loss is 688395.125\n",
            "in training loop, epoch 1, step 523, the loss is 405231.09375\n",
            "in training loop, epoch 1, step 524, the loss is 684487.5\n",
            "in training loop, epoch 1, step 525, the loss is 585434.5625\n",
            "in training loop, epoch 1, step 526, the loss is 925812.25\n",
            "in training loop, epoch 1, step 527, the loss is 442047.4375\n",
            "in training loop, epoch 1, step 528, the loss is 734662.375\n",
            "in training loop, epoch 1, step 529, the loss is 687306.4375\n",
            "in training loop, epoch 1, step 530, the loss is 493186.75\n",
            "in training loop, epoch 1, step 531, the loss is 394688.8125\n",
            "in training loop, epoch 1, step 532, the loss is 595693.375\n",
            "in training loop, epoch 1, step 533, the loss is 741407.75\n",
            "in training loop, epoch 1, step 534, the loss is 496186.0625\n",
            "in training loop, epoch 1, step 535, the loss is 665399.0\n",
            "in training loop, epoch 1, step 536, the loss is 329570.75\n",
            "in training loop, epoch 1, step 537, the loss is 492033.34375\n",
            "in training loop, epoch 1, step 538, the loss is 349527.21875\n",
            "in training loop, epoch 1, step 539, the loss is 582490.625\n",
            "in training loop, epoch 1, step 540, the loss is 520483.1875\n",
            "in training loop, epoch 1, step 541, the loss is 562929.5\n",
            "in training loop, epoch 1, step 542, the loss is 593333.1875\n",
            "in training loop, epoch 1, step 543, the loss is 482042.375\n",
            "in training loop, epoch 1, step 544, the loss is 580254.0625\n",
            "in training loop, epoch 1, step 545, the loss is 586238.5625\n",
            "in training loop, epoch 1, step 546, the loss is 659900.625\n",
            "in training loop, epoch 1, step 547, the loss is 717333.375\n",
            "in training loop, epoch 1, step 548, the loss is 369436.46875\n",
            "in training loop, epoch 1, step 549, the loss is 477434.28125\n",
            "in training loop, epoch 1, step 550, the loss is 610495.6875\n",
            "in training loop, epoch 1, step 551, the loss is 849862.0\n",
            "in training loop, epoch 1, step 552, the loss is 740522.3125\n",
            "in training loop, epoch 1, step 553, the loss is 671463.75\n",
            "in training loop, epoch 1, step 554, the loss is 570578.3125\n",
            "in training loop, epoch 1, step 555, the loss is 646959.1875\n",
            "in training loop, epoch 1, step 556, the loss is 883052.3125\n",
            "in training loop, epoch 1, step 557, the loss is 640496.5\n",
            "in training loop, epoch 1, step 558, the loss is 541904.5625\n",
            "in training loop, epoch 1, step 559, the loss is 898637.1875\n",
            "in training loop, epoch 1, step 560, the loss is 399437.96875\n",
            "in training loop, epoch 1, step 561, the loss is 814765.5625\n",
            "in training loop, epoch 1, step 562, the loss is 611700.75\n",
            "in training loop, epoch 1, step 563, the loss is 442332.15625\n",
            "in training loop, epoch 1, step 564, the loss is 425860.71875\n",
            "in training loop, epoch 1, step 565, the loss is 460301.78125\n",
            "in training loop, epoch 1, step 566, the loss is 641178.3125\n",
            "in training loop, epoch 1, step 567, the loss is 849164.125\n",
            "in training loop, epoch 1, step 568, the loss is 411329.71875\n",
            "in training loop, epoch 1, step 569, the loss is 461966.3125\n",
            "in training loop, epoch 1, step 570, the loss is 426970.28125\n",
            "in training loop, epoch 1, step 571, the loss is 551281.625\n",
            "in training loop, epoch 1, step 572, the loss is 668264.75\n",
            "in training loop, epoch 1, step 573, the loss is 1790796.125\n",
            "in training loop, epoch 1, step 574, the loss is 848833.5\n",
            "in training loop, epoch 1, step 575, the loss is 608410.0\n",
            "in training loop, epoch 1, step 576, the loss is 530695.625\n",
            "in training loop, epoch 1, step 577, the loss is 571134.0\n",
            "in training loop, epoch 1, step 578, the loss is 356646.6875\n",
            "in training loop, epoch 1, step 579, the loss is 638993.625\n",
            "in training loop, epoch 1, step 580, the loss is 760715.5625\n",
            "in training loop, epoch 1, step 581, the loss is 589492.5\n",
            "in training loop, epoch 1, step 582, the loss is 486167.90625\n",
            "in training loop, epoch 1, step 583, the loss is 395942.84375\n",
            "in training loop, epoch 1, step 584, the loss is 300658.46875\n",
            "in training loop, epoch 1, step 585, the loss is 493271.75\n",
            "in training loop, epoch 1, step 586, the loss is 898103.5625\n",
            "in training loop, epoch 1, step 587, the loss is 798133.3125\n",
            "in training loop, epoch 1, step 588, the loss is 536329.8125\n",
            "in training loop, epoch 1, step 589, the loss is 922941.1875\n",
            "in training loop, epoch 1, step 590, the loss is 790512.0\n",
            "in training loop, epoch 1, step 591, the loss is 672387.0\n",
            "in training loop, epoch 1, step 592, the loss is 887463.3125\n",
            "in training loop, epoch 1, step 593, the loss is 737765.1875\n",
            "in training loop, epoch 1, step 594, the loss is 682459.0\n",
            "in training loop, epoch 1, step 595, the loss is 763070.8125\n",
            "in training loop, epoch 1, step 596, the loss is 827568.125\n",
            "in training loop, epoch 1, step 597, the loss is 835032.25\n",
            "in training loop, epoch 1, step 598, the loss is 459355.0\n",
            "in training loop, epoch 1, step 599, the loss is 579149.5\n",
            "in training loop, epoch 1, step 600, the loss is 332314.625\n",
            "in training loop, epoch 1, step 601, the loss is 657629.875\n",
            "in training loop, epoch 1, step 602, the loss is 792268.0625\n",
            "in training loop, epoch 1, step 603, the loss is 533175.875\n",
            "in training loop, epoch 1, step 604, the loss is 603625.75\n",
            "in training loop, epoch 1, step 605, the loss is 563488.25\n",
            "in training loop, epoch 1, step 606, the loss is 669075.6875\n",
            "in training loop, epoch 1, step 607, the loss is 459985.71875\n",
            "in training loop, epoch 1, step 608, the loss is 640351.25\n",
            "in training loop, epoch 1, step 609, the loss is 518268.9375\n",
            "in training loop, epoch 1, step 610, the loss is 295666.6875\n",
            "in training loop, epoch 1, step 611, the loss is 416595.125\n",
            "in training loop, epoch 1, step 612, the loss is 781143.75\n",
            "in training loop, epoch 1, step 613, the loss is 531685.75\n",
            "in training loop, epoch 1, step 614, the loss is 1058958.25\n",
            "in training loop, epoch 1, step 615, the loss is 814136.875\n",
            "in training loop, epoch 1, step 616, the loss is 795744.3125\n",
            "in training loop, epoch 1, step 617, the loss is 1018431.3125\n",
            "in training loop, epoch 1, step 618, the loss is 557195.25\n",
            "in training loop, epoch 1, step 619, the loss is 703362.5625\n",
            "in training loop, epoch 1, step 620, the loss is 721663.75\n",
            "in training loop, epoch 1, step 621, the loss is 665976.875\n",
            "in training loop, epoch 1, step 622, the loss is 640188.5625\n",
            "in training loop, epoch 1, step 623, the loss is 499128.65625\n",
            "in training loop, epoch 1, step 624, the loss is 662131.3125\n",
            "in training loop, epoch 1, step 625, the loss is 740909.5\n",
            "in training loop, epoch 1, step 626, the loss is 466859.21875\n",
            "in training loop, epoch 1, step 627, the loss is 1007385.0625\n",
            "in training loop, epoch 1, step 628, the loss is 281254.34375\n",
            "in training loop, epoch 1, step 629, the loss is 1114208.0\n",
            "in training loop, epoch 1, step 630, the loss is 1178537.625\n",
            "in training loop, epoch 1, step 631, the loss is 499709.75\n",
            "in training loop, epoch 1, step 632, the loss is 585571.25\n",
            "in training loop, epoch 1, step 633, the loss is 874410.4375\n",
            "in training loop, epoch 1, step 634, the loss is 519791.25\n",
            "in training loop, epoch 1, step 635, the loss is 712948.625\n",
            "in training loop, epoch 1, step 636, the loss is 890567.3125\n",
            "in training loop, epoch 1, step 637, the loss is 628804.625\n",
            "in training loop, epoch 1, step 638, the loss is 514954.21875\n",
            "in training loop, epoch 1, step 639, the loss is 747901.75\n",
            "in training loop, epoch 1, step 640, the loss is 462328.96875\n",
            "in training loop, epoch 1, step 641, the loss is 612745.375\n",
            "in training loop, epoch 1, step 642, the loss is 529167.8125\n",
            "in training loop, epoch 1, step 643, the loss is 428904.375\n",
            "in training loop, epoch 1, step 644, the loss is 453522.65625\n",
            "in training loop, epoch 1, step 645, the loss is 657865.0\n",
            "in training loop, epoch 1, step 646, the loss is 353252.0625\n",
            "in training loop, epoch 1, step 647, the loss is 503121.0625\n",
            "in training loop, epoch 1, step 648, the loss is 567174.75\n",
            "in training loop, epoch 1, step 649, the loss is 525913.75\n",
            "in training loop, epoch 1, step 650, the loss is 691575.1875\n",
            "in training loop, epoch 1, step 651, the loss is 719999.0625\n",
            "in training loop, epoch 1, step 652, the loss is 538575.375\n",
            "in training loop, epoch 1, step 653, the loss is 302146.25\n",
            "in training loop, epoch 1, step 654, the loss is 527558.1875\n",
            "in training loop, epoch 1, step 655, the loss is 929073.375\n",
            "in training loop, epoch 1, step 656, the loss is 622226.6875\n",
            "in training loop, epoch 1, step 657, the loss is 632522.3125\n",
            "in training loop, epoch 1, step 658, the loss is 488321.5\n",
            "in training loop, epoch 1, step 659, the loss is 830752.875\n",
            "in training loop, epoch 1, step 660, the loss is 613910.6875\n",
            "in training loop, epoch 1, step 661, the loss is 862706.8125\n",
            "in training loop, epoch 1, step 662, the loss is 709303.625\n",
            "in training loop, epoch 1, step 663, the loss is 593898.75\n",
            "in training loop, epoch 1, step 664, the loss is 584020.75\n",
            "in training loop, epoch 1, step 665, the loss is 652371.875\n",
            "in training loop, epoch 1, step 666, the loss is 442747.125\n",
            "in training loop, epoch 1, step 667, the loss is 415784.0625\n",
            "in training loop, epoch 1, step 668, the loss is 560444.875\n",
            "in training loop, epoch 1, step 669, the loss is 494950.875\n",
            "in training loop, epoch 1, step 670, the loss is 401202.625\n",
            "in training loop, epoch 1, step 671, the loss is 774879.4375\n",
            "in training loop, epoch 1, step 672, the loss is 515957.25\n",
            "in training loop, epoch 1, step 673, the loss is 895889.625\n",
            "in training loop, epoch 1, step 674, the loss is 414548.09375\n",
            "in training loop, epoch 1, step 675, the loss is 613854.875\n",
            "in training loop, epoch 1, step 676, the loss is 548259.3125\n",
            "in training loop, epoch 1, step 677, the loss is 600054.375\n",
            "in training loop, epoch 1, step 678, the loss is 320336.1875\n",
            "in training loop, epoch 1, step 679, the loss is 515773.5\n",
            "in training loop, epoch 1, step 680, the loss is 712367.5625\n",
            "in training loop, epoch 1, step 681, the loss is 654252.5625\n",
            "in training loop, epoch 1, step 682, the loss is 879656.375\n",
            "in training loop, epoch 1, step 683, the loss is 1115058.5\n",
            "in training loop, epoch 1, step 684, the loss is 599459.625\n",
            "in training loop, epoch 1, step 685, the loss is 778500.6875\n",
            "in training loop, epoch 1, step 686, the loss is 1016854.6875\n",
            "in training loop, epoch 1, step 687, the loss is 547209.0625\n",
            "in training loop, epoch 1, step 688, the loss is 856965.75\n",
            "in training loop, epoch 1, step 689, the loss is 472030.9375\n",
            "in training loop, epoch 1, step 690, the loss is 585272.0\n",
            "in training loop, epoch 1, step 691, the loss is 482444.125\n",
            "in training loop, epoch 1, step 692, the loss is 518384.1875\n",
            "in training loop, epoch 1, step 693, the loss is 546134.5625\n",
            "in training loop, epoch 1, step 694, the loss is 570469.8125\n",
            "in training loop, epoch 1, step 695, the loss is 510884.90625\n",
            "in training loop, epoch 1, step 696, the loss is 538505.75\n",
            "in training loop, epoch 1, step 697, the loss is 427465.84375\n",
            "in training loop, epoch 1, step 698, the loss is 667423.8125\n",
            "in training loop, epoch 1, step 699, the loss is 646833.625\n",
            "in training loop, epoch 1, step 700, the loss is 254923.796875\n",
            "in training loop, epoch 1, step 701, the loss is 911809.875\n",
            "in training loop, epoch 1, step 702, the loss is 727432.75\n",
            "in training loop, epoch 1, step 703, the loss is 845344.625\n",
            "in training loop, epoch 1, step 704, the loss is 974240.75\n",
            "in training loop, epoch 1, step 705, the loss is 302427.90625\n",
            "in training loop, epoch 1, step 706, the loss is 483718.125\n",
            "in training loop, epoch 1, step 707, the loss is 545594.9375\n",
            "in training loop, epoch 1, step 708, the loss is 808670.0\n",
            "in training loop, epoch 1, step 709, the loss is 627009.875\n",
            "in training loop, epoch 1, step 710, the loss is 598603.0\n",
            "in training loop, epoch 1, step 711, the loss is 1031348.0625\n",
            "in training loop, epoch 1, step 712, the loss is 566302.5\n",
            "in training loop, epoch 1, step 713, the loss is 1264968.5\n",
            "in training loop, epoch 1, step 714, the loss is 795922.375\n",
            "in training loop, epoch 1, step 715, the loss is 562167.8125\n",
            "in training loop, epoch 1, step 716, the loss is 723801.5\n",
            "in training loop, epoch 1, step 717, the loss is 876132.625\n",
            "in training loop, epoch 1, step 718, the loss is 490717.0625\n",
            "in training loop, epoch 1, step 719, the loss is 746866.875\n",
            "in training loop, epoch 1, step 720, the loss is 389230.625\n",
            "in training loop, epoch 1, step 721, the loss is 458956.5625\n",
            "in training loop, epoch 1, step 722, the loss is 751936.75\n",
            "in training loop, epoch 1, step 723, the loss is 329385.21875\n",
            "in training loop, epoch 1, step 724, the loss is 442385.6875\n",
            "in training loop, epoch 1, step 725, the loss is 643717.25\n",
            "in training loop, epoch 1, step 726, the loss is 854395.625\n",
            "in training loop, epoch 1, step 727, the loss is 1041722.6875\n",
            "in training loop, epoch 1, step 728, the loss is 509020.84375\n",
            "in training loop, epoch 1, step 729, the loss is 540274.875\n",
            "in training loop, epoch 1, step 730, the loss is 568169.75\n",
            "in training loop, epoch 1, step 731, the loss is 332043.875\n",
            "in training loop, epoch 1, step 732, the loss is 511135.9375\n",
            "in training loop, epoch 1, step 733, the loss is 639576.0\n",
            "in training loop, epoch 1, step 734, the loss is 630985.4375\n",
            "in training loop, epoch 1, step 735, the loss is 543544.3125\n",
            "in training loop, epoch 1, step 736, the loss is 657957.75\n",
            "in training loop, epoch 1, step 737, the loss is 360339.8125\n",
            "in training loop, epoch 1, step 738, the loss is 336912.03125\n",
            "in training loop, epoch 1, step 739, the loss is 1191194.5\n",
            "in training loop, epoch 1, step 740, the loss is 505521.53125\n",
            "in training loop, epoch 1, step 741, the loss is 541903.0625\n",
            "in training loop, epoch 1, step 742, the loss is 545526.3125\n",
            "in training loop, epoch 1, step 743, the loss is 729134.0\n",
            "in training loop, epoch 1, step 744, the loss is 436131.53125\n",
            "in training loop, epoch 1, step 745, the loss is 491977.625\n",
            "in training loop, epoch 1, step 746, the loss is 942499.3125\n",
            "in training loop, epoch 1, step 747, the loss is 703273.3125\n",
            "in training loop, epoch 1, step 748, the loss is 630779.5\n",
            "in training loop, epoch 1, step 749, the loss is 1358358.625\n",
            "in training loop, epoch 1, step 750, the loss is 853250.0\n",
            "in training loop, epoch 1, step 751, the loss is 432533.0625\n",
            "in training loop, epoch 1, step 752, the loss is 283102.21875\n",
            "in training loop, epoch 1, step 753, the loss is 560941.125\n",
            "in training loop, epoch 1, step 754, the loss is 367492.0\n",
            "in training loop, epoch 1, step 755, the loss is 607511.375\n",
            "in training loop, epoch 1, step 756, the loss is 1112770.875\n",
            "in training loop, epoch 1, step 757, the loss is 544264.9375\n",
            "in training loop, epoch 1, step 758, the loss is 463643.84375\n",
            "in training loop, epoch 1, step 759, the loss is 715459.9375\n",
            "in training loop, epoch 1, step 760, the loss is 659521.5625\n",
            "in training loop, epoch 1, step 761, the loss is 908281.0625\n",
            "in training loop, epoch 1, step 762, the loss is 686234.0\n",
            "in training loop, epoch 1, step 763, the loss is 928180.625\n",
            "in training loop, epoch 1, step 764, the loss is 435866.40625\n",
            "in training loop, epoch 1, step 765, the loss is 893013.75\n",
            "in training loop, epoch 1, step 766, the loss is 792377.5\n",
            "in training loop, epoch 1, step 767, the loss is 570753.625\n",
            "in training loop, epoch 1, step 768, the loss is 798620.8125\n",
            "in training loop, epoch 1, step 769, the loss is 538469.75\n",
            "in training loop, epoch 1, step 770, the loss is 507265.0\n",
            "in training loop, epoch 1, step 771, the loss is 771823.4375\n",
            "in training loop, epoch 1, step 772, the loss is 629790.875\n",
            "in training loop, epoch 1, step 773, the loss is 734077.4375\n",
            "in training loop, epoch 1, step 774, the loss is 932572.75\n",
            "in training loop, epoch 1, step 775, the loss is 607090.75\n",
            "in training loop, epoch 1, step 776, the loss is 707485.0\n",
            "in training loop, epoch 1, step 777, the loss is 626780.9375\n",
            "in training loop, epoch 1, step 778, the loss is 683266.0\n",
            "in training loop, epoch 1, step 779, the loss is 1030000.25\n",
            "in training loop, epoch 1, step 780, the loss is 493687.03125\n",
            "in training loop, epoch 1, step 781, the loss is 471459.875\n",
            "in training loop, epoch 1, step 782, the loss is 749767.25\n",
            "in training loop, epoch 1, step 783, the loss is 378230.5\n",
            "in training loop, epoch 1, step 784, the loss is 384331.6875\n",
            "in training loop, epoch 1, step 785, the loss is 314698.375\n",
            "in training loop, epoch 1, step 786, the loss is 564414.1875\n",
            "in training loop, epoch 1, step 787, the loss is 602548.0625\n",
            "in training loop, epoch 1, step 788, the loss is 759653.25\n",
            "in training loop, epoch 1, step 789, the loss is 533472.1875\n",
            "in training loop, epoch 1, step 790, the loss is 613945.4375\n",
            "in training loop, epoch 1, step 791, the loss is 766124.1875\n",
            "in training loop, epoch 1, step 792, the loss is 659597.5\n",
            "in training loop, epoch 1, step 793, the loss is 297500.65625\n",
            "in training loop, epoch 1, step 794, the loss is 553247.25\n",
            "in training loop, epoch 1, step 795, the loss is 588730.5625\n",
            "in training loop, epoch 1, step 796, the loss is 588573.75\n",
            "in training loop, epoch 1, step 797, the loss is 909355.75\n",
            "in training loop, epoch 1, step 798, the loss is 622921.8125\n",
            "in training loop, epoch 1, step 799, the loss is 645909.5625\n",
            "in training loop, epoch 1, step 800, the loss is 431896.71875\n",
            "in training loop, epoch 1, step 801, the loss is 818670.125\n",
            "in training loop, epoch 1, step 802, the loss is 636538.3125\n",
            "in training loop, epoch 1, step 803, the loss is 702843.8125\n",
            "in training loop, epoch 1, step 804, the loss is 700130.625\n",
            "in training loop, epoch 1, step 805, the loss is 708522.375\n",
            "in training loop, epoch 1, step 806, the loss is 305805.6875\n",
            "in training loop, epoch 1, step 807, the loss is 399895.375\n",
            "in training loop, epoch 1, step 808, the loss is 840517.375\n",
            "in training loop, epoch 1, step 809, the loss is 701648.0625\n",
            "in training loop, epoch 1, step 810, the loss is 405428.53125\n",
            "in training loop, epoch 1, step 811, the loss is 1173289.25\n",
            "in training loop, epoch 1, step 812, the loss is 536034.25\n",
            "in training loop, epoch 1, step 813, the loss is 633003.75\n",
            "in training loop, epoch 1, step 814, the loss is 811337.0625\n",
            "in training loop, epoch 1, step 815, the loss is 499558.75\n",
            "in training loop, epoch 1, step 816, the loss is 538124.625\n",
            "in training loop, epoch 1, step 817, the loss is 883003.3125\n",
            "in training loop, epoch 1, step 818, the loss is 764283.1875\n",
            "in training loop, epoch 1, step 819, the loss is 729957.4375\n",
            "in training loop, epoch 1, step 820, the loss is 384454.03125\n",
            "in training loop, epoch 1, step 821, the loss is 533546.25\n",
            "in training loop, epoch 1, step 822, the loss is 346776.09375\n",
            "in training loop, epoch 1, step 823, the loss is 460045.9375\n",
            "in training loop, epoch 1, step 824, the loss is 475226.875\n",
            "in training loop, epoch 1, step 825, the loss is 831677.375\n",
            "in training loop, epoch 1, step 826, the loss is 808966.0\n",
            "in training loop, epoch 1, step 827, the loss is 663721.9375\n",
            "in training loop, epoch 1, step 828, the loss is 751844.5625\n",
            "in training loop, epoch 1, step 829, the loss is 681344.0\n",
            "in training loop, epoch 1, step 830, the loss is 519301.75\n",
            "in training loop, epoch 1, step 831, the loss is 434413.375\n",
            "in training loop, epoch 1, step 832, the loss is 693728.25\n",
            "in training loop, epoch 1, step 833, the loss is 868015.25\n",
            "in training loop, epoch 1, step 834, the loss is 397768.5\n",
            "in training loop, epoch 1, step 835, the loss is 412440.1875\n",
            "in training loop, epoch 1, step 836, the loss is 651646.75\n",
            "in training loop, epoch 1, step 837, the loss is 754121.3125\n",
            "in training loop, epoch 1, step 838, the loss is 592165.0625\n",
            "in training loop, epoch 1, step 839, the loss is 528511.25\n",
            "in training loop, epoch 1, step 840, the loss is 630659.0\n",
            "in training loop, epoch 1, step 841, the loss is 364780.09375\n",
            "in training loop, epoch 1, step 842, the loss is 428055.1875\n",
            "in training loop, epoch 1, step 843, the loss is 760302.4375\n",
            "in training loop, epoch 1, step 844, the loss is 709380.3125\n",
            "in training loop, epoch 1, step 845, the loss is 654142.0\n",
            "in training loop, epoch 1, step 846, the loss is 576105.1875\n",
            "in training loop, epoch 1, step 847, the loss is 583594.5625\n",
            "in training loop, epoch 1, step 848, the loss is 563210.375\n",
            "in training loop, epoch 1, step 849, the loss is 488497.28125\n",
            "in training loop, epoch 1, step 850, the loss is 764459.6875\n",
            "in training loop, epoch 1, step 851, the loss is 594374.75\n",
            "in training loop, epoch 1, step 852, the loss is 509993.71875\n",
            "in training loop, epoch 1, step 853, the loss is 520643.1875\n",
            "in training loop, epoch 1, step 854, the loss is 630713.5\n",
            "in training loop, epoch 1, step 855, the loss is 604690.875\n",
            "in training loop, epoch 1, step 856, the loss is 843098.75\n",
            "in training loop, epoch 1, step 857, the loss is 594577.0625\n",
            "in training loop, epoch 1, step 858, the loss is 407075.46875\n",
            "in training loop, epoch 1, step 859, the loss is 1062350.125\n",
            "in training loop, epoch 1, step 860, the loss is 559516.875\n",
            "in training loop, epoch 1, step 861, the loss is 756839.4375\n",
            "in training loop, epoch 1, step 862, the loss is 781472.875\n",
            "in training loop, epoch 1, step 863, the loss is 682803.875\n",
            "in training loop, epoch 1, step 864, the loss is 469239.65625\n",
            "in training loop, epoch 1, step 865, the loss is 642984.6875\n",
            "in training loop, epoch 1, step 866, the loss is 834275.75\n",
            "in training loop, epoch 1, step 867, the loss is 639665.75\n",
            "in training loop, epoch 1, step 868, the loss is 525775.8125\n",
            "in training loop, epoch 1, step 869, the loss is 950969.9375\n",
            "in training loop, epoch 1, step 870, the loss is 654751.6875\n",
            "in training loop, epoch 1, step 871, the loss is 889236.8125\n",
            "in training loop, epoch 1, step 872, the loss is 558914.4375\n",
            "in training loop, epoch 1, step 873, the loss is 511725.75\n",
            "in training loop, epoch 1, step 874, the loss is 595570.375\n",
            "in training loop, epoch 1, step 875, the loss is 430339.625\n",
            "in training loop, epoch 1, step 876, the loss is 631857.75\n",
            "in training loop, epoch 1, step 877, the loss is 524953.125\n",
            "in training loop, epoch 1, step 878, the loss is 464669.25\n",
            "in training loop, epoch 1, step 879, the loss is 653194.4375\n",
            "in training loop, epoch 1, step 880, the loss is 598074.8125\n",
            "in training loop, epoch 1, step 881, the loss is 723495.3125\n",
            "in training loop, epoch 1, step 882, the loss is 719011.1875\n",
            "in training loop, epoch 1, step 883, the loss is 950463.1875\n",
            "in training loop, epoch 1, step 884, the loss is 618987.6875\n",
            "in training loop, epoch 1, step 885, the loss is 625598.3125\n",
            "in training loop, epoch 1, step 886, the loss is 667251.25\n",
            "in training loop, epoch 1, step 887, the loss is 778963.3125\n",
            "in training loop, epoch 1, step 888, the loss is 513866.75\n",
            "in training loop, epoch 1, step 889, the loss is 1256344.125\n",
            "in training loop, epoch 1, step 890, the loss is 590544.3125\n",
            "in training loop, epoch 1, step 891, the loss is 459760.15625\n",
            "in training loop, epoch 1, step 892, the loss is 1039270.25\n",
            "in training loop, epoch 1, step 893, the loss is 832905.5625\n",
            "in training loop, epoch 1, step 894, the loss is 840366.125\n",
            "in training loop, epoch 1, step 895, the loss is 498744.84375\n",
            "in training loop, epoch 1, step 896, the loss is 615965.625\n",
            "in training loop, epoch 1, step 897, the loss is 429207.0625\n",
            "in training loop, epoch 1, step 898, the loss is 814617.125\n",
            "in training loop, epoch 1, step 899, the loss is 891599.4375\n",
            "in training loop, epoch 1, step 900, the loss is 589193.875\n",
            "in training loop, epoch 1, step 901, the loss is 658960.75\n",
            "in training loop, epoch 1, step 902, the loss is 871421.875\n",
            "in training loop, epoch 1, step 903, the loss is 193022.875\n",
            "k-fold 1:: Epoch 1: train loss 656765.8842125277 val loss 625557.6810024752\n",
            "in training loop, epoch 2, step 0, the loss is 640765.125\n",
            "in training loop, epoch 2, step 1, the loss is 470834.125\n",
            "in training loop, epoch 2, step 2, the loss is 512053.9375\n",
            "in training loop, epoch 2, step 3, the loss is 482627.34375\n",
            "in training loop, epoch 2, step 4, the loss is 682083.375\n",
            "in training loop, epoch 2, step 5, the loss is 605055.375\n",
            "in training loop, epoch 2, step 6, the loss is 784397.0\n",
            "in training loop, epoch 2, step 7, the loss is 539551.875\n",
            "in training loop, epoch 2, step 8, the loss is 467367.71875\n",
            "in training loop, epoch 2, step 9, the loss is 356369.40625\n",
            "in training loop, epoch 2, step 10, the loss is 541044.875\n",
            "in training loop, epoch 2, step 11, the loss is 659787.0625\n",
            "in training loop, epoch 2, step 12, the loss is 738832.1875\n",
            "in training loop, epoch 2, step 13, the loss is 557093.75\n",
            "in training loop, epoch 2, step 14, the loss is 611026.5\n",
            "in training loop, epoch 2, step 15, the loss is 592709.1875\n",
            "in training loop, epoch 2, step 16, the loss is 591134.0625\n",
            "in training loop, epoch 2, step 17, the loss is 351943.1875\n",
            "in training loop, epoch 2, step 18, the loss is 369400.09375\n",
            "in training loop, epoch 2, step 19, the loss is 616587.125\n",
            "in training loop, epoch 2, step 20, the loss is 504960.21875\n",
            "in training loop, epoch 2, step 21, the loss is 494140.53125\n",
            "in training loop, epoch 2, step 22, the loss is 448141.625\n",
            "in training loop, epoch 2, step 23, the loss is 400140.46875\n",
            "in training loop, epoch 2, step 24, the loss is 372606.5\n",
            "in training loop, epoch 2, step 25, the loss is 426632.46875\n",
            "in training loop, epoch 2, step 26, the loss is 466595.34375\n",
            "in training loop, epoch 2, step 27, the loss is 602452.375\n",
            "in training loop, epoch 2, step 28, the loss is 577477.8125\n",
            "in training loop, epoch 2, step 29, the loss is 724019.25\n",
            "in training loop, epoch 2, step 30, the loss is 615948.25\n",
            "in training loop, epoch 2, step 31, the loss is 574970.875\n",
            "in training loop, epoch 2, step 32, the loss is 526218.0\n",
            "in training loop, epoch 2, step 33, the loss is 693351.9375\n",
            "in training loop, epoch 2, step 34, the loss is 436999.15625\n",
            "in training loop, epoch 2, step 35, the loss is 591873.5\n",
            "in training loop, epoch 2, step 36, the loss is 378828.21875\n",
            "in training loop, epoch 2, step 37, the loss is 604333.875\n",
            "in training loop, epoch 2, step 38, the loss is 639050.625\n",
            "in training loop, epoch 2, step 39, the loss is 471548.6875\n",
            "in training loop, epoch 2, step 40, the loss is 517156.0\n",
            "in training loop, epoch 2, step 41, the loss is 342060.21875\n",
            "in training loop, epoch 2, step 42, the loss is 617412.9375\n",
            "in training loop, epoch 2, step 43, the loss is 434233.6875\n",
            "in training loop, epoch 2, step 44, the loss is 454955.5625\n",
            "in training loop, epoch 2, step 45, the loss is 401945.875\n",
            "in training loop, epoch 2, step 46, the loss is 428500.3125\n",
            "in training loop, epoch 2, step 47, the loss is 435252.84375\n",
            "in training loop, epoch 2, step 48, the loss is 774412.125\n",
            "in training loop, epoch 2, step 49, the loss is 451174.5\n",
            "in training loop, epoch 2, step 50, the loss is 444930.9375\n",
            "in training loop, epoch 2, step 51, the loss is 435671.4375\n",
            "in training loop, epoch 2, step 52, the loss is 404119.15625\n",
            "in training loop, epoch 2, step 53, the loss is 791826.0\n",
            "in training loop, epoch 2, step 54, the loss is 398853.875\n",
            "in training loop, epoch 2, step 55, the loss is 239960.125\n",
            "in training loop, epoch 2, step 56, the loss is 507649.0\n",
            "in training loop, epoch 2, step 57, the loss is 610396.75\n",
            "in training loop, epoch 2, step 58, the loss is 507086.53125\n",
            "in training loop, epoch 2, step 59, the loss is 807278.75\n",
            "in training loop, epoch 2, step 60, the loss is 412350.4375\n",
            "in training loop, epoch 2, step 61, the loss is 280306.71875\n",
            "in training loop, epoch 2, step 62, the loss is 298023.21875\n",
            "in training loop, epoch 2, step 63, the loss is 649352.625\n",
            "in training loop, epoch 2, step 64, the loss is 736102.3125\n",
            "in training loop, epoch 2, step 65, the loss is 345458.03125\n",
            "in training loop, epoch 2, step 66, the loss is 790001.0625\n",
            "in training loop, epoch 2, step 67, the loss is 631302.0625\n",
            "in training loop, epoch 2, step 68, the loss is 497316.09375\n",
            "in training loop, epoch 2, step 69, the loss is 498380.8125\n",
            "in training loop, epoch 2, step 70, the loss is 494421.59375\n",
            "in training loop, epoch 2, step 71, the loss is 599908.0\n",
            "in training loop, epoch 2, step 72, the loss is 500083.3125\n",
            "in training loop, epoch 2, step 73, the loss is 645926.25\n",
            "in training loop, epoch 2, step 74, the loss is 314436.5625\n",
            "in training loop, epoch 2, step 75, the loss is 600285.0625\n",
            "in training loop, epoch 2, step 76, the loss is 792764.75\n",
            "in training loop, epoch 2, step 77, the loss is 430680.0625\n",
            "in training loop, epoch 2, step 78, the loss is 659263.1875\n",
            "in training loop, epoch 2, step 79, the loss is 782892.6875\n",
            "in training loop, epoch 2, step 80, the loss is 507106.5\n",
            "in training loop, epoch 2, step 81, the loss is 420893.375\n",
            "in training loop, epoch 2, step 82, the loss is 730467.3125\n",
            "in training loop, epoch 2, step 83, the loss is 639058.3125\n",
            "in training loop, epoch 2, step 84, the loss is 492128.125\n",
            "in training loop, epoch 2, step 85, the loss is 483700.5625\n",
            "in training loop, epoch 2, step 86, the loss is 599139.0\n",
            "in training loop, epoch 2, step 87, the loss is 428395.09375\n",
            "in training loop, epoch 2, step 88, the loss is 556482.3125\n",
            "in training loop, epoch 2, step 89, the loss is 446789.0\n",
            "in training loop, epoch 2, step 90, the loss is 402094.65625\n",
            "in training loop, epoch 2, step 91, the loss is 699206.8125\n",
            "in training loop, epoch 2, step 92, the loss is 548884.875\n",
            "in training loop, epoch 2, step 93, the loss is 814908.0\n",
            "in training loop, epoch 2, step 94, the loss is 520103.625\n",
            "in training loop, epoch 2, step 95, the loss is 319082.375\n",
            "in training loop, epoch 2, step 96, the loss is 404095.0\n",
            "in training loop, epoch 2, step 97, the loss is 631513.625\n",
            "in training loop, epoch 2, step 98, the loss is 386732.375\n",
            "in training loop, epoch 2, step 99, the loss is 404075.6875\n",
            "in training loop, epoch 2, step 100, the loss is 344910.84375\n",
            "in training loop, epoch 2, step 101, the loss is 444532.53125\n",
            "in training loop, epoch 2, step 102, the loss is 516722.125\n",
            "in training loop, epoch 2, step 103, the loss is 557512.875\n",
            "in training loop, epoch 2, step 104, the loss is 644584.25\n",
            "in training loop, epoch 2, step 105, the loss is 392953.4375\n",
            "in training loop, epoch 2, step 106, the loss is 469334.40625\n",
            "in training loop, epoch 2, step 107, the loss is 635946.5\n",
            "in training loop, epoch 2, step 108, the loss is 433728.40625\n",
            "in training loop, epoch 2, step 109, the loss is 555313.6875\n",
            "in training loop, epoch 2, step 110, the loss is 315052.5625\n",
            "in training loop, epoch 2, step 111, the loss is 534993.5\n",
            "in training loop, epoch 2, step 112, the loss is 621724.375\n",
            "in training loop, epoch 2, step 113, the loss is 1041488.125\n",
            "in training loop, epoch 2, step 114, the loss is 706334.625\n",
            "in training loop, epoch 2, step 115, the loss is 611182.125\n",
            "in training loop, epoch 2, step 116, the loss is 487399.3125\n",
            "in training loop, epoch 2, step 117, the loss is 570556.125\n",
            "in training loop, epoch 2, step 118, the loss is 395199.90625\n",
            "in training loop, epoch 2, step 119, the loss is 388433.59375\n",
            "in training loop, epoch 2, step 120, the loss is 682311.5\n",
            "in training loop, epoch 2, step 121, the loss is 473892.3125\n",
            "in training loop, epoch 2, step 122, the loss is 452099.375\n",
            "in training loop, epoch 2, step 123, the loss is 719592.8125\n",
            "in training loop, epoch 2, step 124, the loss is 574676.5\n",
            "in training loop, epoch 2, step 125, the loss is 567391.625\n",
            "in training loop, epoch 2, step 126, the loss is 529223.5\n",
            "in training loop, epoch 2, step 127, the loss is 694258.25\n",
            "in training loop, epoch 2, step 128, the loss is 398324.125\n",
            "in training loop, epoch 2, step 129, the loss is 462945.03125\n",
            "in training loop, epoch 2, step 130, the loss is 799166.3125\n",
            "in training loop, epoch 2, step 131, the loss is 766617.5625\n",
            "in training loop, epoch 2, step 132, the loss is 724629.875\n",
            "in training loop, epoch 2, step 133, the loss is 445143.125\n",
            "in training loop, epoch 2, step 134, the loss is 730529.1875\n",
            "in training loop, epoch 2, step 135, the loss is 553782.125\n",
            "in training loop, epoch 2, step 136, the loss is 267131.40625\n",
            "in training loop, epoch 2, step 137, the loss is 718351.9375\n",
            "in training loop, epoch 2, step 138, the loss is 651140.875\n",
            "in training loop, epoch 2, step 139, the loss is 456982.0\n",
            "in training loop, epoch 2, step 140, the loss is 556854.625\n",
            "in training loop, epoch 2, step 141, the loss is 540303.625\n",
            "in training loop, epoch 2, step 142, the loss is 488232.4375\n",
            "in training loop, epoch 2, step 143, the loss is 549060.8125\n",
            "in training loop, epoch 2, step 144, the loss is 459717.09375\n",
            "in training loop, epoch 2, step 145, the loss is 312764.9375\n",
            "in training loop, epoch 2, step 146, the loss is 443108.8125\n",
            "in training loop, epoch 2, step 147, the loss is 714995.9375\n",
            "in training loop, epoch 2, step 148, the loss is 499437.4375\n",
            "in training loop, epoch 2, step 149, the loss is 648538.5625\n",
            "in training loop, epoch 2, step 150, the loss is 512463.71875\n",
            "in training loop, epoch 2, step 151, the loss is 677693.9375\n",
            "in training loop, epoch 2, step 152, the loss is 812495.25\n",
            "in training loop, epoch 2, step 153, the loss is 508493.1875\n",
            "in training loop, epoch 2, step 154, the loss is 800166.625\n",
            "in training loop, epoch 2, step 155, the loss is 577857.0625\n",
            "in training loop, epoch 2, step 156, the loss is 453699.625\n",
            "in training loop, epoch 2, step 157, the loss is 512854.375\n",
            "in training loop, epoch 2, step 158, the loss is 436234.8125\n",
            "in training loop, epoch 2, step 159, the loss is 460696.0\n",
            "in training loop, epoch 2, step 160, the loss is 786121.375\n",
            "in training loop, epoch 2, step 161, the loss is 570572.8125\n",
            "in training loop, epoch 2, step 162, the loss is 850712.625\n",
            "in training loop, epoch 2, step 163, the loss is 429091.8125\n",
            "in training loop, epoch 2, step 164, the loss is 442712.96875\n",
            "in training loop, epoch 2, step 165, the loss is 725252.5\n",
            "in training loop, epoch 2, step 166, the loss is 629102.75\n",
            "in training loop, epoch 2, step 167, the loss is 586464.875\n",
            "in training loop, epoch 2, step 168, the loss is 451019.1875\n",
            "in training loop, epoch 2, step 169, the loss is 594827.875\n",
            "in training loop, epoch 2, step 170, the loss is 859789.875\n",
            "in training loop, epoch 2, step 171, the loss is 724743.125\n",
            "in training loop, epoch 2, step 172, the loss is 679045.375\n",
            "in training loop, epoch 2, step 173, the loss is 425268.28125\n",
            "in training loop, epoch 2, step 174, the loss is 324192.3125\n",
            "in training loop, epoch 2, step 175, the loss is 872846.9375\n",
            "in training loop, epoch 2, step 176, the loss is 662138.875\n",
            "in training loop, epoch 2, step 177, the loss is 252421.53125\n",
            "in training loop, epoch 2, step 178, the loss is 735757.75\n",
            "in training loop, epoch 2, step 179, the loss is 527506.875\n",
            "in training loop, epoch 2, step 180, the loss is 608875.9375\n",
            "in training loop, epoch 2, step 181, the loss is 531756.875\n",
            "in training loop, epoch 2, step 182, the loss is 571863.4375\n",
            "in training loop, epoch 2, step 183, the loss is 524453.9375\n",
            "in training loop, epoch 2, step 184, the loss is 355750.25\n",
            "in training loop, epoch 2, step 185, the loss is 574446.4375\n",
            "in training loop, epoch 2, step 186, the loss is 533570.125\n",
            "in training loop, epoch 2, step 187, the loss is 453267.15625\n",
            "in training loop, epoch 2, step 188, the loss is 325924.5\n",
            "in training loop, epoch 2, step 189, the loss is 676245.0\n",
            "in training loop, epoch 2, step 190, the loss is 577476.5\n",
            "in training loop, epoch 2, step 191, the loss is 776988.875\n",
            "in training loop, epoch 2, step 192, the loss is 567545.5625\n",
            "in training loop, epoch 2, step 193, the loss is 405103.9375\n",
            "in training loop, epoch 2, step 194, the loss is 716404.0\n",
            "in training loop, epoch 2, step 195, the loss is 483432.46875\n",
            "in training loop, epoch 2, step 196, the loss is 541436.375\n",
            "in training loop, epoch 2, step 197, the loss is 646922.25\n",
            "in training loop, epoch 2, step 198, the loss is 560252.0625\n",
            "in training loop, epoch 2, step 199, the loss is 456029.53125\n",
            "in training loop, epoch 2, step 200, the loss is 438145.25\n",
            "in training loop, epoch 2, step 201, the loss is 626632.25\n",
            "in training loop, epoch 2, step 202, the loss is 505100.9375\n",
            "in training loop, epoch 2, step 203, the loss is 873816.5\n",
            "in training loop, epoch 2, step 204, the loss is 449901.3125\n",
            "in training loop, epoch 2, step 205, the loss is 563354.75\n",
            "in training loop, epoch 2, step 206, the loss is 465641.625\n",
            "in training loop, epoch 2, step 207, the loss is 521012.84375\n",
            "in training loop, epoch 2, step 208, the loss is 546587.1875\n",
            "in training loop, epoch 2, step 209, the loss is 859766.125\n",
            "in training loop, epoch 2, step 210, the loss is 314418.4375\n",
            "in training loop, epoch 2, step 211, the loss is 487486.78125\n",
            "in training loop, epoch 2, step 212, the loss is 506114.75\n",
            "in training loop, epoch 2, step 213, the loss is 830132.9375\n",
            "in training loop, epoch 2, step 214, the loss is 701636.8125\n",
            "in training loop, epoch 2, step 215, the loss is 626745.625\n",
            "in training loop, epoch 2, step 216, the loss is 473807.5625\n",
            "in training loop, epoch 2, step 217, the loss is 244699.28125\n",
            "in training loop, epoch 2, step 218, the loss is 565384.625\n",
            "in training loop, epoch 2, step 219, the loss is 502735.46875\n",
            "in training loop, epoch 2, step 220, the loss is 611069.9375\n",
            "in training loop, epoch 2, step 221, the loss is 521282.8125\n",
            "in training loop, epoch 2, step 222, the loss is 573599.375\n",
            "in training loop, epoch 2, step 223, the loss is 771423.625\n",
            "in training loop, epoch 2, step 224, the loss is 738779.875\n",
            "in training loop, epoch 2, step 225, the loss is 548424.875\n",
            "in training loop, epoch 2, step 226, the loss is 533621.875\n",
            "in training loop, epoch 2, step 227, the loss is 359498.125\n",
            "in training loop, epoch 2, step 228, the loss is 549347.5625\n",
            "in training loop, epoch 2, step 229, the loss is 750389.5625\n",
            "in training loop, epoch 2, step 230, the loss is 764368.75\n",
            "in training loop, epoch 2, step 231, the loss is 525459.75\n",
            "in training loop, epoch 2, step 232, the loss is 444882.34375\n",
            "in training loop, epoch 2, step 233, the loss is 488892.90625\n",
            "in training loop, epoch 2, step 234, the loss is 435554.53125\n",
            "in training loop, epoch 2, step 235, the loss is 644609.125\n",
            "in training loop, epoch 2, step 236, the loss is 544468.75\n",
            "in training loop, epoch 2, step 237, the loss is 384873.125\n",
            "in training loop, epoch 2, step 238, the loss is 548743.25\n",
            "in training loop, epoch 2, step 239, the loss is 269928.125\n",
            "in training loop, epoch 2, step 240, the loss is 595636.75\n",
            "in training loop, epoch 2, step 241, the loss is 525226.5625\n",
            "in training loop, epoch 2, step 242, the loss is 767267.25\n",
            "in training loop, epoch 2, step 243, the loss is 612866.0\n",
            "in training loop, epoch 2, step 244, the loss is 547786.375\n",
            "in training loop, epoch 2, step 245, the loss is 417988.78125\n",
            "in training loop, epoch 2, step 246, the loss is 414078.59375\n",
            "in training loop, epoch 2, step 247, the loss is 764782.875\n",
            "in training loop, epoch 2, step 248, the loss is 356983.5\n",
            "in training loop, epoch 2, step 249, the loss is 471297.0\n",
            "in training loop, epoch 2, step 250, the loss is 318638.84375\n",
            "in training loop, epoch 2, step 251, the loss is 725277.75\n",
            "in training loop, epoch 2, step 252, the loss is 390486.3125\n",
            "in training loop, epoch 2, step 253, the loss is 498611.84375\n",
            "in training loop, epoch 2, step 254, the loss is 758459.0625\n",
            "in training loop, epoch 2, step 255, the loss is 575237.5\n",
            "in training loop, epoch 2, step 256, the loss is 613457.1875\n",
            "in training loop, epoch 2, step 257, the loss is 602724.6875\n",
            "in training loop, epoch 2, step 258, the loss is 487478.53125\n",
            "in training loop, epoch 2, step 259, the loss is 456923.28125\n",
            "in training loop, epoch 2, step 260, the loss is 841633.375\n",
            "in training loop, epoch 2, step 261, the loss is 482421.3125\n",
            "in training loop, epoch 2, step 262, the loss is 338114.6875\n",
            "in training loop, epoch 2, step 263, the loss is 621006.875\n",
            "in training loop, epoch 2, step 264, the loss is 739730.3125\n",
            "in training loop, epoch 2, step 265, the loss is 691964.75\n",
            "in training loop, epoch 2, step 266, the loss is 366178.6875\n",
            "in training loop, epoch 2, step 267, the loss is 249153.578125\n",
            "in training loop, epoch 2, step 268, the loss is 411273.6875\n",
            "in training loop, epoch 2, step 269, the loss is 685918.625\n",
            "in training loop, epoch 2, step 270, the loss is 272472.28125\n",
            "in training loop, epoch 2, step 271, the loss is 431885.59375\n",
            "in training loop, epoch 2, step 272, the loss is 580265.3125\n",
            "in training loop, epoch 2, step 273, the loss is 614773.1875\n",
            "in training loop, epoch 2, step 274, the loss is 351004.0\n",
            "in training loop, epoch 2, step 275, the loss is 732091.1875\n",
            "in training loop, epoch 2, step 276, the loss is 805859.5\n",
            "in training loop, epoch 2, step 277, the loss is 549797.5625\n",
            "in training loop, epoch 2, step 278, the loss is 622316.5\n",
            "in training loop, epoch 2, step 279, the loss is 318707.4375\n",
            "in training loop, epoch 2, step 280, the loss is 568563.1875\n",
            "in training loop, epoch 2, step 281, the loss is 534945.1875\n",
            "in training loop, epoch 2, step 282, the loss is 634511.625\n",
            "in training loop, epoch 2, step 283, the loss is 904302.0\n",
            "in training loop, epoch 2, step 284, the loss is 860330.9375\n",
            "in training loop, epoch 2, step 285, the loss is 491422.71875\n",
            "in training loop, epoch 2, step 286, the loss is 468088.84375\n",
            "in training loop, epoch 2, step 287, the loss is 526777.5625\n",
            "in training loop, epoch 2, step 288, the loss is 520265.0625\n",
            "in training loop, epoch 2, step 289, the loss is 372160.09375\n",
            "in training loop, epoch 2, step 290, the loss is 502902.375\n",
            "in training loop, epoch 2, step 291, the loss is 701748.6875\n",
            "in training loop, epoch 2, step 292, the loss is 541256.5\n",
            "in training loop, epoch 2, step 293, the loss is 1089509.0\n",
            "in training loop, epoch 2, step 294, the loss is 1051660.75\n",
            "in training loop, epoch 2, step 295, the loss is 608652.5\n",
            "in training loop, epoch 2, step 296, the loss is 345684.59375\n",
            "in training loop, epoch 2, step 297, the loss is 536481.9375\n",
            "in training loop, epoch 2, step 298, the loss is 556875.5625\n",
            "in training loop, epoch 2, step 299, the loss is 460204.71875\n",
            "in training loop, epoch 2, step 300, the loss is 515387.75\n",
            "in training loop, epoch 2, step 301, the loss is 357482.6875\n",
            "in training loop, epoch 2, step 302, the loss is 932377.1875\n",
            "in training loop, epoch 2, step 303, the loss is 645720.3125\n",
            "in training loop, epoch 2, step 304, the loss is 702738.375\n",
            "in training loop, epoch 2, step 305, the loss is 693837.0625\n",
            "in training loop, epoch 2, step 306, the loss is 629399.75\n",
            "in training loop, epoch 2, step 307, the loss is 648873.0\n",
            "in training loop, epoch 2, step 308, the loss is 779469.75\n",
            "in training loop, epoch 2, step 309, the loss is 544515.25\n",
            "in training loop, epoch 2, step 310, the loss is 715564.875\n",
            "in training loop, epoch 2, step 311, the loss is 555971.125\n",
            "in training loop, epoch 2, step 312, the loss is 323966.8125\n",
            "in training loop, epoch 2, step 313, the loss is 853844.625\n",
            "in training loop, epoch 2, step 314, the loss is 617337.5\n",
            "in training loop, epoch 2, step 315, the loss is 563097.0\n",
            "in training loop, epoch 2, step 316, the loss is 574761.0\n",
            "in training loop, epoch 2, step 317, the loss is 552938.0625\n",
            "in training loop, epoch 2, step 318, the loss is 546890.5625\n",
            "in training loop, epoch 2, step 319, the loss is 557248.75\n",
            "in training loop, epoch 2, step 320, the loss is 460903.71875\n",
            "in training loop, epoch 2, step 321, the loss is 538212.9375\n",
            "in training loop, epoch 2, step 322, the loss is 354594.9375\n",
            "in training loop, epoch 2, step 323, the loss is 855479.0\n",
            "in training loop, epoch 2, step 324, the loss is 551750.125\n",
            "in training loop, epoch 2, step 325, the loss is 641130.125\n",
            "in training loop, epoch 2, step 326, the loss is 730815.75\n",
            "in training loop, epoch 2, step 327, the loss is 397523.9375\n",
            "in training loop, epoch 2, step 328, the loss is 507863.0625\n",
            "in training loop, epoch 2, step 329, the loss is 1000643.875\n",
            "in training loop, epoch 2, step 330, the loss is 668625.9375\n",
            "in training loop, epoch 2, step 331, the loss is 662827.0625\n",
            "in training loop, epoch 2, step 332, the loss is 599376.0\n",
            "in training loop, epoch 2, step 333, the loss is 580679.8125\n",
            "in training loop, epoch 2, step 334, the loss is 681980.6875\n",
            "in training loop, epoch 2, step 335, the loss is 295551.1875\n",
            "in training loop, epoch 2, step 336, the loss is 528206.3125\n",
            "in training loop, epoch 2, step 337, the loss is 408379.65625\n",
            "in training loop, epoch 2, step 338, the loss is 962640.3125\n",
            "in training loop, epoch 2, step 339, the loss is 400287.1875\n",
            "in training loop, epoch 2, step 340, the loss is 555598.875\n",
            "in training loop, epoch 2, step 341, the loss is 498264.625\n",
            "in training loop, epoch 2, step 342, the loss is 623045.6875\n",
            "in training loop, epoch 2, step 343, the loss is 559941.9375\n",
            "in training loop, epoch 2, step 344, the loss is 568764.0625\n",
            "in training loop, epoch 2, step 345, the loss is 378693.28125\n",
            "in training loop, epoch 2, step 346, the loss is 425724.15625\n",
            "in training loop, epoch 2, step 347, the loss is 666716.0625\n",
            "in training loop, epoch 2, step 348, the loss is 642355.875\n",
            "in training loop, epoch 2, step 349, the loss is 550273.6875\n",
            "in training loop, epoch 2, step 350, the loss is 657694.25\n",
            "in training loop, epoch 2, step 351, the loss is 469861.8125\n",
            "in training loop, epoch 2, step 352, the loss is 620514.0625\n",
            "in training loop, epoch 2, step 353, the loss is 477103.84375\n",
            "in training loop, epoch 2, step 354, the loss is 790674.25\n",
            "in training loop, epoch 2, step 355, the loss is 675921.1875\n",
            "in training loop, epoch 2, step 356, the loss is 657697.875\n",
            "in training loop, epoch 2, step 357, the loss is 287483.71875\n",
            "in training loop, epoch 2, step 358, the loss is 347826.15625\n",
            "in training loop, epoch 2, step 359, the loss is 742515.625\n",
            "in training loop, epoch 2, step 360, the loss is 880969.1875\n",
            "in training loop, epoch 2, step 361, the loss is 593466.3125\n",
            "in training loop, epoch 2, step 362, the loss is 961979.5625\n",
            "in training loop, epoch 2, step 363, the loss is 428488.15625\n",
            "in training loop, epoch 2, step 364, the loss is 1009312.875\n",
            "in training loop, epoch 2, step 365, the loss is 642785.5\n",
            "in training loop, epoch 2, step 366, the loss is 924951.5\n",
            "in training loop, epoch 2, step 367, the loss is 827543.9375\n",
            "in training loop, epoch 2, step 368, the loss is 833299.3125\n",
            "in training loop, epoch 2, step 369, the loss is 365786.21875\n",
            "in training loop, epoch 2, step 370, the loss is 842677.125\n",
            "in training loop, epoch 2, step 371, the loss is 784653.375\n",
            "in training loop, epoch 2, step 372, the loss is 716274.25\n",
            "in training loop, epoch 2, step 373, the loss is 929471.25\n",
            "in training loop, epoch 2, step 374, the loss is 615731.6875\n",
            "in training loop, epoch 2, step 375, the loss is 472248.1875\n",
            "in training loop, epoch 2, step 376, the loss is 645108.75\n",
            "in training loop, epoch 2, step 377, the loss is 642250.5\n",
            "in training loop, epoch 2, step 378, the loss is 1441612.375\n",
            "in training loop, epoch 2, step 379, the loss is 379970.34375\n",
            "in training loop, epoch 2, step 380, the loss is 589801.4375\n",
            "in training loop, epoch 2, step 381, the loss is 315131.15625\n",
            "in training loop, epoch 2, step 382, the loss is 1160868.625\n",
            "in training loop, epoch 2, step 383, the loss is 679899.9375\n",
            "in training loop, epoch 2, step 384, the loss is 728806.125\n",
            "in training loop, epoch 2, step 385, the loss is 966362.9375\n",
            "in training loop, epoch 2, step 386, the loss is 988962.875\n",
            "in training loop, epoch 2, step 387, the loss is 1026296.5\n",
            "in training loop, epoch 2, step 388, the loss is 805884.4375\n",
            "in training loop, epoch 2, step 389, the loss is 688253.0625\n",
            "in training loop, epoch 2, step 390, the loss is 725863.875\n",
            "in training loop, epoch 2, step 391, the loss is 799863.0625\n",
            "in training loop, epoch 2, step 392, the loss is 806724.875\n",
            "in training loop, epoch 2, step 393, the loss is 1010294.4375\n",
            "in training loop, epoch 2, step 394, the loss is 359570.40625\n",
            "in training loop, epoch 2, step 395, the loss is 765759.3125\n",
            "in training loop, epoch 2, step 396, the loss is 744374.6875\n",
            "in training loop, epoch 2, step 397, the loss is 639657.75\n",
            "in training loop, epoch 2, step 398, the loss is 787280.375\n",
            "in training loop, epoch 2, step 399, the loss is 620606.5\n",
            "in training loop, epoch 2, step 400, the loss is 578067.625\n",
            "in training loop, epoch 2, step 401, the loss is 791612.3125\n",
            "in training loop, epoch 2, step 402, the loss is 612008.125\n",
            "in training loop, epoch 2, step 403, the loss is 958253.9375\n",
            "in training loop, epoch 2, step 404, the loss is 505395.1875\n",
            "in training loop, epoch 2, step 405, the loss is 617113.75\n",
            "in training loop, epoch 2, step 406, the loss is 807110.875\n",
            "in training loop, epoch 2, step 407, the loss is 822431.375\n",
            "in training loop, epoch 2, step 408, the loss is 812931.1875\n",
            "in training loop, epoch 2, step 409, the loss is 1061673.375\n",
            "in training loop, epoch 2, step 410, the loss is 619906.9375\n",
            "in training loop, epoch 2, step 411, the loss is 546547.125\n",
            "in training loop, epoch 2, step 412, the loss is 697600.5625\n",
            "in training loop, epoch 2, step 413, the loss is 532534.875\n",
            "in training loop, epoch 2, step 414, the loss is 965717.1875\n",
            "in training loop, epoch 2, step 415, the loss is 402089.0\n",
            "in training loop, epoch 2, step 416, the loss is 573993.0625\n",
            "in training loop, epoch 2, step 417, the loss is 520464.4375\n",
            "in training loop, epoch 2, step 418, the loss is 640642.0625\n",
            "in training loop, epoch 2, step 419, the loss is 788692.625\n",
            "in training loop, epoch 2, step 420, the loss is 462676.3125\n",
            "in training loop, epoch 2, step 421, the loss is 711663.9375\n",
            "in training loop, epoch 2, step 422, the loss is 675665.875\n",
            "in training loop, epoch 2, step 423, the loss is 924124.875\n",
            "in training loop, epoch 2, step 424, the loss is 1256032.375\n",
            "in training loop, epoch 2, step 425, the loss is 706100.625\n",
            "in training loop, epoch 2, step 426, the loss is 517605.1875\n",
            "in training loop, epoch 2, step 427, the loss is 687714.6875\n",
            "in training loop, epoch 2, step 428, the loss is 481447.8125\n",
            "in training loop, epoch 2, step 429, the loss is 714895.0625\n",
            "in training loop, epoch 2, step 430, the loss is 468370.6875\n",
            "in training loop, epoch 2, step 431, the loss is 693931.0625\n",
            "in training loop, epoch 2, step 432, the loss is 487827.6875\n",
            "in training loop, epoch 2, step 433, the loss is 778692.875\n",
            "in training loop, epoch 2, step 434, the loss is 483901.0625\n",
            "in training loop, epoch 2, step 435, the loss is 460731.96875\n",
            "in training loop, epoch 2, step 436, the loss is 346633.125\n",
            "in training loop, epoch 2, step 437, the loss is 815792.0625\n",
            "in training loop, epoch 2, step 438, the loss is 376695.125\n",
            "in training loop, epoch 2, step 439, the loss is 658212.1875\n",
            "in training loop, epoch 2, step 440, the loss is 625812.8125\n",
            "in training loop, epoch 2, step 441, the loss is 549422.75\n",
            "in training loop, epoch 2, step 442, the loss is 641842.875\n",
            "in training loop, epoch 2, step 443, the loss is 598252.75\n",
            "in training loop, epoch 2, step 444, the loss is 671173.6875\n",
            "in training loop, epoch 2, step 445, the loss is 511966.5\n",
            "in training loop, epoch 2, step 446, the loss is 728426.6875\n",
            "in training loop, epoch 2, step 447, the loss is 1186229.75\n",
            "in training loop, epoch 2, step 448, the loss is 481554.75\n",
            "in training loop, epoch 2, step 449, the loss is 676410.625\n",
            "in training loop, epoch 2, step 450, the loss is 520413.46875\n",
            "in training loop, epoch 2, step 451, the loss is 705978.3125\n",
            "in training loop, epoch 2, step 452, the loss is 561435.4375\n",
            "in training loop, epoch 2, step 453, the loss is 667961.625\n",
            "in training loop, epoch 2, step 454, the loss is 823804.25\n",
            "in training loop, epoch 2, step 455, the loss is 599102.9375\n",
            "in training loop, epoch 2, step 456, the loss is 351670.75\n",
            "in training loop, epoch 2, step 457, the loss is 543833.6875\n",
            "in training loop, epoch 2, step 458, the loss is 456435.90625\n",
            "in training loop, epoch 2, step 459, the loss is 556877.9375\n",
            "in training loop, epoch 2, step 460, the loss is 717023.5\n",
            "in training loop, epoch 2, step 461, the loss is 791827.1875\n",
            "in training loop, epoch 2, step 462, the loss is 584240.25\n",
            "in training loop, epoch 2, step 463, the loss is 981660.8125\n",
            "in training loop, epoch 2, step 464, the loss is 336562.90625\n",
            "in training loop, epoch 2, step 465, the loss is 930182.125\n",
            "in training loop, epoch 2, step 466, the loss is 605410.75\n",
            "in training loop, epoch 2, step 467, the loss is 576099.375\n",
            "in training loop, epoch 2, step 468, the loss is 692361.3125\n",
            "in training loop, epoch 2, step 469, the loss is 682950.9375\n",
            "in training loop, epoch 2, step 470, the loss is 392172.4375\n",
            "in training loop, epoch 2, step 471, the loss is 620881.5\n",
            "in training loop, epoch 2, step 472, the loss is 722822.125\n",
            "in training loop, epoch 2, step 473, the loss is 687138.875\n",
            "in training loop, epoch 2, step 474, the loss is 639469.625\n",
            "in training loop, epoch 2, step 475, the loss is 753446.4375\n",
            "in training loop, epoch 2, step 476, the loss is 720518.75\n",
            "in training loop, epoch 2, step 477, the loss is 806369.9375\n",
            "in training loop, epoch 2, step 478, the loss is 451044.25\n",
            "in training loop, epoch 2, step 479, the loss is 733438.6875\n",
            "in training loop, epoch 2, step 480, the loss is 570640.5\n",
            "in training loop, epoch 2, step 481, the loss is 482710.8125\n",
            "in training loop, epoch 2, step 482, the loss is 836333.1875\n",
            "in training loop, epoch 2, step 483, the loss is 429685.15625\n",
            "in training loop, epoch 2, step 484, the loss is 373660.96875\n",
            "in training loop, epoch 2, step 485, the loss is 807323.3125\n",
            "in training loop, epoch 2, step 486, the loss is 612766.5\n",
            "in training loop, epoch 2, step 487, the loss is 408326.40625\n",
            "in training loop, epoch 2, step 488, the loss is 690798.5625\n",
            "in training loop, epoch 2, step 489, the loss is 777362.125\n",
            "in training loop, epoch 2, step 490, the loss is 440114.6875\n",
            "in training loop, epoch 2, step 491, the loss is 650155.5\n",
            "in training loop, epoch 2, step 492, the loss is 287801.15625\n",
            "in training loop, epoch 2, step 493, the loss is 513138.0625\n",
            "in training loop, epoch 2, step 494, the loss is 295415.25\n",
            "in training loop, epoch 2, step 495, the loss is 590180.375\n",
            "in training loop, epoch 2, step 496, the loss is 508777.25\n",
            "in training loop, epoch 2, step 497, the loss is 511694.65625\n",
            "in training loop, epoch 2, step 498, the loss is 423038.375\n",
            "in training loop, epoch 2, step 499, the loss is 543998.0\n",
            "in training loop, epoch 2, step 500, the loss is 714110.3125\n",
            "in training loop, epoch 2, step 501, the loss is 956325.0\n",
            "in training loop, epoch 2, step 502, the loss is 617285.75\n",
            "in training loop, epoch 2, step 503, the loss is 682556.1875\n",
            "in training loop, epoch 2, step 504, the loss is 643623.0\n",
            "in training loop, epoch 2, step 505, the loss is 680156.0625\n",
            "in training loop, epoch 2, step 506, the loss is 532567.5\n",
            "in training loop, epoch 2, step 507, the loss is 342119.15625\n",
            "in training loop, epoch 2, step 508, the loss is 499576.25\n",
            "in training loop, epoch 2, step 509, the loss is 659281.75\n",
            "in training loop, epoch 2, step 510, the loss is 447378.71875\n",
            "in training loop, epoch 2, step 511, the loss is 435265.875\n",
            "in training loop, epoch 2, step 512, the loss is 619590.5\n",
            "in training loop, epoch 2, step 513, the loss is 504305.125\n",
            "in training loop, epoch 2, step 514, the loss is 494350.9375\n",
            "in training loop, epoch 2, step 515, the loss is 494085.78125\n",
            "in training loop, epoch 2, step 516, the loss is 647628.375\n",
            "in training loop, epoch 2, step 517, the loss is 666824.6875\n",
            "in training loop, epoch 2, step 518, the loss is 846985.5625\n",
            "in training loop, epoch 2, step 519, the loss is 453092.9375\n",
            "in training loop, epoch 2, step 520, the loss is 494106.28125\n",
            "in training loop, epoch 2, step 521, the loss is 361510.875\n",
            "in training loop, epoch 2, step 522, the loss is 460592.6875\n",
            "in training loop, epoch 2, step 523, the loss is 467678.9375\n",
            "in training loop, epoch 2, step 524, the loss is 655524.8125\n",
            "in training loop, epoch 2, step 525, the loss is 603144.3125\n",
            "in training loop, epoch 2, step 526, the loss is 688662.375\n",
            "in training loop, epoch 2, step 527, the loss is 479721.8125\n",
            "in training loop, epoch 2, step 528, the loss is 660816.8125\n",
            "in training loop, epoch 2, step 529, the loss is 665890.125\n",
            "in training loop, epoch 2, step 530, the loss is 635181.5625\n",
            "in training loop, epoch 2, step 531, the loss is 811460.0625\n",
            "in training loop, epoch 2, step 532, the loss is 493213.1875\n",
            "in training loop, epoch 2, step 533, the loss is 800977.875\n",
            "in training loop, epoch 2, step 534, the loss is 497549.25\n",
            "in training loop, epoch 2, step 535, the loss is 252093.09375\n",
            "in training loop, epoch 2, step 536, the loss is 730418.8125\n",
            "in training loop, epoch 2, step 537, the loss is 745693.625\n",
            "in training loop, epoch 2, step 538, the loss is 458158.03125\n",
            "in training loop, epoch 2, step 539, the loss is 589639.6875\n",
            "in training loop, epoch 2, step 540, the loss is 524921.9375\n",
            "in training loop, epoch 2, step 541, the loss is 562971.625\n",
            "in training loop, epoch 2, step 542, the loss is 584056.0\n",
            "in training loop, epoch 2, step 543, the loss is 754292.125\n",
            "in training loop, epoch 2, step 544, the loss is 628245.5625\n",
            "in training loop, epoch 2, step 545, the loss is 468392.6875\n",
            "in training loop, epoch 2, step 546, the loss is 673291.3125\n",
            "in training loop, epoch 2, step 547, the loss is 373234.25\n",
            "in training loop, epoch 2, step 548, the loss is 447290.0625\n",
            "in training loop, epoch 2, step 549, the loss is 831154.75\n",
            "in training loop, epoch 2, step 550, the loss is 515019.0625\n",
            "in training loop, epoch 2, step 551, the loss is 606458.125\n",
            "in training loop, epoch 2, step 552, the loss is 515808.40625\n",
            "in training loop, epoch 2, step 553, the loss is 815087.6875\n",
            "in training loop, epoch 2, step 554, the loss is 651150.1875\n",
            "in training loop, epoch 2, step 555, the loss is 747906.125\n",
            "in training loop, epoch 2, step 556, the loss is 338739.8125\n",
            "in training loop, epoch 2, step 557, the loss is 491778.96875\n",
            "in training loop, epoch 2, step 558, the loss is 553223.6875\n",
            "in training loop, epoch 2, step 559, the loss is 652550.75\n",
            "in training loop, epoch 2, step 560, the loss is 434633.96875\n",
            "in training loop, epoch 2, step 561, the loss is 589808.5\n",
            "in training loop, epoch 2, step 562, the loss is 703804.25\n",
            "in training loop, epoch 2, step 563, the loss is 553396.3125\n",
            "in training loop, epoch 2, step 564, the loss is 688547.9375\n",
            "in training loop, epoch 2, step 565, the loss is 572773.75\n",
            "in training loop, epoch 2, step 566, the loss is 558949.375\n",
            "in training loop, epoch 2, step 567, the loss is 521651.53125\n",
            "in training loop, epoch 2, step 568, the loss is 638933.875\n",
            "in training loop, epoch 2, step 569, the loss is 712563.8125\n",
            "in training loop, epoch 2, step 570, the loss is 538836.0\n",
            "in training loop, epoch 2, step 571, the loss is 389464.0625\n",
            "in training loop, epoch 2, step 572, the loss is 592120.625\n",
            "in training loop, epoch 2, step 573, the loss is 394100.96875\n",
            "in training loop, epoch 2, step 574, the loss is 491794.96875\n",
            "in training loop, epoch 2, step 575, the loss is 381262.71875\n",
            "in training loop, epoch 2, step 576, the loss is 534106.5625\n",
            "in training loop, epoch 2, step 577, the loss is 438715.9375\n",
            "in training loop, epoch 2, step 578, the loss is 941138.9375\n",
            "in training loop, epoch 2, step 579, the loss is 550722.375\n",
            "in training loop, epoch 2, step 580, the loss is 242808.3125\n",
            "in training loop, epoch 2, step 581, the loss is 408121.5\n",
            "in training loop, epoch 2, step 582, the loss is 692763.125\n",
            "in training loop, epoch 2, step 583, the loss is 682095.0625\n",
            "in training loop, epoch 2, step 584, the loss is 625318.875\n",
            "in training loop, epoch 2, step 585, the loss is 623659.25\n",
            "in training loop, epoch 2, step 586, the loss is 613573.4375\n",
            "in training loop, epoch 2, step 587, the loss is 549955.875\n",
            "in training loop, epoch 2, step 588, the loss is 1177084.125\n",
            "in training loop, epoch 2, step 589, the loss is 467568.65625\n",
            "in training loop, epoch 2, step 590, the loss is 538310.375\n",
            "in training loop, epoch 2, step 591, the loss is 654858.0\n",
            "in training loop, epoch 2, step 592, the loss is 836058.875\n",
            "in training loop, epoch 2, step 593, the loss is 751551.1875\n",
            "in training loop, epoch 2, step 594, the loss is 702149.8125\n",
            "in training loop, epoch 2, step 595, the loss is 786171.3125\n",
            "in training loop, epoch 2, step 596, the loss is 553736.3125\n",
            "in training loop, epoch 2, step 597, the loss is 725011.8125\n",
            "in training loop, epoch 2, step 598, the loss is 684685.75\n",
            "in training loop, epoch 2, step 599, the loss is 538997.5625\n",
            "in training loop, epoch 2, step 600, the loss is 856775.0625\n",
            "in training loop, epoch 2, step 601, the loss is 397213.125\n",
            "in training loop, epoch 2, step 602, the loss is 931146.9375\n",
            "in training loop, epoch 2, step 603, the loss is 657551.625\n",
            "in training loop, epoch 2, step 604, the loss is 741517.75\n",
            "in training loop, epoch 2, step 605, the loss is 533041.4375\n",
            "in training loop, epoch 2, step 606, the loss is 767467.875\n",
            "in training loop, epoch 2, step 607, the loss is 464483.1875\n",
            "in training loop, epoch 2, step 608, the loss is 761934.8125\n",
            "in training loop, epoch 2, step 609, the loss is 688696.875\n",
            "in training loop, epoch 2, step 610, the loss is 658729.375\n",
            "in training loop, epoch 2, step 611, the loss is 580156.3125\n",
            "in training loop, epoch 2, step 612, the loss is 1209129.75\n",
            "in training loop, epoch 2, step 613, the loss is 588230.875\n",
            "in training loop, epoch 2, step 614, the loss is 656412.5625\n",
            "in training loop, epoch 2, step 615, the loss is 649090.375\n",
            "in training loop, epoch 2, step 616, the loss is 748594.625\n",
            "in training loop, epoch 2, step 617, the loss is 796042.625\n",
            "in training loop, epoch 2, step 618, the loss is 354406.375\n",
            "in training loop, epoch 2, step 619, the loss is 850108.1875\n",
            "in training loop, epoch 2, step 620, the loss is 830482.25\n",
            "in training loop, epoch 2, step 621, the loss is 724335.0\n",
            "in training loop, epoch 2, step 622, the loss is 660856.0\n",
            "in training loop, epoch 2, step 623, the loss is 569643.0\n",
            "in training loop, epoch 2, step 624, the loss is 316521.5625\n",
            "in training loop, epoch 2, step 625, the loss is 454675.40625\n",
            "in training loop, epoch 2, step 626, the loss is 457123.1875\n",
            "in training loop, epoch 2, step 627, the loss is 875809.375\n",
            "in training loop, epoch 2, step 628, the loss is 364014.25\n",
            "in training loop, epoch 2, step 629, the loss is 599212.25\n",
            "in training loop, epoch 2, step 630, the loss is 488781.53125\n",
            "in training loop, epoch 2, step 631, the loss is 1028696.875\n",
            "in training loop, epoch 2, step 632, the loss is 503017.25\n",
            "in training loop, epoch 2, step 633, the loss is 498859.625\n",
            "in training loop, epoch 2, step 634, the loss is 875445.0625\n",
            "in training loop, epoch 2, step 635, the loss is 940676.25\n",
            "in training loop, epoch 2, step 636, the loss is 504960.3125\n",
            "in training loop, epoch 2, step 637, the loss is 590855.75\n",
            "in training loop, epoch 2, step 638, the loss is 745374.125\n",
            "in training loop, epoch 2, step 639, the loss is 477312.0\n",
            "in training loop, epoch 2, step 640, the loss is 563516.625\n",
            "in training loop, epoch 2, step 641, the loss is 560495.75\n",
            "in training loop, epoch 2, step 642, the loss is 753647.0625\n",
            "in training loop, epoch 2, step 643, the loss is 632362.75\n",
            "in training loop, epoch 2, step 644, the loss is 571415.625\n",
            "in training loop, epoch 2, step 645, the loss is 387295.46875\n",
            "in training loop, epoch 2, step 646, the loss is 494869.59375\n",
            "in training loop, epoch 2, step 647, the loss is 535707.25\n",
            "in training loop, epoch 2, step 648, the loss is 503940.875\n",
            "in training loop, epoch 2, step 649, the loss is 321882.75\n",
            "in training loop, epoch 2, step 650, the loss is 459184.8125\n",
            "in training loop, epoch 2, step 651, the loss is 237656.5625\n",
            "in training loop, epoch 2, step 652, the loss is 624835.0\n",
            "in training loop, epoch 2, step 653, the loss is 689954.625\n",
            "in training loop, epoch 2, step 654, the loss is 570413.625\n",
            "in training loop, epoch 2, step 655, the loss is 597849.9375\n",
            "in training loop, epoch 2, step 656, the loss is 546227.25\n",
            "in training loop, epoch 2, step 657, the loss is 387540.125\n",
            "in training loop, epoch 2, step 658, the loss is 453087.34375\n",
            "in training loop, epoch 2, step 659, the loss is 465141.9375\n",
            "in training loop, epoch 2, step 660, the loss is 508396.9375\n",
            "in training loop, epoch 2, step 661, the loss is 678795.5\n",
            "in training loop, epoch 2, step 662, the loss is 409597.4375\n",
            "in training loop, epoch 2, step 663, the loss is 537294.4375\n",
            "in training loop, epoch 2, step 664, the loss is 931223.4375\n",
            "in training loop, epoch 2, step 665, the loss is 932388.125\n",
            "in training loop, epoch 2, step 666, the loss is 582152.875\n",
            "in training loop, epoch 2, step 667, the loss is 599196.9375\n",
            "in training loop, epoch 2, step 668, the loss is 830230.1875\n",
            "in training loop, epoch 2, step 669, the loss is 343929.21875\n",
            "in training loop, epoch 2, step 670, the loss is 565090.5625\n",
            "in training loop, epoch 2, step 671, the loss is 509431.0\n",
            "in training loop, epoch 2, step 672, the loss is 584700.125\n",
            "in training loop, epoch 2, step 673, the loss is 524609.5\n",
            "in training loop, epoch 2, step 674, the loss is 523809.59375\n",
            "in training loop, epoch 2, step 675, the loss is 763780.4375\n",
            "in training loop, epoch 2, step 676, the loss is 674503.5625\n",
            "in training loop, epoch 2, step 677, the loss is 1064639.875\n",
            "in training loop, epoch 2, step 678, the loss is 765333.75\n",
            "in training loop, epoch 2, step 679, the loss is 487725.34375\n",
            "in training loop, epoch 2, step 680, the loss is 806219.75\n",
            "in training loop, epoch 2, step 681, the loss is 498322.4375\n",
            "in training loop, epoch 2, step 682, the loss is 585977.875\n",
            "in training loop, epoch 2, step 683, the loss is 824464.875\n",
            "in training loop, epoch 2, step 684, the loss is 687996.625\n",
            "in training loop, epoch 2, step 685, the loss is 458398.875\n",
            "in training loop, epoch 2, step 686, the loss is 326666.25\n",
            "in training loop, epoch 2, step 687, the loss is 504215.78125\n",
            "in training loop, epoch 2, step 688, the loss is 512452.71875\n",
            "in training loop, epoch 2, step 689, the loss is 644806.3125\n",
            "in training loop, epoch 2, step 690, the loss is 347498.96875\n",
            "in training loop, epoch 2, step 691, the loss is 555743.0\n",
            "in training loop, epoch 2, step 692, the loss is 532626.0\n",
            "in training loop, epoch 2, step 693, the loss is 625773.75\n",
            "in training loop, epoch 2, step 694, the loss is 603135.5\n",
            "in training loop, epoch 2, step 695, the loss is 378359.65625\n",
            "in training loop, epoch 2, step 696, the loss is 400309.5625\n",
            "in training loop, epoch 2, step 697, the loss is 527147.75\n",
            "in training loop, epoch 2, step 698, the loss is 606937.8125\n",
            "in training loop, epoch 2, step 699, the loss is 315687.6875\n",
            "in training loop, epoch 2, step 700, the loss is 483418.96875\n",
            "in training loop, epoch 2, step 701, the loss is 797092.75\n",
            "in training loop, epoch 2, step 702, the loss is 756674.6875\n",
            "in training loop, epoch 2, step 703, the loss is 728027.0\n",
            "in training loop, epoch 2, step 704, the loss is 598147.75\n",
            "in training loop, epoch 2, step 705, the loss is 681079.1875\n",
            "in training loop, epoch 2, step 706, the loss is 610416.5625\n",
            "in training loop, epoch 2, step 707, the loss is 348762.5625\n",
            "in training loop, epoch 2, step 708, the loss is 616261.8125\n",
            "in training loop, epoch 2, step 709, the loss is 370044.0625\n",
            "in training loop, epoch 2, step 710, the loss is 406465.1875\n",
            "in training loop, epoch 2, step 711, the loss is 939185.4375\n",
            "in training loop, epoch 2, step 712, the loss is 810002.0\n",
            "in training loop, epoch 2, step 713, the loss is 493472.6875\n",
            "in training loop, epoch 2, step 714, the loss is 483072.40625\n",
            "in training loop, epoch 2, step 715, the loss is 579736.6875\n",
            "in training loop, epoch 2, step 716, the loss is 476910.625\n",
            "in training loop, epoch 2, step 717, the loss is 576322.0625\n",
            "in training loop, epoch 2, step 718, the loss is 325589.375\n",
            "in training loop, epoch 2, step 719, the loss is 461829.34375\n",
            "in training loop, epoch 2, step 720, the loss is 499535.5\n",
            "in training loop, epoch 2, step 721, the loss is 426932.5\n",
            "in training loop, epoch 2, step 722, the loss is 578921.875\n",
            "in training loop, epoch 2, step 723, the loss is 667898.5\n",
            "in training loop, epoch 2, step 724, the loss is 726019.0\n",
            "in training loop, epoch 2, step 725, the loss is 482175.09375\n",
            "in training loop, epoch 2, step 726, the loss is 688088.8125\n",
            "in training loop, epoch 2, step 727, the loss is 640645.625\n",
            "in training loop, epoch 2, step 728, the loss is 631231.5\n",
            "in training loop, epoch 2, step 729, the loss is 528138.25\n",
            "in training loop, epoch 2, step 730, the loss is 621882.25\n",
            "in training loop, epoch 2, step 731, the loss is 349919.46875\n",
            "in training loop, epoch 2, step 732, the loss is 694199.875\n",
            "in training loop, epoch 2, step 733, the loss is 733395.75\n",
            "in training loop, epoch 2, step 734, the loss is 521287.5625\n",
            "in training loop, epoch 2, step 735, the loss is 449602.5\n",
            "in training loop, epoch 2, step 736, the loss is 382585.5\n",
            "in training loop, epoch 2, step 737, the loss is 382436.53125\n",
            "in training loop, epoch 2, step 738, the loss is 587269.8125\n",
            "in training loop, epoch 2, step 739, the loss is 460189.5\n",
            "in training loop, epoch 2, step 740, the loss is 596129.0\n",
            "in training loop, epoch 2, step 741, the loss is 623013.4375\n",
            "in training loop, epoch 2, step 742, the loss is 654644.3125\n",
            "in training loop, epoch 2, step 743, the loss is 468791.125\n",
            "in training loop, epoch 2, step 744, the loss is 687556.4375\n",
            "in training loop, epoch 2, step 745, the loss is 805903.6875\n",
            "in training loop, epoch 2, step 746, the loss is 497589.6875\n",
            "in training loop, epoch 2, step 747, the loss is 677625.1875\n",
            "in training loop, epoch 2, step 748, the loss is 577097.625\n",
            "in training loop, epoch 2, step 749, the loss is 644763.625\n",
            "in training loop, epoch 2, step 750, the loss is 451591.53125\n",
            "in training loop, epoch 2, step 751, the loss is 582581.8125\n",
            "in training loop, epoch 2, step 752, the loss is 458083.65625\n",
            "in training loop, epoch 2, step 753, the loss is 314315.65625\n",
            "in training loop, epoch 2, step 754, the loss is 828463.5625\n",
            "in training loop, epoch 2, step 755, the loss is 449051.4375\n",
            "in training loop, epoch 2, step 756, the loss is 609614.75\n",
            "in training loop, epoch 2, step 757, the loss is 398801.9375\n",
            "in training loop, epoch 2, step 758, the loss is 411132.09375\n",
            "in training loop, epoch 2, step 759, the loss is 655428.5\n",
            "in training loop, epoch 2, step 760, the loss is 521187.125\n",
            "in training loop, epoch 2, step 761, the loss is 506699.1875\n",
            "in training loop, epoch 2, step 762, the loss is 643047.1875\n",
            "in training loop, epoch 2, step 763, the loss is 482958.96875\n",
            "in training loop, epoch 2, step 764, the loss is 418143.875\n",
            "in training loop, epoch 2, step 765, the loss is 835316.3125\n",
            "in training loop, epoch 2, step 766, the loss is 598447.8125\n",
            "in training loop, epoch 2, step 767, the loss is 570713.625\n",
            "in training loop, epoch 2, step 768, the loss is 699797.875\n",
            "in training loop, epoch 2, step 769, the loss is 599043.5625\n",
            "in training loop, epoch 2, step 770, the loss is 507540.5\n",
            "in training loop, epoch 2, step 771, the loss is 562350.625\n",
            "in training loop, epoch 2, step 772, the loss is 616475.6875\n",
            "in training loop, epoch 2, step 773, the loss is 408296.5625\n",
            "in training loop, epoch 2, step 774, the loss is 602882.6875\n",
            "in training loop, epoch 2, step 775, the loss is 597632.375\n",
            "in training loop, epoch 2, step 776, the loss is 542699.9375\n",
            "in training loop, epoch 2, step 777, the loss is 571292.25\n",
            "in training loop, epoch 2, step 778, the loss is 562505.9375\n",
            "in training loop, epoch 2, step 779, the loss is 538137.125\n",
            "in training loop, epoch 2, step 780, the loss is 575494.625\n",
            "in training loop, epoch 2, step 781, the loss is 638201.625\n",
            "in training loop, epoch 2, step 782, the loss is 391981.4375\n",
            "in training loop, epoch 2, step 783, the loss is 395049.59375\n",
            "in training loop, epoch 2, step 784, the loss is 597154.125\n",
            "in training loop, epoch 2, step 785, the loss is 627005.6875\n",
            "in training loop, epoch 2, step 786, the loss is 806995.875\n",
            "in training loop, epoch 2, step 787, the loss is 505766.875\n",
            "in training loop, epoch 2, step 788, the loss is 819769.3125\n",
            "in training loop, epoch 2, step 789, the loss is 759003.6875\n",
            "in training loop, epoch 2, step 790, the loss is 638293.9375\n",
            "in training loop, epoch 2, step 791, the loss is 419159.4375\n",
            "in training loop, epoch 2, step 792, the loss is 665511.1875\n",
            "in training loop, epoch 2, step 793, the loss is 518465.75\n",
            "in training loop, epoch 2, step 794, the loss is 450484.15625\n",
            "in training loop, epoch 2, step 795, the loss is 337522.53125\n",
            "in training loop, epoch 2, step 796, the loss is 530769.0\n",
            "in training loop, epoch 2, step 797, the loss is 663019.3125\n",
            "in training loop, epoch 2, step 798, the loss is 598467.6875\n",
            "in training loop, epoch 2, step 799, the loss is 526634.375\n",
            "in training loop, epoch 2, step 800, the loss is 1113957.5\n",
            "in training loop, epoch 2, step 801, the loss is 871730.8125\n",
            "in training loop, epoch 2, step 802, the loss is 438133.15625\n",
            "in training loop, epoch 2, step 803, the loss is 975491.25\n",
            "in training loop, epoch 2, step 804, the loss is 741581.5625\n",
            "in training loop, epoch 2, step 805, the loss is 452793.5625\n",
            "in training loop, epoch 2, step 806, the loss is 684438.0\n",
            "in training loop, epoch 2, step 807, the loss is 729026.875\n",
            "in training loop, epoch 2, step 808, the loss is 723828.8125\n",
            "in training loop, epoch 2, step 809, the loss is 830630.125\n",
            "in training loop, epoch 2, step 810, the loss is 708216.75\n",
            "in training loop, epoch 2, step 811, the loss is 683376.5625\n",
            "in training loop, epoch 2, step 812, the loss is 543342.4375\n",
            "in training loop, epoch 2, step 813, the loss is 264531.25\n",
            "in training loop, epoch 2, step 814, the loss is 489938.71875\n",
            "in training loop, epoch 2, step 815, the loss is 567734.75\n",
            "in training loop, epoch 2, step 816, the loss is 688650.5625\n",
            "in training loop, epoch 2, step 817, the loss is 944836.4375\n",
            "in training loop, epoch 2, step 818, the loss is 573738.75\n",
            "in training loop, epoch 2, step 819, the loss is 721197.75\n",
            "in training loop, epoch 2, step 820, the loss is 439654.3125\n",
            "in training loop, epoch 2, step 821, the loss is 692496.5\n",
            "in training loop, epoch 2, step 822, the loss is 559979.125\n",
            "in training loop, epoch 2, step 823, the loss is 499617.03125\n",
            "in training loop, epoch 2, step 824, the loss is 558192.375\n",
            "in training loop, epoch 2, step 825, the loss is 753179.125\n",
            "in training loop, epoch 2, step 826, the loss is 1332192.0\n",
            "in training loop, epoch 2, step 827, the loss is 711836.375\n",
            "in training loop, epoch 2, step 828, the loss is 388690.46875\n",
            "in training loop, epoch 2, step 829, the loss is 388959.8125\n",
            "in training loop, epoch 2, step 830, the loss is 606948.25\n",
            "in training loop, epoch 2, step 831, the loss is 517230.40625\n",
            "in training loop, epoch 2, step 832, the loss is 485261.125\n",
            "in training loop, epoch 2, step 833, the loss is 554526.25\n",
            "in training loop, epoch 2, step 834, the loss is 435056.125\n",
            "in training loop, epoch 2, step 835, the loss is 578079.8125\n",
            "in training loop, epoch 2, step 836, the loss is 771645.25\n",
            "in training loop, epoch 2, step 837, the loss is 388082.75\n",
            "in training loop, epoch 2, step 838, the loss is 490976.875\n",
            "in training loop, epoch 2, step 839, the loss is 448074.59375\n",
            "in training loop, epoch 2, step 840, the loss is 608586.0\n",
            "in training loop, epoch 2, step 841, the loss is 561574.5625\n",
            "in training loop, epoch 2, step 842, the loss is 728061.3125\n",
            "in training loop, epoch 2, step 843, the loss is 602214.4375\n",
            "in training loop, epoch 2, step 844, the loss is 804713.0625\n",
            "in training loop, epoch 2, step 845, the loss is 652401.125\n",
            "in training loop, epoch 2, step 846, the loss is 720006.125\n",
            "in training loop, epoch 2, step 847, the loss is 509458.15625\n",
            "in training loop, epoch 2, step 848, the loss is 367701.8125\n",
            "in training loop, epoch 2, step 849, the loss is 657720.4375\n",
            "in training loop, epoch 2, step 850, the loss is 703781.1875\n",
            "in training loop, epoch 2, step 851, the loss is 605722.5\n",
            "in training loop, epoch 2, step 852, the loss is 695018.25\n",
            "in training loop, epoch 2, step 853, the loss is 588277.5\n",
            "in training loop, epoch 2, step 854, the loss is 626524.3125\n",
            "in training loop, epoch 2, step 855, the loss is 508612.9375\n",
            "in training loop, epoch 2, step 856, the loss is 894801.0\n",
            "in training loop, epoch 2, step 857, the loss is 458553.03125\n",
            "in training loop, epoch 2, step 858, the loss is 678942.875\n",
            "in training loop, epoch 2, step 859, the loss is 590538.5\n",
            "in training loop, epoch 2, step 860, the loss is 893888.25\n",
            "in training loop, epoch 2, step 861, the loss is 698897.6875\n",
            "in training loop, epoch 2, step 862, the loss is 628941.4375\n",
            "in training loop, epoch 2, step 863, the loss is 519445.4375\n",
            "in training loop, epoch 2, step 864, the loss is 508556.375\n",
            "in training loop, epoch 2, step 865, the loss is 717330.375\n",
            "in training loop, epoch 2, step 866, the loss is 531418.125\n",
            "in training loop, epoch 2, step 867, the loss is 507089.65625\n",
            "in training loop, epoch 2, step 868, the loss is 511598.75\n",
            "in training loop, epoch 2, step 869, the loss is 357065.84375\n",
            "in training loop, epoch 2, step 870, the loss is 827997.125\n",
            "in training loop, epoch 2, step 871, the loss is 664592.5625\n",
            "in training loop, epoch 2, step 872, the loss is 376575.90625\n",
            "in training loop, epoch 2, step 873, the loss is 611590.75\n",
            "in training loop, epoch 2, step 874, the loss is 594648.1875\n",
            "in training loop, epoch 2, step 875, the loss is 536404.1875\n",
            "in training loop, epoch 2, step 876, the loss is 550213.25\n",
            "in training loop, epoch 2, step 877, the loss is 498007.84375\n",
            "in training loop, epoch 2, step 878, the loss is 883019.75\n",
            "in training loop, epoch 2, step 879, the loss is 463781.03125\n",
            "in training loop, epoch 2, step 880, the loss is 633651.125\n",
            "in training loop, epoch 2, step 881, the loss is 670515.6875\n",
            "in training loop, epoch 2, step 882, the loss is 965739.25\n",
            "in training loop, epoch 2, step 883, the loss is 834902.625\n",
            "in training loop, epoch 2, step 884, the loss is 282401.3125\n",
            "in training loop, epoch 2, step 885, the loss is 863907.25\n",
            "in training loop, epoch 2, step 886, the loss is 716255.625\n",
            "in training loop, epoch 2, step 887, the loss is 604001.3125\n",
            "in training loop, epoch 2, step 888, the loss is 629925.375\n",
            "in training loop, epoch 2, step 889, the loss is 551425.875\n",
            "in training loop, epoch 2, step 890, the loss is 631504.25\n",
            "in training loop, epoch 2, step 891, the loss is 416232.03125\n",
            "in training loop, epoch 2, step 892, the loss is 845682.125\n",
            "in training loop, epoch 2, step 893, the loss is 547928.0\n",
            "in training loop, epoch 2, step 894, the loss is 582608.5625\n",
            "in training loop, epoch 2, step 895, the loss is 606812.9375\n",
            "in training loop, epoch 2, step 896, the loss is 491005.25\n",
            "in training loop, epoch 2, step 897, the loss is 684678.125\n",
            "in training loop, epoch 2, step 898, the loss is 773411.9375\n",
            "in training loop, epoch 2, step 899, the loss is 669674.875\n",
            "in training loop, epoch 2, step 900, the loss is 552487.5\n",
            "in training loop, epoch 2, step 901, the loss is 710374.75\n",
            "in training loop, epoch 2, step 902, the loss is 728925.4375\n",
            "in training loop, epoch 2, step 903, the loss is 265050.84375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "k-fold 1:: Epoch 2: train loss 592511.3346411781 val loss 633601.0683787129\n",
            "in training loop, epoch 3, step 0, the loss is 462445.6875\n",
            "in training loop, epoch 3, step 1, the loss is 286580.40625\n",
            "in training loop, epoch 3, step 2, the loss is 521788.40625\n",
            "in training loop, epoch 3, step 3, the loss is 583558.5625\n",
            "in training loop, epoch 3, step 4, the loss is 583119.6875\n",
            "in training loop, epoch 3, step 5, the loss is 489371.125\n",
            "in training loop, epoch 3, step 6, the loss is 318621.5625\n",
            "in training loop, epoch 3, step 7, the loss is 780860.6875\n",
            "in training loop, epoch 3, step 8, the loss is 446409.875\n",
            "in training loop, epoch 3, step 9, the loss is 404184.34375\n",
            "in training loop, epoch 3, step 10, the loss is 574782.8125\n",
            "in training loop, epoch 3, step 11, the loss is 320706.71875\n",
            "in training loop, epoch 3, step 12, the loss is 336216.40625\n",
            "in training loop, epoch 3, step 13, the loss is 380903.1875\n",
            "in training loop, epoch 3, step 14, the loss is 696485.375\n",
            "in training loop, epoch 3, step 15, the loss is 376023.5625\n",
            "in training loop, epoch 3, step 16, the loss is 418621.84375\n",
            "in training loop, epoch 3, step 17, the loss is 515833.21875\n",
            "in training loop, epoch 3, step 18, the loss is 563336.75\n",
            "in training loop, epoch 3, step 19, the loss is 246927.65625\n",
            "in training loop, epoch 3, step 20, the loss is 522567.78125\n",
            "in training loop, epoch 3, step 21, the loss is 506158.6875\n",
            "in training loop, epoch 3, step 22, the loss is 343675.75\n",
            "in training loop, epoch 3, step 23, the loss is 368670.4375\n",
            "in training loop, epoch 3, step 24, the loss is 538540.0\n",
            "in training loop, epoch 3, step 25, the loss is 383405.0625\n",
            "in training loop, epoch 3, step 26, the loss is 671605.6875\n",
            "in training loop, epoch 3, step 27, the loss is 673693.1875\n",
            "in training loop, epoch 3, step 28, the loss is 391448.90625\n",
            "in training loop, epoch 3, step 29, the loss is 349705.625\n",
            "in training loop, epoch 3, step 30, the loss is 603760.375\n",
            "in training loop, epoch 3, step 31, the loss is 536826.0625\n",
            "in training loop, epoch 3, step 32, the loss is 356955.75\n",
            "in training loop, epoch 3, step 33, the loss is 593651.1875\n",
            "in training loop, epoch 3, step 34, the loss is 552532.0625\n",
            "in training loop, epoch 3, step 35, the loss is 395095.0625\n",
            "in training loop, epoch 3, step 36, the loss is 446214.40625\n",
            "in training loop, epoch 3, step 37, the loss is 430705.65625\n",
            "in training loop, epoch 3, step 38, the loss is 1037525.125\n",
            "in training loop, epoch 3, step 39, the loss is 459361.125\n",
            "in training loop, epoch 3, step 40, the loss is 485115.8125\n",
            "in training loop, epoch 3, step 41, the loss is 412582.3125\n",
            "in training loop, epoch 3, step 42, the loss is 485832.21875\n",
            "in training loop, epoch 3, step 43, the loss is 467385.96875\n",
            "in training loop, epoch 3, step 44, the loss is 549117.9375\n",
            "in training loop, epoch 3, step 45, the loss is 1105347.875\n",
            "in training loop, epoch 3, step 46, the loss is 495826.40625\n",
            "in training loop, epoch 3, step 47, the loss is 623762.125\n",
            "in training loop, epoch 3, step 48, the loss is 544079.625\n",
            "in training loop, epoch 3, step 49, the loss is 620530.125\n",
            "in training loop, epoch 3, step 50, the loss is 413459.03125\n",
            "in training loop, epoch 3, step 51, the loss is 632676.0\n",
            "in training loop, epoch 3, step 52, the loss is 626612.375\n",
            "in training loop, epoch 3, step 53, the loss is 550009.125\n",
            "in training loop, epoch 3, step 54, the loss is 656595.0\n",
            "in training loop, epoch 3, step 55, the loss is 330390.9375\n",
            "in training loop, epoch 3, step 56, the loss is 595013.625\n",
            "in training loop, epoch 3, step 57, the loss is 386252.21875\n",
            "in training loop, epoch 3, step 58, the loss is 815075.6875\n",
            "in training loop, epoch 3, step 59, the loss is 413698.90625\n",
            "in training loop, epoch 3, step 60, the loss is 552717.8125\n",
            "in training loop, epoch 3, step 61, the loss is 515101.5625\n",
            "in training loop, epoch 3, step 62, the loss is 611545.625\n",
            "in training loop, epoch 3, step 63, the loss is 667746.3125\n",
            "in training loop, epoch 3, step 64, the loss is 430309.59375\n",
            "in training loop, epoch 3, step 65, the loss is 693733.1875\n",
            "in training loop, epoch 3, step 66, the loss is 491675.125\n",
            "in training loop, epoch 3, step 67, the loss is 417762.375\n",
            "in training loop, epoch 3, step 68, the loss is 551163.375\n",
            "in training loop, epoch 3, step 69, the loss is 501642.09375\n",
            "in training loop, epoch 3, step 70, the loss is 560971.875\n",
            "in training loop, epoch 3, step 71, the loss is 411244.0\n",
            "in training loop, epoch 3, step 72, the loss is 365548.34375\n",
            "in training loop, epoch 3, step 73, the loss is 448180.90625\n",
            "in training loop, epoch 3, step 74, the loss is 531484.875\n",
            "in training loop, epoch 3, step 75, the loss is 514330.25\n",
            "in training loop, epoch 3, step 76, the loss is 546744.625\n",
            "in training loop, epoch 3, step 77, the loss is 424946.03125\n",
            "in training loop, epoch 3, step 78, the loss is 413928.5\n",
            "in training loop, epoch 3, step 79, the loss is 503350.34375\n",
            "in training loop, epoch 3, step 80, the loss is 573684.375\n",
            "in training loop, epoch 3, step 81, the loss is 402158.78125\n",
            "in training loop, epoch 3, step 82, the loss is 812841.4375\n",
            "in training loop, epoch 3, step 83, the loss is 521815.65625\n",
            "in training loop, epoch 3, step 84, the loss is 618084.625\n",
            "in training loop, epoch 3, step 85, the loss is 278164.03125\n",
            "in training loop, epoch 3, step 86, the loss is 568935.5\n",
            "in training loop, epoch 3, step 87, the loss is 571877.4375\n",
            "in training loop, epoch 3, step 88, the loss is 1180488.75\n",
            "in training loop, epoch 3, step 89, the loss is 1243424.0\n",
            "in training loop, epoch 3, step 90, the loss is 845628.8125\n",
            "in training loop, epoch 3, step 91, the loss is 1210575.5\n",
            "in training loop, epoch 3, step 92, the loss is 1038584.3125\n",
            "in training loop, epoch 3, step 93, the loss is 2495555.75\n",
            "in training loop, epoch 3, step 94, the loss is 5094641.0\n",
            "in training loop, epoch 3, step 95, the loss is 534488.0\n",
            "in training loop, epoch 3, step 96, the loss is 1385151.0\n",
            "in training loop, epoch 3, step 97, the loss is 2036214.125\n",
            "in training loop, epoch 3, step 98, the loss is 2015576.125\n",
            "in training loop, epoch 3, step 99, the loss is 707570.0\n",
            "in training loop, epoch 3, step 100, the loss is 957090.0\n",
            "in training loop, epoch 3, step 101, the loss is 1153239.375\n",
            "in training loop, epoch 3, step 102, the loss is 942661.625\n",
            "in training loop, epoch 3, step 103, the loss is 518311.9375\n",
            "in training loop, epoch 3, step 104, the loss is 1175057.5\n",
            "in training loop, epoch 3, step 105, the loss is 950380.625\n",
            "in training loop, epoch 3, step 106, the loss is 899365.3125\n",
            "in training loop, epoch 3, step 107, the loss is 1039121.125\n",
            "in training loop, epoch 3, step 108, the loss is 644463.5\n",
            "in training loop, epoch 3, step 109, the loss is 433610.0625\n",
            "in training loop, epoch 3, step 110, the loss is 938760.25\n",
            "in training loop, epoch 3, step 111, the loss is 677668.375\n",
            "in training loop, epoch 3, step 112, the loss is 734264.375\n",
            "in training loop, epoch 3, step 113, the loss is 702977.9375\n",
            "in training loop, epoch 3, step 114, the loss is 1329962.625\n",
            "in training loop, epoch 3, step 115, the loss is 744164.9375\n",
            "in training loop, epoch 3, step 116, the loss is 500426.09375\n",
            "in training loop, epoch 3, step 117, the loss is 626415.9375\n",
            "in training loop, epoch 3, step 118, the loss is 951808.0\n",
            "in training loop, epoch 3, step 119, the loss is 637970.0\n",
            "in training loop, epoch 3, step 120, the loss is 882781.625\n",
            "in training loop, epoch 3, step 121, the loss is 846552.5625\n",
            "in training loop, epoch 3, step 122, the loss is 768016.4375\n",
            "in training loop, epoch 3, step 123, the loss is 568825.0\n",
            "in training loop, epoch 3, step 124, the loss is 454180.0\n",
            "in training loop, epoch 3, step 125, the loss is 645690.25\n",
            "in training loop, epoch 3, step 126, the loss is 721862.5\n",
            "in training loop, epoch 3, step 127, the loss is 384177.5625\n",
            "in training loop, epoch 3, step 128, the loss is 646150.0625\n",
            "in training loop, epoch 3, step 129, the loss is 622730.1875\n",
            "in training loop, epoch 3, step 130, the loss is 303073.96875\n",
            "in training loop, epoch 3, step 131, the loss is 516588.0\n",
            "in training loop, epoch 3, step 132, the loss is 424326.34375\n",
            "in training loop, epoch 3, step 133, the loss is 405410.96875\n",
            "in training loop, epoch 3, step 134, the loss is 577361.3125\n",
            "in training loop, epoch 3, step 135, the loss is 664221.0\n",
            "in training loop, epoch 3, step 136, the loss is 534725.875\n",
            "in training loop, epoch 3, step 137, the loss is 694028.1875\n",
            "in training loop, epoch 3, step 138, the loss is 456105.90625\n",
            "in training loop, epoch 3, step 139, the loss is 820468.9375\n",
            "in training loop, epoch 3, step 140, the loss is 352288.0625\n",
            "in training loop, epoch 3, step 141, the loss is 545872.25\n",
            "in training loop, epoch 3, step 142, the loss is 739069.25\n",
            "in training loop, epoch 3, step 143, the loss is 543552.3125\n",
            "in training loop, epoch 3, step 144, the loss is 548904.25\n",
            "in training loop, epoch 3, step 145, the loss is 697704.875\n",
            "in training loop, epoch 3, step 146, the loss is 677493.375\n",
            "in training loop, epoch 3, step 147, the loss is 437585.0625\n",
            "in training loop, epoch 3, step 148, the loss is 592234.75\n",
            "in training loop, epoch 3, step 149, the loss is 381799.75\n",
            "in training loop, epoch 3, step 150, the loss is 507340.0\n",
            "in training loop, epoch 3, step 151, the loss is 700994.1875\n",
            "in training loop, epoch 3, step 152, the loss is 589159.4375\n",
            "in training loop, epoch 3, step 153, the loss is 567849.5625\n",
            "in training loop, epoch 3, step 154, the loss is 379825.875\n",
            "in training loop, epoch 3, step 155, the loss is 421821.4375\n",
            "in training loop, epoch 3, step 156, the loss is 311548.5\n",
            "in training loop, epoch 3, step 157, the loss is 691099.375\n",
            "in training loop, epoch 3, step 158, the loss is 837812.375\n",
            "in training loop, epoch 3, step 159, the loss is 572192.625\n",
            "in training loop, epoch 3, step 160, the loss is 536131.3125\n",
            "in training loop, epoch 3, step 161, the loss is 501814.78125\n",
            "in training loop, epoch 3, step 162, the loss is 632718.4375\n",
            "in training loop, epoch 3, step 163, the loss is 677790.9375\n",
            "in training loop, epoch 3, step 164, the loss is 496295.1875\n",
            "in training loop, epoch 3, step 165, the loss is 389097.625\n",
            "in training loop, epoch 3, step 166, the loss is 366758.8125\n",
            "in training loop, epoch 3, step 167, the loss is 700107.6875\n",
            "in training loop, epoch 3, step 168, the loss is 489116.96875\n",
            "in training loop, epoch 3, step 169, the loss is 588318.75\n",
            "in training loop, epoch 3, step 170, the loss is 466500.6875\n",
            "in training loop, epoch 3, step 171, the loss is 725277.375\n",
            "in training loop, epoch 3, step 172, the loss is 493126.4375\n",
            "in training loop, epoch 3, step 173, the loss is 557182.125\n",
            "in training loop, epoch 3, step 174, the loss is 403653.0625\n",
            "in training loop, epoch 3, step 175, the loss is 347458.625\n",
            "in training loop, epoch 3, step 176, the loss is 702680.3125\n",
            "in training loop, epoch 3, step 177, the loss is 623625.3125\n",
            "in training loop, epoch 3, step 178, the loss is 748909.875\n",
            "in training loop, epoch 3, step 179, the loss is 1100724.75\n",
            "in training loop, epoch 3, step 180, the loss is 668650.875\n",
            "in training loop, epoch 3, step 181, the loss is 610580.0\n",
            "in training loop, epoch 3, step 182, the loss is 575863.5625\n",
            "in training loop, epoch 3, step 183, the loss is 497276.90625\n",
            "in training loop, epoch 3, step 184, the loss is 331804.9375\n",
            "in training loop, epoch 3, step 185, the loss is 494076.3125\n",
            "in training loop, epoch 3, step 186, the loss is 553101.75\n",
            "in training loop, epoch 3, step 187, the loss is 547291.625\n",
            "in training loop, epoch 3, step 188, the loss is 561865.25\n",
            "in training loop, epoch 3, step 189, the loss is 412760.4375\n",
            "in training loop, epoch 3, step 190, the loss is 676779.3125\n",
            "in training loop, epoch 3, step 191, the loss is 521492.28125\n",
            "in training loop, epoch 3, step 192, the loss is 277686.375\n",
            "in training loop, epoch 3, step 193, the loss is 389701.46875\n",
            "in training loop, epoch 3, step 194, the loss is 388298.03125\n",
            "in training loop, epoch 3, step 195, the loss is 622683.1875\n",
            "in training loop, epoch 3, step 196, the loss is 408830.53125\n",
            "in training loop, epoch 3, step 197, the loss is 456053.21875\n",
            "in training loop, epoch 3, step 198, the loss is 1003653.6875\n",
            "in training loop, epoch 3, step 199, the loss is 371741.625\n",
            "in training loop, epoch 3, step 200, the loss is 515769.15625\n",
            "in training loop, epoch 3, step 201, the loss is 609186.6875\n",
            "in training loop, epoch 3, step 202, the loss is 630135.75\n",
            "in training loop, epoch 3, step 203, the loss is 362575.125\n",
            "in training loop, epoch 3, step 204, the loss is 377503.875\n",
            "in training loop, epoch 3, step 205, the loss is 667505.0\n",
            "in training loop, epoch 3, step 206, the loss is 329196.5625\n",
            "in training loop, epoch 3, step 207, the loss is 481155.625\n",
            "in training loop, epoch 3, step 208, the loss is 832791.125\n",
            "in training loop, epoch 3, step 209, the loss is 510614.34375\n",
            "in training loop, epoch 3, step 210, the loss is 354756.71875\n",
            "in training loop, epoch 3, step 211, the loss is 639208.25\n",
            "in training loop, epoch 3, step 212, the loss is 544215.4375\n",
            "in training loop, epoch 3, step 213, the loss is 444076.0\n",
            "in training loop, epoch 3, step 214, the loss is 482520.15625\n",
            "in training loop, epoch 3, step 215, the loss is 635420.375\n",
            "in training loop, epoch 3, step 216, the loss is 660426.6875\n",
            "in training loop, epoch 3, step 217, the loss is 802163.5\n",
            "in training loop, epoch 3, step 218, the loss is 495970.75\n",
            "in training loop, epoch 3, step 219, the loss is 499675.6875\n",
            "in training loop, epoch 3, step 220, the loss is 505458.875\n",
            "in training loop, epoch 3, step 221, the loss is 309489.5625\n",
            "in training loop, epoch 3, step 222, the loss is 463048.875\n",
            "in training loop, epoch 3, step 223, the loss is 642618.0625\n",
            "in training loop, epoch 3, step 224, the loss is 418318.0\n",
            "in training loop, epoch 3, step 225, the loss is 293732.4375\n",
            "in training loop, epoch 3, step 226, the loss is 623098.375\n",
            "in training loop, epoch 3, step 227, the loss is 738870.125\n",
            "in training loop, epoch 3, step 228, the loss is 578740.0\n",
            "in training loop, epoch 3, step 229, the loss is 524925.625\n",
            "in training loop, epoch 3, step 230, the loss is 888570.6875\n",
            "in training loop, epoch 3, step 231, the loss is 342395.96875\n",
            "in training loop, epoch 3, step 232, the loss is 475793.78125\n",
            "in training loop, epoch 3, step 233, the loss is 397048.65625\n",
            "in training loop, epoch 3, step 234, the loss is 721240.5\n",
            "in training loop, epoch 3, step 235, the loss is 483080.6875\n",
            "in training loop, epoch 3, step 236, the loss is 487172.1875\n",
            "in training loop, epoch 3, step 237, the loss is 401378.875\n",
            "in training loop, epoch 3, step 238, the loss is 483932.9375\n",
            "in training loop, epoch 3, step 239, the loss is 451192.5625\n",
            "in training loop, epoch 3, step 240, the loss is 641030.375\n",
            "in training loop, epoch 3, step 241, the loss is 493569.46875\n",
            "in training loop, epoch 3, step 242, the loss is 429977.34375\n",
            "in training loop, epoch 3, step 243, the loss is 456833.53125\n",
            "in training loop, epoch 3, step 244, the loss is 472461.15625\n",
            "in training loop, epoch 3, step 245, the loss is 758235.1875\n",
            "in training loop, epoch 3, step 246, the loss is 386317.40625\n",
            "in training loop, epoch 3, step 247, the loss is 438787.84375\n",
            "in training loop, epoch 3, step 248, the loss is 480515.09375\n",
            "in training loop, epoch 3, step 249, the loss is 319799.90625\n",
            "in training loop, epoch 3, step 250, the loss is 706231.5625\n",
            "in training loop, epoch 3, step 251, the loss is 643817.0625\n",
            "in training loop, epoch 3, step 252, the loss is 584172.4375\n",
            "in training loop, epoch 3, step 253, the loss is 486867.75\n",
            "in training loop, epoch 3, step 254, the loss is 937252.8125\n",
            "in training loop, epoch 3, step 255, the loss is 410951.78125\n",
            "in training loop, epoch 3, step 256, the loss is 386565.3125\n",
            "in training loop, epoch 3, step 257, the loss is 460682.125\n",
            "in training loop, epoch 3, step 258, the loss is 470902.75\n",
            "in training loop, epoch 3, step 259, the loss is 482319.4375\n",
            "in training loop, epoch 3, step 260, the loss is 425458.15625\n",
            "in training loop, epoch 3, step 261, the loss is 643041.75\n",
            "in training loop, epoch 3, step 262, the loss is 330870.65625\n",
            "in training loop, epoch 3, step 263, the loss is 762828.625\n",
            "in training loop, epoch 3, step 264, the loss is 390905.4375\n",
            "in training loop, epoch 3, step 265, the loss is 402462.875\n",
            "in training loop, epoch 3, step 266, the loss is 265757.78125\n",
            "in training loop, epoch 3, step 267, the loss is 399987.0\n",
            "in training loop, epoch 3, step 268, the loss is 482954.75\n",
            "in training loop, epoch 3, step 269, the loss is 514032.9375\n",
            "in training loop, epoch 3, step 270, the loss is 518965.78125\n",
            "in training loop, epoch 3, step 271, the loss is 455715.28125\n",
            "in training loop, epoch 3, step 272, the loss is 1244918.875\n",
            "in training loop, epoch 3, step 273, the loss is 407111.40625\n",
            "in training loop, epoch 3, step 274, the loss is 486903.0625\n",
            "in training loop, epoch 3, step 275, the loss is 595004.3125\n",
            "in training loop, epoch 3, step 276, the loss is 518765.0625\n",
            "in training loop, epoch 3, step 277, the loss is 524522.8125\n",
            "in training loop, epoch 3, step 278, the loss is 535136.875\n",
            "in training loop, epoch 3, step 279, the loss is 381636.625\n",
            "in training loop, epoch 3, step 280, the loss is 559972.1875\n",
            "in training loop, epoch 3, step 281, the loss is 516871.5\n",
            "in training loop, epoch 3, step 282, the loss is 500420.71875\n",
            "in training loop, epoch 3, step 283, the loss is 561608.5\n",
            "in training loop, epoch 3, step 284, the loss is 774185.5\n",
            "in training loop, epoch 3, step 285, the loss is 513742.90625\n",
            "in training loop, epoch 3, step 286, the loss is 557900.0625\n",
            "in training loop, epoch 3, step 287, the loss is 536708.9375\n",
            "in training loop, epoch 3, step 288, the loss is 496176.03125\n",
            "in training loop, epoch 3, step 289, the loss is 456733.5\n",
            "in training loop, epoch 3, step 290, the loss is 289844.8125\n",
            "in training loop, epoch 3, step 291, the loss is 653092.1875\n",
            "in training loop, epoch 3, step 292, the loss is 414501.71875\n",
            "in training loop, epoch 3, step 293, the loss is 686976.625\n",
            "in training loop, epoch 3, step 294, the loss is 657128.8125\n",
            "in training loop, epoch 3, step 295, the loss is 646200.4375\n",
            "in training loop, epoch 3, step 296, the loss is 530397.25\n",
            "in training loop, epoch 3, step 297, the loss is 730111.25\n",
            "in training loop, epoch 3, step 298, the loss is 626061.75\n",
            "in training loop, epoch 3, step 299, the loss is 576187.375\n",
            "in training loop, epoch 3, step 300, the loss is 404868.375\n",
            "in training loop, epoch 3, step 301, the loss is 576616.625\n",
            "in training loop, epoch 3, step 302, the loss is 558297.9375\n",
            "in training loop, epoch 3, step 303, the loss is 487177.5625\n",
            "in training loop, epoch 3, step 304, the loss is 458017.4375\n",
            "in training loop, epoch 3, step 305, the loss is 812063.5\n",
            "in training loop, epoch 3, step 306, the loss is 424733.34375\n",
            "in training loop, epoch 3, step 307, the loss is 658602.5\n",
            "in training loop, epoch 3, step 308, the loss is 409607.75\n",
            "in training loop, epoch 3, step 309, the loss is 396991.25\n",
            "in training loop, epoch 3, step 310, the loss is 702403.4375\n",
            "in training loop, epoch 3, step 311, the loss is 730606.625\n",
            "in training loop, epoch 3, step 312, the loss is 429072.25\n",
            "in training loop, epoch 3, step 313, the loss is 443157.96875\n",
            "in training loop, epoch 3, step 314, the loss is 635734.6875\n",
            "in training loop, epoch 3, step 315, the loss is 455467.5\n",
            "in training loop, epoch 3, step 316, the loss is 524483.5625\n",
            "in training loop, epoch 3, step 317, the loss is 503732.6875\n",
            "in training loop, epoch 3, step 318, the loss is 509342.125\n",
            "in training loop, epoch 3, step 319, the loss is 468330.28125\n",
            "in training loop, epoch 3, step 320, the loss is 626251.8125\n",
            "in training loop, epoch 3, step 321, the loss is 486754.40625\n",
            "in training loop, epoch 3, step 322, the loss is 585665.5625\n",
            "in training loop, epoch 3, step 323, the loss is 831003.0625\n",
            "in training loop, epoch 3, step 324, the loss is 315665.25\n",
            "in training loop, epoch 3, step 325, the loss is 474986.5625\n",
            "in training loop, epoch 3, step 326, the loss is 654152.0\n",
            "in training loop, epoch 3, step 327, the loss is 325176.6875\n",
            "in training loop, epoch 3, step 328, the loss is 456584.4375\n",
            "in training loop, epoch 3, step 329, the loss is 442118.8125\n",
            "in training loop, epoch 3, step 330, the loss is 551941.3125\n",
            "in training loop, epoch 3, step 331, the loss is 396791.4375\n",
            "in training loop, epoch 3, step 332, the loss is 623936.3125\n",
            "in training loop, epoch 3, step 333, the loss is 632230.5\n",
            "in training loop, epoch 3, step 334, the loss is 395166.25\n",
            "in training loop, epoch 3, step 335, the loss is 574464.4375\n",
            "in training loop, epoch 3, step 336, the loss is 602037.125\n",
            "in training loop, epoch 3, step 337, the loss is 323258.0625\n",
            "in training loop, epoch 3, step 338, the loss is 777097.8125\n",
            "in training loop, epoch 3, step 339, the loss is 684259.875\n",
            "in training loop, epoch 3, step 340, the loss is 587265.6875\n",
            "in training loop, epoch 3, step 341, the loss is 470414.28125\n",
            "in training loop, epoch 3, step 342, the loss is 540117.0\n",
            "in training loop, epoch 3, step 343, the loss is 698689.375\n",
            "in training loop, epoch 3, step 344, the loss is 477911.34375\n",
            "in training loop, epoch 3, step 345, the loss is 465504.25\n",
            "in training loop, epoch 3, step 346, the loss is 473170.90625\n",
            "in training loop, epoch 3, step 347, the loss is 454956.375\n",
            "in training loop, epoch 3, step 348, the loss is 337946.84375\n",
            "in training loop, epoch 3, step 349, the loss is 381457.5625\n",
            "in training loop, epoch 3, step 350, the loss is 340541.4375\n",
            "in training loop, epoch 3, step 351, the loss is 502544.53125\n",
            "in training loop, epoch 3, step 352, the loss is 415829.78125\n",
            "in training loop, epoch 3, step 353, the loss is 478990.625\n",
            "in training loop, epoch 3, step 354, the loss is 614190.6875\n",
            "in training loop, epoch 3, step 355, the loss is 363059.75\n",
            "in training loop, epoch 3, step 356, the loss is 470053.1875\n",
            "in training loop, epoch 3, step 357, the loss is 493369.5\n",
            "in training loop, epoch 3, step 358, the loss is 299729.875\n",
            "in training loop, epoch 3, step 359, the loss is 522786.4375\n",
            "in training loop, epoch 3, step 360, the loss is 394507.6875\n",
            "in training loop, epoch 3, step 361, the loss is 541335.375\n",
            "in training loop, epoch 3, step 362, the loss is 572444.125\n",
            "in training loop, epoch 3, step 363, the loss is 605926.5\n",
            "in training loop, epoch 3, step 364, the loss is 556782.125\n",
            "in training loop, epoch 3, step 365, the loss is 500206.5625\n",
            "in training loop, epoch 3, step 366, the loss is 525567.5625\n",
            "in training loop, epoch 3, step 367, the loss is 733975.8125\n",
            "in training loop, epoch 3, step 368, the loss is 483410.53125\n",
            "in training loop, epoch 3, step 369, the loss is 531366.0\n",
            "in training loop, epoch 3, step 370, the loss is 714073.1875\n",
            "in training loop, epoch 3, step 371, the loss is 600870.0625\n",
            "in training loop, epoch 3, step 372, the loss is 475267.125\n",
            "in training loop, epoch 3, step 373, the loss is 475944.125\n",
            "in training loop, epoch 3, step 374, the loss is 408831.3125\n",
            "in training loop, epoch 3, step 375, the loss is 549188.6875\n",
            "in training loop, epoch 3, step 376, the loss is 577969.6875\n",
            "in training loop, epoch 3, step 377, the loss is 747220.875\n",
            "in training loop, epoch 3, step 378, the loss is 719181.25\n",
            "in training loop, epoch 3, step 379, the loss is 538346.375\n",
            "in training loop, epoch 3, step 380, the loss is 474032.90625\n",
            "in training loop, epoch 3, step 381, the loss is 644582.4375\n",
            "in training loop, epoch 3, step 382, the loss is 422293.46875\n",
            "in training loop, epoch 3, step 383, the loss is 532483.8125\n",
            "in training loop, epoch 3, step 384, the loss is 501024.84375\n",
            "in training loop, epoch 3, step 385, the loss is 453025.65625\n",
            "in training loop, epoch 3, step 386, the loss is 553782.4375\n",
            "in training loop, epoch 3, step 387, the loss is 572145.125\n",
            "in training loop, epoch 3, step 388, the loss is 512739.6875\n",
            "in training loop, epoch 3, step 389, the loss is 346144.375\n",
            "in training loop, epoch 3, step 390, the loss is 499550.3125\n",
            "in training loop, epoch 3, step 391, the loss is 498090.59375\n",
            "in training loop, epoch 3, step 392, the loss is 788045.125\n",
            "in training loop, epoch 3, step 393, the loss is 733497.3125\n",
            "in training loop, epoch 3, step 394, the loss is 417216.03125\n",
            "in training loop, epoch 3, step 395, the loss is 512908.75\n",
            "in training loop, epoch 3, step 396, the loss is 520474.5\n",
            "in training loop, epoch 3, step 397, the loss is 494116.03125\n",
            "in training loop, epoch 3, step 398, the loss is 331418.375\n",
            "in training loop, epoch 3, step 399, the loss is 526882.5\n",
            "in training loop, epoch 3, step 400, the loss is 503622.4375\n",
            "in training loop, epoch 3, step 401, the loss is 535540.75\n",
            "in training loop, epoch 3, step 402, the loss is 458497.3125\n",
            "in training loop, epoch 3, step 403, the loss is 686655.0\n",
            "in training loop, epoch 3, step 404, the loss is 444749.0625\n",
            "in training loop, epoch 3, step 405, the loss is 509862.625\n",
            "in training loop, epoch 3, step 406, the loss is 385010.09375\n",
            "in training loop, epoch 3, step 407, the loss is 502858.625\n",
            "in training loop, epoch 3, step 408, the loss is 444296.28125\n",
            "in training loop, epoch 3, step 409, the loss is 516402.90625\n",
            "in training loop, epoch 3, step 410, the loss is 451585.34375\n",
            "in training loop, epoch 3, step 411, the loss is 564408.9375\n",
            "in training loop, epoch 3, step 412, the loss is 424806.3125\n",
            "in training loop, epoch 3, step 413, the loss is 464795.5\n",
            "in training loop, epoch 3, step 414, the loss is 438307.5\n",
            "in training loop, epoch 3, step 415, the loss is 392431.90625\n",
            "in training loop, epoch 3, step 416, the loss is 403717.0\n",
            "in training loop, epoch 3, step 417, the loss is 369820.75\n",
            "in training loop, epoch 3, step 418, the loss is 384214.5\n",
            "in training loop, epoch 3, step 419, the loss is 605611.5625\n",
            "in training loop, epoch 3, step 420, the loss is 336937.6875\n",
            "in training loop, epoch 3, step 421, the loss is 635939.75\n",
            "in training loop, epoch 3, step 422, the loss is 568804.3125\n",
            "in training loop, epoch 3, step 423, the loss is 607616.375\n",
            "in training loop, epoch 3, step 424, the loss is 473665.125\n",
            "in training loop, epoch 3, step 425, the loss is 421907.96875\n",
            "in training loop, epoch 3, step 426, the loss is 549257.125\n",
            "in training loop, epoch 3, step 427, the loss is 519523.3125\n",
            "in training loop, epoch 3, step 428, the loss is 402127.375\n",
            "in training loop, epoch 3, step 429, the loss is 354142.9375\n",
            "in training loop, epoch 3, step 430, the loss is 318613.46875\n",
            "in training loop, epoch 3, step 431, the loss is 309427.90625\n",
            "in training loop, epoch 3, step 432, the loss is 709630.6875\n",
            "in training loop, epoch 3, step 433, the loss is 559399.5\n",
            "in training loop, epoch 3, step 434, the loss is 445913.15625\n",
            "in training loop, epoch 3, step 435, the loss is 253336.703125\n",
            "in training loop, epoch 3, step 436, the loss is 635133.1875\n",
            "in training loop, epoch 3, step 437, the loss is 272924.0625\n",
            "in training loop, epoch 3, step 438, the loss is 544097.6875\n",
            "in training loop, epoch 3, step 439, the loss is 783285.9375\n",
            "in training loop, epoch 3, step 440, the loss is 512906.875\n",
            "in training loop, epoch 3, step 441, the loss is 523442.71875\n",
            "in training loop, epoch 3, step 442, the loss is 436667.5625\n",
            "in training loop, epoch 3, step 443, the loss is 580776.625\n",
            "in training loop, epoch 3, step 444, the loss is 620029.0625\n",
            "in training loop, epoch 3, step 445, the loss is 574577.375\n",
            "in training loop, epoch 3, step 446, the loss is 559054.3125\n",
            "in training loop, epoch 3, step 447, the loss is 677024.0\n",
            "in training loop, epoch 3, step 448, the loss is 938809.5\n",
            "in training loop, epoch 3, step 449, the loss is 326253.3125\n",
            "in training loop, epoch 3, step 450, the loss is 492838.03125\n",
            "in training loop, epoch 3, step 451, the loss is 371823.28125\n",
            "in training loop, epoch 3, step 452, the loss is 712689.1875\n",
            "in training loop, epoch 3, step 453, the loss is 680788.4375\n",
            "in training loop, epoch 3, step 454, the loss is 399604.5\n",
            "in training loop, epoch 3, step 455, the loss is 351109.5\n",
            "in training loop, epoch 3, step 456, the loss is 466234.875\n",
            "in training loop, epoch 3, step 457, the loss is 428765.875\n",
            "in training loop, epoch 3, step 458, the loss is 425126.6875\n",
            "in training loop, epoch 3, step 459, the loss is 602149.8125\n",
            "in training loop, epoch 3, step 460, the loss is 790403.125\n",
            "in training loop, epoch 3, step 461, the loss is 711414.875\n",
            "in training loop, epoch 3, step 462, the loss is 530643.375\n",
            "in training loop, epoch 3, step 463, the loss is 501483.125\n",
            "in training loop, epoch 3, step 464, the loss is 954187.25\n",
            "in training loop, epoch 3, step 465, the loss is 316018.9375\n",
            "in training loop, epoch 3, step 466, the loss is 246977.625\n",
            "in training loop, epoch 3, step 467, the loss is 632883.9375\n",
            "in training loop, epoch 3, step 468, the loss is 475907.65625\n",
            "in training loop, epoch 3, step 469, the loss is 491316.5\n",
            "in training loop, epoch 3, step 470, the loss is 771238.875\n",
            "in training loop, epoch 3, step 471, the loss is 601149.125\n",
            "in training loop, epoch 3, step 472, the loss is 637005.625\n",
            "in training loop, epoch 3, step 473, the loss is 693622.1875\n",
            "in training loop, epoch 3, step 474, the loss is 540920.125\n",
            "in training loop, epoch 3, step 475, the loss is 858609.5\n",
            "in training loop, epoch 3, step 476, the loss is 352630.3125\n",
            "in training loop, epoch 3, step 477, the loss is 681782.4375\n",
            "in training loop, epoch 3, step 478, the loss is 922408.625\n",
            "in training loop, epoch 3, step 479, the loss is 456166.03125\n",
            "in training loop, epoch 3, step 480, the loss is 421754.25\n",
            "in training loop, epoch 3, step 481, the loss is 733849.9375\n",
            "in training loop, epoch 3, step 482, the loss is 566146.1875\n",
            "in training loop, epoch 3, step 483, the loss is 595463.25\n",
            "in training loop, epoch 3, step 484, the loss is 254317.015625\n",
            "in training loop, epoch 3, step 485, the loss is 381137.71875\n",
            "in training loop, epoch 3, step 486, the loss is 571636.875\n",
            "in training loop, epoch 3, step 487, the loss is 469450.96875\n",
            "in training loop, epoch 3, step 488, the loss is 531423.5\n",
            "in training loop, epoch 3, step 489, the loss is 178980.921875\n",
            "in training loop, epoch 3, step 490, the loss is 400893.8125\n",
            "in training loop, epoch 3, step 491, the loss is 644040.25\n",
            "in training loop, epoch 3, step 492, the loss is 599697.375\n",
            "in training loop, epoch 3, step 493, the loss is 624014.625\n",
            "in training loop, epoch 3, step 494, the loss is 502229.75\n",
            "in training loop, epoch 3, step 495, the loss is 400343.1875\n",
            "in training loop, epoch 3, step 496, the loss is 614664.3125\n",
            "in training loop, epoch 3, step 497, the loss is 509198.28125\n",
            "in training loop, epoch 3, step 498, the loss is 583301.0\n",
            "in training loop, epoch 3, step 499, the loss is 525931.375\n",
            "in training loop, epoch 3, step 500, the loss is 434442.3125\n",
            "in training loop, epoch 3, step 501, the loss is 466359.46875\n",
            "in training loop, epoch 3, step 502, the loss is 715833.125\n",
            "in training loop, epoch 3, step 503, the loss is 550932.5625\n",
            "in training loop, epoch 3, step 504, the loss is 569397.625\n",
            "in training loop, epoch 3, step 505, the loss is 320511.65625\n",
            "in training loop, epoch 3, step 506, the loss is 395226.6875\n",
            "in training loop, epoch 3, step 507, the loss is 366070.15625\n",
            "in training loop, epoch 3, step 508, the loss is 426963.8125\n",
            "in training loop, epoch 3, step 509, the loss is 486223.78125\n",
            "in training loop, epoch 3, step 510, the loss is 563423.1875\n",
            "in training loop, epoch 3, step 511, the loss is 455662.125\n",
            "in training loop, epoch 3, step 512, the loss is 589735.8125\n",
            "in training loop, epoch 3, step 513, the loss is 355434.0\n",
            "in training loop, epoch 3, step 514, the loss is 521648.25\n",
            "in training loop, epoch 3, step 515, the loss is 707801.6875\n",
            "in training loop, epoch 3, step 516, the loss is 470366.46875\n",
            "in training loop, epoch 3, step 517, the loss is 483602.3125\n",
            "in training loop, epoch 3, step 518, the loss is 455566.15625\n",
            "in training loop, epoch 3, step 519, the loss is 330749.625\n",
            "in training loop, epoch 3, step 520, the loss is 329096.125\n",
            "in training loop, epoch 3, step 521, the loss is 423823.25\n",
            "in training loop, epoch 3, step 522, the loss is 735014.875\n",
            "in training loop, epoch 3, step 523, the loss is 407927.96875\n",
            "in training loop, epoch 3, step 524, the loss is 566450.4375\n",
            "in training loop, epoch 3, step 525, the loss is 530739.875\n",
            "in training loop, epoch 3, step 526, the loss is 539836.5625\n",
            "in training loop, epoch 3, step 527, the loss is 366668.9375\n",
            "in training loop, epoch 3, step 528, the loss is 631519.625\n",
            "in training loop, epoch 3, step 529, the loss is 738368.75\n",
            "in training loop, epoch 3, step 530, the loss is 341391.5\n",
            "in training loop, epoch 3, step 531, the loss is 563326.5625\n",
            "in training loop, epoch 3, step 532, the loss is 636207.75\n",
            "in training loop, epoch 3, step 533, the loss is 442864.3125\n",
            "in training loop, epoch 3, step 534, the loss is 544225.75\n",
            "in training loop, epoch 3, step 535, the loss is 547580.6875\n",
            "in training loop, epoch 3, step 536, the loss is 492480.21875\n",
            "in training loop, epoch 3, step 537, the loss is 384343.375\n",
            "in training loop, epoch 3, step 538, the loss is 533910.3125\n",
            "in training loop, epoch 3, step 539, the loss is 393728.71875\n",
            "in training loop, epoch 3, step 540, the loss is 401988.3125\n",
            "in training loop, epoch 3, step 541, the loss is 338110.34375\n",
            "in training loop, epoch 3, step 542, the loss is 458752.5\n",
            "in training loop, epoch 3, step 543, the loss is 534008.625\n",
            "in training loop, epoch 3, step 544, the loss is 719202.625\n",
            "in training loop, epoch 3, step 545, the loss is 536255.375\n",
            "in training loop, epoch 3, step 546, the loss is 361873.75\n",
            "in training loop, epoch 3, step 547, the loss is 695708.6875\n",
            "in training loop, epoch 3, step 548, the loss is 474288.40625\n",
            "in training loop, epoch 3, step 549, the loss is 546101.625\n",
            "in training loop, epoch 3, step 550, the loss is 629953.0\n",
            "in training loop, epoch 3, step 551, the loss is 421952.90625\n",
            "in training loop, epoch 3, step 552, the loss is 429658.21875\n",
            "in training loop, epoch 3, step 553, the loss is 599699.75\n",
            "in training loop, epoch 3, step 554, the loss is 410399.28125\n",
            "in training loop, epoch 3, step 555, the loss is 391839.6875\n",
            "in training loop, epoch 3, step 556, the loss is 581583.4375\n",
            "in training loop, epoch 3, step 557, the loss is 679339.3125\n",
            "in training loop, epoch 3, step 558, the loss is 713407.6875\n",
            "in training loop, epoch 3, step 559, the loss is 474070.125\n",
            "in training loop, epoch 3, step 560, the loss is 592530.75\n",
            "in training loop, epoch 3, step 561, the loss is 416879.28125\n",
            "in training loop, epoch 3, step 562, the loss is 551665.25\n",
            "in training loop, epoch 3, step 563, the loss is 439143.0\n",
            "in training loop, epoch 3, step 564, the loss is 509065.625\n",
            "in training loop, epoch 3, step 565, the loss is 883194.75\n",
            "in training loop, epoch 3, step 566, the loss is 488442.96875\n",
            "in training loop, epoch 3, step 567, the loss is 501407.0\n",
            "in training loop, epoch 3, step 568, the loss is 618527.0625\n",
            "in training loop, epoch 3, step 569, the loss is 664842.0625\n",
            "in training loop, epoch 3, step 570, the loss is 434631.09375\n",
            "in training loop, epoch 3, step 571, the loss is 490191.03125\n",
            "in training loop, epoch 3, step 572, the loss is 529304.0\n",
            "in training loop, epoch 3, step 573, the loss is 561791.6875\n",
            "in training loop, epoch 3, step 574, the loss is 682187.8125\n",
            "in training loop, epoch 3, step 575, the loss is 477560.1875\n",
            "in training loop, epoch 3, step 576, the loss is 568170.875\n",
            "in training loop, epoch 3, step 577, the loss is 434986.96875\n",
            "in training loop, epoch 3, step 578, the loss is 350614.5\n",
            "in training loop, epoch 3, step 579, the loss is 488736.03125\n",
            "in training loop, epoch 3, step 580, the loss is 522134.375\n",
            "in training loop, epoch 3, step 581, the loss is 392501.25\n",
            "in training loop, epoch 3, step 582, the loss is 500475.9375\n",
            "in training loop, epoch 3, step 583, the loss is 479406.8125\n",
            "in training loop, epoch 3, step 584, the loss is 647380.5625\n",
            "in training loop, epoch 3, step 585, the loss is 317550.65625\n",
            "in training loop, epoch 3, step 586, the loss is 503946.84375\n",
            "in training loop, epoch 3, step 587, the loss is 530517.9375\n",
            "in training loop, epoch 3, step 588, the loss is 500620.25\n",
            "in training loop, epoch 3, step 589, the loss is 339379.96875\n",
            "in training loop, epoch 3, step 590, the loss is 435903.875\n",
            "in training loop, epoch 3, step 591, the loss is 680996.4375\n",
            "in training loop, epoch 3, step 592, the loss is 800875.0625\n",
            "in training loop, epoch 3, step 593, the loss is 424399.875\n",
            "in training loop, epoch 3, step 594, the loss is 418807.125\n",
            "in training loop, epoch 3, step 595, the loss is 739249.0\n",
            "in training loop, epoch 3, step 596, the loss is 410335.375\n",
            "in training loop, epoch 3, step 597, the loss is 383100.84375\n",
            "in training loop, epoch 3, step 598, the loss is 444437.0625\n",
            "in training loop, epoch 3, step 599, the loss is 574459.3125\n",
            "in training loop, epoch 3, step 600, the loss is 866644.0625\n",
            "in training loop, epoch 3, step 601, the loss is 592243.25\n",
            "in training loop, epoch 3, step 602, the loss is 396217.5625\n",
            "in training loop, epoch 3, step 603, the loss is 556502.125\n",
            "in training loop, epoch 3, step 604, the loss is 610647.5625\n",
            "in training loop, epoch 3, step 605, the loss is 420703.875\n",
            "in training loop, epoch 3, step 606, the loss is 355420.34375\n",
            "in training loop, epoch 3, step 607, the loss is 543707.25\n",
            "in training loop, epoch 3, step 608, the loss is 295159.28125\n",
            "in training loop, epoch 3, step 609, the loss is 807536.875\n",
            "in training loop, epoch 3, step 610, the loss is 530568.375\n",
            "in training loop, epoch 3, step 611, the loss is 485436.65625\n",
            "in training loop, epoch 3, step 612, the loss is 404397.125\n",
            "in training loop, epoch 3, step 613, the loss is 436863.84375\n",
            "in training loop, epoch 3, step 614, the loss is 484809.9375\n",
            "in training loop, epoch 3, step 615, the loss is 444180.34375\n",
            "in training loop, epoch 3, step 616, the loss is 490752.9375\n",
            "in training loop, epoch 3, step 617, the loss is 528899.4375\n",
            "in training loop, epoch 3, step 618, the loss is 605666.5\n",
            "in training loop, epoch 3, step 619, the loss is 702734.0\n",
            "in training loop, epoch 3, step 620, the loss is 478572.8125\n",
            "in training loop, epoch 3, step 621, the loss is 440006.1875\n",
            "in training loop, epoch 3, step 622, the loss is 429508.5625\n",
            "in training loop, epoch 3, step 623, the loss is 642886.875\n",
            "in training loop, epoch 3, step 624, the loss is 435475.1875\n",
            "in training loop, epoch 3, step 625, the loss is 704391.5\n",
            "in training loop, epoch 3, step 626, the loss is 616375.4375\n",
            "in training loop, epoch 3, step 627, the loss is 517577.09375\n",
            "in training loop, epoch 3, step 628, the loss is 623635.625\n",
            "in training loop, epoch 3, step 629, the loss is 467076.125\n",
            "in training loop, epoch 3, step 630, the loss is 466761.65625\n",
            "in training loop, epoch 3, step 631, the loss is 510817.9375\n",
            "in training loop, epoch 3, step 632, the loss is 566281.8125\n",
            "in training loop, epoch 3, step 633, the loss is 762023.625\n",
            "in training loop, epoch 3, step 634, the loss is 406728.1875\n",
            "in training loop, epoch 3, step 635, the loss is 452878.46875\n",
            "in training loop, epoch 3, step 636, the loss is 407792.8125\n",
            "in training loop, epoch 3, step 637, the loss is 351383.625\n",
            "in training loop, epoch 3, step 638, the loss is 384514.625\n",
            "in training loop, epoch 3, step 639, the loss is 589941.4375\n",
            "in training loop, epoch 3, step 640, the loss is 468335.875\n",
            "in training loop, epoch 3, step 641, the loss is 392018.59375\n",
            "in training loop, epoch 3, step 642, the loss is 432448.09375\n",
            "in training loop, epoch 3, step 643, the loss is 667457.875\n",
            "in training loop, epoch 3, step 644, the loss is 345147.5\n",
            "in training loop, epoch 3, step 645, the loss is 406449.59375\n",
            "in training loop, epoch 3, step 646, the loss is 517571.03125\n",
            "in training loop, epoch 3, step 647, the loss is 441108.375\n",
            "in training loop, epoch 3, step 648, the loss is 508007.15625\n",
            "in training loop, epoch 3, step 649, the loss is 540131.4375\n",
            "in training loop, epoch 3, step 650, the loss is 732673.375\n",
            "in training loop, epoch 3, step 651, the loss is 374508.5625\n",
            "in training loop, epoch 3, step 652, the loss is 481615.1875\n",
            "in training loop, epoch 3, step 653, the loss is 444241.03125\n",
            "in training loop, epoch 3, step 654, the loss is 576997.875\n",
            "in training loop, epoch 3, step 655, the loss is 575064.125\n",
            "in training loop, epoch 3, step 656, the loss is 515560.375\n",
            "in training loop, epoch 3, step 657, the loss is 533000.25\n",
            "in training loop, epoch 3, step 658, the loss is 386688.5625\n",
            "in training loop, epoch 3, step 659, the loss is 618470.1875\n",
            "in training loop, epoch 3, step 660, the loss is 567773.25\n",
            "in training loop, epoch 3, step 661, the loss is 571806.9375\n",
            "in training loop, epoch 3, step 662, the loss is 577249.75\n",
            "in training loop, epoch 3, step 663, the loss is 472287.4375\n",
            "in training loop, epoch 3, step 664, the loss is 547511.9375\n",
            "in training loop, epoch 3, step 665, the loss is 544448.5\n",
            "in training loop, epoch 3, step 666, the loss is 172665.03125\n",
            "in training loop, epoch 3, step 667, the loss is 500154.65625\n",
            "in training loop, epoch 3, step 668, the loss is 407277.1875\n",
            "in training loop, epoch 3, step 669, the loss is 499132.0625\n",
            "in training loop, epoch 3, step 670, the loss is 632216.125\n",
            "in training loop, epoch 3, step 671, the loss is 489796.5625\n",
            "in training loop, epoch 3, step 672, the loss is 605829.625\n",
            "in training loop, epoch 3, step 673, the loss is 522967.3125\n",
            "in training loop, epoch 3, step 674, the loss is 465152.75\n",
            "in training loop, epoch 3, step 675, the loss is 461275.9375\n",
            "in training loop, epoch 3, step 676, the loss is 512620.65625\n",
            "in training loop, epoch 3, step 677, the loss is 391123.0\n",
            "in training loop, epoch 3, step 678, the loss is 520060.84375\n",
            "in training loop, epoch 3, step 679, the loss is 420978.90625\n",
            "in training loop, epoch 3, step 680, the loss is 510360.28125\n",
            "in training loop, epoch 3, step 681, the loss is 543701.5\n",
            "in training loop, epoch 3, step 682, the loss is 426090.4375\n",
            "in training loop, epoch 3, step 683, the loss is 611245.875\n",
            "in training loop, epoch 3, step 684, the loss is 605479.875\n",
            "in training loop, epoch 3, step 685, the loss is 596892.8125\n",
            "in training loop, epoch 3, step 686, the loss is 389051.3125\n",
            "in training loop, epoch 3, step 687, the loss is 448715.96875\n",
            "in training loop, epoch 3, step 688, the loss is 446532.6875\n",
            "in training loop, epoch 3, step 689, the loss is 509420.4375\n",
            "in training loop, epoch 3, step 690, the loss is 335424.40625\n",
            "in training loop, epoch 3, step 691, the loss is 485545.3125\n",
            "in training loop, epoch 3, step 692, the loss is 592175.875\n",
            "in training loop, epoch 3, step 693, the loss is 431811.9375\n",
            "in training loop, epoch 3, step 694, the loss is 597935.3125\n",
            "in training loop, epoch 3, step 695, the loss is 692973.75\n",
            "in training loop, epoch 3, step 696, the loss is 447982.84375\n",
            "in training loop, epoch 3, step 697, the loss is 641374.8125\n",
            "in training loop, epoch 3, step 698, the loss is 619844.4375\n",
            "in training loop, epoch 3, step 699, the loss is 350185.96875\n",
            "in training loop, epoch 3, step 700, the loss is 280327.1875\n",
            "in training loop, epoch 3, step 701, the loss is 652957.0\n",
            "in training loop, epoch 3, step 702, the loss is 597358.125\n",
            "in training loop, epoch 3, step 703, the loss is 354710.96875\n",
            "in training loop, epoch 3, step 704, the loss is 397553.5\n",
            "in training loop, epoch 3, step 705, the loss is 437523.0625\n",
            "in training loop, epoch 3, step 706, the loss is 339059.40625\n",
            "in training loop, epoch 3, step 707, the loss is 522245.4375\n",
            "in training loop, epoch 3, step 708, the loss is 604198.75\n",
            "in training loop, epoch 3, step 709, the loss is 493254.84375\n",
            "in training loop, epoch 3, step 710, the loss is 454669.75\n",
            "in training loop, epoch 3, step 711, the loss is 471662.84375\n",
            "in training loop, epoch 3, step 712, the loss is 452818.90625\n",
            "in training loop, epoch 3, step 713, the loss is 252838.1875\n",
            "in training loop, epoch 3, step 714, the loss is 593230.3125\n",
            "in training loop, epoch 3, step 715, the loss is 596139.0625\n",
            "in training loop, epoch 3, step 716, the loss is 624309.4375\n",
            "in training loop, epoch 3, step 717, the loss is 306399.59375\n",
            "in training loop, epoch 3, step 718, the loss is 659813.8125\n",
            "in training loop, epoch 3, step 719, the loss is 423266.25\n",
            "in training loop, epoch 3, step 720, the loss is 560680.1875\n",
            "in training loop, epoch 3, step 721, the loss is 398033.34375\n",
            "in training loop, epoch 3, step 722, the loss is 540781.125\n",
            "in training loop, epoch 3, step 723, the loss is 329815.25\n",
            "in training loop, epoch 3, step 724, the loss is 538067.0625\n",
            "in training loop, epoch 3, step 725, the loss is 517487.53125\n",
            "in training loop, epoch 3, step 726, the loss is 472286.1875\n",
            "in training loop, epoch 3, step 727, the loss is 400793.03125\n",
            "in training loop, epoch 3, step 728, the loss is 797766.4375\n",
            "in training loop, epoch 3, step 729, the loss is 501590.96875\n",
            "in training loop, epoch 3, step 730, the loss is 560489.6875\n",
            "in training loop, epoch 3, step 731, the loss is 612240.5\n",
            "in training loop, epoch 3, step 732, the loss is 490090.1875\n",
            "in training loop, epoch 3, step 733, the loss is 440118.875\n",
            "in training loop, epoch 3, step 734, the loss is 444826.15625\n",
            "in training loop, epoch 3, step 735, the loss is 453106.09375\n",
            "in training loop, epoch 3, step 736, the loss is 514329.59375\n",
            "in training loop, epoch 3, step 737, the loss is 535894.1875\n",
            "in training loop, epoch 3, step 738, the loss is 452045.71875\n",
            "in training loop, epoch 3, step 739, the loss is 564872.0625\n",
            "in training loop, epoch 3, step 740, the loss is 478494.25\n",
            "in training loop, epoch 3, step 741, the loss is 431905.5\n",
            "in training loop, epoch 3, step 742, the loss is 466474.03125\n",
            "in training loop, epoch 3, step 743, the loss is 536761.8125\n",
            "in training loop, epoch 3, step 744, the loss is 396172.59375\n",
            "in training loop, epoch 3, step 745, the loss is 516791.84375\n",
            "in training loop, epoch 3, step 746, the loss is 748463.625\n",
            "in training loop, epoch 3, step 747, the loss is 494915.875\n",
            "in training loop, epoch 3, step 748, the loss is 607445.3125\n",
            "in training loop, epoch 3, step 749, the loss is 718734.5625\n",
            "in training loop, epoch 3, step 750, the loss is 816601.875\n",
            "in training loop, epoch 3, step 751, the loss is 545378.875\n",
            "in training loop, epoch 3, step 752, the loss is 607485.4375\n",
            "in training loop, epoch 3, step 753, the loss is 325925.84375\n",
            "in training loop, epoch 3, step 754, the loss is 411877.3125\n",
            "in training loop, epoch 3, step 755, the loss is 668331.4375\n",
            "in training loop, epoch 3, step 756, the loss is 554134.5625\n",
            "in training loop, epoch 3, step 757, the loss is 653066.6875\n",
            "in training loop, epoch 3, step 758, the loss is 529636.75\n",
            "in training loop, epoch 3, step 759, the loss is 356753.90625\n",
            "in training loop, epoch 3, step 760, the loss is 598509.125\n",
            "in training loop, epoch 3, step 761, the loss is 464107.71875\n",
            "in training loop, epoch 3, step 762, the loss is 612059.6875\n",
            "in training loop, epoch 3, step 763, the loss is 641921.125\n",
            "in training loop, epoch 3, step 764, the loss is 506920.59375\n",
            "in training loop, epoch 3, step 765, the loss is 635567.75\n",
            "in training loop, epoch 3, step 766, the loss is 371309.78125\n",
            "in training loop, epoch 3, step 767, the loss is 488157.15625\n",
            "in training loop, epoch 3, step 768, the loss is 388702.28125\n",
            "in training loop, epoch 3, step 769, the loss is 426979.28125\n",
            "in training loop, epoch 3, step 770, the loss is 455851.75\n",
            "in training loop, epoch 3, step 771, the loss is 470348.46875\n",
            "in training loop, epoch 3, step 772, the loss is 306196.625\n",
            "in training loop, epoch 3, step 773, the loss is 648252.6875\n",
            "in training loop, epoch 3, step 774, the loss is 506306.53125\n",
            "in training loop, epoch 3, step 775, the loss is 295310.4375\n",
            "in training loop, epoch 3, step 776, the loss is 584040.5\n",
            "in training loop, epoch 3, step 777, the loss is 443377.875\n",
            "in training loop, epoch 3, step 778, the loss is 331993.03125\n",
            "in training loop, epoch 3, step 779, the loss is 528485.25\n",
            "in training loop, epoch 3, step 780, the loss is 649280.875\n",
            "in training loop, epoch 3, step 781, the loss is 482601.875\n",
            "in training loop, epoch 3, step 782, the loss is 303655.375\n",
            "in training loop, epoch 3, step 783, the loss is 508643.65625\n",
            "in training loop, epoch 3, step 784, the loss is 655982.25\n",
            "in training loop, epoch 3, step 785, the loss is 560728.625\n",
            "in training loop, epoch 3, step 786, the loss is 491580.5625\n",
            "in training loop, epoch 3, step 787, the loss is 377354.875\n",
            "in training loop, epoch 3, step 788, the loss is 543133.125\n",
            "in training loop, epoch 3, step 789, the loss is 927763.75\n",
            "in training loop, epoch 3, step 790, the loss is 764254.8125\n",
            "in training loop, epoch 3, step 791, the loss is 498277.40625\n",
            "in training loop, epoch 3, step 792, the loss is 475291.09375\n",
            "in training loop, epoch 3, step 793, the loss is 549940.625\n",
            "in training loop, epoch 3, step 794, the loss is 466844.40625\n",
            "in training loop, epoch 3, step 795, the loss is 261857.5625\n",
            "in training loop, epoch 3, step 796, the loss is 367105.65625\n",
            "in training loop, epoch 3, step 797, the loss is 606983.25\n",
            "in training loop, epoch 3, step 798, the loss is 533425.125\n",
            "in training loop, epoch 3, step 799, the loss is 503151.5625\n",
            "in training loop, epoch 3, step 800, the loss is 409917.6875\n",
            "in training loop, epoch 3, step 801, the loss is 365479.9375\n",
            "in training loop, epoch 3, step 802, the loss is 565255.0625\n",
            "in training loop, epoch 3, step 803, the loss is 362566.0\n",
            "in training loop, epoch 3, step 804, the loss is 626262.3125\n",
            "in training loop, epoch 3, step 805, the loss is 500653.9375\n",
            "in training loop, epoch 3, step 806, the loss is 490391.90625\n",
            "in training loop, epoch 3, step 807, the loss is 366205.125\n",
            "in training loop, epoch 3, step 808, the loss is 517331.125\n",
            "in training loop, epoch 3, step 809, the loss is 640874.375\n",
            "in training loop, epoch 3, step 810, the loss is 638543.875\n",
            "in training loop, epoch 3, step 811, the loss is 293589.0625\n",
            "in training loop, epoch 3, step 812, the loss is 591692.9375\n",
            "in training loop, epoch 3, step 813, the loss is 651540.75\n",
            "in training loop, epoch 3, step 814, the loss is 570351.25\n",
            "in training loop, epoch 3, step 815, the loss is 405315.21875\n",
            "in training loop, epoch 3, step 816, the loss is 401759.15625\n",
            "in training loop, epoch 3, step 817, the loss is 542708.625\n",
            "in training loop, epoch 3, step 818, the loss is 326047.15625\n",
            "in training loop, epoch 3, step 819, the loss is 573258.8125\n",
            "in training loop, epoch 3, step 820, the loss is 367490.6875\n",
            "in training loop, epoch 3, step 821, the loss is 400946.65625\n",
            "in training loop, epoch 3, step 822, the loss is 508860.96875\n",
            "in training loop, epoch 3, step 823, the loss is 478267.34375\n",
            "in training loop, epoch 3, step 824, the loss is 411948.875\n",
            "in training loop, epoch 3, step 825, the loss is 445995.03125\n",
            "in training loop, epoch 3, step 826, the loss is 720157.5\n",
            "in training loop, epoch 3, step 827, the loss is 455444.125\n",
            "in training loop, epoch 3, step 828, the loss is 364892.96875\n",
            "in training loop, epoch 3, step 829, the loss is 421411.09375\n",
            "in training loop, epoch 3, step 830, the loss is 759703.125\n",
            "in training loop, epoch 3, step 831, the loss is 561895.25\n",
            "in training loop, epoch 3, step 832, the loss is 628025.125\n",
            "in training loop, epoch 3, step 833, the loss is 426681.84375\n",
            "in training loop, epoch 3, step 834, the loss is 438074.375\n",
            "in training loop, epoch 3, step 835, the loss is 586923.875\n",
            "in training loop, epoch 3, step 836, the loss is 460638.0\n",
            "in training loop, epoch 3, step 837, the loss is 816462.8125\n",
            "in training loop, epoch 3, step 838, the loss is 247721.109375\n",
            "in training loop, epoch 3, step 839, the loss is 574810.25\n",
            "in training loop, epoch 3, step 840, the loss is 613641.4375\n",
            "in training loop, epoch 3, step 841, the loss is 461559.84375\n",
            "in training loop, epoch 3, step 842, the loss is 487360.4375\n",
            "in training loop, epoch 3, step 843, the loss is 570167.8125\n",
            "in training loop, epoch 3, step 844, the loss is 393537.625\n",
            "in training loop, epoch 3, step 845, the loss is 500106.875\n",
            "in training loop, epoch 3, step 846, the loss is 457626.9375\n",
            "in training loop, epoch 3, step 847, the loss is 394586.90625\n",
            "in training loop, epoch 3, step 848, the loss is 520663.15625\n",
            "in training loop, epoch 3, step 849, the loss is 509260.9375\n",
            "in training loop, epoch 3, step 850, the loss is 650546.5\n",
            "in training loop, epoch 3, step 851, the loss is 513159.875\n",
            "in training loop, epoch 3, step 852, the loss is 747830.25\n",
            "in training loop, epoch 3, step 853, the loss is 410936.09375\n",
            "in training loop, epoch 3, step 854, the loss is 515948.875\n",
            "in training loop, epoch 3, step 855, the loss is 486547.21875\n",
            "in training loop, epoch 3, step 856, the loss is 335270.1875\n",
            "in training loop, epoch 3, step 857, the loss is 574145.0625\n",
            "in training loop, epoch 3, step 858, the loss is 554225.875\n",
            "in training loop, epoch 3, step 859, the loss is 637614.75\n",
            "in training loop, epoch 3, step 860, the loss is 349808.875\n",
            "in training loop, epoch 3, step 861, the loss is 396638.90625\n",
            "in training loop, epoch 3, step 862, the loss is 613361.5\n",
            "in training loop, epoch 3, step 863, the loss is 264157.96875\n",
            "in training loop, epoch 3, step 864, the loss is 351923.46875\n",
            "in training loop, epoch 3, step 865, the loss is 602620.375\n",
            "in training loop, epoch 3, step 866, the loss is 487747.28125\n",
            "in training loop, epoch 3, step 867, the loss is 474407.0\n",
            "in training loop, epoch 3, step 868, the loss is 493289.625\n",
            "in training loop, epoch 3, step 869, the loss is 677620.75\n",
            "in training loop, epoch 3, step 870, the loss is 283456.21875\n",
            "in training loop, epoch 3, step 871, the loss is 322513.3125\n",
            "in training loop, epoch 3, step 872, the loss is 499741.34375\n",
            "in training loop, epoch 3, step 873, the loss is 499106.8125\n",
            "in training loop, epoch 3, step 874, the loss is 403557.53125\n",
            "in training loop, epoch 3, step 875, the loss is 514053.375\n",
            "in training loop, epoch 3, step 876, the loss is 379125.03125\n",
            "in training loop, epoch 3, step 877, the loss is 490289.46875\n",
            "in training loop, epoch 3, step 878, the loss is 641536.0625\n",
            "in training loop, epoch 3, step 879, the loss is 359397.78125\n",
            "in training loop, epoch 3, step 880, the loss is 528539.625\n",
            "in training loop, epoch 3, step 881, the loss is 438148.375\n",
            "in training loop, epoch 3, step 882, the loss is 370955.78125\n",
            "in training loop, epoch 3, step 883, the loss is 602038.125\n",
            "in training loop, epoch 3, step 884, the loss is 500137.25\n",
            "in training loop, epoch 3, step 885, the loss is 469130.8125\n",
            "in training loop, epoch 3, step 886, the loss is 338832.4375\n",
            "in training loop, epoch 3, step 887, the loss is 526123.5625\n",
            "in training loop, epoch 3, step 888, the loss is 515081.6875\n",
            "in training loop, epoch 3, step 889, the loss is 410727.375\n",
            "in training loop, epoch 3, step 890, the loss is 598238.5625\n",
            "in training loop, epoch 3, step 891, the loss is 419980.625\n",
            "in training loop, epoch 3, step 892, the loss is 626024.0\n",
            "in training loop, epoch 3, step 893, the loss is 344330.53125\n",
            "in training loop, epoch 3, step 894, the loss is 809030.5\n",
            "in training loop, epoch 3, step 895, the loss is 489540.78125\n",
            "in training loop, epoch 3, step 896, the loss is 755412.6875\n",
            "in training loop, epoch 3, step 897, the loss is 330622.09375\n",
            "in training loop, epoch 3, step 898, the loss is 498497.125\n",
            "in training loop, epoch 3, step 899, the loss is 455277.34375\n",
            "in training loop, epoch 3, step 900, the loss is 455797.4375\n",
            "in training loop, epoch 3, step 901, the loss is 492766.6875\n",
            "in training loop, epoch 3, step 902, the loss is 619135.375\n",
            "in training loop, epoch 3, step 903, the loss is 226163.0\n",
            "k-fold 1:: Epoch 3: train loss 539671.2624792588 val loss 652058.5091274752\n",
            "in training loop, epoch 4, step 0, the loss is 419615.6875\n",
            "in training loop, epoch 4, step 1, the loss is 427401.09375\n",
            "in training loop, epoch 4, step 2, the loss is 984464.25\n",
            "in training loop, epoch 4, step 3, the loss is 413176.1875\n",
            "in training loop, epoch 4, step 4, the loss is 596515.125\n",
            "in training loop, epoch 4, step 5, the loss is 599116.125\n",
            "in training loop, epoch 4, step 6, the loss is 351898.46875\n",
            "in training loop, epoch 4, step 7, the loss is 398274.53125\n",
            "in training loop, epoch 4, step 8, the loss is 490834.5\n",
            "in training loop, epoch 4, step 9, the loss is 393939.09375\n",
            "in training loop, epoch 4, step 10, the loss is 406141.03125\n",
            "in training loop, epoch 4, step 11, the loss is 478590.25\n",
            "in training loop, epoch 4, step 12, the loss is 391507.25\n",
            "in training loop, epoch 4, step 13, the loss is 467976.34375\n",
            "in training loop, epoch 4, step 14, the loss is 441151.25\n",
            "in training loop, epoch 4, step 15, the loss is 294676.375\n",
            "in training loop, epoch 4, step 16, the loss is 480116.0625\n",
            "in training loop, epoch 4, step 17, the loss is 591381.75\n",
            "in training loop, epoch 4, step 18, the loss is 411061.65625\n",
            "in training loop, epoch 4, step 19, the loss is 285864.5\n",
            "in training loop, epoch 4, step 20, the loss is 447338.125\n",
            "in training loop, epoch 4, step 21, the loss is 333655.0\n",
            "in training loop, epoch 4, step 22, the loss is 425289.75\n",
            "in training loop, epoch 4, step 23, the loss is 655916.75\n",
            "in training loop, epoch 4, step 24, the loss is 451271.90625\n",
            "in training loop, epoch 4, step 25, the loss is 304032.75\n",
            "in training loop, epoch 4, step 26, the loss is 277678.71875\n",
            "in training loop, epoch 4, step 27, the loss is 353125.4375\n",
            "in training loop, epoch 4, step 28, the loss is 457795.84375\n",
            "in training loop, epoch 4, step 29, the loss is 338868.5\n",
            "in training loop, epoch 4, step 30, the loss is 898513.9375\n",
            "in training loop, epoch 4, step 31, the loss is 459950.28125\n",
            "in training loop, epoch 4, step 32, the loss is 372126.125\n",
            "in training loop, epoch 4, step 33, the loss is 570330.5\n",
            "in training loop, epoch 4, step 34, the loss is 578064.5\n",
            "in training loop, epoch 4, step 35, the loss is 465373.96875\n",
            "in training loop, epoch 4, step 36, the loss is 463986.625\n",
            "in training loop, epoch 4, step 37, the loss is 431206.0625\n",
            "in training loop, epoch 4, step 38, the loss is 375718.125\n",
            "in training loop, epoch 4, step 39, the loss is 427241.5\n",
            "in training loop, epoch 4, step 40, the loss is 354779.09375\n",
            "in training loop, epoch 4, step 41, the loss is 319763.5625\n",
            "in training loop, epoch 4, step 42, the loss is 213712.90625\n",
            "in training loop, epoch 4, step 43, the loss is 403884.75\n",
            "in training loop, epoch 4, step 44, the loss is 574667.9375\n",
            "in training loop, epoch 4, step 45, the loss is 252607.78125\n",
            "in training loop, epoch 4, step 46, the loss is 286415.28125\n",
            "in training loop, epoch 4, step 47, the loss is 420769.84375\n",
            "in training loop, epoch 4, step 48, the loss is 450454.75\n",
            "in training loop, epoch 4, step 49, the loss is 441844.09375\n",
            "in training loop, epoch 4, step 50, the loss is 244631.4375\n",
            "in training loop, epoch 4, step 51, the loss is 516491.3125\n",
            "in training loop, epoch 4, step 52, the loss is 431557.5\n",
            "in training loop, epoch 4, step 53, the loss is 374826.09375\n",
            "in training loop, epoch 4, step 54, the loss is 456310.46875\n",
            "in training loop, epoch 4, step 55, the loss is 366928.375\n",
            "in training loop, epoch 4, step 56, the loss is 400516.59375\n",
            "in training loop, epoch 4, step 57, the loss is 343376.53125\n",
            "in training loop, epoch 4, step 58, the loss is 662273.25\n",
            "in training loop, epoch 4, step 59, the loss is 432274.21875\n",
            "in training loop, epoch 4, step 60, the loss is 417281.5\n",
            "in training loop, epoch 4, step 61, the loss is 406069.78125\n",
            "in training loop, epoch 4, step 62, the loss is 377850.625\n",
            "in training loop, epoch 4, step 63, the loss is 326922.34375\n",
            "in training loop, epoch 4, step 64, the loss is 515846.53125\n",
            "in training loop, epoch 4, step 65, the loss is 428552.90625\n",
            "in training loop, epoch 4, step 66, the loss is 374966.375\n",
            "in training loop, epoch 4, step 67, the loss is 426795.9375\n",
            "in training loop, epoch 4, step 68, the loss is 471624.84375\n",
            "in training loop, epoch 4, step 69, the loss is 358798.28125\n",
            "in training loop, epoch 4, step 70, the loss is 552165.5\n",
            "in training loop, epoch 4, step 71, the loss is 408902.0\n",
            "in training loop, epoch 4, step 72, the loss is 362391.4375\n",
            "in training loop, epoch 4, step 73, the loss is 567462.125\n",
            "in training loop, epoch 4, step 74, the loss is 268017.53125\n",
            "in training loop, epoch 4, step 75, the loss is 541316.9375\n",
            "in training loop, epoch 4, step 76, the loss is 494676.875\n",
            "in training loop, epoch 4, step 77, the loss is 453714.375\n",
            "in training loop, epoch 4, step 78, the loss is 396111.625\n",
            "in training loop, epoch 4, step 79, the loss is 368409.03125\n",
            "in training loop, epoch 4, step 80, the loss is 523914.75\n",
            "in training loop, epoch 4, step 81, the loss is 483658.0\n",
            "in training loop, epoch 4, step 82, the loss is 382171.71875\n",
            "in training loop, epoch 4, step 83, the loss is 307793.71875\n",
            "in training loop, epoch 4, step 84, the loss is 494766.59375\n",
            "in training loop, epoch 4, step 85, the loss is 918432.8125\n",
            "in training loop, epoch 4, step 86, the loss is 334261.09375\n",
            "in training loop, epoch 4, step 87, the loss is 597704.625\n",
            "in training loop, epoch 4, step 88, the loss is 636048.8125\n",
            "in training loop, epoch 4, step 89, the loss is 784291.5\n",
            "in training loop, epoch 4, step 90, the loss is 451255.9375\n",
            "in training loop, epoch 4, step 91, the loss is 687155.125\n",
            "in training loop, epoch 4, step 92, the loss is 395407.6875\n",
            "in training loop, epoch 4, step 93, the loss is 612730.0625\n",
            "in training loop, epoch 4, step 94, the loss is 600069.3125\n",
            "in training loop, epoch 4, step 95, the loss is 546165.1875\n",
            "in training loop, epoch 4, step 96, the loss is 454666.59375\n",
            "in training loop, epoch 4, step 97, the loss is 471526.9375\n",
            "in training loop, epoch 4, step 98, the loss is 571912.5\n",
            "in training loop, epoch 4, step 99, the loss is 475683.3125\n",
            "in training loop, epoch 4, step 100, the loss is 515758.0\n",
            "in training loop, epoch 4, step 101, the loss is 574940.0625\n",
            "in training loop, epoch 4, step 102, the loss is 506818.0\n",
            "in training loop, epoch 4, step 103, the loss is 501216.375\n",
            "in training loop, epoch 4, step 104, the loss is 462006.5\n",
            "in training loop, epoch 4, step 105, the loss is 413691.0\n",
            "in training loop, epoch 4, step 106, the loss is 506329.59375\n",
            "in training loop, epoch 4, step 107, the loss is 472677.5\n",
            "in training loop, epoch 4, step 108, the loss is 590248.0\n",
            "in training loop, epoch 4, step 109, the loss is 515155.78125\n",
            "in training loop, epoch 4, step 110, the loss is 371832.9375\n",
            "in training loop, epoch 4, step 111, the loss is 353815.9375\n",
            "in training loop, epoch 4, step 112, the loss is 661930.3125\n",
            "in training loop, epoch 4, step 113, the loss is 443763.125\n",
            "in training loop, epoch 4, step 114, the loss is 509585.21875\n",
            "in training loop, epoch 4, step 115, the loss is 542843.8125\n",
            "in training loop, epoch 4, step 116, the loss is 551525.0625\n",
            "in training loop, epoch 4, step 117, the loss is 517983.03125\n",
            "in training loop, epoch 4, step 118, the loss is 303921.875\n",
            "in training loop, epoch 4, step 119, the loss is 416984.125\n",
            "in training loop, epoch 4, step 120, the loss is 550242.4375\n",
            "in training loop, epoch 4, step 121, the loss is 212544.6875\n",
            "in training loop, epoch 4, step 122, the loss is 467792.59375\n",
            "in training loop, epoch 4, step 123, the loss is 324187.75\n",
            "in training loop, epoch 4, step 124, the loss is 235761.046875\n",
            "in training loop, epoch 4, step 125, the loss is 331614.25\n",
            "in training loop, epoch 4, step 126, the loss is 368786.3125\n",
            "in training loop, epoch 4, step 127, the loss is 555878.75\n",
            "in training loop, epoch 4, step 128, the loss is 370002.9375\n",
            "in training loop, epoch 4, step 129, the loss is 414858.25\n",
            "in training loop, epoch 4, step 130, the loss is 705979.625\n",
            "in training loop, epoch 4, step 131, the loss is 412245.5625\n",
            "in training loop, epoch 4, step 132, the loss is 470061.21875\n",
            "in training loop, epoch 4, step 133, the loss is 509301.65625\n",
            "in training loop, epoch 4, step 134, the loss is 464919.15625\n",
            "in training loop, epoch 4, step 135, the loss is 318528.21875\n",
            "in training loop, epoch 4, step 136, the loss is 379192.0\n",
            "in training loop, epoch 4, step 137, the loss is 349455.15625\n",
            "in training loop, epoch 4, step 138, the loss is 533998.6875\n",
            "in training loop, epoch 4, step 139, the loss is 373027.5\n",
            "in training loop, epoch 4, step 140, the loss is 349761.40625\n",
            "in training loop, epoch 4, step 141, the loss is 403683.5625\n",
            "in training loop, epoch 4, step 142, the loss is 235557.375\n",
            "in training loop, epoch 4, step 143, the loss is 293829.53125\n",
            "in training loop, epoch 4, step 144, the loss is 433498.25\n",
            "in training loop, epoch 4, step 145, the loss is 624466.5625\n",
            "in training loop, epoch 4, step 146, the loss is 500999.40625\n",
            "in training loop, epoch 4, step 147, the loss is 480947.34375\n",
            "in training loop, epoch 4, step 148, the loss is 342333.34375\n",
            "in training loop, epoch 4, step 149, the loss is 259333.5625\n",
            "in training loop, epoch 4, step 150, the loss is 646713.75\n",
            "in training loop, epoch 4, step 151, the loss is 384564.65625\n",
            "in training loop, epoch 4, step 152, the loss is 485087.53125\n",
            "in training loop, epoch 4, step 153, the loss is 511201.6875\n",
            "in training loop, epoch 4, step 154, the loss is 361325.71875\n",
            "in training loop, epoch 4, step 155, the loss is 466413.46875\n",
            "in training loop, epoch 4, step 156, the loss is 582738.75\n",
            "in training loop, epoch 4, step 157, the loss is 311236.8125\n",
            "in training loop, epoch 4, step 158, the loss is 604796.625\n",
            "in training loop, epoch 4, step 159, the loss is 448946.6875\n",
            "in training loop, epoch 4, step 160, the loss is 298066.03125\n",
            "in training loop, epoch 4, step 161, the loss is 455307.75\n",
            "in training loop, epoch 4, step 162, the loss is 465746.28125\n",
            "in training loop, epoch 4, step 163, the loss is 336520.71875\n",
            "in training loop, epoch 4, step 164, the loss is 631048.0\n",
            "in training loop, epoch 4, step 165, the loss is 412173.71875\n",
            "in training loop, epoch 4, step 166, the loss is 421000.625\n",
            "in training loop, epoch 4, step 167, the loss is 469671.5\n",
            "in training loop, epoch 4, step 168, the loss is 385910.875\n",
            "in training loop, epoch 4, step 169, the loss is 407453.15625\n",
            "in training loop, epoch 4, step 170, the loss is 381959.25\n",
            "in training loop, epoch 4, step 171, the loss is 460870.9375\n",
            "in training loop, epoch 4, step 172, the loss is 513877.34375\n",
            "in training loop, epoch 4, step 173, the loss is 463779.53125\n",
            "in training loop, epoch 4, step 174, the loss is 414084.25\n",
            "in training loop, epoch 4, step 175, the loss is 442410.25\n",
            "in training loop, epoch 4, step 176, the loss is 433603.3125\n",
            "in training loop, epoch 4, step 177, the loss is 479903.875\n",
            "in training loop, epoch 4, step 178, the loss is 390383.84375\n",
            "in training loop, epoch 4, step 179, the loss is 464113.53125\n",
            "in training loop, epoch 4, step 180, the loss is 456576.65625\n",
            "in training loop, epoch 4, step 181, the loss is 254073.875\n",
            "in training loop, epoch 4, step 182, the loss is 533164.625\n",
            "in training loop, epoch 4, step 183, the loss is 495173.5\n",
            "in training loop, epoch 4, step 184, the loss is 471481.4375\n",
            "in training loop, epoch 4, step 185, the loss is 417777.78125\n",
            "in training loop, epoch 4, step 186, the loss is 490743.40625\n",
            "in training loop, epoch 4, step 187, the loss is 472645.375\n",
            "in training loop, epoch 4, step 188, the loss is 379692.5\n",
            "in training loop, epoch 4, step 189, the loss is 581708.375\n",
            "in training loop, epoch 4, step 190, the loss is 416553.25\n",
            "in training loop, epoch 4, step 191, the loss is 428855.125\n",
            "in training loop, epoch 4, step 192, the loss is 435556.25\n",
            "in training loop, epoch 4, step 193, the loss is 323119.0\n",
            "in training loop, epoch 4, step 194, the loss is 621221.375\n",
            "in training loop, epoch 4, step 195, the loss is 379660.875\n",
            "in training loop, epoch 4, step 196, the loss is 332571.875\n",
            "in training loop, epoch 4, step 197, the loss is 488088.8125\n",
            "in training loop, epoch 4, step 198, the loss is 642888.125\n",
            "in training loop, epoch 4, step 199, the loss is 449138.4375\n",
            "in training loop, epoch 4, step 200, the loss is 521268.71875\n",
            "in training loop, epoch 4, step 201, the loss is 520991.0625\n",
            "in training loop, epoch 4, step 202, the loss is 522140.875\n",
            "in training loop, epoch 4, step 203, the loss is 509740.09375\n",
            "in training loop, epoch 4, step 204, the loss is 561329.8125\n",
            "in training loop, epoch 4, step 205, the loss is 542727.1875\n",
            "in training loop, epoch 4, step 206, the loss is 304221.71875\n",
            "in training loop, epoch 4, step 207, the loss is 410971.53125\n",
            "in training loop, epoch 4, step 208, the loss is 398904.25\n",
            "in training loop, epoch 4, step 209, the loss is 572869.75\n",
            "in training loop, epoch 4, step 210, the loss is 471470.8125\n",
            "in training loop, epoch 4, step 211, the loss is 433898.96875\n",
            "in training loop, epoch 4, step 212, the loss is 549298.9375\n",
            "in training loop, epoch 4, step 213, the loss is 406514.59375\n",
            "in training loop, epoch 4, step 214, the loss is 396564.40625\n",
            "in training loop, epoch 4, step 215, the loss is 404158.0625\n",
            "in training loop, epoch 4, step 216, the loss is 580173.75\n",
            "in training loop, epoch 4, step 217, the loss is 428863.9375\n",
            "in training loop, epoch 4, step 218, the loss is 763018.5\n",
            "in training loop, epoch 4, step 219, the loss is 440332.15625\n",
            "in training loop, epoch 4, step 220, the loss is 618121.625\n",
            "in training loop, epoch 4, step 221, the loss is 480488.3125\n",
            "in training loop, epoch 4, step 222, the loss is 345905.46875\n",
            "in training loop, epoch 4, step 223, the loss is 350846.40625\n",
            "in training loop, epoch 4, step 224, the loss is 517046.8125\n",
            "in training loop, epoch 4, step 225, the loss is 507948.53125\n",
            "in training loop, epoch 4, step 226, the loss is 647843.875\n",
            "in training loop, epoch 4, step 227, the loss is 504883.09375\n",
            "in training loop, epoch 4, step 228, the loss is 476551.09375\n",
            "in training loop, epoch 4, step 229, the loss is 407408.8125\n",
            "in training loop, epoch 4, step 230, the loss is 375772.65625\n",
            "in training loop, epoch 4, step 231, the loss is 765152.6875\n",
            "in training loop, epoch 4, step 232, the loss is 593303.75\n",
            "in training loop, epoch 4, step 233, the loss is 550950.0\n",
            "in training loop, epoch 4, step 234, the loss is 423060.96875\n",
            "in training loop, epoch 4, step 235, the loss is 589788.25\n",
            "in training loop, epoch 4, step 236, the loss is 411850.125\n",
            "in training loop, epoch 4, step 237, the loss is 509623.375\n",
            "in training loop, epoch 4, step 238, the loss is 484563.875\n",
            "in training loop, epoch 4, step 239, the loss is 483428.8125\n",
            "in training loop, epoch 4, step 240, the loss is 387644.4375\n",
            "in training loop, epoch 4, step 241, the loss is 390018.0\n",
            "in training loop, epoch 4, step 242, the loss is 474017.8125\n",
            "in training loop, epoch 4, step 243, the loss is 463498.375\n",
            "in training loop, epoch 4, step 244, the loss is 487016.21875\n",
            "in training loop, epoch 4, step 245, the loss is 851587.6875\n",
            "in training loop, epoch 4, step 246, the loss is 414292.09375\n",
            "in training loop, epoch 4, step 247, the loss is 361419.1875\n",
            "in training loop, epoch 4, step 248, the loss is 648376.0625\n",
            "in training loop, epoch 4, step 249, the loss is 451384.6875\n",
            "in training loop, epoch 4, step 250, the loss is 316708.71875\n",
            "in training loop, epoch 4, step 251, the loss is 507138.78125\n",
            "in training loop, epoch 4, step 252, the loss is 641082.25\n",
            "in training loop, epoch 4, step 253, the loss is 289057.78125\n",
            "in training loop, epoch 4, step 254, the loss is 489261.6875\n",
            "in training loop, epoch 4, step 255, the loss is 367772.25\n",
            "in training loop, epoch 4, step 256, the loss is 443374.75\n",
            "in training loop, epoch 4, step 257, the loss is 416210.5\n",
            "in training loop, epoch 4, step 258, the loss is 307918.6875\n",
            "in training loop, epoch 4, step 259, the loss is 471791.125\n",
            "in training loop, epoch 4, step 260, the loss is 572171.0\n",
            "in training loop, epoch 4, step 261, the loss is 418800.375\n",
            "in training loop, epoch 4, step 262, the loss is 335736.375\n",
            "in training loop, epoch 4, step 263, the loss is 377223.15625\n",
            "in training loop, epoch 4, step 264, the loss is 658935.25\n",
            "in training loop, epoch 4, step 265, the loss is 296248.84375\n",
            "in training loop, epoch 4, step 266, the loss is 481655.78125\n",
            "in training loop, epoch 4, step 267, the loss is 491141.5625\n",
            "in training loop, epoch 4, step 268, the loss is 476771.78125\n",
            "in training loop, epoch 4, step 269, the loss is 478033.28125\n",
            "in training loop, epoch 4, step 270, the loss is 467439.34375\n",
            "in training loop, epoch 4, step 271, the loss is 313646.0625\n",
            "in training loop, epoch 4, step 272, the loss is 340329.375\n",
            "in training loop, epoch 4, step 273, the loss is 306923.84375\n",
            "in training loop, epoch 4, step 274, the loss is 369608.4375\n",
            "in training loop, epoch 4, step 275, the loss is 357412.75\n",
            "in training loop, epoch 4, step 276, the loss is 488911.0\n",
            "in training loop, epoch 4, step 277, the loss is 437674.1875\n",
            "in training loop, epoch 4, step 278, the loss is 264962.90625\n",
            "in training loop, epoch 4, step 279, the loss is 353781.34375\n",
            "in training loop, epoch 4, step 280, the loss is 427586.375\n",
            "in training loop, epoch 4, step 281, the loss is 440290.53125\n",
            "in training loop, epoch 4, step 282, the loss is 521841.0\n",
            "in training loop, epoch 4, step 283, the loss is 457038.96875\n",
            "in training loop, epoch 4, step 284, the loss is 496999.8125\n",
            "in training loop, epoch 4, step 285, the loss is 381074.96875\n",
            "in training loop, epoch 4, step 286, the loss is 327478.84375\n",
            "in training loop, epoch 4, step 287, the loss is 404130.09375\n",
            "in training loop, epoch 4, step 288, the loss is 646945.875\n",
            "in training loop, epoch 4, step 289, the loss is 360840.0\n",
            "in training loop, epoch 4, step 290, the loss is 448753.59375\n",
            "in training loop, epoch 4, step 291, the loss is 451844.03125\n",
            "in training loop, epoch 4, step 292, the loss is 400261.65625\n",
            "in training loop, epoch 4, step 293, the loss is 496746.53125\n",
            "in training loop, epoch 4, step 294, the loss is 542600.0\n",
            "in training loop, epoch 4, step 295, the loss is 309997.8125\n",
            "in training loop, epoch 4, step 296, the loss is 388762.15625\n",
            "in training loop, epoch 4, step 297, the loss is 448478.28125\n",
            "in training loop, epoch 4, step 298, the loss is 715321.1875\n",
            "in training loop, epoch 4, step 299, the loss is 476600.53125\n",
            "in training loop, epoch 4, step 300, the loss is 574986.375\n",
            "in training loop, epoch 4, step 301, the loss is 309856.15625\n",
            "in training loop, epoch 4, step 302, the loss is 367834.34375\n",
            "in training loop, epoch 4, step 303, the loss is 508769.75\n",
            "in training loop, epoch 4, step 304, the loss is 481828.3125\n",
            "in training loop, epoch 4, step 305, the loss is 547825.5\n",
            "in training loop, epoch 4, step 306, the loss is 587982.625\n",
            "in training loop, epoch 4, step 307, the loss is 420729.5\n",
            "in training loop, epoch 4, step 308, the loss is 546044.5\n",
            "in training loop, epoch 4, step 309, the loss is 505858.53125\n",
            "in training loop, epoch 4, step 310, the loss is 422030.15625\n",
            "in training loop, epoch 4, step 311, the loss is 402669.5\n",
            "in training loop, epoch 4, step 312, the loss is 327758.1875\n",
            "in training loop, epoch 4, step 313, the loss is 403970.3125\n",
            "in training loop, epoch 4, step 314, the loss is 595529.5625\n",
            "in training loop, epoch 4, step 315, the loss is 392691.03125\n",
            "in training loop, epoch 4, step 316, the loss is 287888.3125\n",
            "in training loop, epoch 4, step 317, the loss is 419758.28125\n",
            "in training loop, epoch 4, step 318, the loss is 468369.71875\n",
            "in training loop, epoch 4, step 319, the loss is 577941.4375\n",
            "in training loop, epoch 4, step 320, the loss is 446420.53125\n",
            "in training loop, epoch 4, step 321, the loss is 460122.25\n",
            "in training loop, epoch 4, step 322, the loss is 575869.5625\n",
            "in training loop, epoch 4, step 323, the loss is 321611.5625\n",
            "in training loop, epoch 4, step 324, the loss is 507881.5625\n",
            "in training loop, epoch 4, step 325, the loss is 503911.15625\n",
            "in training loop, epoch 4, step 326, the loss is 504929.875\n",
            "in training loop, epoch 4, step 327, the loss is 330911.09375\n",
            "in training loop, epoch 4, step 328, the loss is 419521.125\n",
            "in training loop, epoch 4, step 329, the loss is 456722.375\n",
            "in training loop, epoch 4, step 330, the loss is 355194.21875\n",
            "in training loop, epoch 4, step 331, the loss is 355213.09375\n",
            "in training loop, epoch 4, step 332, the loss is 585411.1875\n",
            "in training loop, epoch 4, step 333, the loss is 459047.78125\n",
            "in training loop, epoch 4, step 334, the loss is 444310.59375\n",
            "in training loop, epoch 4, step 335, the loss is 457101.0\n",
            "in training loop, epoch 4, step 336, the loss is 411557.75\n",
            "in training loop, epoch 4, step 337, the loss is 245422.953125\n",
            "in training loop, epoch 4, step 338, the loss is 479132.4375\n",
            "in training loop, epoch 4, step 339, the loss is 442527.71875\n",
            "in training loop, epoch 4, step 340, the loss is 397176.625\n",
            "in training loop, epoch 4, step 341, the loss is 422072.90625\n",
            "in training loop, epoch 4, step 342, the loss is 498471.875\n",
            "in training loop, epoch 4, step 343, the loss is 433124.375\n",
            "in training loop, epoch 4, step 344, the loss is 410941.375\n",
            "in training loop, epoch 4, step 345, the loss is 444701.15625\n",
            "in training loop, epoch 4, step 346, the loss is 407661.3125\n",
            "in training loop, epoch 4, step 347, the loss is 460104.84375\n",
            "in training loop, epoch 4, step 348, the loss is 492423.0\n",
            "in training loop, epoch 4, step 349, the loss is 432919.0\n",
            "in training loop, epoch 4, step 350, the loss is 624431.25\n",
            "in training loop, epoch 4, step 351, the loss is 376980.1875\n",
            "in training loop, epoch 4, step 352, the loss is 352755.40625\n",
            "in training loop, epoch 4, step 353, the loss is 512945.8125\n",
            "in training loop, epoch 4, step 354, the loss is 381337.8125\n",
            "in training loop, epoch 4, step 355, the loss is 455698.625\n",
            "in training loop, epoch 4, step 356, the loss is 531290.625\n",
            "in training loop, epoch 4, step 357, the loss is 373606.1875\n",
            "in training loop, epoch 4, step 358, the loss is 497567.5625\n",
            "in training loop, epoch 4, step 359, the loss is 581133.25\n",
            "in training loop, epoch 4, step 360, the loss is 535539.75\n",
            "in training loop, epoch 4, step 361, the loss is 297781.90625\n",
            "in training loop, epoch 4, step 362, the loss is 273512.4375\n",
            "in training loop, epoch 4, step 363, the loss is 365761.8125\n",
            "in training loop, epoch 4, step 364, the loss is 470679.4375\n",
            "in training loop, epoch 4, step 365, the loss is 352710.96875\n",
            "in training loop, epoch 4, step 366, the loss is 446638.90625\n",
            "in training loop, epoch 4, step 367, the loss is 598249.0\n",
            "in training loop, epoch 4, step 368, the loss is 563856.125\n",
            "in training loop, epoch 4, step 369, the loss is 561941.875\n",
            "in training loop, epoch 4, step 370, the loss is 480498.5\n",
            "in training loop, epoch 4, step 371, the loss is 289602.21875\n",
            "in training loop, epoch 4, step 372, the loss is 424543.8125\n",
            "in training loop, epoch 4, step 373, the loss is 551894.75\n",
            "in training loop, epoch 4, step 374, the loss is 385662.21875\n",
            "in training loop, epoch 4, step 375, the loss is 419503.375\n",
            "in training loop, epoch 4, step 376, the loss is 507587.09375\n",
            "in training loop, epoch 4, step 377, the loss is 485294.53125\n",
            "in training loop, epoch 4, step 378, the loss is 527739.3125\n",
            "in training loop, epoch 4, step 379, the loss is 382835.5625\n",
            "in training loop, epoch 4, step 380, the loss is 477833.9375\n",
            "in training loop, epoch 4, step 381, the loss is 702489.0\n",
            "in training loop, epoch 4, step 382, the loss is 400897.4375\n",
            "in training loop, epoch 4, step 383, the loss is 390916.40625\n",
            "in training loop, epoch 4, step 384, the loss is 404441.75\n",
            "in training loop, epoch 4, step 385, the loss is 453207.78125\n",
            "in training loop, epoch 4, step 386, the loss is 639842.75\n",
            "in training loop, epoch 4, step 387, the loss is 437904.25\n",
            "in training loop, epoch 4, step 388, the loss is 398614.15625\n",
            "in training loop, epoch 4, step 389, the loss is 604064.8125\n",
            "in training loop, epoch 4, step 390, the loss is 502091.28125\n",
            "in training loop, epoch 4, step 391, the loss is 347110.0\n",
            "in training loop, epoch 4, step 392, the loss is 390247.4375\n",
            "in training loop, epoch 4, step 393, the loss is 447786.40625\n",
            "in training loop, epoch 4, step 394, the loss is 504099.0625\n",
            "in training loop, epoch 4, step 395, the loss is 422407.28125\n",
            "in training loop, epoch 4, step 396, the loss is 348952.1875\n",
            "in training loop, epoch 4, step 397, the loss is 598454.75\n",
            "in training loop, epoch 4, step 398, the loss is 409212.875\n",
            "in training loop, epoch 4, step 399, the loss is 548338.375\n",
            "in training loop, epoch 4, step 400, the loss is 333253.75\n",
            "in training loop, epoch 4, step 401, the loss is 444993.8125\n",
            "in training loop, epoch 4, step 402, the loss is 618761.75\n",
            "in training loop, epoch 4, step 403, the loss is 409839.40625\n",
            "in training loop, epoch 4, step 404, the loss is 506479.84375\n",
            "in training loop, epoch 4, step 405, the loss is 369175.4375\n",
            "in training loop, epoch 4, step 406, the loss is 567477.8125\n",
            "in training loop, epoch 4, step 407, the loss is 376086.65625\n",
            "in training loop, epoch 4, step 408, the loss is 626786.8125\n",
            "in training loop, epoch 4, step 409, the loss is 361146.21875\n",
            "in training loop, epoch 4, step 410, the loss is 304612.90625\n",
            "in training loop, epoch 4, step 411, the loss is 452229.875\n",
            "in training loop, epoch 4, step 412, the loss is 533672.8125\n",
            "in training loop, epoch 4, step 413, the loss is 596556.625\n",
            "in training loop, epoch 4, step 414, the loss is 602996.0625\n",
            "in training loop, epoch 4, step 415, the loss is 344610.65625\n",
            "in training loop, epoch 4, step 416, the loss is 578025.1875\n",
            "in training loop, epoch 4, step 417, the loss is 316510.6875\n",
            "in training loop, epoch 4, step 418, the loss is 445069.59375\n",
            "in training loop, epoch 4, step 419, the loss is 523117.09375\n",
            "in training loop, epoch 4, step 420, the loss is 363346.09375\n",
            "in training loop, epoch 4, step 421, the loss is 401397.84375\n",
            "in training loop, epoch 4, step 422, the loss is 558135.25\n",
            "in training loop, epoch 4, step 423, the loss is 427843.53125\n",
            "in training loop, epoch 4, step 424, the loss is 425052.84375\n",
            "in training loop, epoch 4, step 425, the loss is 383655.375\n",
            "in training loop, epoch 4, step 426, the loss is 470548.5625\n",
            "in training loop, epoch 4, step 427, the loss is 436909.875\n",
            "in training loop, epoch 4, step 428, the loss is 422810.46875\n",
            "in training loop, epoch 4, step 429, the loss is 368834.375\n",
            "in training loop, epoch 4, step 430, the loss is 398515.28125\n",
            "in training loop, epoch 4, step 431, the loss is 232541.390625\n",
            "in training loop, epoch 4, step 432, the loss is 404405.4375\n",
            "in training loop, epoch 4, step 433, the loss is 594213.3125\n",
            "in training loop, epoch 4, step 434, the loss is 479964.71875\n",
            "in training loop, epoch 4, step 435, the loss is 603697.1875\n",
            "in training loop, epoch 4, step 436, the loss is 486435.5\n",
            "in training loop, epoch 4, step 437, the loss is 507273.625\n",
            "in training loop, epoch 4, step 438, the loss is 479929.78125\n",
            "in training loop, epoch 4, step 439, the loss is 434216.75\n",
            "in training loop, epoch 4, step 440, the loss is 363799.71875\n",
            "in training loop, epoch 4, step 441, the loss is 360126.59375\n",
            "in training loop, epoch 4, step 442, the loss is 434100.90625\n",
            "in training loop, epoch 4, step 443, the loss is 422279.3125\n",
            "in training loop, epoch 4, step 444, the loss is 676478.375\n",
            "in training loop, epoch 4, step 445, the loss is 510150.5625\n",
            "in training loop, epoch 4, step 446, the loss is 479478.3125\n",
            "in training loop, epoch 4, step 447, the loss is 495976.0\n",
            "in training loop, epoch 4, step 448, the loss is 358769.53125\n",
            "in training loop, epoch 4, step 449, the loss is 341593.34375\n",
            "in training loop, epoch 4, step 450, the loss is 499815.03125\n",
            "in training loop, epoch 4, step 451, the loss is 468147.28125\n",
            "in training loop, epoch 4, step 452, the loss is 369439.34375\n",
            "in training loop, epoch 4, step 453, the loss is 410231.1875\n",
            "in training loop, epoch 4, step 454, the loss is 390019.5625\n",
            "in training loop, epoch 4, step 455, the loss is 413976.1875\n",
            "in training loop, epoch 4, step 456, the loss is 389435.375\n",
            "in training loop, epoch 4, step 457, the loss is 442236.90625\n",
            "in training loop, epoch 4, step 458, the loss is 426838.78125\n",
            "in training loop, epoch 4, step 459, the loss is 463983.09375\n",
            "in training loop, epoch 4, step 460, the loss is 435612.21875\n",
            "in training loop, epoch 4, step 461, the loss is 399589.5625\n",
            "in training loop, epoch 4, step 462, the loss is 584865.1875\n",
            "in training loop, epoch 4, step 463, the loss is 287365.96875\n",
            "in training loop, epoch 4, step 464, the loss is 441124.1875\n",
            "in training loop, epoch 4, step 465, the loss is 374143.4375\n",
            "in training loop, epoch 4, step 466, the loss is 492285.8125\n",
            "in training loop, epoch 4, step 467, the loss is 447866.90625\n",
            "in training loop, epoch 4, step 468, the loss is 425819.34375\n",
            "in training loop, epoch 4, step 469, the loss is 348730.125\n",
            "in training loop, epoch 4, step 470, the loss is 438438.71875\n",
            "in training loop, epoch 4, step 471, the loss is 371555.5625\n",
            "in training loop, epoch 4, step 472, the loss is 379869.9375\n",
            "in training loop, epoch 4, step 473, the loss is 589840.3125\n",
            "in training loop, epoch 4, step 474, the loss is 382003.0625\n",
            "in training loop, epoch 4, step 475, the loss is 551704.9375\n",
            "in training loop, epoch 4, step 476, the loss is 312846.65625\n",
            "in training loop, epoch 4, step 477, the loss is 503675.1875\n",
            "in training loop, epoch 4, step 478, the loss is 354083.9375\n",
            "in training loop, epoch 4, step 479, the loss is 544871.625\n",
            "in training loop, epoch 4, step 480, the loss is 524441.3125\n",
            "in training loop, epoch 4, step 481, the loss is 482855.84375\n",
            "in training loop, epoch 4, step 482, the loss is 625800.5\n",
            "in training loop, epoch 4, step 483, the loss is 318373.53125\n",
            "in training loop, epoch 4, step 484, the loss is 434911.65625\n",
            "in training loop, epoch 4, step 485, the loss is 519868.9375\n",
            "in training loop, epoch 4, step 486, the loss is 369909.78125\n",
            "in training loop, epoch 4, step 487, the loss is 317428.25\n",
            "in training loop, epoch 4, step 488, the loss is 380926.375\n",
            "in training loop, epoch 4, step 489, the loss is 380999.90625\n",
            "in training loop, epoch 4, step 490, the loss is 479183.3125\n",
            "in training loop, epoch 4, step 491, the loss is 382339.53125\n",
            "in training loop, epoch 4, step 492, the loss is 509752.71875\n",
            "in training loop, epoch 4, step 493, the loss is 405440.53125\n",
            "in training loop, epoch 4, step 494, the loss is 401611.125\n",
            "in training loop, epoch 4, step 495, the loss is 367991.0\n",
            "in training loop, epoch 4, step 496, the loss is 507806.78125\n",
            "in training loop, epoch 4, step 497, the loss is 347161.09375\n",
            "in training loop, epoch 4, step 498, the loss is 467823.1875\n",
            "in training loop, epoch 4, step 499, the loss is 490145.75\n",
            "in training loop, epoch 4, step 500, the loss is 519555.375\n",
            "in training loop, epoch 4, step 501, the loss is 332136.59375\n",
            "in training loop, epoch 4, step 502, the loss is 578279.5625\n",
            "in training loop, epoch 4, step 503, the loss is 497644.28125\n",
            "in training loop, epoch 4, step 504, the loss is 531386.6875\n",
            "in training loop, epoch 4, step 505, the loss is 372761.78125\n",
            "in training loop, epoch 4, step 506, the loss is 488461.84375\n",
            "in training loop, epoch 4, step 507, the loss is 353678.6875\n",
            "in training loop, epoch 4, step 508, the loss is 488303.5625\n",
            "in training loop, epoch 4, step 509, the loss is 689853.6875\n",
            "in training loop, epoch 4, step 510, the loss is 421657.34375\n",
            "in training loop, epoch 4, step 511, the loss is 332451.1875\n",
            "in training loop, epoch 4, step 512, the loss is 365239.375\n",
            "in training loop, epoch 4, step 513, the loss is 397419.28125\n",
            "in training loop, epoch 4, step 514, the loss is 454054.875\n",
            "in training loop, epoch 4, step 515, the loss is 390887.5\n",
            "in training loop, epoch 4, step 516, the loss is 444240.34375\n",
            "in training loop, epoch 4, step 517, the loss is 311836.6875\n",
            "in training loop, epoch 4, step 518, the loss is 498985.8125\n",
            "in training loop, epoch 4, step 519, the loss is 445959.90625\n",
            "in training loop, epoch 4, step 520, the loss is 428052.0625\n",
            "in training loop, epoch 4, step 521, the loss is 356138.875\n",
            "in training loop, epoch 4, step 522, the loss is 418962.15625\n",
            "in training loop, epoch 4, step 523, the loss is 446035.25\n",
            "in training loop, epoch 4, step 524, the loss is 541779.8125\n",
            "in training loop, epoch 4, step 525, the loss is 383006.875\n",
            "in training loop, epoch 4, step 526, the loss is 405455.9375\n",
            "in training loop, epoch 4, step 527, the loss is 418203.875\n",
            "in training loop, epoch 4, step 528, the loss is 425173.90625\n",
            "in training loop, epoch 4, step 529, the loss is 538988.25\n",
            "in training loop, epoch 4, step 530, the loss is 367540.09375\n",
            "in training loop, epoch 4, step 531, the loss is 803337.125\n",
            "in training loop, epoch 4, step 532, the loss is 464616.25\n",
            "in training loop, epoch 4, step 533, the loss is 446464.09375\n",
            "in training loop, epoch 4, step 534, the loss is 474174.875\n",
            "in training loop, epoch 4, step 535, the loss is 443494.78125\n",
            "in training loop, epoch 4, step 536, the loss is 340602.90625\n",
            "in training loop, epoch 4, step 537, the loss is 289274.90625\n",
            "in training loop, epoch 4, step 538, the loss is 620783.5\n",
            "in training loop, epoch 4, step 539, the loss is 367077.46875\n",
            "in training loop, epoch 4, step 540, the loss is 457351.9375\n",
            "in training loop, epoch 4, step 541, the loss is 384838.875\n",
            "in training loop, epoch 4, step 542, the loss is 551906.6875\n",
            "in training loop, epoch 4, step 543, the loss is 344223.1875\n",
            "in training loop, epoch 4, step 544, the loss is 519080.53125\n",
            "in training loop, epoch 4, step 545, the loss is 470864.875\n",
            "in training loop, epoch 4, step 546, the loss is 444361.5\n",
            "in training loop, epoch 4, step 547, the loss is 411493.3125\n",
            "in training loop, epoch 4, step 548, the loss is 496778.21875\n",
            "in training loop, epoch 4, step 549, the loss is 542938.375\n",
            "in training loop, epoch 4, step 550, the loss is 278723.71875\n",
            "in training loop, epoch 4, step 551, the loss is 587762.75\n",
            "in training loop, epoch 4, step 552, the loss is 425456.875\n",
            "in training loop, epoch 4, step 553, the loss is 456782.09375\n",
            "in training loop, epoch 4, step 554, the loss is 492934.5\n",
            "in training loop, epoch 4, step 555, the loss is 400562.03125\n",
            "in training loop, epoch 4, step 556, the loss is 678474.125\n",
            "in training loop, epoch 4, step 557, the loss is 454865.625\n",
            "in training loop, epoch 4, step 558, the loss is 624036.875\n",
            "in training loop, epoch 4, step 559, the loss is 551840.6875\n",
            "in training loop, epoch 4, step 560, the loss is 328381.53125\n",
            "in training loop, epoch 4, step 561, the loss is 560940.4375\n",
            "in training loop, epoch 4, step 562, the loss is 445665.03125\n",
            "in training loop, epoch 4, step 563, the loss is 351619.75\n",
            "in training loop, epoch 4, step 564, the loss is 478333.0\n",
            "in training loop, epoch 4, step 565, the loss is 372980.875\n",
            "in training loop, epoch 4, step 566, the loss is 482043.96875\n",
            "in training loop, epoch 4, step 567, the loss is 333430.40625\n",
            "in training loop, epoch 4, step 568, the loss is 285829.5625\n",
            "in training loop, epoch 4, step 569, the loss is 577623.25\n",
            "in training loop, epoch 4, step 570, the loss is 475900.5625\n",
            "in training loop, epoch 4, step 571, the loss is 471402.90625\n",
            "in training loop, epoch 4, step 572, the loss is 354870.28125\n",
            "in training loop, epoch 4, step 573, the loss is 424155.9375\n",
            "in training loop, epoch 4, step 574, the loss is 427258.25\n",
            "in training loop, epoch 4, step 575, the loss is 425267.625\n",
            "in training loop, epoch 4, step 576, the loss is 680654.625\n",
            "in training loop, epoch 4, step 577, the loss is 365367.5625\n",
            "in training loop, epoch 4, step 578, the loss is 359549.625\n",
            "in training loop, epoch 4, step 579, the loss is 449023.03125\n",
            "in training loop, epoch 4, step 580, the loss is 307733.25\n",
            "in training loop, epoch 4, step 581, the loss is 416441.3125\n",
            "in training loop, epoch 4, step 582, the loss is 534880.25\n",
            "in training loop, epoch 4, step 583, the loss is 505966.21875\n",
            "in training loop, epoch 4, step 584, the loss is 391820.875\n",
            "in training loop, epoch 4, step 585, the loss is 338770.9375\n",
            "in training loop, epoch 4, step 586, the loss is 401339.59375\n",
            "in training loop, epoch 4, step 587, the loss is 479013.3125\n",
            "in training loop, epoch 4, step 588, the loss is 511302.125\n",
            "in training loop, epoch 4, step 589, the loss is 406926.75\n",
            "in training loop, epoch 4, step 590, the loss is 393494.0\n",
            "in training loop, epoch 4, step 591, the loss is 369239.5\n",
            "in training loop, epoch 4, step 592, the loss is 687112.875\n",
            "in training loop, epoch 4, step 593, the loss is 777262.3125\n",
            "in training loop, epoch 4, step 594, the loss is 299283.46875\n",
            "in training loop, epoch 4, step 595, the loss is 537340.75\n",
            "in training loop, epoch 4, step 596, the loss is 347259.375\n",
            "in training loop, epoch 4, step 597, the loss is 547034.1875\n",
            "in training loop, epoch 4, step 598, the loss is 552954.0\n",
            "in training loop, epoch 4, step 599, the loss is 386139.375\n",
            "in training loop, epoch 4, step 600, the loss is 446698.1875\n",
            "in training loop, epoch 4, step 601, the loss is 486645.34375\n",
            "in training loop, epoch 4, step 602, the loss is 476813.1875\n",
            "in training loop, epoch 4, step 603, the loss is 525850.125\n",
            "in training loop, epoch 4, step 604, the loss is 618279.6875\n",
            "in training loop, epoch 4, step 605, the loss is 489756.09375\n",
            "in training loop, epoch 4, step 606, the loss is 328677.78125\n",
            "in training loop, epoch 4, step 607, the loss is 461403.1875\n",
            "in training loop, epoch 4, step 608, the loss is 389988.96875\n",
            "in training loop, epoch 4, step 609, the loss is 419084.65625\n",
            "in training loop, epoch 4, step 610, the loss is 549952.6875\n",
            "in training loop, epoch 4, step 611, the loss is 486795.3125\n",
            "in training loop, epoch 4, step 612, the loss is 711872.125\n",
            "in training loop, epoch 4, step 613, the loss is 519777.96875\n",
            "in training loop, epoch 4, step 614, the loss is 384149.5625\n",
            "in training loop, epoch 4, step 615, the loss is 540808.3125\n",
            "in training loop, epoch 4, step 616, the loss is 464109.15625\n",
            "in training loop, epoch 4, step 617, the loss is 583043.5\n",
            "in training loop, epoch 4, step 618, the loss is 401807.4375\n",
            "in training loop, epoch 4, step 619, the loss is 457866.1875\n",
            "in training loop, epoch 4, step 620, the loss is 809534.9375\n",
            "in training loop, epoch 4, step 621, the loss is 522311.75\n",
            "in training loop, epoch 4, step 622, the loss is 201035.265625\n",
            "in training loop, epoch 4, step 623, the loss is 559915.3125\n",
            "in training loop, epoch 4, step 624, the loss is 423651.25\n",
            "in training loop, epoch 4, step 625, the loss is 861096.6875\n",
            "in training loop, epoch 4, step 626, the loss is 579678.4375\n",
            "in training loop, epoch 4, step 627, the loss is 453711.5625\n",
            "in training loop, epoch 4, step 628, the loss is 568374.375\n",
            "in training loop, epoch 4, step 629, the loss is 652870.875\n",
            "in training loop, epoch 4, step 630, the loss is 459461.6875\n",
            "in training loop, epoch 4, step 631, the loss is 472095.03125\n",
            "in training loop, epoch 4, step 632, the loss is 533031.8125\n",
            "in training loop, epoch 4, step 633, the loss is 439417.0\n",
            "in training loop, epoch 4, step 634, the loss is 392716.1875\n",
            "in training loop, epoch 4, step 635, the loss is 653955.25\n",
            "in training loop, epoch 4, step 636, the loss is 519914.1875\n",
            "in training loop, epoch 4, step 637, the loss is 348082.375\n",
            "in training loop, epoch 4, step 638, the loss is 527593.8125\n",
            "in training loop, epoch 4, step 639, the loss is 389755.625\n",
            "in training loop, epoch 4, step 640, the loss is 514326.09375\n",
            "in training loop, epoch 4, step 641, the loss is 612183.8125\n",
            "in training loop, epoch 4, step 642, the loss is 488257.0\n",
            "in training loop, epoch 4, step 643, the loss is 367485.5\n",
            "in training loop, epoch 4, step 644, the loss is 404917.875\n",
            "in training loop, epoch 4, step 645, the loss is 530304.125\n",
            "in training loop, epoch 4, step 646, the loss is 547464.25\n",
            "in training loop, epoch 4, step 647, the loss is 271986.5625\n",
            "in training loop, epoch 4, step 648, the loss is 482598.8125\n",
            "in training loop, epoch 4, step 649, the loss is 383241.21875\n",
            "in training loop, epoch 4, step 650, the loss is 583771.5\n",
            "in training loop, epoch 4, step 651, the loss is 522615.15625\n",
            "in training loop, epoch 4, step 652, the loss is 414232.6875\n",
            "in training loop, epoch 4, step 653, the loss is 492790.71875\n",
            "in training loop, epoch 4, step 654, the loss is 746289.0\n",
            "in training loop, epoch 4, step 655, the loss is 558139.4375\n",
            "in training loop, epoch 4, step 656, the loss is 444760.625\n",
            "in training loop, epoch 4, step 657, the loss is 640358.75\n",
            "in training loop, epoch 4, step 658, the loss is 396950.5625\n",
            "in training loop, epoch 4, step 659, the loss is 579848.0625\n",
            "in training loop, epoch 4, step 660, the loss is 442015.625\n",
            "in training loop, epoch 4, step 661, the loss is 381791.78125\n",
            "in training loop, epoch 4, step 662, the loss is 429102.4375\n",
            "in training loop, epoch 4, step 663, the loss is 329810.4375\n",
            "in training loop, epoch 4, step 664, the loss is 637328.0\n",
            "in training loop, epoch 4, step 665, the loss is 410655.4375\n",
            "in training loop, epoch 4, step 666, the loss is 454296.4375\n",
            "in training loop, epoch 4, step 667, the loss is 542456.5625\n",
            "in training loop, epoch 4, step 668, the loss is 418641.21875\n",
            "in training loop, epoch 4, step 669, the loss is 587917.375\n",
            "in training loop, epoch 4, step 670, the loss is 293621.78125\n",
            "in training loop, epoch 4, step 671, the loss is 556527.4375\n",
            "in training loop, epoch 4, step 672, the loss is 537613.875\n",
            "in training loop, epoch 4, step 673, the loss is 444963.21875\n",
            "in training loop, epoch 4, step 674, the loss is 656624.75\n",
            "in training loop, epoch 4, step 675, the loss is 419071.09375\n",
            "in training loop, epoch 4, step 676, the loss is 503890.40625\n",
            "in training loop, epoch 4, step 677, the loss is 488368.21875\n",
            "in training loop, epoch 4, step 678, the loss is 431613.5625\n",
            "in training loop, epoch 4, step 679, the loss is 363554.09375\n",
            "in training loop, epoch 4, step 680, the loss is 756402.4375\n",
            "in training loop, epoch 4, step 681, the loss is 296988.5625\n",
            "in training loop, epoch 4, step 682, the loss is 516426.5\n",
            "in training loop, epoch 4, step 683, the loss is 416586.875\n",
            "in training loop, epoch 4, step 684, the loss is 592199.25\n",
            "in training loop, epoch 4, step 685, the loss is 318717.53125\n",
            "in training loop, epoch 4, step 686, the loss is 284704.125\n",
            "in training loop, epoch 4, step 687, the loss is 434391.625\n",
            "in training loop, epoch 4, step 688, the loss is 650966.5\n",
            "in training loop, epoch 4, step 689, the loss is 618362.5\n",
            "in training loop, epoch 4, step 690, the loss is 302165.90625\n",
            "in training loop, epoch 4, step 691, the loss is 350278.1875\n",
            "in training loop, epoch 4, step 692, the loss is 371648.6875\n",
            "in training loop, epoch 4, step 693, the loss is 453879.125\n",
            "in training loop, epoch 4, step 694, the loss is 557812.8125\n",
            "in training loop, epoch 4, step 695, the loss is 388395.8125\n",
            "in training loop, epoch 4, step 696, the loss is 376020.59375\n",
            "in training loop, epoch 4, step 697, the loss is 481222.5\n",
            "in training loop, epoch 4, step 698, the loss is 435743.4375\n",
            "in training loop, epoch 4, step 699, the loss is 392217.53125\n",
            "in training loop, epoch 4, step 700, the loss is 409725.625\n",
            "in training loop, epoch 4, step 701, the loss is 465668.3125\n",
            "in training loop, epoch 4, step 702, the loss is 584908.25\n",
            "in training loop, epoch 4, step 703, the loss is 508367.25\n",
            "in training loop, epoch 4, step 704, the loss is 428480.71875\n",
            "in training loop, epoch 4, step 705, the loss is 511320.96875\n",
            "in training loop, epoch 4, step 706, the loss is 481479.6875\n",
            "in training loop, epoch 4, step 707, the loss is 558884.0\n",
            "in training loop, epoch 4, step 708, the loss is 431929.59375\n",
            "in training loop, epoch 4, step 709, the loss is 547021.5\n",
            "in training loop, epoch 4, step 710, the loss is 453864.3125\n",
            "in training loop, epoch 4, step 711, the loss is 529943.0625\n",
            "in training loop, epoch 4, step 712, the loss is 569322.375\n",
            "in training loop, epoch 4, step 713, the loss is 417116.46875\n",
            "in training loop, epoch 4, step 714, the loss is 400870.15625\n",
            "in training loop, epoch 4, step 715, the loss is 356549.21875\n",
            "in training loop, epoch 4, step 716, the loss is 513227.28125\n",
            "in training loop, epoch 4, step 717, the loss is 591278.75\n",
            "in training loop, epoch 4, step 718, the loss is 460249.625\n",
            "in training loop, epoch 4, step 719, the loss is 434515.8125\n",
            "in training loop, epoch 4, step 720, the loss is 472301.5625\n",
            "in training loop, epoch 4, step 721, the loss is 465493.0\n",
            "in training loop, epoch 4, step 722, the loss is 512425.3125\n",
            "in training loop, epoch 4, step 723, the loss is 446878.625\n",
            "in training loop, epoch 4, step 724, the loss is 632562.9375\n",
            "in training loop, epoch 4, step 725, the loss is 588867.3125\n",
            "in training loop, epoch 4, step 726, the loss is 469572.9375\n",
            "in training loop, epoch 4, step 727, the loss is 501657.4375\n",
            "in training loop, epoch 4, step 728, the loss is 346045.5625\n",
            "in training loop, epoch 4, step 729, the loss is 387741.84375\n",
            "in training loop, epoch 4, step 730, the loss is 484621.34375\n",
            "in training loop, epoch 4, step 731, the loss is 406108.0\n",
            "in training loop, epoch 4, step 732, the loss is 477533.96875\n",
            "in training loop, epoch 4, step 733, the loss is 559537.0625\n",
            "in training loop, epoch 4, step 734, the loss is 574308.3125\n",
            "in training loop, epoch 4, step 735, the loss is 202251.453125\n",
            "in training loop, epoch 4, step 736, the loss is 320489.59375\n",
            "in training loop, epoch 4, step 737, the loss is 669614.5\n",
            "in training loop, epoch 4, step 738, the loss is 487432.40625\n",
            "in training loop, epoch 4, step 739, the loss is 688545.375\n",
            "in training loop, epoch 4, step 740, the loss is 546621.75\n",
            "in training loop, epoch 4, step 741, the loss is 313557.3125\n",
            "in training loop, epoch 4, step 742, the loss is 722178.6875\n",
            "in training loop, epoch 4, step 743, the loss is 373374.8125\n",
            "in training loop, epoch 4, step 744, the loss is 743960.1875\n",
            "in training loop, epoch 4, step 745, the loss is 327568.5\n",
            "in training loop, epoch 4, step 746, the loss is 557625.375\n",
            "in training loop, epoch 4, step 747, the loss is 527689.375\n",
            "in training loop, epoch 4, step 748, the loss is 397368.875\n",
            "in training loop, epoch 4, step 749, the loss is 347444.0625\n",
            "in training loop, epoch 4, step 750, the loss is 419704.375\n",
            "in training loop, epoch 4, step 751, the loss is 546924.625\n",
            "in training loop, epoch 4, step 752, the loss is 577352.5\n",
            "in training loop, epoch 4, step 753, the loss is 320051.5625\n",
            "in training loop, epoch 4, step 754, the loss is 419508.25\n",
            "in training loop, epoch 4, step 755, the loss is 501058.9375\n",
            "in training loop, epoch 4, step 756, the loss is 422346.9375\n",
            "in training loop, epoch 4, step 757, the loss is 454597.28125\n",
            "in training loop, epoch 4, step 758, the loss is 385502.0\n",
            "in training loop, epoch 4, step 759, the loss is 570837.0\n",
            "in training loop, epoch 4, step 760, the loss is 829555.5625\n",
            "in training loop, epoch 4, step 761, the loss is 365497.8125\n",
            "in training loop, epoch 4, step 762, the loss is 430657.5\n",
            "in training loop, epoch 4, step 763, the loss is 369509.78125\n",
            "in training loop, epoch 4, step 764, the loss is 401224.28125\n",
            "in training loop, epoch 4, step 765, the loss is 745876.0\n",
            "in training loop, epoch 4, step 766, the loss is 517230.875\n",
            "in training loop, epoch 4, step 767, the loss is 723758.875\n",
            "in training loop, epoch 4, step 768, the loss is 351710.40625\n",
            "in training loop, epoch 4, step 769, the loss is 438295.65625\n",
            "in training loop, epoch 4, step 770, the loss is 433717.78125\n",
            "in training loop, epoch 4, step 771, the loss is 604552.875\n",
            "in training loop, epoch 4, step 772, the loss is 470706.125\n",
            "in training loop, epoch 4, step 773, the loss is 476392.25\n",
            "in training loop, epoch 4, step 774, the loss is 540478.5625\n",
            "in training loop, epoch 4, step 775, the loss is 661477.25\n",
            "in training loop, epoch 4, step 776, the loss is 553345.75\n",
            "in training loop, epoch 4, step 777, the loss is 604559.25\n",
            "in training loop, epoch 4, step 778, the loss is 484367.15625\n",
            "in training loop, epoch 4, step 779, the loss is 426614.625\n",
            "in training loop, epoch 4, step 780, the loss is 471333.9375\n",
            "in training loop, epoch 4, step 781, the loss is 521125.0625\n",
            "in training loop, epoch 4, step 782, the loss is 588375.375\n",
            "in training loop, epoch 4, step 783, the loss is 501065.625\n",
            "in training loop, epoch 4, step 784, the loss is 954503.8125\n",
            "in training loop, epoch 4, step 785, the loss is 445427.34375\n",
            "in training loop, epoch 4, step 786, the loss is 470131.6875\n",
            "in training loop, epoch 4, step 787, the loss is 685484.3125\n",
            "in training loop, epoch 4, step 788, the loss is 322670.09375\n",
            "in training loop, epoch 4, step 789, the loss is 710260.6875\n",
            "in training loop, epoch 4, step 790, the loss is 462722.59375\n",
            "in training loop, epoch 4, step 791, the loss is 345435.0\n",
            "in training loop, epoch 4, step 792, the loss is 485934.375\n",
            "in training loop, epoch 4, step 793, the loss is 396692.96875\n",
            "in training loop, epoch 4, step 794, the loss is 426299.59375\n",
            "in training loop, epoch 4, step 795, the loss is 683365.0\n",
            "in training loop, epoch 4, step 796, the loss is 379860.96875\n",
            "in training loop, epoch 4, step 797, the loss is 626678.125\n",
            "in training loop, epoch 4, step 798, the loss is 483172.875\n",
            "in training loop, epoch 4, step 799, the loss is 371008.625\n",
            "in training loop, epoch 4, step 800, the loss is 444038.625\n",
            "in training loop, epoch 4, step 801, the loss is 364359.5625\n",
            "in training loop, epoch 4, step 802, the loss is 693052.5625\n",
            "in training loop, epoch 4, step 803, the loss is 435569.71875\n",
            "in training loop, epoch 4, step 804, the loss is 531926.0625\n",
            "in training loop, epoch 4, step 805, the loss is 469334.09375\n",
            "in training loop, epoch 4, step 806, the loss is 425163.5\n",
            "in training loop, epoch 4, step 807, the loss is 463272.0625\n",
            "in training loop, epoch 4, step 808, the loss is 382017.75\n",
            "in training loop, epoch 4, step 809, the loss is 608765.625\n",
            "in training loop, epoch 4, step 810, the loss is 429229.53125\n",
            "in training loop, epoch 4, step 811, the loss is 384211.75\n",
            "in training loop, epoch 4, step 812, the loss is 451869.5\n",
            "in training loop, epoch 4, step 813, the loss is 507287.15625\n",
            "in training loop, epoch 4, step 814, the loss is 402132.0\n",
            "in training loop, epoch 4, step 815, the loss is 404534.9375\n",
            "in training loop, epoch 4, step 816, the loss is 482848.6875\n",
            "in training loop, epoch 4, step 817, the loss is 461267.0625\n",
            "in training loop, epoch 4, step 818, the loss is 783880.375\n",
            "in training loop, epoch 4, step 819, the loss is 444139.65625\n",
            "in training loop, epoch 4, step 820, the loss is 353583.75\n",
            "in training loop, epoch 4, step 821, the loss is 423726.59375\n",
            "in training loop, epoch 4, step 822, the loss is 508221.03125\n",
            "in training loop, epoch 4, step 823, the loss is 825591.375\n",
            "in training loop, epoch 4, step 824, the loss is 277955.46875\n",
            "in training loop, epoch 4, step 825, the loss is 469371.625\n",
            "in training loop, epoch 4, step 826, the loss is 235214.53125\n",
            "in training loop, epoch 4, step 827, the loss is 403024.90625\n",
            "in training loop, epoch 4, step 828, the loss is 511368.65625\n",
            "in training loop, epoch 4, step 829, the loss is 628163.5\n",
            "in training loop, epoch 4, step 830, the loss is 256392.46875\n",
            "in training loop, epoch 4, step 831, the loss is 333393.15625\n",
            "in training loop, epoch 4, step 832, the loss is 585316.6875\n",
            "in training loop, epoch 4, step 833, the loss is 594590.25\n",
            "in training loop, epoch 4, step 834, the loss is 516320.53125\n",
            "in training loop, epoch 4, step 835, the loss is 489184.03125\n",
            "in training loop, epoch 4, step 836, the loss is 395485.75\n",
            "in training loop, epoch 4, step 837, the loss is 362241.59375\n",
            "in training loop, epoch 4, step 838, the loss is 446002.0\n",
            "in training loop, epoch 4, step 839, the loss is 545917.75\n",
            "in training loop, epoch 4, step 840, the loss is 629450.8125\n",
            "in training loop, epoch 4, step 841, the loss is 507777.8125\n",
            "in training loop, epoch 4, step 842, the loss is 656616.5625\n",
            "in training loop, epoch 4, step 843, the loss is 719640.0\n",
            "in training loop, epoch 4, step 844, the loss is 755859.875\n",
            "in training loop, epoch 4, step 845, the loss is 474874.15625\n",
            "in training loop, epoch 4, step 846, the loss is 458614.0\n",
            "in training loop, epoch 4, step 847, the loss is 677308.875\n",
            "in training loop, epoch 4, step 848, the loss is 496456.65625\n",
            "in training loop, epoch 4, step 849, the loss is 754753.875\n",
            "in training loop, epoch 4, step 850, the loss is 448788.40625\n",
            "in training loop, epoch 4, step 851, the loss is 434931.71875\n",
            "in training loop, epoch 4, step 852, the loss is 480242.4375\n",
            "in training loop, epoch 4, step 853, the loss is 508243.5625\n",
            "in training loop, epoch 4, step 854, the loss is 582297.75\n",
            "in training loop, epoch 4, step 855, the loss is 739126.375\n",
            "in training loop, epoch 4, step 856, the loss is 521728.625\n",
            "in training loop, epoch 4, step 857, the loss is 460470.21875\n",
            "in training loop, epoch 4, step 858, the loss is 599508.9375\n",
            "in training loop, epoch 4, step 859, the loss is 485088.21875\n",
            "in training loop, epoch 4, step 860, the loss is 682564.0\n",
            "in training loop, epoch 4, step 861, the loss is 473999.0\n",
            "in training loop, epoch 4, step 862, the loss is 888924.5\n",
            "in training loop, epoch 4, step 863, the loss is 504503.875\n",
            "in training loop, epoch 4, step 864, the loss is 356045.375\n",
            "in training loop, epoch 4, step 865, the loss is 318037.90625\n",
            "in training loop, epoch 4, step 866, the loss is 621393.625\n",
            "in training loop, epoch 4, step 867, the loss is 412145.21875\n",
            "in training loop, epoch 4, step 868, the loss is 477304.84375\n",
            "in training loop, epoch 4, step 869, the loss is 464888.125\n",
            "in training loop, epoch 4, step 870, the loss is 653655.5\n",
            "in training loop, epoch 4, step 871, the loss is 494206.875\n",
            "in training loop, epoch 4, step 872, the loss is 348520.59375\n",
            "in training loop, epoch 4, step 873, the loss is 458700.78125\n",
            "in training loop, epoch 4, step 874, the loss is 521157.59375\n",
            "in training loop, epoch 4, step 875, the loss is 362976.9375\n",
            "in training loop, epoch 4, step 876, the loss is 478805.0\n",
            "in training loop, epoch 4, step 877, the loss is 488216.71875\n",
            "in training loop, epoch 4, step 878, the loss is 608022.5625\n",
            "in training loop, epoch 4, step 879, the loss is 415864.9375\n",
            "in training loop, epoch 4, step 880, the loss is 526856.1875\n",
            "in training loop, epoch 4, step 881, the loss is 257564.109375\n",
            "in training loop, epoch 4, step 882, the loss is 530183.625\n",
            "in training loop, epoch 4, step 883, the loss is 452365.15625\n",
            "in training loop, epoch 4, step 884, the loss is 373175.46875\n",
            "in training loop, epoch 4, step 885, the loss is 448520.75\n",
            "in training loop, epoch 4, step 886, the loss is 556181.875\n",
            "in training loop, epoch 4, step 887, the loss is 470280.78125\n",
            "in training loop, epoch 4, step 888, the loss is 518286.40625\n",
            "in training loop, epoch 4, step 889, the loss is 510631.78125\n",
            "in training loop, epoch 4, step 890, the loss is 426457.3125\n",
            "in training loop, epoch 4, step 891, the loss is 391709.53125\n",
            "in training loop, epoch 4, step 892, the loss is 757043.8125\n",
            "in training loop, epoch 4, step 893, the loss is 518778.09375\n",
            "in training loop, epoch 4, step 894, the loss is 319868.625\n",
            "in training loop, epoch 4, step 895, the loss is 424448.15625\n",
            "in training loop, epoch 4, step 896, the loss is 589832.875\n",
            "in training loop, epoch 4, step 897, the loss is 293555.5625\n",
            "in training loop, epoch 4, step 898, the loss is 391342.375\n",
            "in training loop, epoch 4, step 899, the loss is 457638.875\n",
            "in training loop, epoch 4, step 900, the loss is 527340.125\n",
            "in training loop, epoch 4, step 901, the loss is 554373.125\n",
            "in training loop, epoch 4, step 902, the loss is 403809.40625\n",
            "in training loop, epoch 4, step 903, the loss is 279912.40625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "k-fold 1:: Epoch 4: train loss 465371.5586628872 val loss 658607.5259900991\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AxisError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-a83d57fe7de8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0mtestWrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_reconstruction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m   \u001b[0mtrain_loss_mean_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss_mean_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m   \u001b[0mtest_loss_mean_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss_mean_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m   \u001b[0mval_loss_mean_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss_mean_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   3368\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3369\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3370\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3372\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mrcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_count_reduce_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0;31m# Make this warning show up first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_count_reduce_items\u001b[0;34m(arr, axis)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mitems\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_axis_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAxisError\u001b[0m: axis 0 is out of bounds for array of dimension 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utbuno3_o_wv",
        "outputId": "df808059-3ab7-4bb8-b2f1-691f2a8cce9f"
      },
      "source": [
        "a=[1,2]\n",
        "b= np.mean(a,axis=0)\n",
        "c= b.tolist()\n",
        "c"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3I63RgNmk73"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}