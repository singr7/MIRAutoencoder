{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNLSTMAutoEncoder_with48bins.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNh4CqIvmUn6aYwDjZ/wUv5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singr7/MIRAutoencoder/blob/master/CNNLSTMAutoEncoder_with48bins.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9JqXiXhqdkN"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORCKRmYPHk-z"
      },
      "source": [
        "#Mount the google drive\n",
        "#Create list of numpy files for western and indian dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPnD6kkyHfx8",
        "outputId": "690bedf3-314a-4266-da1e-38eca13b0ecd"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "western_files = []\n",
        "western_file_dir = \"/content/drive/My Drive/MusicResearchColabNB/MelFeatures/Western_mel_numpy\"\n",
        "for r,d, fileList in os.walk(western_file_dir):\n",
        "  for file in fileList:\n",
        "    western_files.append(os.path.join(r,file))\n",
        "\n",
        "indian_files = []\n",
        "indian_file_dir = \"/content/drive/My Drive/MusicResearchColabNB/MelFeatures/Indian_mel_numpy\"\n",
        "for r,d, fileList in os.walk(indian_file_dir):\n",
        "  for file in fileList:\n",
        "    indian_files.append(os.path.join(r,file))\n",
        "\n",
        "print(len(western_files))\n",
        "print(len(indian_files))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "7894\n",
            "2008\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMQbF-ylLmdK"
      },
      "source": [
        "# Balance the western dataset by taking files equal to Indian dataset files = 2008"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXHKJAOLL-iX",
        "outputId": "55d91761-45d6-4d77-f1d1-9cc69ffb69b6"
      },
      "source": [
        "import random \n",
        "#randomize the selection. To avoid getting a different random sample with every run, use seed\n",
        "random.seed(234)\n",
        "bal_western_files = random.sample(western_files,2008)\n",
        "len(bal_western_files)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2008"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeBwpwaTX3Jo"
      },
      "source": [
        "#Define configuration class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP4323imT53f"
      },
      "source": [
        "class Configuration:\n",
        "  seq_len = 200  # taking half of the original timesteps extracted \n",
        "  input_dim = 48  #num of mels\n",
        "  embedding_dim = 64\n",
        "  batch_size = 2\n",
        "  base_dir = \"/content/drive/My Drive/MusicResearchColabNB/vajra/westernAE_48bins\"   # need to be edited..\n",
        "  loss_function = torch.nn.MSELoss(reduction='sum')\n",
        "  lr=1e-3  # I edited it from 1e-3 to 1e-5\n",
        "  n_epochs = 4\n",
        "  model_file = \"/content/drive/My Drive/MusicResearchColabNB/vajra/westernAE_48bins/models/mel.pkl\"  #need need edits\n",
        "  results_dir = os.path.join(base_dir, \"./results\")  # may need edits\n",
        "  checkpoint_model_file = \"/content/drive/My Drive/MusicResearchColabNB/vajra/westernAE_48bins/models/mel_checkpoint.pkl\" #may need edits\n",
        "  kernel_size = 3  #why?\n",
        "  k_folds = 10 "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RALRBXgZZBA"
      },
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "  def __init__(self, seq_len, n_features, embedding_dim=64, kernel_size=3, stride=1):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.seq_len, self.n_features = seq_len, n_features\n",
        "    self.embedding_dim, self.hidden_dim = embedding_dim, 2 * embedding_dim\n",
        "\n",
        "\n",
        "    self.conv = nn.Conv1d(in_channels=seq_len,out_channels=seq_len,kernel_size=kernel_size,stride=stride, groups=seq_len)\n",
        "    conv_op_dim = int(((n_features - kernel_size)/ stride) + 1)\n",
        "\n",
        "    self.rnn1 = nn.LSTM(\n",
        "      input_size=conv_op_dim,\n",
        "      hidden_size=self.hidden_dim,\n",
        "      num_layers=1,\n",
        "      batch_first=True\n",
        "    )\n",
        "    self.rnn2 = nn.LSTM(\n",
        "      input_size=self.hidden_dim,\n",
        "      hidden_size=embedding_dim,\n",
        "      num_layers=1,\n",
        "      batch_first=True\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    #x = x.reshape((10, self.seq_len, self.n_features))\n",
        "   # print('In Encoder')\n",
        "   # print(x.shape)\n",
        "    x = self.conv(x)\n",
        "    x, (_, _) = self.rnn1(x)\n",
        "    x, (hidden_n, _) = self.rnn2(x)\n",
        "    return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkW-A8TzZdGT"
      },
      "source": [
        "class Decoder(torch.nn.Module):\n",
        "  def __init__(self, seq_len, embedding_dim=64, n_features=48):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.seq_len, self.embedding_dim = seq_len, embedding_dim\n",
        "    self.hidden_dim, self.n_features = 2 * embedding_dim, n_features\n",
        "    self.rnn1 = nn.LSTM(\n",
        "      input_size=embedding_dim,\n",
        "      hidden_size=embedding_dim,\n",
        "      num_layers=1,\n",
        "      batch_first=True\n",
        "    )\n",
        "    self.rnn2 = nn.LSTM(\n",
        "      input_size=embedding_dim,\n",
        "      hidden_size=self.hidden_dim,\n",
        "      num_layers=1,\n",
        "      batch_first=True\n",
        "    )\n",
        "    self.output_layer = nn.Linear(self.hidden_dim * self.seq_len, n_features * self.seq_len)\n",
        "  def forward(self, x):\n",
        "    x, (hidden_n, cell_n) = self.rnn1(x)\n",
        "    x, (hidden_n, cell_n) = self.rnn2(x)\n",
        "    #print(\"in decoder\", x.shape)\n",
        "    x = x.contiguous()\n",
        "    x = x.view(x.shape[0], -1)\n",
        "    x = self.output_layer(x)\n",
        "    return x.reshape(x.shape[0],self.seq_len, self.n_features)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mqrvU5MZfEA"
      },
      "source": [
        "class RecurrentAutoencoder(torch.nn.Module):\n",
        "  def __init__(self, seq_len, n_features, embedding_dim=64, device='cpu'):\n",
        "    super(RecurrentAutoencoder, self).__init__()\n",
        "    self.encoder = Encoder(seq_len, n_features, embedding_dim).to(device)\n",
        "    self.decoder = Decoder(seq_len, embedding_dim, n_features).to(device)\n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)\n",
        "    x = self.decoder(x)\n",
        "    return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRQ-t9aNZiUD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32faf639-f62e-4b5d-8392-f560c47d8c18"
      },
      "source": [
        "x = torch.randn(10, 48, 400)\n",
        "print(x.shape)\n",
        "x = x.permute(0, 2, 1)\n",
        "print(x.shape)\n",
        "\n",
        "encoder = Encoder(400, 48, embedding_dim=64, kernel_size=3, stride=1)\n",
        "encoded = encoder(x)\n",
        "print(encoded.shape)\n",
        "\n",
        "decoder = Decoder(400, 64, 48)\n",
        "decoded = decoder(encoded)\n",
        "print(decoded.shape)\n",
        "\n",
        "rae = RecurrentAutoencoder(400, 48, 64)\n",
        "output = rae(x)\n",
        "\n",
        "print(output.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 48, 400])\n",
            "torch.Size([10, 400, 48])\n",
            "torch.Size([10, 400, 64])\n",
            "torch.Size([10, 400, 48])\n",
            "torch.Size([10, 400, 48])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDpec6ICZnKN"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data.sampler import SequentialSampler\n",
        "\n",
        "class CustomDatasetMel(Dataset):\n",
        "\n",
        "    def __init__(self, dataList):\n",
        "        self.data = dataList\n",
        "        #self.labels = labelList\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        import numpy as np\n",
        "        fileName = self.data[index]\n",
        "        \n",
        "        mel_spect = np.load(fileName)\n",
        "        data = torch.tensor(mel_spect[:,:200], dtype=torch.float)\n",
        "        data = data.permute(1, 0)\n",
        "        #data = torch.unsqueeze(data, dim =0)\n",
        "\n",
        "        #label = torch.tensor(self.labels[index])\n",
        "        return data"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uAv1gLc6Zwwr",
        "outputId": "83213286-44f1-4c12-c1aa-1356d675783b"
      },
      "source": [
        "import copy\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "class TrainingWrapper:\n",
        "\n",
        "  def __init__(self, config, training_loader, test_loader, device, val_loader=None, cross=10):\n",
        "    self.config = config\n",
        "    self.training_loader = training_loader\n",
        "    self.test_loader = test_loader\n",
        "    self.val_loader = val_loader\n",
        "    self.device = device\n",
        "    self.model = RecurrentAutoencoder(self.config.seq_len, self.config.input_dim, self.config.embedding_dim, device=self.device)\n",
        "    self.model = self.model.to(self.device)\n",
        "    self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.lr)\n",
        "    self.criterion = self.config.loss_function.to(self.device)\n",
        "    self.history = dict(train=[], val=[], cross_val=[])\n",
        "    self.best_model_wts = copy.deepcopy(self.model.state_dict())\n",
        "    self.best_loss = 10000.0\n",
        "    #print(self.config.base_dir + self.config.model_file)\n",
        "    torch.save(self.model.state_dict(),  self.config.model_file)\n",
        "    self.cross = cross\n",
        "    \n",
        "\n",
        "  def visualizeTraining(self, epoch, trn_losses, tst_losses, val_losses, save_dir,cross):\n",
        "    # visualize the loss as the network trained\n",
        "    import matplotlib.pyplot as plt\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    plt.plot(range(0, len(trn_losses)), trn_losses, label='Training Loss')\n",
        "    if tst_losses:\n",
        "      plt.plot(range(0, len(tst_losses)), tst_losses, label='Validation Loss')\n",
        "    if val_losses:\n",
        "      plt.plot(range(0, len(val_losses)), val_losses, label='Cross Validation Loss')\n",
        "\n",
        "    minposs = tst_losses.index(min(tst_losses))\n",
        "    plt.axvline(minposs, linestyle='--', color='r', label='Early Stopping Checkpoint')\n",
        "\n",
        "\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('loss')\n",
        "    # plt.ylim(0, 0.5)  # consistent scale\n",
        "    # plt.xlim(0, len(trn_losses))  # consistent scale\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    fig.savefig(os.path.join(save_dir , 'loss_plot_{}.png'.format(cross)), bbox_inches='tight')\n",
        "\n",
        "  def train(self):\n",
        "    self.model.load_state_dict(torch.load(config.checkpoint_model_file))\n",
        "    for epoch in range(1, self.config.n_epochs + 1):\n",
        "      self.model = self.model.train()\n",
        "      train_losses = []\n",
        "      for i, data in enumerate(self.training_loader,0):\n",
        "        x = data\n",
        "        self.optimizer.zero_grad()\n",
        "        x = x.to(self.device)\n",
        "\n",
        "        output = self.model(x)\n",
        "        loss = self.criterion(output, x)\n",
        "\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        train_losses.append(loss.item())\n",
        "        print(\"in training loop, epoch {}, step {}, the loss is {}\".format(epoch, i, loss.item()))\n",
        "\n",
        "      val_losses = []\n",
        "      self.model = self.model.eval()\n",
        "      with torch.no_grad():\n",
        "        for i, data in enumerate(self.test_loader):\n",
        "          x = data\n",
        "          x = x.to(device)\n",
        "          output = self.model(x)\n",
        "          loss = self.criterion(output, x)\n",
        "          val_losses.append(loss.item())\n",
        "\n",
        "\n",
        "      cross_val_losses = []\n",
        "      self.model = self.model.eval()\n",
        "      with torch.no_grad():\n",
        "        for i, data in enumerate(self.val_loader):\n",
        "          x = data\n",
        "          x = x.to(device)\n",
        "          output = self.model(x)\n",
        "          loss = self.criterion(output, x)\n",
        "          cross_val_losses.append(loss.item())\n",
        "\n",
        "      train_loss = np.mean(train_losses)\n",
        "      val_loss = np.mean(val_losses)\n",
        "      cross_val_loss = np.mean(cross_val_losses)\n",
        "\n",
        "\n",
        "      self.history['train'].append(train_loss)\n",
        "      self.history['val'].append(val_loss)\n",
        "      self.history['cross_val'].append(cross_val_loss)\n",
        "\n",
        "      if val_loss < self.best_loss:\n",
        "        self.best_loss = val_loss\n",
        "        self.best_model_wts = copy.deepcopy(self.model.state_dict())\n",
        "      #torch.save(self.model.state_dict(),  self.config.checkpoint_model_file)\n",
        "      if epoch % 2 == 0:\n",
        "        torch.save(self.model.state_dict(),  self.config.checkpoint_model_file)\n",
        "        self.visualizeTraining(epoch, trn_losses= self.history['train'], tst_losses=self.history['val'], val_losses =self.history['cross_val'], save_dir=self.config.base_dir + \"/results\",cross=fold)\n",
        "      print(f'k-fold {fold}:: Epoch {epoch}: train loss {train_loss} val loss {val_loss}')\n",
        "    self.model.load_state_dict(self.best_model_wts)\n",
        "    torch.save(self.model.state_dict(), self.config.model_file)\n",
        "    return self.model.eval(), self.history\n",
        "\n",
        "\n",
        "mode = 'train'\n",
        "data = \"mel\"\n",
        "config = Configuration()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import random\n",
        "import os\n",
        "\n",
        "def seed_everything(seed=1234):\n",
        "  \n",
        "  random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  np.random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def visualizeTraining(epoch, trn_losses, tst_losses, val_losses, save_dir):\n",
        "    # visualize the loss as the network trained\n",
        "    import matplotlib.pyplot as plt\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    plt.plot(range(0, len(trn_losses)), trn_losses, label='Training Loss')\n",
        "    \n",
        "    plt.plot(range(0, len(tst_losses)), tst_losses, label='Validation Loss')\n",
        "    #if val_losses:\n",
        "    plt.plot(range(0, len(val_losses)), val_losses, label='Cross Validation Loss')\n",
        "\n",
        "    minposs = tst_losses.index(min(tst_losses))\n",
        "    plt.axvline(minposs, linestyle='--', color='r', label='Early Stopping Checkpoint')\n",
        "\n",
        "\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('loss')\n",
        "    # plt.ylim(0, 0.5)  # consistent scale\n",
        "    # plt.xlim(0, len(trn_losses))  # consistent scale\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    fig.savefig(os.path.join(save_dir , 'loss_plot_{}.png'.format(\"MEAN\")), bbox_inches='tight')\n",
        "\n",
        "seed_everything()\n",
        "\n",
        "train_data = bal_western_files\n",
        "val_data = indian_files\n",
        "\n",
        "\n",
        "\n",
        "# Cross validation runs\n",
        "# use sklearn KFolds\n",
        "from sklearn.model_selection import KFold\n",
        "kfold = KFold(n_splits=config.k_folds , shuffle=True)\n",
        "\n",
        "train_dataset = CustomDatasetMel(train_data)\n",
        "val_dataset = CustomDatasetMel(val_data)\n",
        "#Load the cross val dataset which is Full Indian dataset\n",
        "#It is identical for all K-folds\n",
        "crossval_loader = torch.utils.data.DataLoader(\n",
        "                      val_dataset,\n",
        "                      batch_size=config.batch_size, \n",
        "                      sampler=SequentialSampler(val_dataset), \n",
        "                      drop_last=False)  \n",
        "\n",
        "train_loss_mean_list = []\n",
        "test_loss_mean_list = []\n",
        "val_loss_mean_list = []\n",
        "\n",
        "for fold, (train_ids, test_ids) in enumerate(kfold.split(train_dataset)):\n",
        "  print(f'FOLD {fold}')\n",
        "  print('--------------------------------')\n",
        "    \n",
        "    # Sample elements randomly from a given list of ids, no replacement.\n",
        "  train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "  test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
        "  \n",
        "    # Define data loaders for training and testing data in this fold\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "                      train_dataset, \n",
        "                      batch_size=config.batch_size,\n",
        "                      sampler=train_subsampler,\n",
        "                      drop_last=False)\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "                      train_dataset,\n",
        "                      batch_size=config.batch_size,\n",
        "                      sampler=test_subsampler,\n",
        "                      drop_last=False)\n",
        "    \n",
        "  print(\"length of of train_loader is {} & length of traindataset is {}\".format(len(train_loader),len(train_dataset)))\n",
        "  print(\"length of of test_loader is {}\".format(len(test_loader)))\n",
        "  print(\"length of of val_loader is {}\".format(len(crossval_loader)))\n",
        "    \n",
        "  if mode==\"train\":\n",
        "    trainingWrapper = TrainingWrapper(config=config, training_loader=train_loader, test_loader=test_loader, device=device, val_loader=crossval_loader, cross=fold)\n",
        "    model, history = trainingWrapper.train()\n",
        "    \n",
        "    try:\n",
        "      train_loss_mean_list.append(history['train'])\n",
        "      test_loss_mean_list.append(history['val'])\n",
        "      val_loss_mean_list.append(history['cross_val'])\n",
        "    except:\n",
        "      print('Appendoing to train_loss_mean_list')\n",
        "\n",
        "    \n",
        "    print(f'At fold {fold} with train_loss:')\n",
        "\n",
        "    if data==\"mnist\":\n",
        "      #trainingWrapper.show_reconstruction(test_loader=test_loader, n_images=50)\n",
        "      pass\n",
        "\n",
        "  elif mode==\"test\":\n",
        "    testWrapper = TestingWrapper(config=config, device=device)\n",
        "    testWrapper.save_reconstruction(test_loader)\n",
        "  try:\n",
        "    print('Before doing mean loss list calc')\n",
        "    train_loss_mean_list_np = np.mean(train_loss_mean_list, axis=0)\n",
        "    test_loss_mean_list_np = np.mean(test_loss_mean_list, axis=0)\n",
        "    val_loss_mean_list_np = np.mean(val_loss_mean_list, axis=0)\n",
        "\n",
        "  except:\n",
        "    print('# 1 Error happened while doing mean loss list#####')\n",
        "    \n",
        "    \n",
        "    print(\"train_loss_mean_list\",train_loss_mean_list)\n",
        "    print(\"test_loss_mean_list\",test_loss_mean_list)\n",
        "    print(\"val_loss_mean_list\",val_loss_mean_list)\n",
        "\n",
        "  try:\n",
        "    tn_loss =train_loss_mean_list_np.tolist()\n",
        "    tt_loss= test_loss_mean_list_np.tolist()\n",
        "    v_loss= val_loss_mean_list_np.tolist()\n",
        "\n",
        "    visualizeTraining(0, tn_loss, tt_loss, v_loss, save_dir = config.base_dir + \"/results\")  \n",
        "  except:\n",
        "    print('Out of here..')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOLD 0\n",
            "--------------------------------\n",
            "length of of train_loader is 904 & length of traindataset is 2008\n",
            "length of of test_loader is 101\n",
            "length of of val_loader is 1004\n",
            "in training loop, epoch 1, step 0, the loss is 286191.3125\n",
            "in training loop, epoch 1, step 1, the loss is 395862.78125\n",
            "in training loop, epoch 1, step 2, the loss is 316609.3125\n",
            "in training loop, epoch 1, step 3, the loss is 359692.0\n",
            "in training loop, epoch 1, step 4, the loss is 435704.21875\n",
            "in training loop, epoch 1, step 5, the loss is 443838.5\n",
            "in training loop, epoch 1, step 6, the loss is 429193.53125\n",
            "in training loop, epoch 1, step 7, the loss is 300446.8125\n",
            "in training loop, epoch 1, step 8, the loss is 286934.4375\n",
            "in training loop, epoch 1, step 9, the loss is 335012.625\n",
            "in training loop, epoch 1, step 10, the loss is 510005.6875\n",
            "in training loop, epoch 1, step 11, the loss is 398381.40625\n",
            "in training loop, epoch 1, step 12, the loss is 365863.34375\n",
            "in training loop, epoch 1, step 13, the loss is 270006.84375\n",
            "in training loop, epoch 1, step 14, the loss is 386333.40625\n",
            "in training loop, epoch 1, step 15, the loss is 341092.6875\n",
            "in training loop, epoch 1, step 16, the loss is 629540.75\n",
            "in training loop, epoch 1, step 17, the loss is 443073.84375\n",
            "in training loop, epoch 1, step 18, the loss is 288169.78125\n",
            "in training loop, epoch 1, step 19, the loss is 407821.96875\n",
            "in training loop, epoch 1, step 20, the loss is 318986.28125\n",
            "in training loop, epoch 1, step 21, the loss is 473989.21875\n",
            "in training loop, epoch 1, step 22, the loss is 288812.96875\n",
            "in training loop, epoch 1, step 23, the loss is 313833.4375\n",
            "in training loop, epoch 1, step 24, the loss is 425754.1875\n",
            "in training loop, epoch 1, step 25, the loss is 296693.9375\n",
            "in training loop, epoch 1, step 26, the loss is 329726.90625\n",
            "in training loop, epoch 1, step 27, the loss is 289818.1875\n",
            "in training loop, epoch 1, step 28, the loss is 396679.125\n",
            "in training loop, epoch 1, step 29, the loss is 254669.3125\n",
            "in training loop, epoch 1, step 30, the loss is 304399.5\n",
            "in training loop, epoch 1, step 31, the loss is 368712.25\n",
            "in training loop, epoch 1, step 32, the loss is 299934.8125\n",
            "in training loop, epoch 1, step 33, the loss is 272458.75\n",
            "in training loop, epoch 1, step 34, the loss is 314932.3125\n",
            "in training loop, epoch 1, step 35, the loss is 258105.5\n",
            "in training loop, epoch 1, step 36, the loss is 278838.125\n",
            "in training loop, epoch 1, step 37, the loss is 535057.4375\n",
            "in training loop, epoch 1, step 38, the loss is 342740.90625\n",
            "in training loop, epoch 1, step 39, the loss is 293069.875\n",
            "in training loop, epoch 1, step 40, the loss is 268902.6875\n",
            "in training loop, epoch 1, step 41, the loss is 219220.96875\n",
            "in training loop, epoch 1, step 42, the loss is 387024.6875\n",
            "in training loop, epoch 1, step 43, the loss is 281343.5\n",
            "in training loop, epoch 1, step 44, the loss is 298551.4375\n",
            "in training loop, epoch 1, step 45, the loss is 270730.8125\n",
            "in training loop, epoch 1, step 46, the loss is 306062.96875\n",
            "in training loop, epoch 1, step 47, the loss is 241708.5625\n",
            "in training loop, epoch 1, step 48, the loss is 439085.4375\n",
            "in training loop, epoch 1, step 49, the loss is 340418.875\n",
            "in training loop, epoch 1, step 50, the loss is 228843.78125\n",
            "in training loop, epoch 1, step 51, the loss is 373860.46875\n",
            "in training loop, epoch 1, step 52, the loss is 472109.5\n",
            "in training loop, epoch 1, step 53, the loss is 248515.578125\n",
            "in training loop, epoch 1, step 54, the loss is 333895.71875\n",
            "in training loop, epoch 1, step 55, the loss is 364017.0625\n",
            "in training loop, epoch 1, step 56, the loss is 347067.65625\n",
            "in training loop, epoch 1, step 57, the loss is 224449.9375\n",
            "in training loop, epoch 1, step 58, the loss is 401951.625\n",
            "in training loop, epoch 1, step 59, the loss is 541383.0\n",
            "in training loop, epoch 1, step 60, the loss is 326215.90625\n",
            "in training loop, epoch 1, step 61, the loss is 416279.84375\n",
            "in training loop, epoch 1, step 62, the loss is 261734.03125\n",
            "in training loop, epoch 1, step 63, the loss is 416614.8125\n",
            "in training loop, epoch 1, step 64, the loss is 404094.75\n",
            "in training loop, epoch 1, step 65, the loss is 392647.0\n",
            "in training loop, epoch 1, step 66, the loss is 337239.84375\n",
            "in training loop, epoch 1, step 67, the loss is 338557.34375\n",
            "in training loop, epoch 1, step 68, the loss is 247672.609375\n",
            "in training loop, epoch 1, step 69, the loss is 409194.5\n",
            "in training loop, epoch 1, step 70, the loss is 352973.34375\n",
            "in training loop, epoch 1, step 71, the loss is 255451.03125\n",
            "in training loop, epoch 1, step 72, the loss is 256764.984375\n",
            "in training loop, epoch 1, step 73, the loss is 241963.734375\n",
            "in training loop, epoch 1, step 74, the loss is 411412.0\n",
            "in training loop, epoch 1, step 75, the loss is 348107.0\n",
            "in training loop, epoch 1, step 76, the loss is 170035.65625\n",
            "in training loop, epoch 1, step 77, the loss is 260467.25\n",
            "in training loop, epoch 1, step 78, the loss is 287080.53125\n",
            "in training loop, epoch 1, step 79, the loss is 264767.75\n",
            "in training loop, epoch 1, step 80, the loss is 241900.515625\n",
            "in training loop, epoch 1, step 81, the loss is 401682.15625\n",
            "in training loop, epoch 1, step 82, the loss is 397041.0625\n",
            "in training loop, epoch 1, step 83, the loss is 348350.34375\n",
            "in training loop, epoch 1, step 84, the loss is 429680.9375\n",
            "in training loop, epoch 1, step 85, the loss is 202607.71875\n",
            "in training loop, epoch 1, step 86, the loss is 365125.65625\n",
            "in training loop, epoch 1, step 87, the loss is 384888.375\n",
            "in training loop, epoch 1, step 88, the loss is 330291.0\n",
            "in training loop, epoch 1, step 89, the loss is 292324.15625\n",
            "in training loop, epoch 1, step 90, the loss is 280966.03125\n",
            "in training loop, epoch 1, step 91, the loss is 251070.375\n",
            "in training loop, epoch 1, step 92, the loss is 223550.5625\n",
            "in training loop, epoch 1, step 93, the loss is 256649.09375\n",
            "in training loop, epoch 1, step 94, the loss is 211645.234375\n",
            "in training loop, epoch 1, step 95, the loss is 277239.84375\n",
            "in training loop, epoch 1, step 96, the loss is 349891.6875\n",
            "in training loop, epoch 1, step 97, the loss is 322913.3125\n",
            "in training loop, epoch 1, step 98, the loss is 204387.671875\n",
            "in training loop, epoch 1, step 99, the loss is 278319.1875\n",
            "in training loop, epoch 1, step 100, the loss is 299848.375\n",
            "in training loop, epoch 1, step 101, the loss is 341586.46875\n",
            "in training loop, epoch 1, step 102, the loss is 366547.6875\n",
            "in training loop, epoch 1, step 103, the loss is 442506.21875\n",
            "in training loop, epoch 1, step 104, the loss is 199642.171875\n",
            "in training loop, epoch 1, step 105, the loss is 361495.96875\n",
            "in training loop, epoch 1, step 106, the loss is 350206.375\n",
            "in training loop, epoch 1, step 107, the loss is 259354.453125\n",
            "in training loop, epoch 1, step 108, the loss is 269813.09375\n",
            "in training loop, epoch 1, step 109, the loss is 244106.71875\n",
            "in training loop, epoch 1, step 110, the loss is 348499.78125\n",
            "in training loop, epoch 1, step 111, the loss is 358124.9375\n",
            "in training loop, epoch 1, step 112, the loss is 359412.34375\n",
            "in training loop, epoch 1, step 113, the loss is 288539.1875\n",
            "in training loop, epoch 1, step 114, the loss is 336808.0625\n",
            "in training loop, epoch 1, step 115, the loss is 363167.53125\n",
            "in training loop, epoch 1, step 116, the loss is 216393.96875\n",
            "in training loop, epoch 1, step 117, the loss is 197643.984375\n",
            "in training loop, epoch 1, step 118, the loss is 432773.75\n",
            "in training loop, epoch 1, step 119, the loss is 284054.46875\n",
            "in training loop, epoch 1, step 120, the loss is 319804.53125\n",
            "in training loop, epoch 1, step 121, the loss is 264505.1875\n",
            "in training loop, epoch 1, step 122, the loss is 289347.1875\n",
            "in training loop, epoch 1, step 123, the loss is 267191.0625\n",
            "in training loop, epoch 1, step 124, the loss is 312429.875\n",
            "in training loop, epoch 1, step 125, the loss is 221869.0\n",
            "in training loop, epoch 1, step 126, the loss is 248708.125\n",
            "in training loop, epoch 1, step 127, the loss is 261390.609375\n",
            "in training loop, epoch 1, step 128, the loss is 312040.28125\n",
            "in training loop, epoch 1, step 129, the loss is 325724.125\n",
            "in training loop, epoch 1, step 130, the loss is 240375.71875\n",
            "in training loop, epoch 1, step 131, the loss is 258826.546875\n",
            "in training loop, epoch 1, step 132, the loss is 344328.1875\n",
            "in training loop, epoch 1, step 133, the loss is 305359.5625\n",
            "in training loop, epoch 1, step 134, the loss is 297025.59375\n",
            "in training loop, epoch 1, step 135, the loss is 263615.96875\n",
            "in training loop, epoch 1, step 136, the loss is 267007.65625\n",
            "in training loop, epoch 1, step 137, the loss is 265027.0625\n",
            "in training loop, epoch 1, step 138, the loss is 295723.8125\n",
            "in training loop, epoch 1, step 139, the loss is 394592.59375\n",
            "in training loop, epoch 1, step 140, the loss is 293769.4375\n",
            "in training loop, epoch 1, step 141, the loss is 243792.03125\n",
            "in training loop, epoch 1, step 142, the loss is 253760.171875\n",
            "in training loop, epoch 1, step 143, the loss is 225733.625\n",
            "in training loop, epoch 1, step 144, the loss is 345427.53125\n",
            "in training loop, epoch 1, step 145, the loss is 211295.609375\n",
            "in training loop, epoch 1, step 146, the loss is 296527.0625\n",
            "in training loop, epoch 1, step 147, the loss is 271186.8125\n",
            "in training loop, epoch 1, step 148, the loss is 215713.5625\n",
            "in training loop, epoch 1, step 149, the loss is 190826.34375\n",
            "in training loop, epoch 1, step 150, the loss is 444657.90625\n",
            "in training loop, epoch 1, step 151, the loss is 234782.71875\n",
            "in training loop, epoch 1, step 152, the loss is 258516.890625\n",
            "in training loop, epoch 1, step 153, the loss is 266756.78125\n",
            "in training loop, epoch 1, step 154, the loss is 303856.0625\n",
            "in training loop, epoch 1, step 155, the loss is 311501.03125\n",
            "in training loop, epoch 1, step 156, the loss is 183734.875\n",
            "in training loop, epoch 1, step 157, the loss is 192055.1875\n",
            "in training loop, epoch 1, step 158, the loss is 373587.09375\n",
            "in training loop, epoch 1, step 159, the loss is 266178.65625\n",
            "in training loop, epoch 1, step 160, the loss is 240304.65625\n",
            "in training loop, epoch 1, step 161, the loss is 201516.578125\n",
            "in training loop, epoch 1, step 162, the loss is 218437.59375\n",
            "in training loop, epoch 1, step 163, the loss is 214511.8125\n",
            "in training loop, epoch 1, step 164, the loss is 367264.75\n",
            "in training loop, epoch 1, step 165, the loss is 345729.5\n",
            "in training loop, epoch 1, step 166, the loss is 333970.875\n",
            "in training loop, epoch 1, step 167, the loss is 286284.625\n",
            "in training loop, epoch 1, step 168, the loss is 193538.578125\n",
            "in training loop, epoch 1, step 169, the loss is 373953.78125\n",
            "in training loop, epoch 1, step 170, the loss is 326199.375\n",
            "in training loop, epoch 1, step 171, the loss is 308449.3125\n",
            "in training loop, epoch 1, step 172, the loss is 268310.09375\n",
            "in training loop, epoch 1, step 173, the loss is 323208.78125\n",
            "in training loop, epoch 1, step 174, the loss is 255783.921875\n",
            "in training loop, epoch 1, step 175, the loss is 395702.90625\n",
            "in training loop, epoch 1, step 176, the loss is 249885.84375\n",
            "in training loop, epoch 1, step 177, the loss is 237188.6875\n",
            "in training loop, epoch 1, step 178, the loss is 282095.75\n",
            "in training loop, epoch 1, step 179, the loss is 300816.625\n",
            "in training loop, epoch 1, step 180, the loss is 363657.625\n",
            "in training loop, epoch 1, step 181, the loss is 210352.0\n",
            "in training loop, epoch 1, step 182, the loss is 410478.78125\n",
            "in training loop, epoch 1, step 183, the loss is 269899.09375\n",
            "in training loop, epoch 1, step 184, the loss is 294795.625\n",
            "in training loop, epoch 1, step 185, the loss is 205645.078125\n",
            "in training loop, epoch 1, step 186, the loss is 216950.109375\n",
            "in training loop, epoch 1, step 187, the loss is 262142.0\n",
            "in training loop, epoch 1, step 188, the loss is 312391.6875\n",
            "in training loop, epoch 1, step 189, the loss is 271821.0\n",
            "in training loop, epoch 1, step 190, the loss is 373558.125\n",
            "in training loop, epoch 1, step 191, the loss is 218178.28125\n",
            "in training loop, epoch 1, step 192, the loss is 370734.09375\n",
            "in training loop, epoch 1, step 193, the loss is 344177.90625\n",
            "in training loop, epoch 1, step 194, the loss is 270901.65625\n",
            "in training loop, epoch 1, step 195, the loss is 345538.25\n",
            "in training loop, epoch 1, step 196, the loss is 357845.0\n",
            "in training loop, epoch 1, step 197, the loss is 309464.5625\n",
            "in training loop, epoch 1, step 198, the loss is 556710.1875\n",
            "in training loop, epoch 1, step 199, the loss is 271252.90625\n",
            "in training loop, epoch 1, step 200, the loss is 317001.25\n",
            "in training loop, epoch 1, step 201, the loss is 256202.71875\n",
            "in training loop, epoch 1, step 202, the loss is 192210.375\n",
            "in training loop, epoch 1, step 203, the loss is 270191.28125\n",
            "in training loop, epoch 1, step 204, the loss is 268088.75\n",
            "in training loop, epoch 1, step 205, the loss is 242735.1875\n",
            "in training loop, epoch 1, step 206, the loss is 241548.375\n",
            "in training loop, epoch 1, step 207, the loss is 404930.875\n",
            "in training loop, epoch 1, step 208, the loss is 413552.78125\n",
            "in training loop, epoch 1, step 209, the loss is 241171.0\n",
            "in training loop, epoch 1, step 210, the loss is 269429.3125\n",
            "in training loop, epoch 1, step 211, the loss is 325461.40625\n",
            "in training loop, epoch 1, step 212, the loss is 346407.59375\n",
            "in training loop, epoch 1, step 213, the loss is 317250.375\n",
            "in training loop, epoch 1, step 214, the loss is 350584.8125\n",
            "in training loop, epoch 1, step 215, the loss is 312703.4375\n",
            "in training loop, epoch 1, step 216, the loss is 427691.71875\n",
            "in training loop, epoch 1, step 217, the loss is 365371.25\n",
            "in training loop, epoch 1, step 218, the loss is 380581.5\n",
            "in training loop, epoch 1, step 219, the loss is 333190.75\n",
            "in training loop, epoch 1, step 220, the loss is 258106.859375\n",
            "in training loop, epoch 1, step 221, the loss is 302218.6875\n",
            "in training loop, epoch 1, step 222, the loss is 297530.28125\n",
            "in training loop, epoch 1, step 223, the loss is 287784.1875\n",
            "in training loop, epoch 1, step 224, the loss is 406043.0625\n",
            "in training loop, epoch 1, step 225, the loss is 427916.09375\n",
            "in training loop, epoch 1, step 226, the loss is 317244.75\n",
            "in training loop, epoch 1, step 227, the loss is 254471.15625\n",
            "in training loop, epoch 1, step 228, the loss is 260846.90625\n",
            "in training loop, epoch 1, step 229, the loss is 316002.8125\n",
            "in training loop, epoch 1, step 230, the loss is 297756.1875\n",
            "in training loop, epoch 1, step 231, the loss is 217172.421875\n",
            "in training loop, epoch 1, step 232, the loss is 360997.5\n",
            "in training loop, epoch 1, step 233, the loss is 299482.375\n",
            "in training loop, epoch 1, step 234, the loss is 250347.296875\n",
            "in training loop, epoch 1, step 235, the loss is 275970.65625\n",
            "in training loop, epoch 1, step 236, the loss is 359681.84375\n",
            "in training loop, epoch 1, step 237, the loss is 343103.15625\n",
            "in training loop, epoch 1, step 238, the loss is 339020.90625\n",
            "in training loop, epoch 1, step 239, the loss is 285865.59375\n",
            "in training loop, epoch 1, step 240, the loss is 310309.5625\n",
            "in training loop, epoch 1, step 241, the loss is 375866.59375\n",
            "in training loop, epoch 1, step 242, the loss is 290152.40625\n",
            "in training loop, epoch 1, step 243, the loss is 349656.09375\n",
            "in training loop, epoch 1, step 244, the loss is 404248.3125\n",
            "in training loop, epoch 1, step 245, the loss is 238564.75\n",
            "in training loop, epoch 1, step 246, the loss is 297383.8125\n",
            "in training loop, epoch 1, step 247, the loss is 340302.25\n",
            "in training loop, epoch 1, step 248, the loss is 358968.5\n",
            "in training loop, epoch 1, step 249, the loss is 264892.625\n",
            "in training loop, epoch 1, step 250, the loss is 311671.6875\n",
            "in training loop, epoch 1, step 251, the loss is 314567.15625\n",
            "in training loop, epoch 1, step 252, the loss is 336156.34375\n",
            "in training loop, epoch 1, step 253, the loss is 279213.96875\n",
            "in training loop, epoch 1, step 254, the loss is 227412.46875\n",
            "in training loop, epoch 1, step 255, the loss is 220715.59375\n",
            "in training loop, epoch 1, step 256, the loss is 307805.4375\n",
            "in training loop, epoch 1, step 257, the loss is 262172.0\n",
            "in training loop, epoch 1, step 258, the loss is 271689.75\n",
            "in training loop, epoch 1, step 259, the loss is 362488.84375\n",
            "in training loop, epoch 1, step 260, the loss is 249145.65625\n",
            "in training loop, epoch 1, step 261, the loss is 261853.828125\n",
            "in training loop, epoch 1, step 262, the loss is 278146.3125\n",
            "in training loop, epoch 1, step 263, the loss is 251599.296875\n",
            "in training loop, epoch 1, step 264, the loss is 261967.1875\n",
            "in training loop, epoch 1, step 265, the loss is 370665.6875\n",
            "in training loop, epoch 1, step 266, the loss is 261349.09375\n",
            "in training loop, epoch 1, step 267, the loss is 191965.03125\n",
            "in training loop, epoch 1, step 268, the loss is 293237.125\n",
            "in training loop, epoch 1, step 269, the loss is 204329.84375\n",
            "in training loop, epoch 1, step 270, the loss is 286448.90625\n",
            "in training loop, epoch 1, step 271, the loss is 354939.34375\n",
            "in training loop, epoch 1, step 272, the loss is 285484.15625\n",
            "in training loop, epoch 1, step 273, the loss is 242282.90625\n",
            "in training loop, epoch 1, step 274, the loss is 395446.375\n",
            "in training loop, epoch 1, step 275, the loss is 243906.515625\n",
            "in training loop, epoch 1, step 276, the loss is 206162.078125\n",
            "in training loop, epoch 1, step 277, the loss is 361434.9375\n",
            "in training loop, epoch 1, step 278, the loss is 298580.09375\n",
            "in training loop, epoch 1, step 279, the loss is 388345.1875\n",
            "in training loop, epoch 1, step 280, the loss is 340674.75\n",
            "in training loop, epoch 1, step 281, the loss is 321677.4375\n",
            "in training loop, epoch 1, step 282, the loss is 476773.84375\n",
            "in training loop, epoch 1, step 283, the loss is 328358.3125\n",
            "in training loop, epoch 1, step 284, the loss is 449338.21875\n",
            "in training loop, epoch 1, step 285, the loss is 310546.96875\n",
            "in training loop, epoch 1, step 286, the loss is 206800.875\n",
            "in training loop, epoch 1, step 287, the loss is 334219.0\n",
            "in training loop, epoch 1, step 288, the loss is 397474.53125\n",
            "in training loop, epoch 1, step 289, the loss is 254020.0625\n",
            "in training loop, epoch 1, step 290, the loss is 245434.859375\n",
            "in training loop, epoch 1, step 291, the loss is 395269.4375\n",
            "in training loop, epoch 1, step 292, the loss is 224547.921875\n",
            "in training loop, epoch 1, step 293, the loss is 458906.3125\n",
            "in training loop, epoch 1, step 294, the loss is 324844.375\n",
            "in training loop, epoch 1, step 295, the loss is 337003.6875\n",
            "in training loop, epoch 1, step 296, the loss is 457563.71875\n",
            "in training loop, epoch 1, step 297, the loss is 301194.90625\n",
            "in training loop, epoch 1, step 298, the loss is 432711.90625\n",
            "in training loop, epoch 1, step 299, the loss is 263840.59375\n",
            "in training loop, epoch 1, step 300, the loss is 319980.5\n",
            "in training loop, epoch 1, step 301, the loss is 338936.21875\n",
            "in training loop, epoch 1, step 302, the loss is 344270.21875\n",
            "in training loop, epoch 1, step 303, the loss is 337920.78125\n",
            "in training loop, epoch 1, step 304, the loss is 256102.5625\n",
            "in training loop, epoch 1, step 305, the loss is 230572.21875\n",
            "in training loop, epoch 1, step 306, the loss is 283297.84375\n",
            "in training loop, epoch 1, step 307, the loss is 280094.75\n",
            "in training loop, epoch 1, step 308, the loss is 269740.3125\n",
            "in training loop, epoch 1, step 309, the loss is 408223.28125\n",
            "in training loop, epoch 1, step 310, the loss is 332254.9375\n",
            "in training loop, epoch 1, step 311, the loss is 190260.734375\n",
            "in training loop, epoch 1, step 312, the loss is 240931.71875\n",
            "in training loop, epoch 1, step 313, the loss is 375443.0\n",
            "in training loop, epoch 1, step 314, the loss is 296514.75\n",
            "in training loop, epoch 1, step 315, the loss is 343924.0\n",
            "in training loop, epoch 1, step 316, the loss is 264494.40625\n",
            "in training loop, epoch 1, step 317, the loss is 245434.71875\n",
            "in training loop, epoch 1, step 318, the loss is 224968.921875\n",
            "in training loop, epoch 1, step 319, the loss is 247628.140625\n",
            "in training loop, epoch 1, step 320, the loss is 455735.96875\n",
            "in training loop, epoch 1, step 321, the loss is 318717.15625\n",
            "in training loop, epoch 1, step 322, the loss is 212421.296875\n",
            "in training loop, epoch 1, step 323, the loss is 231642.984375\n",
            "in training loop, epoch 1, step 324, the loss is 488427.4375\n",
            "in training loop, epoch 1, step 325, the loss is 194578.265625\n",
            "in training loop, epoch 1, step 326, the loss is 265470.125\n",
            "in training loop, epoch 1, step 327, the loss is 258494.46875\n",
            "in training loop, epoch 1, step 328, the loss is 365855.0\n",
            "in training loop, epoch 1, step 329, the loss is 321112.21875\n",
            "in training loop, epoch 1, step 330, the loss is 290509.53125\n",
            "in training loop, epoch 1, step 331, the loss is 384725.03125\n",
            "in training loop, epoch 1, step 332, the loss is 202938.828125\n",
            "in training loop, epoch 1, step 333, the loss is 199416.0\n",
            "in training loop, epoch 1, step 334, the loss is 259566.359375\n",
            "in training loop, epoch 1, step 335, the loss is 199478.0\n",
            "in training loop, epoch 1, step 336, the loss is 267692.0\n",
            "in training loop, epoch 1, step 337, the loss is 274646.9375\n",
            "in training loop, epoch 1, step 338, the loss is 234064.953125\n",
            "in training loop, epoch 1, step 339, the loss is 361798.625\n",
            "in training loop, epoch 1, step 340, the loss is 322514.75\n",
            "in training loop, epoch 1, step 341, the loss is 305291.8125\n",
            "in training loop, epoch 1, step 342, the loss is 293450.71875\n",
            "in training loop, epoch 1, step 343, the loss is 308549.65625\n",
            "in training loop, epoch 1, step 344, the loss is 388615.0\n",
            "in training loop, epoch 1, step 345, the loss is 371557.34375\n",
            "in training loop, epoch 1, step 346, the loss is 284792.40625\n",
            "in training loop, epoch 1, step 347, the loss is 307596.375\n",
            "in training loop, epoch 1, step 348, the loss is 363238.46875\n",
            "in training loop, epoch 1, step 349, the loss is 261650.203125\n",
            "in training loop, epoch 1, step 350, the loss is 293217.78125\n",
            "in training loop, epoch 1, step 351, the loss is 338293.375\n",
            "in training loop, epoch 1, step 352, the loss is 270269.65625\n",
            "in training loop, epoch 1, step 353, the loss is 286252.375\n",
            "in training loop, epoch 1, step 354, the loss is 355515.875\n",
            "in training loop, epoch 1, step 355, the loss is 334567.875\n",
            "in training loop, epoch 1, step 356, the loss is 295522.4375\n",
            "in training loop, epoch 1, step 357, the loss is 248055.984375\n",
            "in training loop, epoch 1, step 358, the loss is 301647.0\n",
            "in training loop, epoch 1, step 359, the loss is 353323.0625\n",
            "in training loop, epoch 1, step 360, the loss is 292802.46875\n",
            "in training loop, epoch 1, step 361, the loss is 326910.0625\n",
            "in training loop, epoch 1, step 362, the loss is 275808.5625\n",
            "in training loop, epoch 1, step 363, the loss is 313510.375\n",
            "in training loop, epoch 1, step 364, the loss is 281881.0\n",
            "in training loop, epoch 1, step 365, the loss is 313251.75\n",
            "in training loop, epoch 1, step 366, the loss is 283599.09375\n",
            "in training loop, epoch 1, step 367, the loss is 437898.875\n",
            "in training loop, epoch 1, step 368, the loss is 343824.71875\n",
            "in training loop, epoch 1, step 369, the loss is 314691.625\n",
            "in training loop, epoch 1, step 370, the loss is 362883.90625\n",
            "in training loop, epoch 1, step 371, the loss is 382973.75\n",
            "in training loop, epoch 1, step 372, the loss is 226940.46875\n",
            "in training loop, epoch 1, step 373, the loss is 347412.4375\n",
            "in training loop, epoch 1, step 374, the loss is 369866.125\n",
            "in training loop, epoch 1, step 375, the loss is 280925.15625\n",
            "in training loop, epoch 1, step 376, the loss is 317023.28125\n",
            "in training loop, epoch 1, step 377, the loss is 455468.90625\n",
            "in training loop, epoch 1, step 378, the loss is 290587.65625\n",
            "in training loop, epoch 1, step 379, the loss is 264527.6875\n",
            "in training loop, epoch 1, step 380, the loss is 239762.03125\n",
            "in training loop, epoch 1, step 381, the loss is 264566.65625\n",
            "in training loop, epoch 1, step 382, the loss is 315126.375\n",
            "in training loop, epoch 1, step 383, the loss is 237760.265625\n",
            "in training loop, epoch 1, step 384, the loss is 280284.0625\n",
            "in training loop, epoch 1, step 385, the loss is 328484.25\n",
            "in training loop, epoch 1, step 386, the loss is 377796.9375\n",
            "in training loop, epoch 1, step 387, the loss is 195688.796875\n",
            "in training loop, epoch 1, step 388, the loss is 240309.0\n",
            "in training loop, epoch 1, step 389, the loss is 265125.625\n",
            "in training loop, epoch 1, step 390, the loss is 272002.5625\n",
            "in training loop, epoch 1, step 391, the loss is 264355.375\n",
            "in training loop, epoch 1, step 392, the loss is 308093.71875\n",
            "in training loop, epoch 1, step 393, the loss is 284492.6875\n",
            "in training loop, epoch 1, step 394, the loss is 329807.625\n",
            "in training loop, epoch 1, step 395, the loss is 247465.28125\n",
            "in training loop, epoch 1, step 396, the loss is 259749.390625\n",
            "in training loop, epoch 1, step 397, the loss is 264533.40625\n",
            "in training loop, epoch 1, step 398, the loss is 239654.234375\n",
            "in training loop, epoch 1, step 399, the loss is 311239.1875\n",
            "in training loop, epoch 1, step 400, the loss is 217583.296875\n",
            "in training loop, epoch 1, step 401, the loss is 266119.59375\n",
            "in training loop, epoch 1, step 402, the loss is 246999.03125\n",
            "in training loop, epoch 1, step 403, the loss is 249419.078125\n",
            "in training loop, epoch 1, step 404, the loss is 334140.4375\n",
            "in training loop, epoch 1, step 405, the loss is 257090.25\n",
            "in training loop, epoch 1, step 406, the loss is 248991.1875\n",
            "in training loop, epoch 1, step 407, the loss is 403012.0\n",
            "in training loop, epoch 1, step 408, the loss is 308296.9375\n",
            "in training loop, epoch 1, step 409, the loss is 277143.125\n",
            "in training loop, epoch 1, step 410, the loss is 236022.625\n",
            "in training loop, epoch 1, step 411, the loss is 289412.125\n",
            "in training loop, epoch 1, step 412, the loss is 277616.75\n",
            "in training loop, epoch 1, step 413, the loss is 362123.5\n",
            "in training loop, epoch 1, step 414, the loss is 312992.1875\n",
            "in training loop, epoch 1, step 415, the loss is 203112.453125\n",
            "in training loop, epoch 1, step 416, the loss is 267656.3125\n",
            "in training loop, epoch 1, step 417, the loss is 321748.6875\n",
            "in training loop, epoch 1, step 418, the loss is 326150.34375\n",
            "in training loop, epoch 1, step 419, the loss is 226659.609375\n",
            "in training loop, epoch 1, step 420, the loss is 240721.703125\n",
            "in training loop, epoch 1, step 421, the loss is 405326.15625\n",
            "in training loop, epoch 1, step 422, the loss is 338717.53125\n",
            "in training loop, epoch 1, step 423, the loss is 404218.875\n",
            "in training loop, epoch 1, step 424, the loss is 277890.8125\n",
            "in training loop, epoch 1, step 425, the loss is 306189.34375\n",
            "in training loop, epoch 1, step 426, the loss is 506760.59375\n",
            "in training loop, epoch 1, step 427, the loss is 410777.53125\n",
            "in training loop, epoch 1, step 428, the loss is 301121.34375\n",
            "in training loop, epoch 1, step 429, the loss is 352054.21875\n",
            "in training loop, epoch 1, step 430, the loss is 223130.09375\n",
            "in training loop, epoch 1, step 431, the loss is 335034.15625\n",
            "in training loop, epoch 1, step 432, the loss is 350799.25\n",
            "in training loop, epoch 1, step 433, the loss is 302552.34375\n",
            "in training loop, epoch 1, step 434, the loss is 329012.53125\n",
            "in training loop, epoch 1, step 435, the loss is 260160.78125\n",
            "in training loop, epoch 1, step 436, the loss is 418516.8125\n",
            "in training loop, epoch 1, step 437, the loss is 353129.9375\n",
            "in training loop, epoch 1, step 438, the loss is 338659.0\n",
            "in training loop, epoch 1, step 439, the loss is 260681.421875\n",
            "in training loop, epoch 1, step 440, the loss is 219259.625\n",
            "in training loop, epoch 1, step 441, the loss is 429246.125\n",
            "in training loop, epoch 1, step 442, the loss is 231690.65625\n",
            "in training loop, epoch 1, step 443, the loss is 358458.0625\n",
            "in training loop, epoch 1, step 444, the loss is 296997.125\n",
            "in training loop, epoch 1, step 445, the loss is 245530.15625\n",
            "in training loop, epoch 1, step 446, the loss is 249843.875\n",
            "in training loop, epoch 1, step 447, the loss is 226140.421875\n",
            "in training loop, epoch 1, step 448, the loss is 327011.75\n",
            "in training loop, epoch 1, step 449, the loss is 548280.4375\n",
            "in training loop, epoch 1, step 450, the loss is 424253.90625\n",
            "in training loop, epoch 1, step 451, the loss is 288890.03125\n",
            "in training loop, epoch 1, step 452, the loss is 249673.625\n",
            "in training loop, epoch 1, step 453, the loss is 304548.53125\n",
            "in training loop, epoch 1, step 454, the loss is 477125.59375\n",
            "in training loop, epoch 1, step 455, the loss is 356702.5\n",
            "in training loop, epoch 1, step 456, the loss is 312783.40625\n",
            "in training loop, epoch 1, step 457, the loss is 321882.75\n",
            "in training loop, epoch 1, step 458, the loss is 295733.84375\n",
            "in training loop, epoch 1, step 459, the loss is 393929.09375\n",
            "in training loop, epoch 1, step 460, the loss is 320856.71875\n",
            "in training loop, epoch 1, step 461, the loss is 237351.921875\n",
            "in training loop, epoch 1, step 462, the loss is 310856.375\n",
            "in training loop, epoch 1, step 463, the loss is 298814.75\n",
            "in training loop, epoch 1, step 464, the loss is 269237.8125\n",
            "in training loop, epoch 1, step 465, the loss is 292573.96875\n",
            "in training loop, epoch 1, step 466, the loss is 452076.71875\n",
            "in training loop, epoch 1, step 467, the loss is 385148.9375\n",
            "in training loop, epoch 1, step 468, the loss is 290180.5\n",
            "in training loop, epoch 1, step 469, the loss is 305554.5625\n",
            "in training loop, epoch 1, step 470, the loss is 278181.625\n",
            "in training loop, epoch 1, step 471, the loss is 450923.53125\n",
            "in training loop, epoch 1, step 472, the loss is 568018.25\n",
            "in training loop, epoch 1, step 473, the loss is 264536.15625\n",
            "in training loop, epoch 1, step 474, the loss is 345673.8125\n",
            "in training loop, epoch 1, step 475, the loss is 421734.34375\n",
            "in training loop, epoch 1, step 476, the loss is 435746.5\n",
            "in training loop, epoch 1, step 477, the loss is 316812.9375\n",
            "in training loop, epoch 1, step 478, the loss is 330484.5\n",
            "in training loop, epoch 1, step 479, the loss is 290905.25\n",
            "in training loop, epoch 1, step 480, the loss is 296722.59375\n",
            "in training loop, epoch 1, step 481, the loss is 248430.921875\n",
            "in training loop, epoch 1, step 482, the loss is 305967.125\n",
            "in training loop, epoch 1, step 483, the loss is 291551.6875\n",
            "in training loop, epoch 1, step 484, the loss is 313236.3125\n",
            "in training loop, epoch 1, step 485, the loss is 379803.40625\n",
            "in training loop, epoch 1, step 486, the loss is 330358.0\n",
            "in training loop, epoch 1, step 487, the loss is 287371.25\n",
            "in training loop, epoch 1, step 488, the loss is 243906.078125\n",
            "in training loop, epoch 1, step 489, the loss is 259343.765625\n",
            "in training loop, epoch 1, step 490, the loss is 283935.1875\n",
            "in training loop, epoch 1, step 491, the loss is 272363.34375\n",
            "in training loop, epoch 1, step 492, the loss is 368066.0\n",
            "in training loop, epoch 1, step 493, the loss is 294275.65625\n",
            "in training loop, epoch 1, step 494, the loss is 369395.5\n",
            "in training loop, epoch 1, step 495, the loss is 344038.96875\n",
            "in training loop, epoch 1, step 496, the loss is 267946.125\n",
            "in training loop, epoch 1, step 497, the loss is 279225.9375\n",
            "in training loop, epoch 1, step 498, the loss is 271428.8125\n",
            "in training loop, epoch 1, step 499, the loss is 298777.8125\n",
            "in training loop, epoch 1, step 500, the loss is 328994.4375\n",
            "in training loop, epoch 1, step 501, the loss is 208173.96875\n",
            "in training loop, epoch 1, step 502, the loss is 293791.65625\n",
            "in training loop, epoch 1, step 503, the loss is 364966.5\n",
            "in training loop, epoch 1, step 504, the loss is 271172.78125\n",
            "in training loop, epoch 1, step 505, the loss is 288976.25\n",
            "in training loop, epoch 1, step 506, the loss is 273481.6875\n",
            "in training loop, epoch 1, step 507, the loss is 317374.4375\n",
            "in training loop, epoch 1, step 508, the loss is 305179.46875\n",
            "in training loop, epoch 1, step 509, the loss is 341161.125\n",
            "in training loop, epoch 1, step 510, the loss is 252254.84375\n",
            "in training loop, epoch 1, step 511, the loss is 315250.1875\n",
            "in training loop, epoch 1, step 512, the loss is 218587.28125\n",
            "in training loop, epoch 1, step 513, the loss is 306586.875\n",
            "in training loop, epoch 1, step 514, the loss is 436586.5\n",
            "in training loop, epoch 1, step 515, the loss is 282453.15625\n",
            "in training loop, epoch 1, step 516, the loss is 304066.9375\n",
            "in training loop, epoch 1, step 517, the loss is 319709.4375\n",
            "in training loop, epoch 1, step 518, the loss is 297383.65625\n",
            "in training loop, epoch 1, step 519, the loss is 425881.59375\n",
            "in training loop, epoch 1, step 520, the loss is 286697.21875\n",
            "in training loop, epoch 1, step 521, the loss is 298629.625\n",
            "in training loop, epoch 1, step 522, the loss is 277570.1875\n",
            "in training loop, epoch 1, step 523, the loss is 286265.90625\n",
            "in training loop, epoch 1, step 524, the loss is 360203.125\n",
            "in training loop, epoch 1, step 525, the loss is 226948.5625\n",
            "in training loop, epoch 1, step 526, the loss is 390206.3125\n",
            "in training loop, epoch 1, step 527, the loss is 336789.71875\n",
            "in training loop, epoch 1, step 528, the loss is 248363.65625\n",
            "in training loop, epoch 1, step 529, the loss is 277304.5625\n",
            "in training loop, epoch 1, step 530, the loss is 319540.875\n",
            "in training loop, epoch 1, step 531, the loss is 365517.75\n",
            "in training loop, epoch 1, step 532, the loss is 279594.1875\n",
            "in training loop, epoch 1, step 533, the loss is 250471.40625\n",
            "in training loop, epoch 1, step 534, the loss is 399310.875\n",
            "in training loop, epoch 1, step 535, the loss is 229638.046875\n",
            "in training loop, epoch 1, step 536, the loss is 323294.625\n",
            "in training loop, epoch 1, step 537, the loss is 267131.21875\n",
            "in training loop, epoch 1, step 538, the loss is 260936.15625\n",
            "in training loop, epoch 1, step 539, the loss is 326190.90625\n",
            "in training loop, epoch 1, step 540, the loss is 348854.3125\n",
            "in training loop, epoch 1, step 541, the loss is 264084.875\n",
            "in training loop, epoch 1, step 542, the loss is 223523.703125\n",
            "in training loop, epoch 1, step 543, the loss is 220914.71875\n",
            "in training loop, epoch 1, step 544, the loss is 229822.265625\n",
            "in training loop, epoch 1, step 545, the loss is 367944.96875\n",
            "in training loop, epoch 1, step 546, the loss is 233446.3125\n",
            "in training loop, epoch 1, step 547, the loss is 371731.03125\n",
            "in training loop, epoch 1, step 548, the loss is 295745.03125\n",
            "in training loop, epoch 1, step 549, the loss is 295982.40625\n",
            "in training loop, epoch 1, step 550, the loss is 450013.9375\n",
            "in training loop, epoch 1, step 551, the loss is 263908.375\n",
            "in training loop, epoch 1, step 552, the loss is 255878.859375\n",
            "in training loop, epoch 1, step 553, the loss is 274610.96875\n",
            "in training loop, epoch 1, step 554, the loss is 374759.34375\n",
            "in training loop, epoch 1, step 555, the loss is 301460.15625\n",
            "in training loop, epoch 1, step 556, the loss is 294462.21875\n",
            "in training loop, epoch 1, step 557, the loss is 333045.25\n",
            "in training loop, epoch 1, step 558, the loss is 358291.6875\n",
            "in training loop, epoch 1, step 559, the loss is 261817.1875\n",
            "in training loop, epoch 1, step 560, the loss is 365718.28125\n",
            "in training loop, epoch 1, step 561, the loss is 224491.296875\n",
            "in training loop, epoch 1, step 562, the loss is 282428.25\n",
            "in training loop, epoch 1, step 563, the loss is 230779.890625\n",
            "in training loop, epoch 1, step 564, the loss is 318633.71875\n",
            "in training loop, epoch 1, step 565, the loss is 259556.875\n",
            "in training loop, epoch 1, step 566, the loss is 359525.5\n",
            "in training loop, epoch 1, step 567, the loss is 468581.25\n",
            "in training loop, epoch 1, step 568, the loss is 464431.25\n",
            "in training loop, epoch 1, step 569, the loss is 199754.890625\n",
            "in training loop, epoch 1, step 570, the loss is 316794.15625\n",
            "in training loop, epoch 1, step 571, the loss is 294062.15625\n",
            "in training loop, epoch 1, step 572, the loss is 272997.96875\n",
            "in training loop, epoch 1, step 573, the loss is 361120.09375\n",
            "in training loop, epoch 1, step 574, the loss is 305165.8125\n",
            "in training loop, epoch 1, step 575, the loss is 302167.28125\n",
            "in training loop, epoch 1, step 576, the loss is 352380.5625\n",
            "in training loop, epoch 1, step 577, the loss is 385040.5\n",
            "in training loop, epoch 1, step 578, the loss is 295942.25\n",
            "in training loop, epoch 1, step 579, the loss is 313453.75\n",
            "in training loop, epoch 1, step 580, the loss is 260254.09375\n",
            "in training loop, epoch 1, step 581, the loss is 336478.59375\n",
            "in training loop, epoch 1, step 582, the loss is 349062.03125\n",
            "in training loop, epoch 1, step 583, the loss is 211320.046875\n",
            "in training loop, epoch 1, step 584, the loss is 279709.875\n",
            "in training loop, epoch 1, step 585, the loss is 276053.5\n",
            "in training loop, epoch 1, step 586, the loss is 275049.40625\n",
            "in training loop, epoch 1, step 587, the loss is 294410.125\n",
            "in training loop, epoch 1, step 588, the loss is 372165.46875\n",
            "in training loop, epoch 1, step 589, the loss is 326980.0625\n",
            "in training loop, epoch 1, step 590, the loss is 524958.0\n",
            "in training loop, epoch 1, step 591, the loss is 377478.40625\n",
            "in training loop, epoch 1, step 592, the loss is 334205.09375\n",
            "in training loop, epoch 1, step 593, the loss is 366431.875\n",
            "in training loop, epoch 1, step 594, the loss is 248310.25\n",
            "in training loop, epoch 1, step 595, the loss is 354849.25\n",
            "in training loop, epoch 1, step 596, the loss is 399629.65625\n",
            "in training loop, epoch 1, step 597, the loss is 283135.21875\n",
            "in training loop, epoch 1, step 598, the loss is 302785.53125\n",
            "in training loop, epoch 1, step 599, the loss is 435101.71875\n",
            "in training loop, epoch 1, step 600, the loss is 356239.5625\n",
            "in training loop, epoch 1, step 601, the loss is 419705.96875\n",
            "in training loop, epoch 1, step 602, the loss is 455523.78125\n",
            "in training loop, epoch 1, step 603, the loss is 279779.28125\n",
            "in training loop, epoch 1, step 604, the loss is 283905.78125\n",
            "in training loop, epoch 1, step 605, the loss is 325046.03125\n",
            "in training loop, epoch 1, step 606, the loss is 329222.59375\n",
            "in training loop, epoch 1, step 607, the loss is 347963.28125\n",
            "in training loop, epoch 1, step 608, the loss is 287343.46875\n",
            "in training loop, epoch 1, step 609, the loss is 317467.875\n",
            "in training loop, epoch 1, step 610, the loss is 297883.8125\n",
            "in training loop, epoch 1, step 611, the loss is 321611.09375\n",
            "in training loop, epoch 1, step 612, the loss is 304175.25\n",
            "in training loop, epoch 1, step 613, the loss is 243997.9375\n",
            "in training loop, epoch 1, step 614, the loss is 320688.125\n",
            "in training loop, epoch 1, step 615, the loss is 290797.6875\n",
            "in training loop, epoch 1, step 616, the loss is 302629.6875\n",
            "in training loop, epoch 1, step 617, the loss is 266467.96875\n",
            "in training loop, epoch 1, step 618, the loss is 418319.9375\n",
            "in training loop, epoch 1, step 619, the loss is 369994.78125\n",
            "in training loop, epoch 1, step 620, the loss is 341999.15625\n",
            "in training loop, epoch 1, step 621, the loss is 246971.5\n",
            "in training loop, epoch 1, step 622, the loss is 338353.75\n",
            "in training loop, epoch 1, step 623, the loss is 333218.5\n",
            "in training loop, epoch 1, step 624, the loss is 249438.296875\n",
            "in training loop, epoch 1, step 625, the loss is 321018.53125\n",
            "in training loop, epoch 1, step 626, the loss is 297206.78125\n",
            "in training loop, epoch 1, step 627, the loss is 255224.171875\n",
            "in training loop, epoch 1, step 628, the loss is 315331.40625\n",
            "in training loop, epoch 1, step 629, the loss is 252473.34375\n",
            "in training loop, epoch 1, step 630, the loss is 348425.0625\n",
            "in training loop, epoch 1, step 631, the loss is 414097.9375\n",
            "in training loop, epoch 1, step 632, the loss is 261021.0625\n",
            "in training loop, epoch 1, step 633, the loss is 354408.21875\n",
            "in training loop, epoch 1, step 634, the loss is 234003.171875\n",
            "in training loop, epoch 1, step 635, the loss is 303924.5\n",
            "in training loop, epoch 1, step 636, the loss is 266572.65625\n",
            "in training loop, epoch 1, step 637, the loss is 387625.6875\n",
            "in training loop, epoch 1, step 638, the loss is 418448.90625\n",
            "in training loop, epoch 1, step 639, the loss is 462724.0\n",
            "in training loop, epoch 1, step 640, the loss is 319345.25\n",
            "in training loop, epoch 1, step 641, the loss is 275946.65625\n",
            "in training loop, epoch 1, step 642, the loss is 286158.78125\n",
            "in training loop, epoch 1, step 643, the loss is 364656.53125\n",
            "in training loop, epoch 1, step 644, the loss is 393158.96875\n",
            "in training loop, epoch 1, step 645, the loss is 258844.09375\n",
            "in training loop, epoch 1, step 646, the loss is 318885.0\n",
            "in training loop, epoch 1, step 647, the loss is 286335.15625\n",
            "in training loop, epoch 1, step 648, the loss is 368975.84375\n",
            "in training loop, epoch 1, step 649, the loss is 270308.28125\n",
            "in training loop, epoch 1, step 650, the loss is 311457.84375\n",
            "in training loop, epoch 1, step 651, the loss is 295552.375\n",
            "in training loop, epoch 1, step 652, the loss is 262165.6875\n",
            "in training loop, epoch 1, step 653, the loss is 283347.40625\n",
            "in training loop, epoch 1, step 654, the loss is 315792.90625\n",
            "in training loop, epoch 1, step 655, the loss is 271194.84375\n",
            "in training loop, epoch 1, step 656, the loss is 173732.46875\n",
            "in training loop, epoch 1, step 657, the loss is 344476.6875\n",
            "in training loop, epoch 1, step 658, the loss is 355302.53125\n",
            "in training loop, epoch 1, step 659, the loss is 318357.875\n",
            "in training loop, epoch 1, step 660, the loss is 360200.71875\n",
            "in training loop, epoch 1, step 661, the loss is 326821.875\n",
            "in training loop, epoch 1, step 662, the loss is 291952.625\n",
            "in training loop, epoch 1, step 663, the loss is 295600.4375\n",
            "in training loop, epoch 1, step 664, the loss is 253435.4375\n",
            "in training loop, epoch 1, step 665, the loss is 513473.9375\n",
            "in training loop, epoch 1, step 666, the loss is 432830.65625\n",
            "in training loop, epoch 1, step 667, the loss is 366600.5\n",
            "in training loop, epoch 1, step 668, the loss is 368038.46875\n",
            "in training loop, epoch 1, step 669, the loss is 434827.15625\n",
            "in training loop, epoch 1, step 670, the loss is 338864.25\n",
            "in training loop, epoch 1, step 671, the loss is 357686.9375\n",
            "in training loop, epoch 1, step 672, the loss is 222254.9375\n",
            "in training loop, epoch 1, step 673, the loss is 399939.0\n",
            "in training loop, epoch 1, step 674, the loss is 226296.0\n",
            "in training loop, epoch 1, step 675, the loss is 350896.875\n",
            "in training loop, epoch 1, step 676, the loss is 253754.328125\n",
            "in training loop, epoch 1, step 677, the loss is 235794.890625\n",
            "in training loop, epoch 1, step 678, the loss is 310064.40625\n",
            "in training loop, epoch 1, step 679, the loss is 425133.75\n",
            "in training loop, epoch 1, step 680, the loss is 378711.75\n",
            "in training loop, epoch 1, step 681, the loss is 297624.75\n",
            "in training loop, epoch 1, step 682, the loss is 373162.3125\n",
            "in training loop, epoch 1, step 683, the loss is 316692.4375\n",
            "in training loop, epoch 1, step 684, the loss is 277775.71875\n",
            "in training loop, epoch 1, step 685, the loss is 305565.0625\n",
            "in training loop, epoch 1, step 686, the loss is 356951.8125\n",
            "in training loop, epoch 1, step 687, the loss is 299922.0625\n",
            "in training loop, epoch 1, step 688, the loss is 333920.84375\n",
            "in training loop, epoch 1, step 689, the loss is 281243.96875\n",
            "in training loop, epoch 1, step 690, the loss is 239803.734375\n",
            "in training loop, epoch 1, step 691, the loss is 323244.71875\n",
            "in training loop, epoch 1, step 692, the loss is 340596.6875\n",
            "in training loop, epoch 1, step 693, the loss is 280182.25\n",
            "in training loop, epoch 1, step 694, the loss is 308715.625\n",
            "in training loop, epoch 1, step 695, the loss is 314896.125\n",
            "in training loop, epoch 1, step 696, the loss is 364184.6875\n",
            "in training loop, epoch 1, step 697, the loss is 326028.59375\n",
            "in training loop, epoch 1, step 698, the loss is 304160.0625\n",
            "in training loop, epoch 1, step 699, the loss is 335334.59375\n",
            "in training loop, epoch 1, step 700, the loss is 358611.875\n",
            "in training loop, epoch 1, step 701, the loss is 271866.1875\n",
            "in training loop, epoch 1, step 702, the loss is 387816.03125\n",
            "in training loop, epoch 1, step 703, the loss is 371209.96875\n",
            "in training loop, epoch 1, step 704, the loss is 304530.0625\n",
            "in training loop, epoch 1, step 705, the loss is 338076.1875\n",
            "in training loop, epoch 1, step 706, the loss is 311170.1875\n",
            "in training loop, epoch 1, step 707, the loss is 371044.03125\n",
            "in training loop, epoch 1, step 708, the loss is 279347.71875\n",
            "in training loop, epoch 1, step 709, the loss is 287606.5625\n",
            "in training loop, epoch 1, step 710, the loss is 205719.1875\n",
            "in training loop, epoch 1, step 711, the loss is 405863.4375\n",
            "in training loop, epoch 1, step 712, the loss is 255056.25\n",
            "in training loop, epoch 1, step 713, the loss is 246360.5625\n",
            "in training loop, epoch 1, step 714, the loss is 220137.921875\n",
            "in training loop, epoch 1, step 715, the loss is 260376.703125\n",
            "in training loop, epoch 1, step 716, the loss is 306921.625\n",
            "in training loop, epoch 1, step 717, the loss is 253492.796875\n",
            "in training loop, epoch 1, step 718, the loss is 463749.5625\n",
            "in training loop, epoch 1, step 719, the loss is 340882.5\n",
            "in training loop, epoch 1, step 720, the loss is 258724.4375\n",
            "in training loop, epoch 1, step 721, the loss is 266292.25\n",
            "in training loop, epoch 1, step 722, the loss is 315521.5\n",
            "in training loop, epoch 1, step 723, the loss is 206106.90625\n",
            "in training loop, epoch 1, step 724, the loss is 281419.65625\n",
            "in training loop, epoch 1, step 725, the loss is 283528.46875\n",
            "in training loop, epoch 1, step 726, the loss is 308837.6875\n",
            "in training loop, epoch 1, step 727, the loss is 234096.40625\n",
            "in training loop, epoch 1, step 728, the loss is 451765.90625\n",
            "in training loop, epoch 1, step 729, the loss is 283813.875\n",
            "in training loop, epoch 1, step 730, the loss is 327438.15625\n",
            "in training loop, epoch 1, step 731, the loss is 374692.1875\n",
            "in training loop, epoch 1, step 732, the loss is 349513.8125\n",
            "in training loop, epoch 1, step 733, the loss is 324993.5625\n",
            "in training loop, epoch 1, step 734, the loss is 367637.6875\n",
            "in training loop, epoch 1, step 735, the loss is 329958.03125\n",
            "in training loop, epoch 1, step 736, the loss is 258350.828125\n",
            "in training loop, epoch 1, step 737, the loss is 271895.90625\n",
            "in training loop, epoch 1, step 738, the loss is 247355.453125\n",
            "in training loop, epoch 1, step 739, the loss is 282360.0625\n",
            "in training loop, epoch 1, step 740, the loss is 426674.25\n",
            "in training loop, epoch 1, step 741, the loss is 266413.1875\n",
            "in training loop, epoch 1, step 742, the loss is 264462.78125\n",
            "in training loop, epoch 1, step 743, the loss is 300694.59375\n",
            "in training loop, epoch 1, step 744, the loss is 276852.75\n",
            "in training loop, epoch 1, step 745, the loss is 291096.34375\n",
            "in training loop, epoch 1, step 746, the loss is 370271.15625\n",
            "in training loop, epoch 1, step 747, the loss is 383057.59375\n",
            "in training loop, epoch 1, step 748, the loss is 320213.8125\n",
            "in training loop, epoch 1, step 749, the loss is 303498.15625\n",
            "in training loop, epoch 1, step 750, the loss is 330203.5625\n",
            "in training loop, epoch 1, step 751, the loss is 341897.875\n",
            "in training loop, epoch 1, step 752, the loss is 370759.75\n",
            "in training loop, epoch 1, step 753, the loss is 453721.375\n",
            "in training loop, epoch 1, step 754, the loss is 345814.96875\n",
            "in training loop, epoch 1, step 755, the loss is 243548.453125\n",
            "in training loop, epoch 1, step 756, the loss is 291062.6875\n",
            "in training loop, epoch 1, step 757, the loss is 356450.8125\n",
            "in training loop, epoch 1, step 758, the loss is 299707.1875\n",
            "in training loop, epoch 1, step 759, the loss is 330444.3125\n",
            "in training loop, epoch 1, step 760, the loss is 523165.28125\n",
            "in training loop, epoch 1, step 761, the loss is 278701.5625\n",
            "in training loop, epoch 1, step 762, the loss is 270223.875\n",
            "in training loop, epoch 1, step 763, the loss is 480718.375\n",
            "in training loop, epoch 1, step 764, the loss is 330410.75\n",
            "in training loop, epoch 1, step 765, the loss is 407760.9375\n",
            "in training loop, epoch 1, step 766, the loss is 447454.96875\n",
            "in training loop, epoch 1, step 767, the loss is 396501.65625\n",
            "in training loop, epoch 1, step 768, the loss is 462234.90625\n",
            "in training loop, epoch 1, step 769, the loss is 383533.28125\n",
            "in training loop, epoch 1, step 770, the loss is 349598.84375\n",
            "in training loop, epoch 1, step 771, the loss is 318638.6875\n",
            "in training loop, epoch 1, step 772, the loss is 232768.671875\n",
            "in training loop, epoch 1, step 773, the loss is 256262.0625\n",
            "in training loop, epoch 1, step 774, the loss is 312183.875\n",
            "in training loop, epoch 1, step 775, the loss is 417778.90625\n",
            "in training loop, epoch 1, step 776, the loss is 331050.09375\n",
            "in training loop, epoch 1, step 777, the loss is 331513.3125\n",
            "in training loop, epoch 1, step 778, the loss is 400049.40625\n",
            "in training loop, epoch 1, step 779, the loss is 327145.8125\n",
            "in training loop, epoch 1, step 780, the loss is 471324.6875\n",
            "in training loop, epoch 1, step 781, the loss is 416981.34375\n",
            "in training loop, epoch 1, step 782, the loss is 309737.21875\n",
            "in training loop, epoch 1, step 783, the loss is 267266.15625\n",
            "in training loop, epoch 1, step 784, the loss is 528066.75\n",
            "in training loop, epoch 1, step 785, the loss is 319565.375\n",
            "in training loop, epoch 1, step 786, the loss is 301336.71875\n",
            "in training loop, epoch 1, step 787, the loss is 404857.59375\n",
            "in training loop, epoch 1, step 788, the loss is 375867.25\n",
            "in training loop, epoch 1, step 789, the loss is 426182.375\n",
            "in training loop, epoch 1, step 790, the loss is 393481.53125\n",
            "in training loop, epoch 1, step 791, the loss is 270738.4375\n",
            "in training loop, epoch 1, step 792, the loss is 336426.875\n",
            "in training loop, epoch 1, step 793, the loss is 332345.34375\n",
            "in training loop, epoch 1, step 794, the loss is 551239.6875\n",
            "in training loop, epoch 1, step 795, the loss is 237724.015625\n",
            "in training loop, epoch 1, step 796, the loss is 365739.71875\n",
            "in training loop, epoch 1, step 797, the loss is 411232.71875\n",
            "in training loop, epoch 1, step 798, the loss is 397167.71875\n",
            "in training loop, epoch 1, step 799, the loss is 355808.25\n",
            "in training loop, epoch 1, step 800, the loss is 354703.0\n",
            "in training loop, epoch 1, step 801, the loss is 359478.09375\n",
            "in training loop, epoch 1, step 802, the loss is 383245.40625\n",
            "in training loop, epoch 1, step 803, the loss is 306625.875\n",
            "in training loop, epoch 1, step 804, the loss is 339185.71875\n",
            "in training loop, epoch 1, step 805, the loss is 342053.78125\n",
            "in training loop, epoch 1, step 806, the loss is 359088.46875\n",
            "in training loop, epoch 1, step 807, the loss is 362170.125\n",
            "in training loop, epoch 1, step 808, the loss is 275217.03125\n",
            "in training loop, epoch 1, step 809, the loss is 367169.53125\n",
            "in training loop, epoch 1, step 810, the loss is 434378.03125\n",
            "in training loop, epoch 1, step 811, the loss is 303326.90625\n",
            "in training loop, epoch 1, step 812, the loss is 359118.71875\n",
            "in training loop, epoch 1, step 813, the loss is 327492.40625\n",
            "in training loop, epoch 1, step 814, the loss is 457791.1875\n",
            "in training loop, epoch 1, step 815, the loss is 346920.65625\n",
            "in training loop, epoch 1, step 816, the loss is 286641.6875\n",
            "in training loop, epoch 1, step 817, the loss is 337280.375\n",
            "in training loop, epoch 1, step 818, the loss is 230247.234375\n",
            "in training loop, epoch 1, step 819, the loss is 332248.4375\n",
            "in training loop, epoch 1, step 820, the loss is 304829.375\n",
            "in training loop, epoch 1, step 821, the loss is 296987.96875\n",
            "in training loop, epoch 1, step 822, the loss is 349475.96875\n",
            "in training loop, epoch 1, step 823, the loss is 291415.375\n",
            "in training loop, epoch 1, step 824, the loss is 326731.3125\n",
            "in training loop, epoch 1, step 825, the loss is 470287.40625\n",
            "in training loop, epoch 1, step 826, the loss is 458218.25\n",
            "in training loop, epoch 1, step 827, the loss is 240281.453125\n",
            "in training loop, epoch 1, step 828, the loss is 284208.8125\n",
            "in training loop, epoch 1, step 829, the loss is 373502.84375\n",
            "in training loop, epoch 1, step 830, the loss is 391791.5\n",
            "in training loop, epoch 1, step 831, the loss is 411941.71875\n",
            "in training loop, epoch 1, step 832, the loss is 417818.59375\n",
            "in training loop, epoch 1, step 833, the loss is 307300.5625\n",
            "in training loop, epoch 1, step 834, the loss is 338175.75\n",
            "in training loop, epoch 1, step 835, the loss is 367475.53125\n",
            "in training loop, epoch 1, step 836, the loss is 349173.875\n",
            "in training loop, epoch 1, step 837, the loss is 322455.5\n",
            "in training loop, epoch 1, step 838, the loss is 325720.0\n",
            "in training loop, epoch 1, step 839, the loss is 239094.78125\n",
            "in training loop, epoch 1, step 840, the loss is 456376.8125\n",
            "in training loop, epoch 1, step 841, the loss is 406616.65625\n",
            "in training loop, epoch 1, step 842, the loss is 316233.8125\n",
            "in training loop, epoch 1, step 843, the loss is 237606.5625\n",
            "in training loop, epoch 1, step 844, the loss is 346604.0625\n",
            "in training loop, epoch 1, step 845, the loss is 286859.84375\n",
            "in training loop, epoch 1, step 846, the loss is 321002.1875\n",
            "in training loop, epoch 1, step 847, the loss is 333058.6875\n",
            "in training loop, epoch 1, step 848, the loss is 404932.4375\n",
            "in training loop, epoch 1, step 849, the loss is 304953.34375\n",
            "in training loop, epoch 1, step 850, the loss is 364590.28125\n",
            "in training loop, epoch 1, step 851, the loss is 407375.8125\n",
            "in training loop, epoch 1, step 852, the loss is 441279.3125\n",
            "in training loop, epoch 1, step 853, the loss is 294497.21875\n",
            "in training loop, epoch 1, step 854, the loss is 333880.28125\n",
            "in training loop, epoch 1, step 855, the loss is 321920.8125\n",
            "in training loop, epoch 1, step 856, the loss is 265450.09375\n",
            "in training loop, epoch 1, step 857, the loss is 475177.9375\n",
            "in training loop, epoch 1, step 858, the loss is 366990.875\n",
            "in training loop, epoch 1, step 859, the loss is 287075.0625\n",
            "in training loop, epoch 1, step 860, the loss is 304064.5\n",
            "in training loop, epoch 1, step 861, the loss is 260805.859375\n",
            "in training loop, epoch 1, step 862, the loss is 321435.3125\n",
            "in training loop, epoch 1, step 863, the loss is 317502.375\n",
            "in training loop, epoch 1, step 864, the loss is 261296.328125\n",
            "in training loop, epoch 1, step 865, the loss is 313835.4375\n",
            "in training loop, epoch 1, step 866, the loss is 301708.4375\n",
            "in training loop, epoch 1, step 867, the loss is 161601.0625\n",
            "in training loop, epoch 1, step 868, the loss is 482000.78125\n",
            "in training loop, epoch 1, step 869, the loss is 224048.15625\n",
            "in training loop, epoch 1, step 870, the loss is 251645.0625\n",
            "in training loop, epoch 1, step 871, the loss is 383793.25\n",
            "in training loop, epoch 1, step 872, the loss is 342732.9375\n",
            "in training loop, epoch 1, step 873, the loss is 412935.625\n",
            "in training loop, epoch 1, step 874, the loss is 370413.65625\n",
            "in training loop, epoch 1, step 875, the loss is 279543.25\n",
            "in training loop, epoch 1, step 876, the loss is 268009.875\n",
            "in training loop, epoch 1, step 877, the loss is 337115.28125\n",
            "in training loop, epoch 1, step 878, the loss is 343947.15625\n",
            "in training loop, epoch 1, step 879, the loss is 317697.6875\n",
            "in training loop, epoch 1, step 880, the loss is 497635.84375\n",
            "in training loop, epoch 1, step 881, the loss is 277698.9375\n",
            "in training loop, epoch 1, step 882, the loss is 360716.8125\n",
            "in training loop, epoch 1, step 883, the loss is 372473.375\n",
            "in training loop, epoch 1, step 884, the loss is 378966.53125\n",
            "in training loop, epoch 1, step 885, the loss is 277935.875\n",
            "in training loop, epoch 1, step 886, the loss is 307279.90625\n",
            "in training loop, epoch 1, step 887, the loss is 388315.625\n",
            "in training loop, epoch 1, step 888, the loss is 755533.75\n",
            "in training loop, epoch 1, step 889, the loss is 362875.1875\n",
            "in training loop, epoch 1, step 890, the loss is 350011.34375\n",
            "in training loop, epoch 1, step 891, the loss is 465711.5\n",
            "in training loop, epoch 1, step 892, the loss is 475964.125\n",
            "in training loop, epoch 1, step 893, the loss is 479465.5625\n",
            "in training loop, epoch 1, step 894, the loss is 371664.8125\n",
            "in training loop, epoch 1, step 895, the loss is 624140.4375\n",
            "in training loop, epoch 1, step 896, the loss is 570179.1875\n",
            "in training loop, epoch 1, step 897, the loss is 391737.75\n",
            "in training loop, epoch 1, step 898, the loss is 411428.46875\n",
            "in training loop, epoch 1, step 899, the loss is 552403.6875\n",
            "in training loop, epoch 1, step 900, the loss is 475642.59375\n",
            "in training loop, epoch 1, step 901, the loss is 415054.4375\n",
            "in training loop, epoch 1, step 902, the loss is 539882.0\n",
            "in training loop, epoch 1, step 903, the loss is 199976.78125\n",
            "k-fold 0:: Epoch 1: train loss 320428.3146951051 val loss 574179.895420792\n",
            "in training loop, epoch 2, step 0, the loss is 313912.5625\n",
            "in training loop, epoch 2, step 1, the loss is 311131.4375\n",
            "in training loop, epoch 2, step 2, the loss is 330218.625\n",
            "in training loop, epoch 2, step 3, the loss is 400094.96875\n",
            "in training loop, epoch 2, step 4, the loss is 528066.875\n",
            "in training loop, epoch 2, step 5, the loss is 657244.375\n",
            "in training loop, epoch 2, step 6, the loss is 251125.875\n",
            "in training loop, epoch 2, step 7, the loss is 476608.625\n",
            "in training loop, epoch 2, step 8, the loss is 345140.40625\n",
            "in training loop, epoch 2, step 9, the loss is 266719.5\n",
            "in training loop, epoch 2, step 10, the loss is 259674.25\n",
            "in training loop, epoch 2, step 11, the loss is 318100.5\n",
            "in training loop, epoch 2, step 12, the loss is 355071.59375\n",
            "in training loop, epoch 2, step 13, the loss is 300330.78125\n",
            "in training loop, epoch 2, step 14, the loss is 222300.875\n",
            "in training loop, epoch 2, step 15, the loss is 315264.375\n",
            "in training loop, epoch 2, step 16, the loss is 411041.25\n",
            "in training loop, epoch 2, step 17, the loss is 283331.96875\n",
            "in training loop, epoch 2, step 18, the loss is 237014.421875\n",
            "in training loop, epoch 2, step 19, the loss is 255819.890625\n",
            "in training loop, epoch 2, step 20, the loss is 339828.1875\n",
            "in training loop, epoch 2, step 21, the loss is 357759.15625\n",
            "in training loop, epoch 2, step 22, the loss is 356930.6875\n",
            "in training loop, epoch 2, step 23, the loss is 293038.625\n",
            "in training loop, epoch 2, step 24, the loss is 330212.21875\n",
            "in training loop, epoch 2, step 25, the loss is 226695.84375\n",
            "in training loop, epoch 2, step 26, the loss is 274826.8125\n",
            "in training loop, epoch 2, step 27, the loss is 362837.46875\n",
            "in training loop, epoch 2, step 28, the loss is 254447.1875\n",
            "in training loop, epoch 2, step 29, the loss is 263124.03125\n",
            "in training loop, epoch 2, step 30, the loss is 268256.25\n",
            "in training loop, epoch 2, step 31, the loss is 251638.34375\n",
            "in training loop, epoch 2, step 32, the loss is 239360.203125\n",
            "in training loop, epoch 2, step 33, the loss is 218060.171875\n",
            "in training loop, epoch 2, step 34, the loss is 266337.875\n",
            "in training loop, epoch 2, step 35, the loss is 273847.53125\n",
            "in training loop, epoch 2, step 36, the loss is 216418.34375\n",
            "in training loop, epoch 2, step 37, the loss is 208331.46875\n",
            "in training loop, epoch 2, step 38, the loss is 287871.375\n",
            "in training loop, epoch 2, step 39, the loss is 226564.3125\n",
            "in training loop, epoch 2, step 40, the loss is 267683.375\n",
            "in training loop, epoch 2, step 41, the loss is 235326.765625\n",
            "in training loop, epoch 2, step 42, the loss is 232113.6875\n",
            "in training loop, epoch 2, step 43, the loss is 211878.171875\n",
            "in training loop, epoch 2, step 44, the loss is 159635.453125\n",
            "in training loop, epoch 2, step 45, the loss is 455612.5\n",
            "in training loop, epoch 2, step 46, the loss is 271416.75\n",
            "in training loop, epoch 2, step 47, the loss is 216685.09375\n",
            "in training loop, epoch 2, step 48, the loss is 301796.53125\n",
            "in training loop, epoch 2, step 49, the loss is 282479.28125\n",
            "in training loop, epoch 2, step 50, the loss is 217830.0625\n",
            "in training loop, epoch 2, step 51, the loss is 270667.125\n",
            "in training loop, epoch 2, step 52, the loss is 259474.953125\n",
            "in training loop, epoch 2, step 53, the loss is 230418.734375\n",
            "in training loop, epoch 2, step 54, the loss is 411916.75\n",
            "in training loop, epoch 2, step 55, the loss is 327659.25\n",
            "in training loop, epoch 2, step 56, the loss is 267488.5\n",
            "in training loop, epoch 2, step 57, the loss is 238974.9375\n",
            "in training loop, epoch 2, step 58, the loss is 269867.90625\n",
            "in training loop, epoch 2, step 59, the loss is 243318.265625\n",
            "in training loop, epoch 2, step 60, the loss is 262026.0625\n",
            "in training loop, epoch 2, step 61, the loss is 230976.078125\n",
            "in training loop, epoch 2, step 62, the loss is 490641.34375\n",
            "in training loop, epoch 2, step 63, the loss is 223456.3125\n",
            "in training loop, epoch 2, step 64, the loss is 351655.03125\n",
            "in training loop, epoch 2, step 65, the loss is 350604.875\n",
            "in training loop, epoch 2, step 66, the loss is 527923.1875\n",
            "in training loop, epoch 2, step 67, the loss is 285529.3125\n",
            "in training loop, epoch 2, step 68, the loss is 295693.5625\n",
            "in training loop, epoch 2, step 69, the loss is 237759.609375\n",
            "in training loop, epoch 2, step 70, the loss is 400642.4375\n",
            "in training loop, epoch 2, step 71, the loss is 306333.25\n",
            "in training loop, epoch 2, step 72, the loss is 389203.4375\n",
            "in training loop, epoch 2, step 73, the loss is 361736.875\n",
            "in training loop, epoch 2, step 74, the loss is 349889.125\n",
            "in training loop, epoch 2, step 75, the loss is 361425.71875\n",
            "in training loop, epoch 2, step 76, the loss is 449985.59375\n",
            "in training loop, epoch 2, step 77, the loss is 298528.46875\n",
            "in training loop, epoch 2, step 78, the loss is 215868.90625\n",
            "in training loop, epoch 2, step 79, the loss is 269866.96875\n",
            "in training loop, epoch 2, step 80, the loss is 201204.515625\n",
            "in training loop, epoch 2, step 81, the loss is 292706.25\n",
            "in training loop, epoch 2, step 82, the loss is 293517.96875\n",
            "in training loop, epoch 2, step 83, the loss is 205115.21875\n",
            "in training loop, epoch 2, step 84, the loss is 231995.9375\n",
            "in training loop, epoch 2, step 85, the loss is 210770.234375\n",
            "in training loop, epoch 2, step 86, the loss is 273961.28125\n",
            "in training loop, epoch 2, step 87, the loss is 217090.078125\n",
            "in training loop, epoch 2, step 88, the loss is 255992.046875\n",
            "in training loop, epoch 2, step 89, the loss is 497797.9375\n",
            "in training loop, epoch 2, step 90, the loss is 233114.765625\n",
            "in training loop, epoch 2, step 91, the loss is 294414.84375\n",
            "in training loop, epoch 2, step 92, the loss is 233270.796875\n",
            "in training loop, epoch 2, step 93, the loss is 244238.421875\n",
            "in training loop, epoch 2, step 94, the loss is 301444.21875\n",
            "in training loop, epoch 2, step 95, the loss is 285302.09375\n",
            "in training loop, epoch 2, step 96, the loss is 265725.40625\n",
            "in training loop, epoch 2, step 97, the loss is 329818.125\n",
            "in training loop, epoch 2, step 98, the loss is 255468.890625\n",
            "in training loop, epoch 2, step 99, the loss is 310746.34375\n",
            "in training loop, epoch 2, step 100, the loss is 296331.125\n",
            "in training loop, epoch 2, step 101, the loss is 314787.5625\n",
            "in training loop, epoch 2, step 102, the loss is 290106.375\n",
            "in training loop, epoch 2, step 103, the loss is 241439.53125\n",
            "in training loop, epoch 2, step 104, the loss is 236687.15625\n",
            "in training loop, epoch 2, step 105, the loss is 237850.0\n",
            "in training loop, epoch 2, step 106, the loss is 292648.78125\n",
            "in training loop, epoch 2, step 107, the loss is 276555.15625\n",
            "in training loop, epoch 2, step 108, the loss is 177477.796875\n",
            "in training loop, epoch 2, step 109, the loss is 246151.5625\n",
            "in training loop, epoch 2, step 110, the loss is 312392.59375\n",
            "in training loop, epoch 2, step 111, the loss is 348452.59375\n",
            "in training loop, epoch 2, step 112, the loss is 247949.65625\n",
            "in training loop, epoch 2, step 113, the loss is 290129.75\n",
            "in training loop, epoch 2, step 114, the loss is 318552.25\n",
            "in training loop, epoch 2, step 115, the loss is 278982.28125\n",
            "in training loop, epoch 2, step 116, the loss is 187592.109375\n",
            "in training loop, epoch 2, step 117, the loss is 222486.578125\n",
            "in training loop, epoch 2, step 118, the loss is 225900.609375\n",
            "in training loop, epoch 2, step 119, the loss is 180471.390625\n",
            "in training loop, epoch 2, step 120, the loss is 195905.78125\n",
            "in training loop, epoch 2, step 121, the loss is 359159.71875\n",
            "in training loop, epoch 2, step 122, the loss is 524754.875\n",
            "in training loop, epoch 2, step 123, the loss is 246271.234375\n",
            "in training loop, epoch 2, step 124, the loss is 237247.828125\n",
            "in training loop, epoch 2, step 125, the loss is 286092.375\n",
            "in training loop, epoch 2, step 126, the loss is 298342.21875\n",
            "in training loop, epoch 2, step 127, the loss is 277412.46875\n",
            "in training loop, epoch 2, step 128, the loss is 283895.90625\n",
            "in training loop, epoch 2, step 129, the loss is 277743.9375\n",
            "in training loop, epoch 2, step 130, the loss is 306462.90625\n",
            "in training loop, epoch 2, step 131, the loss is 375104.46875\n",
            "in training loop, epoch 2, step 132, the loss is 283148.9375\n",
            "in training loop, epoch 2, step 133, the loss is 351163.40625\n",
            "in training loop, epoch 2, step 134, the loss is 235439.515625\n",
            "in training loop, epoch 2, step 135, the loss is 253483.0625\n",
            "in training loop, epoch 2, step 136, the loss is 299490.875\n",
            "in training loop, epoch 2, step 137, the loss is 252748.40625\n",
            "in training loop, epoch 2, step 138, the loss is 597197.0\n",
            "in training loop, epoch 2, step 139, the loss is 194020.0\n",
            "in training loop, epoch 2, step 140, the loss is 253509.9375\n",
            "in training loop, epoch 2, step 141, the loss is 239938.9375\n",
            "in training loop, epoch 2, step 142, the loss is 241713.46875\n",
            "in training loop, epoch 2, step 143, the loss is 443466.6875\n",
            "in training loop, epoch 2, step 144, the loss is 298329.5625\n",
            "in training loop, epoch 2, step 145, the loss is 268297.34375\n",
            "in training loop, epoch 2, step 146, the loss is 301761.65625\n",
            "in training loop, epoch 2, step 147, the loss is 260371.015625\n",
            "in training loop, epoch 2, step 148, the loss is 342242.9375\n",
            "in training loop, epoch 2, step 149, the loss is 320050.3125\n",
            "in training loop, epoch 2, step 150, the loss is 264857.5\n",
            "in training loop, epoch 2, step 151, the loss is 224688.78125\n",
            "in training loop, epoch 2, step 152, the loss is 317576.625\n",
            "in training loop, epoch 2, step 153, the loss is 292241.78125\n",
            "in training loop, epoch 2, step 154, the loss is 325846.71875\n",
            "in training loop, epoch 2, step 155, the loss is 250089.609375\n",
            "in training loop, epoch 2, step 156, the loss is 237024.46875\n",
            "in training loop, epoch 2, step 157, the loss is 253714.21875\n",
            "in training loop, epoch 2, step 158, the loss is 288951.0625\n",
            "in training loop, epoch 2, step 159, the loss is 410029.5\n",
            "in training loop, epoch 2, step 160, the loss is 237852.59375\n",
            "in training loop, epoch 2, step 161, the loss is 291076.34375\n",
            "in training loop, epoch 2, step 162, the loss is 262778.0625\n",
            "in training loop, epoch 2, step 163, the loss is 290406.4375\n",
            "in training loop, epoch 2, step 164, the loss is 247567.609375\n",
            "in training loop, epoch 2, step 165, the loss is 289948.3125\n",
            "in training loop, epoch 2, step 166, the loss is 231367.78125\n",
            "in training loop, epoch 2, step 167, the loss is 257204.6875\n",
            "in training loop, epoch 2, step 168, the loss is 247569.6875\n",
            "in training loop, epoch 2, step 169, the loss is 727467.375\n",
            "in training loop, epoch 2, step 170, the loss is 285337.03125\n",
            "in training loop, epoch 2, step 171, the loss is 205138.421875\n",
            "in training loop, epoch 2, step 172, the loss is 426362.875\n",
            "in training loop, epoch 2, step 173, the loss is 369977.65625\n",
            "in training loop, epoch 2, step 174, the loss is 409800.0625\n",
            "in training loop, epoch 2, step 175, the loss is 482882.5\n",
            "in training loop, epoch 2, step 176, the loss is 284644.78125\n",
            "in training loop, epoch 2, step 177, the loss is 313657.28125\n",
            "in training loop, epoch 2, step 178, the loss is 263046.84375\n",
            "in training loop, epoch 2, step 179, the loss is 254201.296875\n",
            "in training loop, epoch 2, step 180, the loss is 282519.46875\n",
            "in training loop, epoch 2, step 181, the loss is 241467.890625\n",
            "in training loop, epoch 2, step 182, the loss is 274344.53125\n",
            "in training loop, epoch 2, step 183, the loss is 405333.46875\n",
            "in training loop, epoch 2, step 184, the loss is 226269.28125\n",
            "in training loop, epoch 2, step 185, the loss is 347932.3125\n",
            "in training loop, epoch 2, step 186, the loss is 237175.265625\n",
            "in training loop, epoch 2, step 187, the loss is 278031.03125\n",
            "in training loop, epoch 2, step 188, the loss is 327059.03125\n",
            "in training loop, epoch 2, step 189, the loss is 316383.5625\n",
            "in training loop, epoch 2, step 190, the loss is 225821.859375\n",
            "in training loop, epoch 2, step 191, the loss is 273905.6875\n",
            "in training loop, epoch 2, step 192, the loss is 167243.109375\n",
            "in training loop, epoch 2, step 193, the loss is 256363.96875\n",
            "in training loop, epoch 2, step 194, the loss is 234102.1875\n",
            "in training loop, epoch 2, step 195, the loss is 261735.65625\n",
            "in training loop, epoch 2, step 196, the loss is 287997.0\n",
            "in training loop, epoch 2, step 197, the loss is 341911.0625\n",
            "in training loop, epoch 2, step 198, the loss is 276253.4375\n",
            "in training loop, epoch 2, step 199, the loss is 350611.125\n",
            "in training loop, epoch 2, step 200, the loss is 290920.03125\n",
            "in training loop, epoch 2, step 201, the loss is 319372.34375\n",
            "in training loop, epoch 2, step 202, the loss is 201946.9375\n",
            "in training loop, epoch 2, step 203, the loss is 248429.71875\n",
            "in training loop, epoch 2, step 204, the loss is 314724.25\n",
            "in training loop, epoch 2, step 205, the loss is 211689.1875\n",
            "in training loop, epoch 2, step 206, the loss is 324236.0\n",
            "in training loop, epoch 2, step 207, the loss is 229902.9375\n",
            "in training loop, epoch 2, step 208, the loss is 266441.125\n",
            "in training loop, epoch 2, step 209, the loss is 381123.71875\n",
            "in training loop, epoch 2, step 210, the loss is 268123.375\n",
            "in training loop, epoch 2, step 211, the loss is 277712.0\n",
            "in training loop, epoch 2, step 212, the loss is 178332.546875\n",
            "in training loop, epoch 2, step 213, the loss is 258232.296875\n",
            "in training loop, epoch 2, step 214, the loss is 288218.59375\n",
            "in training loop, epoch 2, step 215, the loss is 267182.6875\n",
            "in training loop, epoch 2, step 216, the loss is 212233.484375\n",
            "in training loop, epoch 2, step 217, the loss is 281352.09375\n",
            "in training loop, epoch 2, step 218, the loss is 274777.375\n",
            "in training loop, epoch 2, step 219, the loss is 212613.53125\n",
            "in training loop, epoch 2, step 220, the loss is 296326.96875\n",
            "in training loop, epoch 2, step 221, the loss is 268638.625\n",
            "in training loop, epoch 2, step 222, the loss is 292060.40625\n",
            "in training loop, epoch 2, step 223, the loss is 187848.171875\n",
            "in training loop, epoch 2, step 224, the loss is 355813.96875\n",
            "in training loop, epoch 2, step 225, the loss is 310334.375\n",
            "in training loop, epoch 2, step 226, the loss is 250451.296875\n",
            "in training loop, epoch 2, step 227, the loss is 241827.6875\n",
            "in training loop, epoch 2, step 228, the loss is 251804.8125\n",
            "in training loop, epoch 2, step 229, the loss is 316829.125\n",
            "in training loop, epoch 2, step 230, the loss is 254326.171875\n",
            "in training loop, epoch 2, step 231, the loss is 263626.71875\n",
            "in training loop, epoch 2, step 232, the loss is 238250.65625\n",
            "in training loop, epoch 2, step 233, the loss is 349417.0\n",
            "in training loop, epoch 2, step 234, the loss is 313584.59375\n",
            "in training loop, epoch 2, step 235, the loss is 288370.4375\n",
            "in training loop, epoch 2, step 236, the loss is 229324.71875\n",
            "in training loop, epoch 2, step 237, the loss is 285856.25\n",
            "in training loop, epoch 2, step 238, the loss is 404473.125\n",
            "in training loop, epoch 2, step 239, the loss is 245755.25\n",
            "in training loop, epoch 2, step 240, the loss is 310573.90625\n",
            "in training loop, epoch 2, step 241, the loss is 236452.8125\n",
            "in training loop, epoch 2, step 242, the loss is 351357.8125\n",
            "in training loop, epoch 2, step 243, the loss is 226862.046875\n",
            "in training loop, epoch 2, step 244, the loss is 225764.890625\n",
            "in training loop, epoch 2, step 245, the loss is 319124.0\n",
            "in training loop, epoch 2, step 246, the loss is 258085.390625\n",
            "in training loop, epoch 2, step 247, the loss is 284049.75\n",
            "in training loop, epoch 2, step 248, the loss is 214361.46875\n",
            "in training loop, epoch 2, step 249, the loss is 222837.890625\n",
            "in training loop, epoch 2, step 250, the loss is 213982.15625\n",
            "in training loop, epoch 2, step 251, the loss is 226405.796875\n",
            "in training loop, epoch 2, step 252, the loss is 223478.96875\n",
            "in training loop, epoch 2, step 253, the loss is 283538.65625\n",
            "in training loop, epoch 2, step 254, the loss is 242286.09375\n",
            "in training loop, epoch 2, step 255, the loss is 287795.25\n",
            "in training loop, epoch 2, step 256, the loss is 256176.03125\n",
            "in training loop, epoch 2, step 257, the loss is 289018.03125\n",
            "in training loop, epoch 2, step 258, the loss is 322628.46875\n",
            "in training loop, epoch 2, step 259, the loss is 224897.8125\n",
            "in training loop, epoch 2, step 260, the loss is 264591.0625\n",
            "in training loop, epoch 2, step 261, the loss is 213023.375\n",
            "in training loop, epoch 2, step 262, the loss is 213257.5625\n",
            "in training loop, epoch 2, step 263, the loss is 310363.40625\n",
            "in training loop, epoch 2, step 264, the loss is 358157.03125\n",
            "in training loop, epoch 2, step 265, the loss is 228464.71875\n",
            "in training loop, epoch 2, step 266, the loss is 184404.578125\n",
            "in training loop, epoch 2, step 267, the loss is 231778.09375\n",
            "in training loop, epoch 2, step 268, the loss is 228776.5\n",
            "in training loop, epoch 2, step 269, the loss is 307282.78125\n",
            "in training loop, epoch 2, step 270, the loss is 222074.34375\n",
            "in training loop, epoch 2, step 271, the loss is 659993.25\n",
            "in training loop, epoch 2, step 272, the loss is 305947.0\n",
            "in training loop, epoch 2, step 273, the loss is 582278.375\n",
            "in training loop, epoch 2, step 274, the loss is 357859.28125\n",
            "in training loop, epoch 2, step 275, the loss is 317456.8125\n",
            "in training loop, epoch 2, step 276, the loss is 423290.4375\n",
            "in training loop, epoch 2, step 277, the loss is 262149.5\n",
            "in training loop, epoch 2, step 278, the loss is 406047.28125\n",
            "in training loop, epoch 2, step 279, the loss is 247068.8125\n",
            "in training loop, epoch 2, step 280, the loss is 398488.5625\n",
            "in training loop, epoch 2, step 281, the loss is 309715.375\n",
            "in training loop, epoch 2, step 282, the loss is 424726.8125\n",
            "in training loop, epoch 2, step 283, the loss is 312568.75\n",
            "in training loop, epoch 2, step 284, the loss is 282268.9375\n",
            "in training loop, epoch 2, step 285, the loss is 314392.21875\n",
            "in training loop, epoch 2, step 286, the loss is 490042.0\n",
            "in training loop, epoch 2, step 287, the loss is 436366.15625\n",
            "in training loop, epoch 2, step 288, the loss is 311254.125\n",
            "in training loop, epoch 2, step 289, the loss is 365736.53125\n",
            "in training loop, epoch 2, step 290, the loss is 317515.5625\n",
            "in training loop, epoch 2, step 291, the loss is 237879.25\n",
            "in training loop, epoch 2, step 292, the loss is 252463.40625\n",
            "in training loop, epoch 2, step 293, the loss is 303542.8125\n",
            "in training loop, epoch 2, step 294, the loss is 451535.5\n",
            "in training loop, epoch 2, step 295, the loss is 274882.3125\n",
            "in training loop, epoch 2, step 296, the loss is 263860.1875\n",
            "in training loop, epoch 2, step 297, the loss is 233989.078125\n",
            "in training loop, epoch 2, step 298, the loss is 343830.125\n",
            "in training loop, epoch 2, step 299, the loss is 255572.40625\n",
            "in training loop, epoch 2, step 300, the loss is 286675.40625\n",
            "in training loop, epoch 2, step 301, the loss is 295705.09375\n",
            "in training loop, epoch 2, step 302, the loss is 276073.9375\n",
            "in training loop, epoch 2, step 303, the loss is 299596.78125\n",
            "in training loop, epoch 2, step 304, the loss is 182570.75\n",
            "in training loop, epoch 2, step 305, the loss is 230907.03125\n",
            "in training loop, epoch 2, step 306, the loss is 280160.625\n",
            "in training loop, epoch 2, step 307, the loss is 264668.53125\n",
            "in training loop, epoch 2, step 308, the loss is 263912.9375\n",
            "in training loop, epoch 2, step 309, the loss is 403070.375\n",
            "in training loop, epoch 2, step 310, the loss is 186628.40625\n",
            "in training loop, epoch 2, step 311, the loss is 273018.625\n",
            "in training loop, epoch 2, step 312, the loss is 323626.6875\n",
            "in training loop, epoch 2, step 313, the loss is 216947.078125\n",
            "in training loop, epoch 2, step 314, the loss is 309836.75\n",
            "in training loop, epoch 2, step 315, the loss is 255772.09375\n",
            "in training loop, epoch 2, step 316, the loss is 281718.59375\n",
            "in training loop, epoch 2, step 317, the loss is 294492.3125\n",
            "in training loop, epoch 2, step 318, the loss is 257679.75\n",
            "in training loop, epoch 2, step 319, the loss is 231078.25\n",
            "in training loop, epoch 2, step 320, the loss is 252043.53125\n",
            "in training loop, epoch 2, step 321, the loss is 275722.5\n",
            "in training loop, epoch 2, step 322, the loss is 284997.28125\n",
            "in training loop, epoch 2, step 323, the loss is 239296.0625\n",
            "in training loop, epoch 2, step 324, the loss is 218711.890625\n",
            "in training loop, epoch 2, step 325, the loss is 219744.703125\n",
            "in training loop, epoch 2, step 326, the loss is 236144.8125\n",
            "in training loop, epoch 2, step 327, the loss is 244716.828125\n",
            "in training loop, epoch 2, step 328, the loss is 223455.03125\n",
            "in training loop, epoch 2, step 329, the loss is 167393.109375\n",
            "in training loop, epoch 2, step 330, the loss is 291438.9375\n",
            "in training loop, epoch 2, step 331, the loss is 210388.65625\n",
            "in training loop, epoch 2, step 332, the loss is 298795.21875\n",
            "in training loop, epoch 2, step 333, the loss is 231871.34375\n",
            "in training loop, epoch 2, step 334, the loss is 212255.046875\n",
            "in training loop, epoch 2, step 335, the loss is 213225.96875\n",
            "in training loop, epoch 2, step 336, the loss is 286787.5625\n",
            "in training loop, epoch 2, step 337, the loss is 218144.078125\n",
            "in training loop, epoch 2, step 338, the loss is 204446.609375\n",
            "in training loop, epoch 2, step 339, the loss is 172457.234375\n",
            "in training loop, epoch 2, step 340, the loss is 169143.0625\n",
            "in training loop, epoch 2, step 341, the loss is 250923.03125\n",
            "in training loop, epoch 2, step 342, the loss is 177402.53125\n",
            "in training loop, epoch 2, step 343, the loss is 245820.84375\n",
            "in training loop, epoch 2, step 344, the loss is 157948.96875\n",
            "in training loop, epoch 2, step 345, the loss is 228243.484375\n",
            "in training loop, epoch 2, step 346, the loss is 263776.625\n",
            "in training loop, epoch 2, step 347, the loss is 233904.546875\n",
            "in training loop, epoch 2, step 348, the loss is 221741.25\n",
            "in training loop, epoch 2, step 349, the loss is 203140.375\n",
            "in training loop, epoch 2, step 350, the loss is 213761.203125\n",
            "in training loop, epoch 2, step 351, the loss is 239972.953125\n",
            "in training loop, epoch 2, step 352, the loss is 243191.328125\n",
            "in training loop, epoch 2, step 353, the loss is 216052.96875\n",
            "in training loop, epoch 2, step 354, the loss is 193604.40625\n",
            "in training loop, epoch 2, step 355, the loss is 243602.625\n",
            "in training loop, epoch 2, step 356, the loss is 215757.125\n",
            "in training loop, epoch 2, step 357, the loss is 270657.1875\n",
            "in training loop, epoch 2, step 358, the loss is 267526.875\n",
            "in training loop, epoch 2, step 359, the loss is 313934.625\n",
            "in training loop, epoch 2, step 360, the loss is 236172.0625\n",
            "in training loop, epoch 2, step 361, the loss is 210365.0625\n",
            "in training loop, epoch 2, step 362, the loss is 250843.34375\n",
            "in training loop, epoch 2, step 363, the loss is 257252.3125\n",
            "in training loop, epoch 2, step 364, the loss is 249180.46875\n",
            "in training loop, epoch 2, step 365, the loss is 207219.109375\n",
            "in training loop, epoch 2, step 366, the loss is 208684.015625\n",
            "in training loop, epoch 2, step 367, the loss is 250955.5625\n",
            "in training loop, epoch 2, step 368, the loss is 264713.1875\n",
            "in training loop, epoch 2, step 369, the loss is 258026.09375\n",
            "in training loop, epoch 2, step 370, the loss is 214398.703125\n",
            "in training loop, epoch 2, step 371, the loss is 340159.75\n",
            "in training loop, epoch 2, step 372, the loss is 240491.625\n",
            "in training loop, epoch 2, step 373, the loss is 268473.09375\n",
            "in training loop, epoch 2, step 374, the loss is 250139.734375\n",
            "in training loop, epoch 2, step 375, the loss is 287819.375\n",
            "in training loop, epoch 2, step 376, the loss is 328906.40625\n",
            "in training loop, epoch 2, step 377, the loss is 275506.90625\n",
            "in training loop, epoch 2, step 378, the loss is 219590.0625\n",
            "in training loop, epoch 2, step 379, the loss is 217231.84375\n",
            "in training loop, epoch 2, step 380, the loss is 185454.015625\n",
            "in training loop, epoch 2, step 381, the loss is 211017.9375\n",
            "in training loop, epoch 2, step 382, the loss is 217785.671875\n",
            "in training loop, epoch 2, step 383, the loss is 349832.84375\n",
            "in training loop, epoch 2, step 384, the loss is 279785.875\n",
            "in training loop, epoch 2, step 385, the loss is 228368.9375\n",
            "in training loop, epoch 2, step 386, the loss is 298962.40625\n",
            "in training loop, epoch 2, step 387, the loss is 239168.15625\n",
            "in training loop, epoch 2, step 388, the loss is 265429.78125\n",
            "in training loop, epoch 2, step 389, the loss is 232947.90625\n",
            "in training loop, epoch 2, step 390, the loss is 230187.46875\n",
            "in training loop, epoch 2, step 391, the loss is 206725.40625\n",
            "in training loop, epoch 2, step 392, the loss is 257310.234375\n",
            "in training loop, epoch 2, step 393, the loss is 283017.75\n",
            "in training loop, epoch 2, step 394, the loss is 267035.1875\n",
            "in training loop, epoch 2, step 395, the loss is 224248.578125\n",
            "in training loop, epoch 2, step 396, the loss is 254122.703125\n",
            "in training loop, epoch 2, step 397, the loss is 230337.71875\n",
            "in training loop, epoch 2, step 398, the loss is 374613.21875\n",
            "in training loop, epoch 2, step 399, the loss is 291144.1875\n",
            "in training loop, epoch 2, step 400, the loss is 218381.828125\n",
            "in training loop, epoch 2, step 401, the loss is 235071.140625\n",
            "in training loop, epoch 2, step 402, the loss is 333611.4375\n",
            "in training loop, epoch 2, step 403, the loss is 229264.453125\n",
            "in training loop, epoch 2, step 404, the loss is 251887.796875\n",
            "in training loop, epoch 2, step 405, the loss is 270006.0\n",
            "in training loop, epoch 2, step 406, the loss is 204922.015625\n",
            "in training loop, epoch 2, step 407, the loss is 277750.65625\n",
            "in training loop, epoch 2, step 408, the loss is 270721.875\n",
            "in training loop, epoch 2, step 409, the loss is 232592.59375\n",
            "in training loop, epoch 2, step 410, the loss is 219795.484375\n",
            "in training loop, epoch 2, step 411, the loss is 275646.65625\n",
            "in training loop, epoch 2, step 412, the loss is 299982.375\n",
            "in training loop, epoch 2, step 413, the loss is 231495.9375\n",
            "in training loop, epoch 2, step 414, the loss is 233268.1875\n",
            "in training loop, epoch 2, step 415, the loss is 203548.828125\n",
            "in training loop, epoch 2, step 416, the loss is 259271.15625\n",
            "in training loop, epoch 2, step 417, the loss is 445244.3125\n",
            "in training loop, epoch 2, step 418, the loss is 377093.6875\n",
            "in training loop, epoch 2, step 419, the loss is 276018.0\n",
            "in training loop, epoch 2, step 420, the loss is 249709.15625\n",
            "in training loop, epoch 2, step 421, the loss is 269076.9375\n",
            "in training loop, epoch 2, step 422, the loss is 320282.03125\n",
            "in training loop, epoch 2, step 423, the loss is 295699.03125\n",
            "in training loop, epoch 2, step 424, the loss is 257787.375\n",
            "in training loop, epoch 2, step 425, the loss is 326746.25\n",
            "in training loop, epoch 2, step 426, the loss is 264740.90625\n",
            "in training loop, epoch 2, step 427, the loss is 222144.84375\n",
            "in training loop, epoch 2, step 428, the loss is 213455.78125\n",
            "in training loop, epoch 2, step 429, the loss is 250546.09375\n",
            "in training loop, epoch 2, step 430, the loss is 241490.734375\n",
            "in training loop, epoch 2, step 431, the loss is 285558.0\n",
            "in training loop, epoch 2, step 432, the loss is 344466.34375\n",
            "in training loop, epoch 2, step 433, the loss is 245770.078125\n",
            "in training loop, epoch 2, step 434, the loss is 266593.46875\n",
            "in training loop, epoch 2, step 435, the loss is 288397.71875\n",
            "in training loop, epoch 2, step 436, the loss is 231224.125\n",
            "in training loop, epoch 2, step 437, the loss is 243433.96875\n",
            "in training loop, epoch 2, step 438, the loss is 235845.890625\n",
            "in training loop, epoch 2, step 439, the loss is 235421.359375\n",
            "in training loop, epoch 2, step 440, the loss is 203460.875\n",
            "in training loop, epoch 2, step 441, the loss is 216001.796875\n",
            "in training loop, epoch 2, step 442, the loss is 199490.203125\n",
            "in training loop, epoch 2, step 443, the loss is 273416.65625\n",
            "in training loop, epoch 2, step 444, the loss is 277709.46875\n",
            "in training loop, epoch 2, step 445, the loss is 236283.8125\n",
            "in training loop, epoch 2, step 446, the loss is 229108.484375\n",
            "in training loop, epoch 2, step 447, the loss is 264877.40625\n",
            "in training loop, epoch 2, step 448, the loss is 269720.40625\n",
            "in training loop, epoch 2, step 449, the loss is 267038.0\n",
            "in training loop, epoch 2, step 450, the loss is 207726.234375\n",
            "in training loop, epoch 2, step 451, the loss is 210558.5\n",
            "in training loop, epoch 2, step 452, the loss is 166180.609375\n",
            "in training loop, epoch 2, step 453, the loss is 234451.65625\n",
            "in training loop, epoch 2, step 454, the loss is 195465.671875\n",
            "in training loop, epoch 2, step 455, the loss is 279059.9375\n",
            "in training loop, epoch 2, step 456, the loss is 229475.6875\n",
            "in training loop, epoch 2, step 457, the loss is 230294.3125\n",
            "in training loop, epoch 2, step 458, the loss is 289277.90625\n",
            "in training loop, epoch 2, step 459, the loss is 214148.84375\n",
            "in training loop, epoch 2, step 460, the loss is 219089.46875\n",
            "in training loop, epoch 2, step 461, the loss is 220941.3125\n",
            "in training loop, epoch 2, step 462, the loss is 193545.90625\n",
            "in training loop, epoch 2, step 463, the loss is 231424.0625\n",
            "in training loop, epoch 2, step 464, the loss is 197489.203125\n",
            "in training loop, epoch 2, step 465, the loss is 252881.125\n",
            "in training loop, epoch 2, step 466, the loss is 256453.390625\n",
            "in training loop, epoch 2, step 467, the loss is 279167.5\n",
            "in training loop, epoch 2, step 468, the loss is 210216.875\n",
            "in training loop, epoch 2, step 469, the loss is 260992.34375\n",
            "in training loop, epoch 2, step 470, the loss is 210380.609375\n",
            "in training loop, epoch 2, step 471, the loss is 285544.9375\n",
            "in training loop, epoch 2, step 472, the loss is 258844.859375\n",
            "in training loop, epoch 2, step 473, the loss is 287045.6875\n",
            "in training loop, epoch 2, step 474, the loss is 221418.90625\n",
            "in training loop, epoch 2, step 475, the loss is 283352.3125\n",
            "in training loop, epoch 2, step 476, the loss is 250477.0\n",
            "in training loop, epoch 2, step 477, the loss is 253399.71875\n",
            "in training loop, epoch 2, step 478, the loss is 198881.9375\n",
            "in training loop, epoch 2, step 479, the loss is 254214.859375\n",
            "in training loop, epoch 2, step 480, the loss is 219937.078125\n",
            "in training loop, epoch 2, step 481, the loss is 290558.9375\n",
            "in training loop, epoch 2, step 482, the loss is 205500.125\n",
            "in training loop, epoch 2, step 483, the loss is 263342.15625\n",
            "in training loop, epoch 2, step 484, the loss is 299193.375\n",
            "in training loop, epoch 2, step 485, the loss is 343358.15625\n",
            "in training loop, epoch 2, step 486, the loss is 274397.59375\n",
            "in training loop, epoch 2, step 487, the loss is 269025.5625\n",
            "in training loop, epoch 2, step 488, the loss is 478450.4375\n",
            "in training loop, epoch 2, step 489, the loss is 292751.78125\n",
            "in training loop, epoch 2, step 490, the loss is 339582.75\n",
            "in training loop, epoch 2, step 491, the loss is 293167.15625\n",
            "in training loop, epoch 2, step 492, the loss is 337496.125\n",
            "in training loop, epoch 2, step 493, the loss is 387615.90625\n",
            "in training loop, epoch 2, step 494, the loss is 362267.28125\n",
            "in training loop, epoch 2, step 495, the loss is 221421.390625\n",
            "in training loop, epoch 2, step 496, the loss is 292201.125\n",
            "in training loop, epoch 2, step 497, the loss is 255790.53125\n",
            "in training loop, epoch 2, step 498, the loss is 263228.84375\n",
            "in training loop, epoch 2, step 499, the loss is 227484.625\n",
            "in training loop, epoch 2, step 500, the loss is 227530.03125\n",
            "in training loop, epoch 2, step 501, the loss is 241398.78125\n",
            "in training loop, epoch 2, step 502, the loss is 426677.4375\n",
            "in training loop, epoch 2, step 503, the loss is 313072.9375\n",
            "in training loop, epoch 2, step 504, the loss is 274582.75\n",
            "in training loop, epoch 2, step 505, the loss is 234238.125\n",
            "in training loop, epoch 2, step 506, the loss is 269126.90625\n",
            "in training loop, epoch 2, step 507, the loss is 279860.125\n",
            "in training loop, epoch 2, step 508, the loss is 291568.46875\n",
            "in training loop, epoch 2, step 509, the loss is 234596.015625\n",
            "in training loop, epoch 2, step 510, the loss is 232202.03125\n",
            "in training loop, epoch 2, step 511, the loss is 242813.421875\n",
            "in training loop, epoch 2, step 512, the loss is 287809.6875\n",
            "in training loop, epoch 2, step 513, the loss is 302268.75\n",
            "in training loop, epoch 2, step 514, the loss is 258008.5\n",
            "in training loop, epoch 2, step 515, the loss is 176711.796875\n",
            "in training loop, epoch 2, step 516, the loss is 211677.171875\n",
            "in training loop, epoch 2, step 517, the loss is 284817.28125\n",
            "in training loop, epoch 2, step 518, the loss is 317647.59375\n",
            "in training loop, epoch 2, step 519, the loss is 226076.546875\n",
            "in training loop, epoch 2, step 520, the loss is 306748.625\n",
            "in training loop, epoch 2, step 521, the loss is 312788.34375\n",
            "in training loop, epoch 2, step 522, the loss is 303183.5625\n",
            "in training loop, epoch 2, step 523, the loss is 306650.3125\n",
            "in training loop, epoch 2, step 524, the loss is 303632.5625\n",
            "in training loop, epoch 2, step 525, the loss is 298119.34375\n",
            "in training loop, epoch 2, step 526, the loss is 257758.171875\n",
            "in training loop, epoch 2, step 527, the loss is 316786.75\n",
            "in training loop, epoch 2, step 528, the loss is 241752.8125\n",
            "in training loop, epoch 2, step 529, the loss is 245290.53125\n",
            "in training loop, epoch 2, step 530, the loss is 250942.640625\n",
            "in training loop, epoch 2, step 531, the loss is 279119.3125\n",
            "in training loop, epoch 2, step 532, the loss is 271381.15625\n",
            "in training loop, epoch 2, step 533, the loss is 262245.84375\n",
            "in training loop, epoch 2, step 534, the loss is 241922.875\n",
            "in training loop, epoch 2, step 535, the loss is 223115.375\n",
            "in training loop, epoch 2, step 536, the loss is 345324.9375\n",
            "in training loop, epoch 2, step 537, the loss is 245527.265625\n",
            "in training loop, epoch 2, step 538, the loss is 328705.4375\n",
            "in training loop, epoch 2, step 539, the loss is 206394.46875\n",
            "in training loop, epoch 2, step 540, the loss is 287241.3125\n",
            "in training loop, epoch 2, step 541, the loss is 357199.4375\n",
            "in training loop, epoch 2, step 542, the loss is 387381.84375\n",
            "in training loop, epoch 2, step 543, the loss is 268811.9375\n",
            "in training loop, epoch 2, step 544, the loss is 221753.578125\n",
            "in training loop, epoch 2, step 545, the loss is 285570.90625\n",
            "in training loop, epoch 2, step 546, the loss is 214487.4375\n",
            "in training loop, epoch 2, step 547, the loss is 217260.140625\n",
            "in training loop, epoch 2, step 548, the loss is 187125.828125\n",
            "in training loop, epoch 2, step 549, the loss is 204146.546875\n",
            "in training loop, epoch 2, step 550, the loss is 236823.671875\n",
            "in training loop, epoch 2, step 551, the loss is 315412.375\n",
            "in training loop, epoch 2, step 552, the loss is 276619.5625\n",
            "in training loop, epoch 2, step 553, the loss is 242755.3125\n",
            "in training loop, epoch 2, step 554, the loss is 216145.71875\n",
            "in training loop, epoch 2, step 555, the loss is 337742.75\n",
            "in training loop, epoch 2, step 556, the loss is 234512.625\n",
            "in training loop, epoch 2, step 557, the loss is 238727.203125\n",
            "in training loop, epoch 2, step 558, the loss is 237573.96875\n",
            "in training loop, epoch 2, step 559, the loss is 277796.90625\n",
            "in training loop, epoch 2, step 560, the loss is 242217.828125\n",
            "in training loop, epoch 2, step 561, the loss is 309213.9375\n",
            "in training loop, epoch 2, step 562, the loss is 247190.09375\n",
            "in training loop, epoch 2, step 563, the loss is 170682.375\n",
            "in training loop, epoch 2, step 564, the loss is 226741.9375\n",
            "in training loop, epoch 2, step 565, the loss is 327553.53125\n",
            "in training loop, epoch 2, step 566, the loss is 243660.125\n",
            "in training loop, epoch 2, step 567, the loss is 271176.1875\n",
            "in training loop, epoch 2, step 568, the loss is 205912.75\n",
            "in training loop, epoch 2, step 569, the loss is 235346.1875\n",
            "in training loop, epoch 2, step 570, the loss is 271651.875\n",
            "in training loop, epoch 2, step 571, the loss is 275396.75\n",
            "in training loop, epoch 2, step 572, the loss is 213708.8125\n",
            "in training loop, epoch 2, step 573, the loss is 204713.109375\n",
            "in training loop, epoch 2, step 574, the loss is 297801.9375\n",
            "in training loop, epoch 2, step 575, the loss is 239395.734375\n",
            "in training loop, epoch 2, step 576, the loss is 247117.890625\n",
            "in training loop, epoch 2, step 577, the loss is 276930.1875\n",
            "in training loop, epoch 2, step 578, the loss is 256306.8125\n",
            "in training loop, epoch 2, step 579, the loss is 278362.46875\n",
            "in training loop, epoch 2, step 580, the loss is 790077.6875\n",
            "in training loop, epoch 2, step 581, the loss is 209361.90625\n",
            "in training loop, epoch 2, step 582, the loss is 257271.0625\n",
            "in training loop, epoch 2, step 583, the loss is 294642.96875\n",
            "in training loop, epoch 2, step 584, the loss is 196424.734375\n",
            "in training loop, epoch 2, step 585, the loss is 227845.21875\n",
            "in training loop, epoch 2, step 586, the loss is 251719.765625\n",
            "in training loop, epoch 2, step 587, the loss is 316732.9375\n",
            "in training loop, epoch 2, step 588, the loss is 227681.4375\n",
            "in training loop, epoch 2, step 589, the loss is 557054.0625\n",
            "in training loop, epoch 2, step 590, the loss is 298877.71875\n",
            "in training loop, epoch 2, step 591, the loss is 212996.4375\n",
            "in training loop, epoch 2, step 592, the loss is 230844.84375\n",
            "in training loop, epoch 2, step 593, the loss is 252070.578125\n",
            "in training loop, epoch 2, step 594, the loss is 274356.25\n",
            "in training loop, epoch 2, step 595, the loss is 283672.84375\n",
            "in training loop, epoch 2, step 596, the loss is 326824.25\n",
            "in training loop, epoch 2, step 597, the loss is 321371.0625\n",
            "in training loop, epoch 2, step 598, the loss is 294212.59375\n",
            "in training loop, epoch 2, step 599, the loss is 269807.03125\n",
            "in training loop, epoch 2, step 600, the loss is 272297.75\n",
            "in training loop, epoch 2, step 601, the loss is 247133.859375\n",
            "in training loop, epoch 2, step 602, the loss is 263948.3125\n",
            "in training loop, epoch 2, step 603, the loss is 354831.625\n",
            "in training loop, epoch 2, step 604, the loss is 316751.40625\n",
            "in training loop, epoch 2, step 605, the loss is 229858.875\n",
            "in training loop, epoch 2, step 606, the loss is 285161.8125\n",
            "in training loop, epoch 2, step 607, the loss is 356540.96875\n",
            "in training loop, epoch 2, step 608, the loss is 303665.3125\n",
            "in training loop, epoch 2, step 609, the loss is 242165.65625\n",
            "in training loop, epoch 2, step 610, the loss is 267169.875\n",
            "in training loop, epoch 2, step 611, the loss is 205967.515625\n",
            "in training loop, epoch 2, step 612, the loss is 440713.875\n",
            "in training loop, epoch 2, step 613, the loss is 261220.015625\n",
            "in training loop, epoch 2, step 614, the loss is 377074.65625\n",
            "in training loop, epoch 2, step 615, the loss is 229281.6875\n",
            "in training loop, epoch 2, step 616, the loss is 328035.78125\n",
            "in training loop, epoch 2, step 617, the loss is 252572.90625\n",
            "in training loop, epoch 2, step 618, the loss is 239649.5\n",
            "in training loop, epoch 2, step 619, the loss is 359438.21875\n",
            "in training loop, epoch 2, step 620, the loss is 367675.5\n",
            "in training loop, epoch 2, step 621, the loss is 185830.21875\n",
            "in training loop, epoch 2, step 622, the loss is 271904.0625\n",
            "in training loop, epoch 2, step 623, the loss is 306489.96875\n",
            "in training loop, epoch 2, step 624, the loss is 283182.78125\n",
            "in training loop, epoch 2, step 625, the loss is 362034.53125\n",
            "in training loop, epoch 2, step 626, the loss is 264817.53125\n",
            "in training loop, epoch 2, step 627, the loss is 189633.953125\n",
            "in training loop, epoch 2, step 628, the loss is 330279.40625\n",
            "in training loop, epoch 2, step 629, the loss is 291546.4375\n",
            "in training loop, epoch 2, step 630, the loss is 209135.59375\n",
            "in training loop, epoch 2, step 631, the loss is 267933.34375\n",
            "in training loop, epoch 2, step 632, the loss is 246464.484375\n",
            "in training loop, epoch 2, step 633, the loss is 291965.9375\n",
            "in training loop, epoch 2, step 634, the loss is 249265.75\n",
            "in training loop, epoch 2, step 635, the loss is 296412.0\n",
            "in training loop, epoch 2, step 636, the loss is 242267.34375\n",
            "in training loop, epoch 2, step 637, the loss is 290457.0\n",
            "in training loop, epoch 2, step 638, the loss is 247057.90625\n",
            "in training loop, epoch 2, step 639, the loss is 332203.4375\n",
            "in training loop, epoch 2, step 640, the loss is 241892.265625\n",
            "in training loop, epoch 2, step 641, the loss is 236422.84375\n",
            "in training loop, epoch 2, step 642, the loss is 294974.34375\n",
            "in training loop, epoch 2, step 643, the loss is 260491.09375\n",
            "in training loop, epoch 2, step 644, the loss is 214375.46875\n",
            "in training loop, epoch 2, step 645, the loss is 271255.09375\n",
            "in training loop, epoch 2, step 646, the loss is 325237.25\n",
            "in training loop, epoch 2, step 647, the loss is 232969.640625\n",
            "in training loop, epoch 2, step 648, the loss is 253686.21875\n",
            "in training loop, epoch 2, step 649, the loss is 267867.96875\n",
            "in training loop, epoch 2, step 650, the loss is 257834.265625\n",
            "in training loop, epoch 2, step 651, the loss is 257884.3125\n",
            "in training loop, epoch 2, step 652, the loss is 207157.078125\n",
            "in training loop, epoch 2, step 653, the loss is 217093.40625\n",
            "in training loop, epoch 2, step 654, the loss is 311001.75\n",
            "in training loop, epoch 2, step 655, the loss is 193060.859375\n",
            "in training loop, epoch 2, step 656, the loss is 281854.75\n",
            "in training loop, epoch 2, step 657, the loss is 202567.65625\n",
            "in training loop, epoch 2, step 658, the loss is 207169.84375\n",
            "in training loop, epoch 2, step 659, the loss is 187352.140625\n",
            "in training loop, epoch 2, step 660, the loss is 188603.015625\n",
            "in training loop, epoch 2, step 661, the loss is 184628.296875\n",
            "in training loop, epoch 2, step 662, the loss is 343676.125\n",
            "in training loop, epoch 2, step 663, the loss is 239395.875\n",
            "in training loop, epoch 2, step 664, the loss is 216384.453125\n",
            "in training loop, epoch 2, step 665, the loss is 299973.71875\n",
            "in training loop, epoch 2, step 666, the loss is 300982.6875\n",
            "in training loop, epoch 2, step 667, the loss is 269925.8125\n",
            "in training loop, epoch 2, step 668, the loss is 198629.796875\n",
            "in training loop, epoch 2, step 669, the loss is 350853.75\n",
            "in training loop, epoch 2, step 670, the loss is 230883.203125\n",
            "in training loop, epoch 2, step 671, the loss is 289404.34375\n",
            "in training loop, epoch 2, step 672, the loss is 286769.65625\n",
            "in training loop, epoch 2, step 673, the loss is 237301.5625\n",
            "in training loop, epoch 2, step 674, the loss is 247157.1875\n",
            "in training loop, epoch 2, step 675, the loss is 158059.375\n",
            "in training loop, epoch 2, step 676, the loss is 341428.375\n",
            "in training loop, epoch 2, step 677, the loss is 320320.875\n",
            "in training loop, epoch 2, step 678, the loss is 204416.671875\n",
            "in training loop, epoch 2, step 679, the loss is 191470.46875\n",
            "in training loop, epoch 2, step 680, the loss is 245247.515625\n",
            "in training loop, epoch 2, step 681, the loss is 302081.875\n",
            "in training loop, epoch 2, step 682, the loss is 238502.140625\n",
            "in training loop, epoch 2, step 683, the loss is 205283.375\n",
            "in training loop, epoch 2, step 684, the loss is 215229.1875\n",
            "in training loop, epoch 2, step 685, the loss is 278809.6875\n",
            "in training loop, epoch 2, step 686, the loss is 249769.6875\n",
            "in training loop, epoch 2, step 687, the loss is 326512.46875\n",
            "in training loop, epoch 2, step 688, the loss is 211599.703125\n",
            "in training loop, epoch 2, step 689, the loss is 271349.6875\n",
            "in training loop, epoch 2, step 690, the loss is 250779.453125\n",
            "in training loop, epoch 2, step 691, the loss is 234553.3125\n",
            "in training loop, epoch 2, step 692, the loss is 293992.53125\n",
            "in training loop, epoch 2, step 693, the loss is 214557.984375\n",
            "in training loop, epoch 2, step 694, the loss is 239722.546875\n",
            "in training loop, epoch 2, step 695, the loss is 232913.578125\n",
            "in training loop, epoch 2, step 696, the loss is 282298.28125\n",
            "in training loop, epoch 2, step 697, the loss is 233786.3125\n",
            "in training loop, epoch 2, step 698, the loss is 276975.65625\n",
            "in training loop, epoch 2, step 699, the loss is 283501.875\n",
            "in training loop, epoch 2, step 700, the loss is 191364.234375\n",
            "in training loop, epoch 2, step 701, the loss is 210442.25\n",
            "in training loop, epoch 2, step 702, the loss is 288578.8125\n",
            "in training loop, epoch 2, step 703, the loss is 290818.90625\n",
            "in training loop, epoch 2, step 704, the loss is 232522.734375\n",
            "in training loop, epoch 2, step 705, the loss is 323366.15625\n",
            "in training loop, epoch 2, step 706, the loss is 289759.59375\n",
            "in training loop, epoch 2, step 707, the loss is 298897.53125\n",
            "in training loop, epoch 2, step 708, the loss is 271642.75\n",
            "in training loop, epoch 2, step 709, the loss is 250346.53125\n",
            "in training loop, epoch 2, step 710, the loss is 226661.53125\n",
            "in training loop, epoch 2, step 711, the loss is 363687.59375\n",
            "in training loop, epoch 2, step 712, the loss is 211463.515625\n",
            "in training loop, epoch 2, step 713, the loss is 192681.34375\n",
            "in training loop, epoch 2, step 714, the loss is 269015.1875\n",
            "in training loop, epoch 2, step 715, the loss is 307023.875\n",
            "in training loop, epoch 2, step 716, the loss is 221319.5\n",
            "in training loop, epoch 2, step 717, the loss is 283205.1875\n",
            "in training loop, epoch 2, step 718, the loss is 289239.15625\n",
            "in training loop, epoch 2, step 719, the loss is 204315.21875\n",
            "in training loop, epoch 2, step 720, the loss is 301106.4375\n",
            "in training loop, epoch 2, step 721, the loss is 297363.34375\n",
            "in training loop, epoch 2, step 722, the loss is 197736.484375\n",
            "in training loop, epoch 2, step 723, the loss is 216413.0\n",
            "in training loop, epoch 2, step 724, the loss is 272757.03125\n",
            "in training loop, epoch 2, step 725, the loss is 346625.21875\n",
            "in training loop, epoch 2, step 726, the loss is 218070.421875\n",
            "in training loop, epoch 2, step 727, the loss is 213787.234375\n",
            "in training loop, epoch 2, step 728, the loss is 300677.03125\n",
            "in training loop, epoch 2, step 729, the loss is 257386.265625\n",
            "in training loop, epoch 2, step 730, the loss is 213715.875\n",
            "in training loop, epoch 2, step 731, the loss is 229227.40625\n",
            "in training loop, epoch 2, step 732, the loss is 249481.78125\n",
            "in training loop, epoch 2, step 733, the loss is 185494.84375\n",
            "in training loop, epoch 2, step 734, the loss is 248236.40625\n",
            "in training loop, epoch 2, step 735, the loss is 264503.65625\n",
            "in training loop, epoch 2, step 736, the loss is 221652.40625\n",
            "in training loop, epoch 2, step 737, the loss is 188048.171875\n",
            "in training loop, epoch 2, step 738, the loss is 242253.84375\n",
            "in training loop, epoch 2, step 739, the loss is 252961.703125\n",
            "in training loop, epoch 2, step 740, the loss is 241689.390625\n",
            "in training loop, epoch 2, step 741, the loss is 228188.78125\n",
            "in training loop, epoch 2, step 742, the loss is 265670.9375\n",
            "in training loop, epoch 2, step 743, the loss is 246517.53125\n",
            "in training loop, epoch 2, step 744, the loss is 363003.8125\n",
            "in training loop, epoch 2, step 745, the loss is 224597.3125\n",
            "in training loop, epoch 2, step 746, the loss is 213440.46875\n",
            "in training loop, epoch 2, step 747, the loss is 216158.953125\n",
            "in training loop, epoch 2, step 748, the loss is 311377.53125\n",
            "in training loop, epoch 2, step 749, the loss is 254069.125\n",
            "in training loop, epoch 2, step 750, the loss is 268423.6875\n",
            "in training loop, epoch 2, step 751, the loss is 169567.625\n",
            "in training loop, epoch 2, step 752, the loss is 225496.640625\n",
            "in training loop, epoch 2, step 753, the loss is 282226.53125\n",
            "in training loop, epoch 2, step 754, the loss is 249318.671875\n",
            "in training loop, epoch 2, step 755, the loss is 235469.109375\n",
            "in training loop, epoch 2, step 756, the loss is 199523.703125\n",
            "in training loop, epoch 2, step 757, the loss is 244043.078125\n",
            "in training loop, epoch 2, step 758, the loss is 196811.796875\n",
            "in training loop, epoch 2, step 759, the loss is 248791.34375\n",
            "in training loop, epoch 2, step 760, the loss is 246015.515625\n",
            "in training loop, epoch 2, step 761, the loss is 229192.515625\n",
            "in training loop, epoch 2, step 762, the loss is 279135.5625\n",
            "in training loop, epoch 2, step 763, the loss is 248787.640625\n",
            "in training loop, epoch 2, step 764, the loss is 291041.84375\n",
            "in training loop, epoch 2, step 765, the loss is 305466.90625\n",
            "in training loop, epoch 2, step 766, the loss is 236034.3125\n",
            "in training loop, epoch 2, step 767, the loss is 332481.125\n",
            "in training loop, epoch 2, step 768, the loss is 297851.3125\n",
            "in training loop, epoch 2, step 769, the loss is 301402.09375\n",
            "in training loop, epoch 2, step 770, the loss is 253493.640625\n",
            "in training loop, epoch 2, step 771, the loss is 226157.734375\n",
            "in training loop, epoch 2, step 772, the loss is 257484.59375\n",
            "in training loop, epoch 2, step 773, the loss is 278675.5625\n",
            "in training loop, epoch 2, step 774, the loss is 215955.0625\n",
            "in training loop, epoch 2, step 775, the loss is 243917.125\n",
            "in training loop, epoch 2, step 776, the loss is 209091.15625\n",
            "in training loop, epoch 2, step 777, the loss is 265999.96875\n",
            "in training loop, epoch 2, step 778, the loss is 225169.5625\n",
            "in training loop, epoch 2, step 779, the loss is 272411.375\n",
            "in training loop, epoch 2, step 780, the loss is 191523.546875\n",
            "in training loop, epoch 2, step 781, the loss is 261689.375\n",
            "in training loop, epoch 2, step 782, the loss is 226976.4375\n",
            "in training loop, epoch 2, step 783, the loss is 204372.03125\n",
            "in training loop, epoch 2, step 784, the loss is 280197.84375\n",
            "in training loop, epoch 2, step 785, the loss is 243529.453125\n",
            "in training loop, epoch 2, step 786, the loss is 223136.015625\n",
            "in training loop, epoch 2, step 787, the loss is 192390.703125\n",
            "in training loop, epoch 2, step 788, the loss is 251048.1875\n",
            "in training loop, epoch 2, step 789, the loss is 252181.75\n",
            "in training loop, epoch 2, step 790, the loss is 264196.09375\n",
            "in training loop, epoch 2, step 791, the loss is 209089.90625\n",
            "in training loop, epoch 2, step 792, the loss is 306709.40625\n",
            "in training loop, epoch 2, step 793, the loss is 232538.140625\n",
            "in training loop, epoch 2, step 794, the loss is 248545.453125\n",
            "in training loop, epoch 2, step 795, the loss is 221037.046875\n",
            "in training loop, epoch 2, step 796, the loss is 227380.796875\n",
            "in training loop, epoch 2, step 797, the loss is 283928.65625\n",
            "in training loop, epoch 2, step 798, the loss is 283570.25\n",
            "in training loop, epoch 2, step 799, the loss is 274517.34375\n",
            "in training loop, epoch 2, step 800, the loss is 184257.265625\n",
            "in training loop, epoch 2, step 801, the loss is 346555.1875\n",
            "in training loop, epoch 2, step 802, the loss is 354961.53125\n",
            "in training loop, epoch 2, step 803, the loss is 251649.484375\n",
            "in training loop, epoch 2, step 804, the loss is 313050.25\n",
            "in training loop, epoch 2, step 805, the loss is 293858.8125\n",
            "in training loop, epoch 2, step 806, the loss is 342855.5625\n",
            "in training loop, epoch 2, step 807, the loss is 364505.21875\n",
            "in training loop, epoch 2, step 808, the loss is 206044.1875\n",
            "in training loop, epoch 2, step 809, the loss is 471407.59375\n",
            "in training loop, epoch 2, step 810, the loss is 402435.9375\n",
            "in training loop, epoch 2, step 811, the loss is 340751.53125\n",
            "in training loop, epoch 2, step 812, the loss is 239600.21875\n",
            "in training loop, epoch 2, step 813, the loss is 326226.21875\n",
            "in training loop, epoch 2, step 814, the loss is 344173.40625\n",
            "in training loop, epoch 2, step 815, the loss is 275435.96875\n",
            "in training loop, epoch 2, step 816, the loss is 193149.78125\n",
            "in training loop, epoch 2, step 817, the loss is 484250.6875\n",
            "in training loop, epoch 2, step 818, the loss is 372852.90625\n",
            "in training loop, epoch 2, step 819, the loss is 260640.03125\n",
            "in training loop, epoch 2, step 820, the loss is 309019.09375\n",
            "in training loop, epoch 2, step 821, the loss is 322715.125\n",
            "in training loop, epoch 2, step 822, the loss is 224429.234375\n",
            "in training loop, epoch 2, step 823, the loss is 345592.09375\n",
            "in training loop, epoch 2, step 824, the loss is 266274.9375\n",
            "in training loop, epoch 2, step 825, the loss is 198054.65625\n",
            "in training loop, epoch 2, step 826, the loss is 238348.59375\n",
            "in training loop, epoch 2, step 827, the loss is 237011.265625\n",
            "in training loop, epoch 2, step 828, the loss is 366901.1875\n",
            "in training loop, epoch 2, step 829, the loss is 350397.1875\n",
            "in training loop, epoch 2, step 830, the loss is 315883.90625\n",
            "in training loop, epoch 2, step 831, the loss is 302173.96875\n",
            "in training loop, epoch 2, step 832, the loss is 348214.5625\n",
            "in training loop, epoch 2, step 833, the loss is 302381.78125\n",
            "in training loop, epoch 2, step 834, the loss is 358628.1875\n",
            "in training loop, epoch 2, step 835, the loss is 238302.90625\n",
            "in training loop, epoch 2, step 836, the loss is 237273.3125\n",
            "in training loop, epoch 2, step 837, the loss is 187357.25\n",
            "in training loop, epoch 2, step 838, the loss is 252294.296875\n",
            "in training loop, epoch 2, step 839, the loss is 332821.6875\n",
            "in training loop, epoch 2, step 840, the loss is 266645.15625\n",
            "in training loop, epoch 2, step 841, the loss is 187888.34375\n",
            "in training loop, epoch 2, step 842, the loss is 247391.78125\n",
            "in training loop, epoch 2, step 843, the loss is 257284.015625\n",
            "in training loop, epoch 2, step 844, the loss is 369333.21875\n",
            "in training loop, epoch 2, step 845, the loss is 306502.46875\n",
            "in training loop, epoch 2, step 846, the loss is 262387.53125\n",
            "in training loop, epoch 2, step 847, the loss is 276236.625\n",
            "in training loop, epoch 2, step 848, the loss is 188065.59375\n",
            "in training loop, epoch 2, step 849, the loss is 366031.34375\n",
            "in training loop, epoch 2, step 850, the loss is 190742.03125\n",
            "in training loop, epoch 2, step 851, the loss is 254702.171875\n",
            "in training loop, epoch 2, step 852, the loss is 299905.0625\n",
            "in training loop, epoch 2, step 853, the loss is 535303.0625\n",
            "in training loop, epoch 2, step 854, the loss is 211585.484375\n",
            "in training loop, epoch 2, step 855, the loss is 267185.5625\n",
            "in training loop, epoch 2, step 856, the loss is 257584.515625\n",
            "in training loop, epoch 2, step 857, the loss is 371899.5625\n",
            "in training loop, epoch 2, step 858, the loss is 297396.03125\n",
            "in training loop, epoch 2, step 859, the loss is 259222.796875\n",
            "in training loop, epoch 2, step 860, the loss is 229478.265625\n",
            "in training loop, epoch 2, step 861, the loss is 314911.4375\n",
            "in training loop, epoch 2, step 862, the loss is 487557.8125\n",
            "in training loop, epoch 2, step 863, the loss is 254594.21875\n",
            "in training loop, epoch 2, step 864, the loss is 221835.203125\n",
            "in training loop, epoch 2, step 865, the loss is 328216.0\n",
            "in training loop, epoch 2, step 866, the loss is 254430.78125\n",
            "in training loop, epoch 2, step 867, the loss is 429373.4375\n",
            "in training loop, epoch 2, step 868, the loss is 238197.90625\n",
            "in training loop, epoch 2, step 869, the loss is 267850.34375\n",
            "in training loop, epoch 2, step 870, the loss is 293574.0625\n",
            "in training loop, epoch 2, step 871, the loss is 333254.78125\n",
            "in training loop, epoch 2, step 872, the loss is 247389.59375\n",
            "in training loop, epoch 2, step 873, the loss is 292419.09375\n",
            "in training loop, epoch 2, step 874, the loss is 210255.765625\n",
            "in training loop, epoch 2, step 875, the loss is 243662.921875\n",
            "in training loop, epoch 2, step 876, the loss is 317634.5\n",
            "in training loop, epoch 2, step 877, the loss is 206110.140625\n",
            "in training loop, epoch 2, step 878, the loss is 239177.0\n",
            "in training loop, epoch 2, step 879, the loss is 240181.203125\n",
            "in training loop, epoch 2, step 880, the loss is 315078.59375\n",
            "in training loop, epoch 2, step 881, the loss is 269875.40625\n",
            "in training loop, epoch 2, step 882, the loss is 153468.671875\n",
            "in training loop, epoch 2, step 883, the loss is 254643.5\n",
            "in training loop, epoch 2, step 884, the loss is 340605.9375\n",
            "in training loop, epoch 2, step 885, the loss is 272009.34375\n",
            "in training loop, epoch 2, step 886, the loss is 338433.1875\n",
            "in training loop, epoch 2, step 887, the loss is 226508.453125\n",
            "in training loop, epoch 2, step 888, the loss is 248363.125\n",
            "in training loop, epoch 2, step 889, the loss is 291896.9375\n",
            "in training loop, epoch 2, step 890, the loss is 255003.078125\n",
            "in training loop, epoch 2, step 891, the loss is 266196.0625\n",
            "in training loop, epoch 2, step 892, the loss is 362699.53125\n",
            "in training loop, epoch 2, step 893, the loss is 240547.78125\n",
            "in training loop, epoch 2, step 894, the loss is 298698.84375\n",
            "in training loop, epoch 2, step 895, the loss is 358210.9375\n",
            "in training loop, epoch 2, step 896, the loss is 260146.921875\n",
            "in training loop, epoch 2, step 897, the loss is 246875.78125\n",
            "in training loop, epoch 2, step 898, the loss is 498785.25\n",
            "in training loop, epoch 2, step 899, the loss is 279251.40625\n",
            "in training loop, epoch 2, step 900, the loss is 315642.90625\n",
            "in training loop, epoch 2, step 901, the loss is 325680.4375\n",
            "in training loop, epoch 2, step 902, the loss is 390553.875\n",
            "in training loop, epoch 2, step 903, the loss is 195224.28125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "k-fold 0:: Epoch 2: train loss 274639.4610066372 val loss 499365.442914604\n",
            "in training loop, epoch 3, step 0, the loss is 191929.78125\n",
            "in training loop, epoch 3, step 1, the loss is 262305.5\n",
            "in training loop, epoch 3, step 2, the loss is 153051.265625\n",
            "in training loop, epoch 3, step 3, the loss is 163581.546875\n",
            "in training loop, epoch 3, step 4, the loss is 216324.859375\n",
            "in training loop, epoch 3, step 5, the loss is 193626.328125\n",
            "in training loop, epoch 3, step 6, the loss is 212893.0625\n",
            "in training loop, epoch 3, step 7, the loss is 280247.40625\n",
            "in training loop, epoch 3, step 8, the loss is 315477.375\n",
            "in training loop, epoch 3, step 9, the loss is 191582.25\n",
            "in training loop, epoch 3, step 10, the loss is 201016.96875\n",
            "in training loop, epoch 3, step 11, the loss is 302792.03125\n",
            "in training loop, epoch 3, step 12, the loss is 224879.5\n",
            "in training loop, epoch 3, step 13, the loss is 279683.9375\n",
            "in training loop, epoch 3, step 14, the loss is 225177.765625\n",
            "in training loop, epoch 3, step 15, the loss is 164735.21875\n",
            "in training loop, epoch 3, step 16, the loss is 223240.96875\n",
            "in training loop, epoch 3, step 17, the loss is 227628.9375\n",
            "in training loop, epoch 3, step 18, the loss is 228626.609375\n",
            "in training loop, epoch 3, step 19, the loss is 176557.09375\n",
            "in training loop, epoch 3, step 20, the loss is 161272.09375\n",
            "in training loop, epoch 3, step 21, the loss is 208103.0625\n",
            "in training loop, epoch 3, step 22, the loss is 217864.9375\n",
            "in training loop, epoch 3, step 23, the loss is 124959.5390625\n",
            "in training loop, epoch 3, step 24, the loss is 221692.015625\n",
            "in training loop, epoch 3, step 25, the loss is 179048.640625\n",
            "in training loop, epoch 3, step 26, the loss is 161711.53125\n",
            "in training loop, epoch 3, step 27, the loss is 240168.921875\n",
            "in training loop, epoch 3, step 28, the loss is 164120.203125\n",
            "in training loop, epoch 3, step 29, the loss is 106578.2109375\n",
            "in training loop, epoch 3, step 30, the loss is 185952.953125\n",
            "in training loop, epoch 3, step 31, the loss is 154720.953125\n",
            "in training loop, epoch 3, step 32, the loss is 184067.609375\n",
            "in training loop, epoch 3, step 33, the loss is 215777.53125\n",
            "in training loop, epoch 3, step 34, the loss is 206955.265625\n",
            "in training loop, epoch 3, step 35, the loss is 615238.4375\n",
            "in training loop, epoch 3, step 36, the loss is 178851.59375\n",
            "in training loop, epoch 3, step 37, the loss is 222326.890625\n",
            "in training loop, epoch 3, step 38, the loss is 143255.21875\n",
            "in training loop, epoch 3, step 39, the loss is 154841.96875\n",
            "in training loop, epoch 3, step 40, the loss is 453268.1875\n",
            "in training loop, epoch 3, step 41, the loss is 180391.59375\n",
            "in training loop, epoch 3, step 42, the loss is 245684.6875\n",
            "in training loop, epoch 3, step 43, the loss is 181606.390625\n",
            "in training loop, epoch 3, step 44, the loss is 287325.9375\n",
            "in training loop, epoch 3, step 45, the loss is 219792.984375\n",
            "in training loop, epoch 3, step 46, the loss is 270539.15625\n",
            "in training loop, epoch 3, step 47, the loss is 364794.8125\n",
            "in training loop, epoch 3, step 48, the loss is 248011.46875\n",
            "in training loop, epoch 3, step 49, the loss is 217581.078125\n",
            "in training loop, epoch 3, step 50, the loss is 195348.4375\n",
            "in training loop, epoch 3, step 51, the loss is 259445.9375\n",
            "in training loop, epoch 3, step 52, the loss is 328503.875\n",
            "in training loop, epoch 3, step 53, the loss is 308083.125\n",
            "in training loop, epoch 3, step 54, the loss is 230468.734375\n",
            "in training loop, epoch 3, step 55, the loss is 217052.140625\n",
            "in training loop, epoch 3, step 56, the loss is 195182.53125\n",
            "in training loop, epoch 3, step 57, the loss is 214342.640625\n",
            "in training loop, epoch 3, step 58, the loss is 181559.0625\n",
            "in training loop, epoch 3, step 59, the loss is 257268.328125\n",
            "in training loop, epoch 3, step 60, the loss is 266026.6875\n",
            "in training loop, epoch 3, step 61, the loss is 266784.84375\n",
            "in training loop, epoch 3, step 62, the loss is 168521.1875\n",
            "in training loop, epoch 3, step 63, the loss is 267403.875\n",
            "in training loop, epoch 3, step 64, the loss is 299582.0625\n",
            "in training loop, epoch 3, step 65, the loss is 321728.59375\n",
            "in training loop, epoch 3, step 66, the loss is 287888.53125\n",
            "in training loop, epoch 3, step 67, the loss is 270547.125\n",
            "in training loop, epoch 3, step 68, the loss is 130566.6875\n",
            "in training loop, epoch 3, step 69, the loss is 374365.96875\n",
            "in training loop, epoch 3, step 70, the loss is 251802.921875\n",
            "in training loop, epoch 3, step 71, the loss is 202128.609375\n",
            "in training loop, epoch 3, step 72, the loss is 222172.09375\n",
            "in training loop, epoch 3, step 73, the loss is 173853.28125\n",
            "in training loop, epoch 3, step 74, the loss is 209468.53125\n",
            "in training loop, epoch 3, step 75, the loss is 301952.59375\n",
            "in training loop, epoch 3, step 76, the loss is 264691.15625\n",
            "in training loop, epoch 3, step 77, the loss is 206517.921875\n",
            "in training loop, epoch 3, step 78, the loss is 242419.015625\n",
            "in training loop, epoch 3, step 79, the loss is 289934.71875\n",
            "in training loop, epoch 3, step 80, the loss is 358068.28125\n",
            "in training loop, epoch 3, step 81, the loss is 195404.4375\n",
            "in training loop, epoch 3, step 82, the loss is 266654.53125\n",
            "in training loop, epoch 3, step 83, the loss is 284222.65625\n",
            "in training loop, epoch 3, step 84, the loss is 217218.015625\n",
            "in training loop, epoch 3, step 85, the loss is 253287.296875\n",
            "in training loop, epoch 3, step 86, the loss is 173276.078125\n",
            "in training loop, epoch 3, step 87, the loss is 193560.859375\n",
            "in training loop, epoch 3, step 88, the loss is 204129.84375\n",
            "in training loop, epoch 3, step 89, the loss is 309625.75\n",
            "in training loop, epoch 3, step 90, the loss is 217402.625\n",
            "in training loop, epoch 3, step 91, the loss is 152669.75\n",
            "in training loop, epoch 3, step 92, the loss is 176907.859375\n",
            "in training loop, epoch 3, step 93, the loss is 334850.21875\n",
            "in training loop, epoch 3, step 94, the loss is 232037.359375\n",
            "in training loop, epoch 3, step 95, the loss is 222055.203125\n",
            "in training loop, epoch 3, step 96, the loss is 187825.0625\n",
            "in training loop, epoch 3, step 97, the loss is 241098.59375\n",
            "in training loop, epoch 3, step 98, the loss is 197342.71875\n",
            "in training loop, epoch 3, step 99, the loss is 168077.296875\n",
            "in training loop, epoch 3, step 100, the loss is 192755.609375\n",
            "in training loop, epoch 3, step 101, the loss is 134935.578125\n",
            "in training loop, epoch 3, step 102, the loss is 190217.890625\n",
            "in training loop, epoch 3, step 103, the loss is 189758.09375\n",
            "in training loop, epoch 3, step 104, the loss is 199599.9375\n",
            "in training loop, epoch 3, step 105, the loss is 147225.84375\n",
            "in training loop, epoch 3, step 106, the loss is 151343.4375\n",
            "in training loop, epoch 3, step 107, the loss is 164124.9375\n",
            "in training loop, epoch 3, step 108, the loss is 183555.078125\n",
            "in training loop, epoch 3, step 109, the loss is 282816.9375\n",
            "in training loop, epoch 3, step 110, the loss is 200531.765625\n",
            "in training loop, epoch 3, step 111, the loss is 198004.328125\n",
            "in training loop, epoch 3, step 112, the loss is 245574.453125\n",
            "in training loop, epoch 3, step 113, the loss is 204042.375\n",
            "in training loop, epoch 3, step 114, the loss is 170927.09375\n",
            "in training loop, epoch 3, step 115, the loss is 235165.1875\n",
            "in training loop, epoch 3, step 116, the loss is 199539.65625\n",
            "in training loop, epoch 3, step 117, the loss is 242187.625\n",
            "in training loop, epoch 3, step 118, the loss is 235844.140625\n",
            "in training loop, epoch 3, step 119, the loss is 236954.53125\n",
            "in training loop, epoch 3, step 120, the loss is 329449.25\n",
            "in training loop, epoch 3, step 121, the loss is 170046.625\n",
            "in training loop, epoch 3, step 122, the loss is 191837.5\n",
            "in training loop, epoch 3, step 123, the loss is 192693.578125\n",
            "in training loop, epoch 3, step 124, the loss is 239668.0625\n",
            "in training loop, epoch 3, step 125, the loss is 261383.046875\n",
            "in training loop, epoch 3, step 126, the loss is 172492.4375\n",
            "in training loop, epoch 3, step 127, the loss is 195666.5\n",
            "in training loop, epoch 3, step 128, the loss is 189460.125\n",
            "in training loop, epoch 3, step 129, the loss is 192339.78125\n",
            "in training loop, epoch 3, step 130, the loss is 219897.25\n",
            "in training loop, epoch 3, step 131, the loss is 206229.6875\n",
            "in training loop, epoch 3, step 132, the loss is 158796.859375\n",
            "in training loop, epoch 3, step 133, the loss is 224666.296875\n",
            "in training loop, epoch 3, step 134, the loss is 172607.65625\n",
            "in training loop, epoch 3, step 135, the loss is 231318.71875\n",
            "in training loop, epoch 3, step 136, the loss is 240581.390625\n",
            "in training loop, epoch 3, step 137, the loss is 246903.5625\n",
            "in training loop, epoch 3, step 138, the loss is 238714.65625\n",
            "in training loop, epoch 3, step 139, the loss is 252717.5625\n",
            "in training loop, epoch 3, step 140, the loss is 195490.8125\n",
            "in training loop, epoch 3, step 141, the loss is 169527.78125\n",
            "in training loop, epoch 3, step 142, the loss is 255197.046875\n",
            "in training loop, epoch 3, step 143, the loss is 275321.75\n",
            "in training loop, epoch 3, step 144, the loss is 173544.484375\n",
            "in training loop, epoch 3, step 145, the loss is 265601.65625\n",
            "in training loop, epoch 3, step 146, the loss is 189260.375\n",
            "in training loop, epoch 3, step 147, the loss is 180231.203125\n",
            "in training loop, epoch 3, step 148, the loss is 199714.984375\n",
            "in training loop, epoch 3, step 149, the loss is 239576.640625\n",
            "in training loop, epoch 3, step 150, the loss is 177471.0\n",
            "in training loop, epoch 3, step 151, the loss is 181826.859375\n",
            "in training loop, epoch 3, step 152, the loss is 205603.484375\n",
            "in training loop, epoch 3, step 153, the loss is 227318.921875\n",
            "in training loop, epoch 3, step 154, the loss is 182553.21875\n",
            "in training loop, epoch 3, step 155, the loss is 143631.5\n",
            "in training loop, epoch 3, step 156, the loss is 200868.96875\n",
            "in training loop, epoch 3, step 157, the loss is 199202.53125\n",
            "in training loop, epoch 3, step 158, the loss is 204278.078125\n",
            "in training loop, epoch 3, step 159, the loss is 169701.4375\n",
            "in training loop, epoch 3, step 160, the loss is 225444.140625\n",
            "in training loop, epoch 3, step 161, the loss is 172411.96875\n",
            "in training loop, epoch 3, step 162, the loss is 161173.4375\n",
            "in training loop, epoch 3, step 163, the loss is 216315.515625\n",
            "in training loop, epoch 3, step 164, the loss is 309955.75\n",
            "in training loop, epoch 3, step 165, the loss is 230440.375\n",
            "in training loop, epoch 3, step 166, the loss is 246514.125\n",
            "in training loop, epoch 3, step 167, the loss is 192136.90625\n",
            "in training loop, epoch 3, step 168, the loss is 190285.203125\n",
            "in training loop, epoch 3, step 169, the loss is 238828.1875\n",
            "in training loop, epoch 3, step 170, the loss is 220845.140625\n",
            "in training loop, epoch 3, step 171, the loss is 168108.078125\n",
            "in training loop, epoch 3, step 172, the loss is 233696.234375\n",
            "in training loop, epoch 3, step 173, the loss is 139074.1875\n",
            "in training loop, epoch 3, step 174, the loss is 208357.28125\n",
            "in training loop, epoch 3, step 175, the loss is 187870.03125\n",
            "in training loop, epoch 3, step 176, the loss is 251944.65625\n",
            "in training loop, epoch 3, step 177, the loss is 181704.578125\n",
            "in training loop, epoch 3, step 178, the loss is 178196.8125\n",
            "in training loop, epoch 3, step 179, the loss is 130739.4140625\n",
            "in training loop, epoch 3, step 180, the loss is 227943.9375\n",
            "in training loop, epoch 3, step 181, the loss is 478831.1875\n",
            "in training loop, epoch 3, step 182, the loss is 136913.75\n",
            "in training loop, epoch 3, step 183, the loss is 231645.3125\n",
            "in training loop, epoch 3, step 184, the loss is 191110.8125\n",
            "in training loop, epoch 3, step 185, the loss is 213616.953125\n",
            "in training loop, epoch 3, step 186, the loss is 646487.0625\n",
            "in training loop, epoch 3, step 187, the loss is 158013.3125\n",
            "in training loop, epoch 3, step 188, the loss is 247053.234375\n",
            "in training loop, epoch 3, step 189, the loss is 244587.46875\n",
            "in training loop, epoch 3, step 190, the loss is 271992.9375\n",
            "in training loop, epoch 3, step 191, the loss is 898099.0625\n",
            "in training loop, epoch 3, step 192, the loss is 348275.0\n",
            "in training loop, epoch 3, step 193, the loss is 240704.328125\n",
            "in training loop, epoch 3, step 194, the loss is 226406.21875\n",
            "in training loop, epoch 3, step 195, the loss is 190981.0625\n",
            "in training loop, epoch 3, step 196, the loss is 241369.4375\n",
            "in training loop, epoch 3, step 197, the loss is 302505.09375\n",
            "in training loop, epoch 3, step 198, the loss is 397503.28125\n",
            "in training loop, epoch 3, step 199, the loss is 244542.65625\n",
            "in training loop, epoch 3, step 200, the loss is 236985.453125\n",
            "in training loop, epoch 3, step 201, the loss is 666106.125\n",
            "in training loop, epoch 3, step 202, the loss is 418854.625\n",
            "in training loop, epoch 3, step 203, the loss is 258796.796875\n",
            "in training loop, epoch 3, step 204, the loss is 294267.875\n",
            "in training loop, epoch 3, step 205, the loss is 349452.96875\n",
            "in training loop, epoch 3, step 206, the loss is 842770.0\n",
            "in training loop, epoch 3, step 207, the loss is 298858.15625\n",
            "in training loop, epoch 3, step 208, the loss is 314882.625\n",
            "in training loop, epoch 3, step 209, the loss is 274404.21875\n",
            "in training loop, epoch 3, step 210, the loss is 617986.125\n",
            "in training loop, epoch 3, step 211, the loss is 313083.8125\n",
            "in training loop, epoch 3, step 212, the loss is 306878.40625\n",
            "in training loop, epoch 3, step 213, the loss is 248817.109375\n",
            "in training loop, epoch 3, step 214, the loss is 254300.4375\n",
            "in training loop, epoch 3, step 215, the loss is 299272.40625\n",
            "in training loop, epoch 3, step 216, the loss is 430629.46875\n",
            "in training loop, epoch 3, step 217, the loss is 240598.375\n",
            "in training loop, epoch 3, step 218, the loss is 294129.875\n",
            "in training loop, epoch 3, step 219, the loss is 538527.5625\n",
            "in training loop, epoch 3, step 220, the loss is 332591.78125\n",
            "in training loop, epoch 3, step 221, the loss is 247714.0\n",
            "in training loop, epoch 3, step 222, the loss is 326232.75\n",
            "in training loop, epoch 3, step 223, the loss is 210115.96875\n",
            "in training loop, epoch 3, step 224, the loss is 336817.5625\n",
            "in training loop, epoch 3, step 225, the loss is 379999.1875\n",
            "in training loop, epoch 3, step 226, the loss is 345457.53125\n",
            "in training loop, epoch 3, step 227, the loss is 318411.0625\n",
            "in training loop, epoch 3, step 228, the loss is 350077.125\n",
            "in training loop, epoch 3, step 229, the loss is 308553.125\n",
            "in training loop, epoch 3, step 230, the loss is 274335.75\n",
            "in training loop, epoch 3, step 231, the loss is 356161.96875\n",
            "in training loop, epoch 3, step 232, the loss is 310550.5625\n",
            "in training loop, epoch 3, step 233, the loss is 231030.734375\n",
            "in training loop, epoch 3, step 234, the loss is 310626.53125\n",
            "in training loop, epoch 3, step 235, the loss is 211156.046875\n",
            "in training loop, epoch 3, step 236, the loss is 253231.28125\n",
            "in training loop, epoch 3, step 237, the loss is 306410.90625\n",
            "in training loop, epoch 3, step 238, the loss is 262215.4375\n",
            "in training loop, epoch 3, step 239, the loss is 139280.171875\n",
            "in training loop, epoch 3, step 240, the loss is 235288.328125\n",
            "in training loop, epoch 3, step 241, the loss is 219660.9375\n",
            "in training loop, epoch 3, step 242, the loss is 261148.5\n",
            "in training loop, epoch 3, step 243, the loss is 290917.5\n",
            "in training loop, epoch 3, step 244, the loss is 254959.375\n",
            "in training loop, epoch 3, step 245, the loss is 161754.828125\n",
            "in training loop, epoch 3, step 246, the loss is 261612.625\n",
            "in training loop, epoch 3, step 247, the loss is 195374.484375\n",
            "in training loop, epoch 3, step 248, the loss is 274369.15625\n",
            "in training loop, epoch 3, step 249, the loss is 301287.34375\n",
            "in training loop, epoch 3, step 250, the loss is 277194.09375\n",
            "in training loop, epoch 3, step 251, the loss is 348364.0\n",
            "in training loop, epoch 3, step 252, the loss is 224048.796875\n",
            "in training loop, epoch 3, step 253, the loss is 211396.84375\n",
            "in training loop, epoch 3, step 254, the loss is 197564.375\n",
            "in training loop, epoch 3, step 255, the loss is 286291.9375\n",
            "in training loop, epoch 3, step 256, the loss is 250190.890625\n",
            "in training loop, epoch 3, step 257, the loss is 241058.75\n",
            "in training loop, epoch 3, step 258, the loss is 382729.625\n",
            "in training loop, epoch 3, step 259, the loss is 226594.203125\n",
            "in training loop, epoch 3, step 260, the loss is 209673.28125\n",
            "in training loop, epoch 3, step 261, the loss is 183402.9375\n",
            "in training loop, epoch 3, step 262, the loss is 201088.296875\n",
            "in training loop, epoch 3, step 263, the loss is 330051.375\n",
            "in training loop, epoch 3, step 264, the loss is 159096.296875\n",
            "in training loop, epoch 3, step 265, the loss is 268088.125\n",
            "in training loop, epoch 3, step 266, the loss is 191769.59375\n",
            "in training loop, epoch 3, step 267, the loss is 231045.6875\n",
            "in training loop, epoch 3, step 268, the loss is 253854.796875\n",
            "in training loop, epoch 3, step 269, the loss is 356912.40625\n",
            "in training loop, epoch 3, step 270, the loss is 273340.25\n",
            "in training loop, epoch 3, step 271, the loss is 151133.46875\n",
            "in training loop, epoch 3, step 272, the loss is 335292.9375\n",
            "in training loop, epoch 3, step 273, the loss is 320161.125\n",
            "in training loop, epoch 3, step 274, the loss is 324776.46875\n",
            "in training loop, epoch 3, step 275, the loss is 174542.296875\n",
            "in training loop, epoch 3, step 276, the loss is 314454.125\n",
            "in training loop, epoch 3, step 277, the loss is 328294.65625\n",
            "in training loop, epoch 3, step 278, the loss is 330684.0\n",
            "in training loop, epoch 3, step 279, the loss is 210844.53125\n",
            "in training loop, epoch 3, step 280, the loss is 225377.859375\n",
            "in training loop, epoch 3, step 281, the loss is 218455.640625\n",
            "in training loop, epoch 3, step 282, the loss is 986349.375\n",
            "in training loop, epoch 3, step 283, the loss is 304619.9375\n",
            "in training loop, epoch 3, step 284, the loss is 216090.46875\n",
            "in training loop, epoch 3, step 285, the loss is 239310.578125\n",
            "in training loop, epoch 3, step 286, the loss is 243524.90625\n",
            "in training loop, epoch 3, step 287, the loss is 218926.46875\n",
            "in training loop, epoch 3, step 288, the loss is 341560.0\n",
            "in training loop, epoch 3, step 289, the loss is 863785.125\n",
            "in training loop, epoch 3, step 290, the loss is 277150.25\n",
            "in training loop, epoch 3, step 291, the loss is 378891.96875\n",
            "in training loop, epoch 3, step 292, the loss is 370956.71875\n",
            "in training loop, epoch 3, step 293, the loss is 429097.65625\n",
            "in training loop, epoch 3, step 294, the loss is 312063.375\n",
            "in training loop, epoch 3, step 295, the loss is 243876.5625\n",
            "in training loop, epoch 3, step 296, the loss is 366483.125\n",
            "in training loop, epoch 3, step 297, the loss is 263743.0\n",
            "in training loop, epoch 3, step 298, the loss is 1155139.625\n",
            "in training loop, epoch 3, step 299, the loss is 302338.375\n",
            "in training loop, epoch 3, step 300, the loss is 233342.328125\n",
            "in training loop, epoch 3, step 301, the loss is 363995.53125\n",
            "in training loop, epoch 3, step 302, the loss is 314794.59375\n",
            "in training loop, epoch 3, step 303, the loss is 287700.3125\n",
            "in training loop, epoch 3, step 304, the loss is 283707.03125\n",
            "in training loop, epoch 3, step 305, the loss is 318328.3125\n",
            "in training loop, epoch 3, step 306, the loss is 423946.6875\n",
            "in training loop, epoch 3, step 307, the loss is 444444.25\n",
            "in training loop, epoch 3, step 308, the loss is 383979.6875\n",
            "in training loop, epoch 3, step 309, the loss is 392503.9375\n",
            "in training loop, epoch 3, step 310, the loss is 301273.0625\n",
            "in training loop, epoch 3, step 311, the loss is 303715.625\n",
            "in training loop, epoch 3, step 312, the loss is 343335.875\n",
            "in training loop, epoch 3, step 313, the loss is 334636.53125\n",
            "in training loop, epoch 3, step 314, the loss is 395886.21875\n",
            "in training loop, epoch 3, step 315, the loss is 353540.625\n",
            "in training loop, epoch 3, step 316, the loss is 322419.28125\n",
            "in training loop, epoch 3, step 317, the loss is 281702.21875\n",
            "in training loop, epoch 3, step 318, the loss is 336849.84375\n",
            "in training loop, epoch 3, step 319, the loss is 297224.3125\n",
            "in training loop, epoch 3, step 320, the loss is 267520.25\n",
            "in training loop, epoch 3, step 321, the loss is 345961.375\n",
            "in training loop, epoch 3, step 322, the loss is 288571.40625\n",
            "in training loop, epoch 3, step 323, the loss is 295522.9375\n",
            "in training loop, epoch 3, step 324, the loss is 301657.625\n",
            "in training loop, epoch 3, step 325, the loss is 290851.9375\n",
            "in training loop, epoch 3, step 326, the loss is 216756.546875\n",
            "in training loop, epoch 3, step 327, the loss is 229231.6875\n",
            "in training loop, epoch 3, step 328, the loss is 277646.84375\n",
            "in training loop, epoch 3, step 329, the loss is 249336.625\n",
            "in training loop, epoch 3, step 330, the loss is 235469.53125\n",
            "in training loop, epoch 3, step 331, the loss is 383315.71875\n",
            "in training loop, epoch 3, step 332, the loss is 355543.0625\n",
            "in training loop, epoch 3, step 333, the loss is 301072.5\n",
            "in training loop, epoch 3, step 334, the loss is 261835.65625\n",
            "in training loop, epoch 3, step 335, the loss is 311332.03125\n",
            "in training loop, epoch 3, step 336, the loss is 275504.0\n",
            "in training loop, epoch 3, step 337, the loss is 172474.609375\n",
            "in training loop, epoch 3, step 338, the loss is 230035.875\n",
            "in training loop, epoch 3, step 339, the loss is 210256.5625\n",
            "in training loop, epoch 3, step 340, the loss is 215256.15625\n",
            "in training loop, epoch 3, step 341, the loss is 215957.96875\n",
            "in training loop, epoch 3, step 342, the loss is 323516.03125\n",
            "in training loop, epoch 3, step 343, the loss is 204722.40625\n",
            "in training loop, epoch 3, step 344, the loss is 315039.8125\n",
            "in training loop, epoch 3, step 345, the loss is 300366.90625\n",
            "in training loop, epoch 3, step 346, the loss is 218946.5625\n",
            "in training loop, epoch 3, step 347, the loss is 325054.71875\n",
            "in training loop, epoch 3, step 348, the loss is 422026.875\n",
            "in training loop, epoch 3, step 349, the loss is 333423.25\n",
            "in training loop, epoch 3, step 350, the loss is 268941.0625\n",
            "in training loop, epoch 3, step 351, the loss is 313671.25\n",
            "in training loop, epoch 3, step 352, the loss is 280815.5625\n",
            "in training loop, epoch 3, step 353, the loss is 349981.84375\n",
            "in training loop, epoch 3, step 354, the loss is 296134.15625\n",
            "in training loop, epoch 3, step 355, the loss is 327805.6875\n",
            "in training loop, epoch 3, step 356, the loss is 319557.25\n",
            "in training loop, epoch 3, step 357, the loss is 259407.578125\n",
            "in training loop, epoch 3, step 358, the loss is 274795.90625\n",
            "in training loop, epoch 3, step 359, the loss is 229408.640625\n",
            "in training loop, epoch 3, step 360, the loss is 227894.875\n",
            "in training loop, epoch 3, step 361, the loss is 206280.53125\n",
            "in training loop, epoch 3, step 362, the loss is 198605.453125\n",
            "in training loop, epoch 3, step 363, the loss is 313703.875\n",
            "in training loop, epoch 3, step 364, the loss is 244033.96875\n",
            "in training loop, epoch 3, step 365, the loss is 269749.34375\n",
            "in training loop, epoch 3, step 366, the loss is 210819.375\n",
            "in training loop, epoch 3, step 367, the loss is 206027.859375\n",
            "in training loop, epoch 3, step 368, the loss is 323871.71875\n",
            "in training loop, epoch 3, step 369, the loss is 275386.40625\n",
            "in training loop, epoch 3, step 370, the loss is 227021.3125\n",
            "in training loop, epoch 3, step 371, the loss is 245309.390625\n",
            "in training loop, epoch 3, step 372, the loss is 158139.34375\n",
            "in training loop, epoch 3, step 373, the loss is 302137.5625\n",
            "in training loop, epoch 3, step 374, the loss is 278457.21875\n",
            "in training loop, epoch 3, step 375, the loss is 228205.265625\n",
            "in training loop, epoch 3, step 376, the loss is 269931.5\n",
            "in training loop, epoch 3, step 377, the loss is 222975.875\n",
            "in training loop, epoch 3, step 378, the loss is 179960.140625\n",
            "in training loop, epoch 3, step 379, the loss is 183709.0\n",
            "in training loop, epoch 3, step 380, the loss is 328703.125\n",
            "in training loop, epoch 3, step 381, the loss is 264229.28125\n",
            "in training loop, epoch 3, step 382, the loss is 225782.484375\n",
            "in training loop, epoch 3, step 383, the loss is 235056.828125\n",
            "in training loop, epoch 3, step 384, the loss is 206459.046875\n",
            "in training loop, epoch 3, step 385, the loss is 242254.9375\n",
            "in training loop, epoch 3, step 386, the loss is 261891.90625\n",
            "in training loop, epoch 3, step 387, the loss is 190555.875\n",
            "in training loop, epoch 3, step 388, the loss is 219938.21875\n",
            "in training loop, epoch 3, step 389, the loss is 277239.46875\n",
            "in training loop, epoch 3, step 390, the loss is 206765.6875\n",
            "in training loop, epoch 3, step 391, the loss is 190829.0625\n",
            "in training loop, epoch 3, step 392, the loss is 278834.65625\n",
            "in training loop, epoch 3, step 393, the loss is 282526.28125\n",
            "in training loop, epoch 3, step 394, the loss is 231246.125\n",
            "in training loop, epoch 3, step 395, the loss is 242578.5625\n",
            "in training loop, epoch 3, step 396, the loss is 187860.25\n",
            "in training loop, epoch 3, step 397, the loss is 269133.6875\n",
            "in training loop, epoch 3, step 398, the loss is 236282.921875\n",
            "in training loop, epoch 3, step 399, the loss is 290657.15625\n",
            "in training loop, epoch 3, step 400, the loss is 348876.6875\n",
            "in training loop, epoch 3, step 401, the loss is 243935.28125\n",
            "in training loop, epoch 3, step 402, the loss is 292839.75\n",
            "in training loop, epoch 3, step 403, the loss is 318075.8125\n",
            "in training loop, epoch 3, step 404, the loss is 411904.15625\n",
            "in training loop, epoch 3, step 405, the loss is 306945.96875\n",
            "in training loop, epoch 3, step 406, the loss is 197474.921875\n",
            "in training loop, epoch 3, step 407, the loss is 296019.4375\n",
            "in training loop, epoch 3, step 408, the loss is 236307.15625\n",
            "in training loop, epoch 3, step 409, the loss is 288219.875\n",
            "in training loop, epoch 3, step 410, the loss is 250819.125\n",
            "in training loop, epoch 3, step 411, the loss is 256071.6875\n",
            "in training loop, epoch 3, step 412, the loss is 202574.78125\n",
            "in training loop, epoch 3, step 413, the loss is 206436.265625\n",
            "in training loop, epoch 3, step 414, the loss is 194335.328125\n",
            "in training loop, epoch 3, step 415, the loss is 217477.0625\n",
            "in training loop, epoch 3, step 416, the loss is 278601.9375\n",
            "in training loop, epoch 3, step 417, the loss is 223807.265625\n",
            "in training loop, epoch 3, step 418, the loss is 215102.03125\n",
            "in training loop, epoch 3, step 419, the loss is 219234.125\n",
            "in training loop, epoch 3, step 420, the loss is 210709.640625\n",
            "in training loop, epoch 3, step 421, the loss is 238554.609375\n",
            "in training loop, epoch 3, step 422, the loss is 251507.15625\n",
            "in training loop, epoch 3, step 423, the loss is 204862.46875\n",
            "in training loop, epoch 3, step 424, the loss is 301342.4375\n",
            "in training loop, epoch 3, step 425, the loss is 330127.125\n",
            "in training loop, epoch 3, step 426, the loss is 264350.125\n",
            "in training loop, epoch 3, step 427, the loss is 216758.3125\n",
            "in training loop, epoch 3, step 428, the loss is 178202.453125\n",
            "in training loop, epoch 3, step 429, the loss is 238072.796875\n",
            "in training loop, epoch 3, step 430, the loss is 158404.03125\n",
            "in training loop, epoch 3, step 431, the loss is 227969.921875\n",
            "in training loop, epoch 3, step 432, the loss is 216801.65625\n",
            "in training loop, epoch 3, step 433, the loss is 236675.34375\n",
            "in training loop, epoch 3, step 434, the loss is 421984.5625\n",
            "in training loop, epoch 3, step 435, the loss is 173760.484375\n",
            "in training loop, epoch 3, step 436, the loss is 200667.15625\n",
            "in training loop, epoch 3, step 437, the loss is 242190.125\n",
            "in training loop, epoch 3, step 438, the loss is 256373.375\n",
            "in training loop, epoch 3, step 439, the loss is 406591.375\n",
            "in training loop, epoch 3, step 440, the loss is 202553.875\n",
            "in training loop, epoch 3, step 441, the loss is 262341.96875\n",
            "in training loop, epoch 3, step 442, the loss is 257307.375\n",
            "in training loop, epoch 3, step 443, the loss is 201563.328125\n",
            "in training loop, epoch 3, step 444, the loss is 328168.90625\n",
            "in training loop, epoch 3, step 445, the loss is 209817.25\n",
            "in training loop, epoch 3, step 446, the loss is 236981.265625\n",
            "in training loop, epoch 3, step 447, the loss is 250674.75\n",
            "in training loop, epoch 3, step 448, the loss is 156082.109375\n",
            "in training loop, epoch 3, step 449, the loss is 203923.9375\n",
            "in training loop, epoch 3, step 450, the loss is 216375.234375\n",
            "in training loop, epoch 3, step 451, the loss is 249673.9375\n",
            "in training loop, epoch 3, step 452, the loss is 276040.375\n",
            "in training loop, epoch 3, step 453, the loss is 211124.21875\n",
            "in training loop, epoch 3, step 454, the loss is 209862.875\n",
            "in training loop, epoch 3, step 455, the loss is 244451.375\n",
            "in training loop, epoch 3, step 456, the loss is 308493.15625\n",
            "in training loop, epoch 3, step 457, the loss is 317289.1875\n",
            "in training loop, epoch 3, step 458, the loss is 202743.859375\n",
            "in training loop, epoch 3, step 459, the loss is 215410.984375\n",
            "in training loop, epoch 3, step 460, the loss is 218641.359375\n",
            "in training loop, epoch 3, step 461, the loss is 243780.453125\n",
            "in training loop, epoch 3, step 462, the loss is 203720.21875\n",
            "in training loop, epoch 3, step 463, the loss is 272883.21875\n",
            "in training loop, epoch 3, step 464, the loss is 182396.625\n",
            "in training loop, epoch 3, step 465, the loss is 232469.296875\n",
            "in training loop, epoch 3, step 466, the loss is 210085.953125\n",
            "in training loop, epoch 3, step 467, the loss is 228651.5\n",
            "in training loop, epoch 3, step 468, the loss is 292074.28125\n",
            "in training loop, epoch 3, step 469, the loss is 224930.15625\n",
            "in training loop, epoch 3, step 470, the loss is 221769.984375\n",
            "in training loop, epoch 3, step 471, the loss is 263176.5625\n",
            "in training loop, epoch 3, step 472, the loss is 251418.625\n",
            "in training loop, epoch 3, step 473, the loss is 193847.09375\n",
            "in training loop, epoch 3, step 474, the loss is 210082.90625\n",
            "in training loop, epoch 3, step 475, the loss is 217830.015625\n",
            "in training loop, epoch 3, step 476, the loss is 228673.09375\n",
            "in training loop, epoch 3, step 477, the loss is 188546.359375\n",
            "in training loop, epoch 3, step 478, the loss is 365432.875\n",
            "in training loop, epoch 3, step 479, the loss is 278847.25\n",
            "in training loop, epoch 3, step 480, the loss is 166402.90625\n",
            "in training loop, epoch 3, step 481, the loss is 370706.1875\n",
            "in training loop, epoch 3, step 482, the loss is 237683.40625\n",
            "in training loop, epoch 3, step 483, the loss is 219831.15625\n",
            "in training loop, epoch 3, step 484, the loss is 319656.90625\n",
            "in training loop, epoch 3, step 485, the loss is 190385.3125\n",
            "in training loop, epoch 3, step 486, the loss is 196421.234375\n",
            "in training loop, epoch 3, step 487, the loss is 228712.71875\n",
            "in training loop, epoch 3, step 488, the loss is 273172.0625\n",
            "in training loop, epoch 3, step 489, the loss is 277088.25\n",
            "in training loop, epoch 3, step 490, the loss is 245335.3125\n",
            "in training loop, epoch 3, step 491, the loss is 187293.390625\n",
            "in training loop, epoch 3, step 492, the loss is 296577.84375\n",
            "in training loop, epoch 3, step 493, the loss is 208117.578125\n",
            "in training loop, epoch 3, step 494, the loss is 225258.1875\n",
            "in training loop, epoch 3, step 495, the loss is 233991.828125\n",
            "in training loop, epoch 3, step 496, the loss is 447949.25\n",
            "in training loop, epoch 3, step 497, the loss is 228831.78125\n",
            "in training loop, epoch 3, step 498, the loss is 215046.640625\n",
            "in training loop, epoch 3, step 499, the loss is 226699.0\n",
            "in training loop, epoch 3, step 500, the loss is 256649.15625\n",
            "in training loop, epoch 3, step 501, the loss is 234652.234375\n",
            "in training loop, epoch 3, step 502, the loss is 274246.375\n",
            "in training loop, epoch 3, step 503, the loss is 324353.53125\n",
            "in training loop, epoch 3, step 504, the loss is 231084.21875\n",
            "in training loop, epoch 3, step 505, the loss is 163130.84375\n",
            "in training loop, epoch 3, step 506, the loss is 288231.53125\n",
            "in training loop, epoch 3, step 507, the loss is 253600.203125\n",
            "in training loop, epoch 3, step 508, the loss is 235710.0\n",
            "in training loop, epoch 3, step 509, the loss is 248622.203125\n",
            "in training loop, epoch 3, step 510, the loss is 246986.375\n",
            "in training loop, epoch 3, step 511, the loss is 155515.40625\n",
            "in training loop, epoch 3, step 512, the loss is 203849.84375\n",
            "in training loop, epoch 3, step 513, the loss is 266199.0\n",
            "in training loop, epoch 3, step 514, the loss is 185195.0\n",
            "in training loop, epoch 3, step 515, the loss is 199528.3125\n",
            "in training loop, epoch 3, step 516, the loss is 186131.015625\n",
            "in training loop, epoch 3, step 517, the loss is 239324.8125\n",
            "in training loop, epoch 3, step 518, the loss is 237428.125\n",
            "in training loop, epoch 3, step 519, the loss is 214285.796875\n",
            "in training loop, epoch 3, step 520, the loss is 205007.65625\n",
            "in training loop, epoch 3, step 521, the loss is 203818.703125\n",
            "in training loop, epoch 3, step 522, the loss is 198859.65625\n",
            "in training loop, epoch 3, step 523, the loss is 222102.90625\n",
            "in training loop, epoch 3, step 524, the loss is 177399.15625\n",
            "in training loop, epoch 3, step 525, the loss is 247691.046875\n",
            "in training loop, epoch 3, step 526, the loss is 277207.625\n",
            "in training loop, epoch 3, step 527, the loss is 258704.359375\n",
            "in training loop, epoch 3, step 528, the loss is 267370.15625\n",
            "in training loop, epoch 3, step 529, the loss is 239155.5625\n",
            "in training loop, epoch 3, step 530, the loss is 255256.390625\n",
            "in training loop, epoch 3, step 531, the loss is 283401.90625\n",
            "in training loop, epoch 3, step 532, the loss is 241005.203125\n",
            "in training loop, epoch 3, step 533, the loss is 191913.25\n",
            "in training loop, epoch 3, step 534, the loss is 213833.25\n",
            "in training loop, epoch 3, step 535, the loss is 234128.984375\n",
            "in training loop, epoch 3, step 536, the loss is 181198.421875\n",
            "in training loop, epoch 3, step 537, the loss is 284314.1875\n",
            "in training loop, epoch 3, step 538, the loss is 219717.1875\n",
            "in training loop, epoch 3, step 539, the loss is 276972.9375\n",
            "in training loop, epoch 3, step 540, the loss is 183787.34375\n",
            "in training loop, epoch 3, step 541, the loss is 439030.71875\n",
            "in training loop, epoch 3, step 542, the loss is 196377.234375\n",
            "in training loop, epoch 3, step 543, the loss is 241011.90625\n",
            "in training loop, epoch 3, step 544, the loss is 248676.84375\n",
            "in training loop, epoch 3, step 545, the loss is 287433.28125\n",
            "in training loop, epoch 3, step 546, the loss is 414742.8125\n",
            "in training loop, epoch 3, step 547, the loss is 314815.375\n",
            "in training loop, epoch 3, step 548, the loss is 308017.875\n",
            "in training loop, epoch 3, step 549, the loss is 327463.3125\n",
            "in training loop, epoch 3, step 550, the loss is 264978.40625\n",
            "in training loop, epoch 3, step 551, the loss is 258946.71875\n",
            "in training loop, epoch 3, step 552, the loss is 333528.28125\n",
            "in training loop, epoch 3, step 553, the loss is 369635.90625\n",
            "in training loop, epoch 3, step 554, the loss is 281117.03125\n",
            "in training loop, epoch 3, step 555, the loss is 237211.34375\n",
            "in training loop, epoch 3, step 556, the loss is 325114.21875\n",
            "in training loop, epoch 3, step 557, the loss is 329487.25\n",
            "in training loop, epoch 3, step 558, the loss is 310154.6875\n",
            "in training loop, epoch 3, step 559, the loss is 297765.75\n",
            "in training loop, epoch 3, step 560, the loss is 233754.0625\n",
            "in training loop, epoch 3, step 561, the loss is 257280.59375\n",
            "in training loop, epoch 3, step 562, the loss is 250328.265625\n",
            "in training loop, epoch 3, step 563, the loss is 231815.78125\n",
            "in training loop, epoch 3, step 564, the loss is 210425.515625\n",
            "in training loop, epoch 3, step 565, the loss is 233718.78125\n",
            "in training loop, epoch 3, step 566, the loss is 284242.6875\n",
            "in training loop, epoch 3, step 567, the loss is 207535.171875\n",
            "in training loop, epoch 3, step 568, the loss is 221012.8125\n",
            "in training loop, epoch 3, step 569, the loss is 194131.796875\n",
            "in training loop, epoch 3, step 570, the loss is 265422.5\n",
            "in training loop, epoch 3, step 571, the loss is 281873.875\n",
            "in training loop, epoch 3, step 572, the loss is 276416.78125\n",
            "in training loop, epoch 3, step 573, the loss is 238875.40625\n",
            "in training loop, epoch 3, step 574, the loss is 188269.515625\n",
            "in training loop, epoch 3, step 575, the loss is 226461.296875\n",
            "in training loop, epoch 3, step 576, the loss is 267857.65625\n",
            "in training loop, epoch 3, step 577, the loss is 205438.46875\n",
            "in training loop, epoch 3, step 578, the loss is 307363.0\n",
            "in training loop, epoch 3, step 579, the loss is 169728.453125\n",
            "in training loop, epoch 3, step 580, the loss is 416938.59375\n",
            "in training loop, epoch 3, step 581, the loss is 219272.078125\n",
            "in training loop, epoch 3, step 582, the loss is 236520.40625\n",
            "in training loop, epoch 3, step 583, the loss is 240298.328125\n",
            "in training loop, epoch 3, step 584, the loss is 402581.875\n",
            "in training loop, epoch 3, step 585, the loss is 244291.15625\n",
            "in training loop, epoch 3, step 586, the loss is 299275.65625\n",
            "in training loop, epoch 3, step 587, the loss is 176385.1875\n",
            "in training loop, epoch 3, step 588, the loss is 230481.265625\n",
            "in training loop, epoch 3, step 589, the loss is 226541.75\n",
            "in training loop, epoch 3, step 590, the loss is 215262.859375\n",
            "in training loop, epoch 3, step 591, the loss is 231572.84375\n",
            "in training loop, epoch 3, step 592, the loss is 350569.6875\n",
            "in training loop, epoch 3, step 593, the loss is 208969.4375\n",
            "in training loop, epoch 3, step 594, the loss is 194064.71875\n",
            "in training loop, epoch 3, step 595, the loss is 213251.140625\n",
            "in training loop, epoch 3, step 596, the loss is 209389.671875\n",
            "in training loop, epoch 3, step 597, the loss is 248777.890625\n",
            "in training loop, epoch 3, step 598, the loss is 234070.875\n",
            "in training loop, epoch 3, step 599, the loss is 222159.21875\n",
            "in training loop, epoch 3, step 600, the loss is 290264.875\n",
            "in training loop, epoch 3, step 601, the loss is 240046.765625\n",
            "in training loop, epoch 3, step 602, the loss is 194651.5\n",
            "in training loop, epoch 3, step 603, the loss is 261248.1875\n",
            "in training loop, epoch 3, step 604, the loss is 232880.515625\n",
            "in training loop, epoch 3, step 605, the loss is 193629.953125\n",
            "in training loop, epoch 3, step 606, the loss is 254596.265625\n",
            "in training loop, epoch 3, step 607, the loss is 198849.890625\n",
            "in training loop, epoch 3, step 608, the loss is 255013.640625\n",
            "in training loop, epoch 3, step 609, the loss is 336987.5625\n",
            "in training loop, epoch 3, step 610, the loss is 176993.3125\n",
            "in training loop, epoch 3, step 611, the loss is 317649.4375\n",
            "in training loop, epoch 3, step 612, the loss is 311533.875\n",
            "in training loop, epoch 3, step 613, the loss is 306656.6875\n",
            "in training loop, epoch 3, step 614, the loss is 352191.9375\n",
            "in training loop, epoch 3, step 615, the loss is 224016.6875\n",
            "in training loop, epoch 3, step 616, the loss is 237847.25\n",
            "in training loop, epoch 3, step 617, the loss is 226272.390625\n",
            "in training loop, epoch 3, step 618, the loss is 281640.75\n",
            "in training loop, epoch 3, step 619, the loss is 222745.3125\n",
            "in training loop, epoch 3, step 620, the loss is 337867.0625\n",
            "in training loop, epoch 3, step 621, the loss is 204821.53125\n",
            "in training loop, epoch 3, step 622, the loss is 385599.15625\n",
            "in training loop, epoch 3, step 623, the loss is 243621.15625\n",
            "in training loop, epoch 3, step 624, the loss is 227813.359375\n",
            "in training loop, epoch 3, step 625, the loss is 264026.15625\n",
            "in training loop, epoch 3, step 626, the loss is 219322.296875\n",
            "in training loop, epoch 3, step 627, the loss is 260641.859375\n",
            "in training loop, epoch 3, step 628, the loss is 278056.09375\n",
            "in training loop, epoch 3, step 629, the loss is 214273.65625\n",
            "in training loop, epoch 3, step 630, the loss is 253903.71875\n",
            "in training loop, epoch 3, step 631, the loss is 263430.96875\n",
            "in training loop, epoch 3, step 632, the loss is 206685.875\n",
            "in training loop, epoch 3, step 633, the loss is 220017.796875\n",
            "in training loop, epoch 3, step 634, the loss is 414617.78125\n",
            "in training loop, epoch 3, step 635, the loss is 244390.640625\n",
            "in training loop, epoch 3, step 636, the loss is 195827.109375\n",
            "in training loop, epoch 3, step 637, the loss is 265003.15625\n",
            "in training loop, epoch 3, step 638, the loss is 193635.765625\n",
            "in training loop, epoch 3, step 639, the loss is 269755.78125\n",
            "in training loop, epoch 3, step 640, the loss is 307099.0625\n",
            "in training loop, epoch 3, step 641, the loss is 219686.5625\n",
            "in training loop, epoch 3, step 642, the loss is 234003.9375\n",
            "in training loop, epoch 3, step 643, the loss is 237860.53125\n",
            "in training loop, epoch 3, step 644, the loss is 207182.328125\n",
            "in training loop, epoch 3, step 645, the loss is 273672.0\n",
            "in training loop, epoch 3, step 646, the loss is 215004.65625\n",
            "in training loop, epoch 3, step 647, the loss is 196416.40625\n",
            "in training loop, epoch 3, step 648, the loss is 217301.390625\n",
            "in training loop, epoch 3, step 649, the loss is 285705.6875\n",
            "in training loop, epoch 3, step 650, the loss is 240705.484375\n",
            "in training loop, epoch 3, step 651, the loss is 263292.1875\n",
            "in training loop, epoch 3, step 652, the loss is 227459.53125\n",
            "in training loop, epoch 3, step 653, the loss is 213121.25\n",
            "in training loop, epoch 3, step 654, the loss is 207107.890625\n",
            "in training loop, epoch 3, step 655, the loss is 257357.0625\n",
            "in training loop, epoch 3, step 656, the loss is 220740.21875\n",
            "in training loop, epoch 3, step 657, the loss is 259662.28125\n",
            "in training loop, epoch 3, step 658, the loss is 227604.046875\n",
            "in training loop, epoch 3, step 659, the loss is 220156.859375\n",
            "in training loop, epoch 3, step 660, the loss is 239725.03125\n",
            "in training loop, epoch 3, step 661, the loss is 293719.03125\n",
            "in training loop, epoch 3, step 662, the loss is 236981.796875\n",
            "in training loop, epoch 3, step 663, the loss is 275251.125\n",
            "in training loop, epoch 3, step 664, the loss is 295149.40625\n",
            "in training loop, epoch 3, step 665, the loss is 211014.0625\n",
            "in training loop, epoch 3, step 666, the loss is 234410.28125\n",
            "in training loop, epoch 3, step 667, the loss is 164543.5\n",
            "in training loop, epoch 3, step 668, the loss is 234825.390625\n",
            "in training loop, epoch 3, step 669, the loss is 208637.53125\n",
            "in training loop, epoch 3, step 670, the loss is 264101.0\n",
            "in training loop, epoch 3, step 671, the loss is 228676.375\n",
            "in training loop, epoch 3, step 672, the loss is 210263.296875\n",
            "in training loop, epoch 3, step 673, the loss is 311890.375\n",
            "in training loop, epoch 3, step 674, the loss is 186679.140625\n",
            "in training loop, epoch 3, step 675, the loss is 215500.140625\n",
            "in training loop, epoch 3, step 676, the loss is 252230.34375\n",
            "in training loop, epoch 3, step 677, the loss is 205283.96875\n",
            "in training loop, epoch 3, step 678, the loss is 224892.921875\n",
            "in training loop, epoch 3, step 679, the loss is 198300.3125\n",
            "in training loop, epoch 3, step 680, the loss is 270013.59375\n",
            "in training loop, epoch 3, step 681, the loss is 229528.890625\n",
            "in training loop, epoch 3, step 682, the loss is 218686.53125\n",
            "in training loop, epoch 3, step 683, the loss is 294250.4375\n",
            "in training loop, epoch 3, step 684, the loss is 221803.015625\n",
            "in training loop, epoch 3, step 685, the loss is 239649.046875\n",
            "in training loop, epoch 3, step 686, the loss is 242444.671875\n",
            "in training loop, epoch 3, step 687, the loss is 302744.78125\n",
            "in training loop, epoch 3, step 688, the loss is 224879.484375\n",
            "in training loop, epoch 3, step 689, the loss is 241739.109375\n",
            "in training loop, epoch 3, step 690, the loss is 169283.59375\n",
            "in training loop, epoch 3, step 691, the loss is 203202.984375\n",
            "in training loop, epoch 3, step 692, the loss is 242034.875\n",
            "in training loop, epoch 3, step 693, the loss is 203562.328125\n",
            "in training loop, epoch 3, step 694, the loss is 187516.453125\n",
            "in training loop, epoch 3, step 695, the loss is 240023.921875\n",
            "in training loop, epoch 3, step 696, the loss is 260375.703125\n",
            "in training loop, epoch 3, step 697, the loss is 166128.90625\n",
            "in training loop, epoch 3, step 698, the loss is 249408.671875\n",
            "in training loop, epoch 3, step 699, the loss is 213617.125\n",
            "in training loop, epoch 3, step 700, the loss is 210003.8125\n",
            "in training loop, epoch 3, step 701, the loss is 195913.625\n",
            "in training loop, epoch 3, step 702, the loss is 199322.703125\n",
            "in training loop, epoch 3, step 703, the loss is 208319.65625\n",
            "in training loop, epoch 3, step 704, the loss is 252897.203125\n",
            "in training loop, epoch 3, step 705, the loss is 237595.125\n",
            "in training loop, epoch 3, step 706, the loss is 190997.796875\n",
            "in training loop, epoch 3, step 707, the loss is 193881.578125\n",
            "in training loop, epoch 3, step 708, the loss is 263353.65625\n",
            "in training loop, epoch 3, step 709, the loss is 257977.203125\n",
            "in training loop, epoch 3, step 710, the loss is 255613.03125\n",
            "in training loop, epoch 3, step 711, the loss is 187711.96875\n",
            "in training loop, epoch 3, step 712, the loss is 209802.890625\n",
            "in training loop, epoch 3, step 713, the loss is 213509.046875\n",
            "in training loop, epoch 3, step 714, the loss is 339858.75\n",
            "in training loop, epoch 3, step 715, the loss is 201567.875\n",
            "in training loop, epoch 3, step 716, the loss is 215533.78125\n",
            "in training loop, epoch 3, step 717, the loss is 245975.328125\n",
            "in training loop, epoch 3, step 718, the loss is 220170.65625\n",
            "in training loop, epoch 3, step 719, the loss is 300547.84375\n",
            "in training loop, epoch 3, step 720, the loss is 261525.5625\n",
            "in training loop, epoch 3, step 721, the loss is 272559.5625\n",
            "in training loop, epoch 3, step 722, the loss is 221785.46875\n",
            "in training loop, epoch 3, step 723, the loss is 235341.03125\n",
            "in training loop, epoch 3, step 724, the loss is 234401.75\n",
            "in training loop, epoch 3, step 725, the loss is 243813.890625\n",
            "in training loop, epoch 3, step 726, the loss is 171674.140625\n",
            "in training loop, epoch 3, step 727, the loss is 210820.0\n",
            "in training loop, epoch 3, step 728, the loss is 144600.78125\n",
            "in training loop, epoch 3, step 729, the loss is 252936.8125\n",
            "in training loop, epoch 3, step 730, the loss is 201442.421875\n",
            "in training loop, epoch 3, step 731, the loss is 257990.65625\n",
            "in training loop, epoch 3, step 732, the loss is 369187.53125\n",
            "in training loop, epoch 3, step 733, the loss is 247963.25\n",
            "in training loop, epoch 3, step 734, the loss is 190739.765625\n",
            "in training loop, epoch 3, step 735, the loss is 238566.609375\n",
            "in training loop, epoch 3, step 736, the loss is 193251.015625\n",
            "in training loop, epoch 3, step 737, the loss is 312818.21875\n",
            "in training loop, epoch 3, step 738, the loss is 301507.9375\n",
            "in training loop, epoch 3, step 739, the loss is 287827.40625\n",
            "in training loop, epoch 3, step 740, the loss is 170950.6875\n",
            "in training loop, epoch 3, step 741, the loss is 248846.09375\n",
            "in training loop, epoch 3, step 742, the loss is 267177.3125\n",
            "in training loop, epoch 3, step 743, the loss is 385358.8125\n",
            "in training loop, epoch 3, step 744, the loss is 208972.25\n",
            "in training loop, epoch 3, step 745, the loss is 204304.515625\n",
            "in training loop, epoch 3, step 746, the loss is 379957.90625\n",
            "in training loop, epoch 3, step 747, the loss is 242936.25\n",
            "in training loop, epoch 3, step 748, the loss is 258405.6875\n",
            "in training loop, epoch 3, step 749, the loss is 229340.65625\n",
            "in training loop, epoch 3, step 750, the loss is 352762.59375\n",
            "in training loop, epoch 3, step 751, the loss is 311422.46875\n",
            "in training loop, epoch 3, step 752, the loss is 278007.25\n",
            "in training loop, epoch 3, step 753, the loss is 292151.96875\n",
            "in training loop, epoch 3, step 754, the loss is 277528.625\n",
            "in training loop, epoch 3, step 755, the loss is 263378.03125\n",
            "in training loop, epoch 3, step 756, the loss is 265832.40625\n",
            "in training loop, epoch 3, step 757, the loss is 252843.5\n",
            "in training loop, epoch 3, step 758, the loss is 239035.8125\n",
            "in training loop, epoch 3, step 759, the loss is 263642.75\n",
            "in training loop, epoch 3, step 760, the loss is 324866.90625\n",
            "in training loop, epoch 3, step 761, the loss is 257032.71875\n",
            "in training loop, epoch 3, step 762, the loss is 241878.765625\n",
            "in training loop, epoch 3, step 763, the loss is 233367.296875\n",
            "in training loop, epoch 3, step 764, the loss is 226645.734375\n",
            "in training loop, epoch 3, step 765, the loss is 248137.421875\n",
            "in training loop, epoch 3, step 766, the loss is 444790.0625\n",
            "in training loop, epoch 3, step 767, the loss is 233288.1875\n",
            "in training loop, epoch 3, step 768, the loss is 289941.21875\n",
            "in training loop, epoch 3, step 769, the loss is 210221.296875\n",
            "in training loop, epoch 3, step 770, the loss is 286894.78125\n",
            "in training loop, epoch 3, step 771, the loss is 269324.78125\n",
            "in training loop, epoch 3, step 772, the loss is 180312.9375\n",
            "in training loop, epoch 3, step 773, the loss is 286778.46875\n",
            "in training loop, epoch 3, step 774, the loss is 245748.671875\n",
            "in training loop, epoch 3, step 775, the loss is 231570.78125\n",
            "in training loop, epoch 3, step 776, the loss is 297434.71875\n",
            "in training loop, epoch 3, step 777, the loss is 166594.8125\n",
            "in training loop, epoch 3, step 778, the loss is 389859.65625\n",
            "in training loop, epoch 3, step 779, the loss is 280044.875\n",
            "in training loop, epoch 3, step 780, the loss is 266175.71875\n",
            "in training loop, epoch 3, step 781, the loss is 329343.65625\n",
            "in training loop, epoch 3, step 782, the loss is 300528.1875\n",
            "in training loop, epoch 3, step 783, the loss is 211714.46875\n",
            "in training loop, epoch 3, step 784, the loss is 263507.34375\n",
            "in training loop, epoch 3, step 785, the loss is 261817.46875\n",
            "in training loop, epoch 3, step 786, the loss is 275375.6875\n",
            "in training loop, epoch 3, step 787, the loss is 316197.6875\n",
            "in training loop, epoch 3, step 788, the loss is 203389.609375\n",
            "in training loop, epoch 3, step 789, the loss is 300900.375\n",
            "in training loop, epoch 3, step 790, the loss is 261313.171875\n",
            "in training loop, epoch 3, step 791, the loss is 275832.25\n",
            "in training loop, epoch 3, step 792, the loss is 204136.125\n",
            "in training loop, epoch 3, step 793, the loss is 226209.015625\n",
            "in training loop, epoch 3, step 794, the loss is 280780.5625\n",
            "in training loop, epoch 3, step 795, the loss is 276508.0\n",
            "in training loop, epoch 3, step 796, the loss is 222790.34375\n",
            "in training loop, epoch 3, step 797, the loss is 211680.84375\n",
            "in training loop, epoch 3, step 798, the loss is 307302.1875\n",
            "in training loop, epoch 3, step 799, the loss is 226573.71875\n",
            "in training loop, epoch 3, step 800, the loss is 206147.359375\n",
            "in training loop, epoch 3, step 801, the loss is 208014.703125\n",
            "in training loop, epoch 3, step 802, the loss is 209715.578125\n",
            "in training loop, epoch 3, step 803, the loss is 292418.125\n",
            "in training loop, epoch 3, step 804, the loss is 256564.984375\n",
            "in training loop, epoch 3, step 805, the loss is 280523.28125\n",
            "in training loop, epoch 3, step 806, the loss is 187925.8125\n",
            "in training loop, epoch 3, step 807, the loss is 228888.953125\n",
            "in training loop, epoch 3, step 808, the loss is 177955.9375\n",
            "in training loop, epoch 3, step 809, the loss is 230663.546875\n",
            "in training loop, epoch 3, step 810, the loss is 216192.59375\n",
            "in training loop, epoch 3, step 811, the loss is 269753.4375\n",
            "in training loop, epoch 3, step 812, the loss is 220841.84375\n",
            "in training loop, epoch 3, step 813, the loss is 191988.6875\n",
            "in training loop, epoch 3, step 814, the loss is 228071.734375\n",
            "in training loop, epoch 3, step 815, the loss is 167455.125\n",
            "in training loop, epoch 3, step 816, the loss is 238851.734375\n",
            "in training loop, epoch 3, step 817, the loss is 291820.0625\n",
            "in training loop, epoch 3, step 818, the loss is 273203.875\n",
            "in training loop, epoch 3, step 819, the loss is 239069.625\n",
            "in training loop, epoch 3, step 820, the loss is 220705.984375\n",
            "in training loop, epoch 3, step 821, the loss is 210981.75\n",
            "in training loop, epoch 3, step 822, the loss is 310354.1875\n",
            "in training loop, epoch 3, step 823, the loss is 269218.25\n",
            "in training loop, epoch 3, step 824, the loss is 283907.25\n",
            "in training loop, epoch 3, step 825, the loss is 238191.046875\n",
            "in training loop, epoch 3, step 826, the loss is 208541.359375\n",
            "in training loop, epoch 3, step 827, the loss is 289510.65625\n",
            "in training loop, epoch 3, step 828, the loss is 181700.734375\n",
            "in training loop, epoch 3, step 829, the loss is 240638.09375\n",
            "in training loop, epoch 3, step 830, the loss is 217865.0\n",
            "in training loop, epoch 3, step 831, the loss is 194799.53125\n",
            "in training loop, epoch 3, step 832, the loss is 273195.40625\n",
            "in training loop, epoch 3, step 833, the loss is 276348.84375\n",
            "in training loop, epoch 3, step 834, the loss is 295433.9375\n",
            "in training loop, epoch 3, step 835, the loss is 244525.5\n",
            "in training loop, epoch 3, step 836, the loss is 233078.109375\n",
            "in training loop, epoch 3, step 837, the loss is 294290.25\n",
            "in training loop, epoch 3, step 838, the loss is 203469.078125\n",
            "in training loop, epoch 3, step 839, the loss is 211682.8125\n",
            "in training loop, epoch 3, step 840, the loss is 337818.4375\n",
            "in training loop, epoch 3, step 841, the loss is 284980.34375\n",
            "in training loop, epoch 3, step 842, the loss is 207010.140625\n",
            "in training loop, epoch 3, step 843, the loss is 277505.1875\n",
            "in training loop, epoch 3, step 844, the loss is 229678.875\n",
            "in training loop, epoch 3, step 845, the loss is 233726.390625\n",
            "in training loop, epoch 3, step 846, the loss is 234023.28125\n",
            "in training loop, epoch 3, step 847, the loss is 190852.8125\n",
            "in training loop, epoch 3, step 848, the loss is 199576.890625\n",
            "in training loop, epoch 3, step 849, the loss is 217419.484375\n",
            "in training loop, epoch 3, step 850, the loss is 233695.8125\n",
            "in training loop, epoch 3, step 851, the loss is 349592.0625\n",
            "in training loop, epoch 3, step 852, the loss is 240834.375\n",
            "in training loop, epoch 3, step 853, the loss is 285613.1875\n",
            "in training loop, epoch 3, step 854, the loss is 176561.75\n",
            "in training loop, epoch 3, step 855, the loss is 214863.703125\n",
            "in training loop, epoch 3, step 856, the loss is 260515.5\n",
            "in training loop, epoch 3, step 857, the loss is 278787.90625\n",
            "in training loop, epoch 3, step 858, the loss is 243233.375\n",
            "in training loop, epoch 3, step 859, the loss is 216981.40625\n",
            "in training loop, epoch 3, step 860, the loss is 203363.828125\n",
            "in training loop, epoch 3, step 861, the loss is 268418.9375\n",
            "in training loop, epoch 3, step 862, the loss is 209340.21875\n",
            "in training loop, epoch 3, step 863, the loss is 201473.15625\n",
            "in training loop, epoch 3, step 864, the loss is 235217.21875\n",
            "in training loop, epoch 3, step 865, the loss is 240279.484375\n",
            "in training loop, epoch 3, step 866, the loss is 185581.5\n",
            "in training loop, epoch 3, step 867, the loss is 307987.9375\n",
            "in training loop, epoch 3, step 868, the loss is 247041.453125\n",
            "in training loop, epoch 3, step 869, the loss is 264289.3125\n",
            "in training loop, epoch 3, step 870, the loss is 263891.9375\n",
            "in training loop, epoch 3, step 871, the loss is 219823.046875\n",
            "in training loop, epoch 3, step 872, the loss is 208133.890625\n",
            "in training loop, epoch 3, step 873, the loss is 278432.84375\n",
            "in training loop, epoch 3, step 874, the loss is 247775.09375\n",
            "in training loop, epoch 3, step 875, the loss is 246386.921875\n",
            "in training loop, epoch 3, step 876, the loss is 203520.265625\n",
            "in training loop, epoch 3, step 877, the loss is 270687.1875\n",
            "in training loop, epoch 3, step 878, the loss is 198739.78125\n",
            "in training loop, epoch 3, step 879, the loss is 149006.5625\n",
            "in training loop, epoch 3, step 880, the loss is 177411.796875\n",
            "in training loop, epoch 3, step 881, the loss is 180865.1875\n",
            "in training loop, epoch 3, step 882, the loss is 264062.6875\n",
            "in training loop, epoch 3, step 883, the loss is 255092.359375\n",
            "in training loop, epoch 3, step 884, the loss is 238714.859375\n",
            "in training loop, epoch 3, step 885, the loss is 204883.015625\n",
            "in training loop, epoch 3, step 886, the loss is 206499.375\n",
            "in training loop, epoch 3, step 887, the loss is 134867.828125\n",
            "in training loop, epoch 3, step 888, the loss is 278072.25\n",
            "in training loop, epoch 3, step 889, the loss is 311315.03125\n",
            "in training loop, epoch 3, step 890, the loss is 209920.984375\n",
            "in training loop, epoch 3, step 891, the loss is 189371.171875\n",
            "in training loop, epoch 3, step 892, the loss is 368159.3125\n",
            "in training loop, epoch 3, step 893, the loss is 276326.5625\n",
            "in training loop, epoch 3, step 894, the loss is 248546.71875\n",
            "in training loop, epoch 3, step 895, the loss is 192975.65625\n",
            "in training loop, epoch 3, step 896, the loss is 228766.40625\n",
            "in training loop, epoch 3, step 897, the loss is 286772.5\n",
            "in training loop, epoch 3, step 898, the loss is 339337.0\n",
            "in training loop, epoch 3, step 899, the loss is 211625.40625\n",
            "in training loop, epoch 3, step 900, the loss is 337944.21875\n",
            "in training loop, epoch 3, step 901, the loss is 373060.375\n",
            "in training loop, epoch 3, step 902, the loss is 360056.15625\n",
            "in training loop, epoch 3, step 903, the loss is 182401.140625\n",
            "k-fold 0:: Epoch 3: train loss 253878.67997269082 val loss 514105.1124690594\n",
            "in training loop, epoch 4, step 0, the loss is 126519.5625\n",
            "in training loop, epoch 4, step 1, the loss is 181874.5625\n",
            "in training loop, epoch 4, step 2, the loss is 150812.0625\n",
            "in training loop, epoch 4, step 3, the loss is 251710.9375\n",
            "in training loop, epoch 4, step 4, the loss is 219367.21875\n",
            "in training loop, epoch 4, step 5, the loss is 206569.203125\n",
            "in training loop, epoch 4, step 6, the loss is 206602.515625\n",
            "in training loop, epoch 4, step 7, the loss is 183339.90625\n",
            "in training loop, epoch 4, step 8, the loss is 188371.203125\n",
            "in training loop, epoch 4, step 9, the loss is 172712.84375\n",
            "in training loop, epoch 4, step 10, the loss is 204456.703125\n",
            "in training loop, epoch 4, step 11, the loss is 192274.53125\n",
            "in training loop, epoch 4, step 12, the loss is 179652.21875\n",
            "in training loop, epoch 4, step 13, the loss is 154844.484375\n",
            "in training loop, epoch 4, step 14, the loss is 185777.203125\n",
            "in training loop, epoch 4, step 15, the loss is 121808.625\n",
            "in training loop, epoch 4, step 16, the loss is 176368.859375\n",
            "in training loop, epoch 4, step 17, the loss is 159410.75\n",
            "in training loop, epoch 4, step 18, the loss is 122292.734375\n",
            "in training loop, epoch 4, step 19, the loss is 82738.7890625\n",
            "in training loop, epoch 4, step 20, the loss is 265673.375\n",
            "in training loop, epoch 4, step 21, the loss is 168018.765625\n",
            "in training loop, epoch 4, step 22, the loss is 147715.453125\n",
            "in training loop, epoch 4, step 23, the loss is 182445.765625\n",
            "in training loop, epoch 4, step 24, the loss is 201918.984375\n",
            "in training loop, epoch 4, step 25, the loss is 168605.875\n",
            "in training loop, epoch 4, step 26, the loss is 166410.859375\n",
            "in training loop, epoch 4, step 27, the loss is 174192.09375\n",
            "in training loop, epoch 4, step 28, the loss is 182837.046875\n",
            "in training loop, epoch 4, step 29, the loss is 125849.9765625\n",
            "in training loop, epoch 4, step 30, the loss is 158738.875\n",
            "in training loop, epoch 4, step 31, the loss is 129208.5703125\n",
            "in training loop, epoch 4, step 32, the loss is 105497.71875\n",
            "in training loop, epoch 4, step 33, the loss is 202251.515625\n",
            "in training loop, epoch 4, step 34, the loss is 167021.25\n",
            "in training loop, epoch 4, step 35, the loss is 131327.78125\n",
            "in training loop, epoch 4, step 36, the loss is 185676.9375\n",
            "in training loop, epoch 4, step 37, the loss is 167943.8125\n",
            "in training loop, epoch 4, step 38, the loss is 247372.09375\n",
            "in training loop, epoch 4, step 39, the loss is 142852.078125\n",
            "in training loop, epoch 4, step 40, the loss is 116926.703125\n",
            "in training loop, epoch 4, step 41, the loss is 247016.546875\n",
            "in training loop, epoch 4, step 42, the loss is 142213.28125\n",
            "in training loop, epoch 4, step 43, the loss is 143205.265625\n",
            "in training loop, epoch 4, step 44, the loss is 153531.484375\n",
            "in training loop, epoch 4, step 45, the loss is 168868.265625\n",
            "in training loop, epoch 4, step 46, the loss is 197725.671875\n",
            "in training loop, epoch 4, step 47, the loss is 180809.640625\n",
            "in training loop, epoch 4, step 48, the loss is 120560.9921875\n",
            "in training loop, epoch 4, step 49, the loss is 150280.78125\n",
            "in training loop, epoch 4, step 50, the loss is 142256.296875\n",
            "in training loop, epoch 4, step 51, the loss is 116223.5546875\n",
            "in training loop, epoch 4, step 52, the loss is 181960.140625\n",
            "in training loop, epoch 4, step 53, the loss is 159333.859375\n",
            "in training loop, epoch 4, step 54, the loss is 174081.34375\n",
            "in training loop, epoch 4, step 55, the loss is 158817.234375\n",
            "in training loop, epoch 4, step 56, the loss is 114781.8515625\n",
            "in training loop, epoch 4, step 57, the loss is 151164.953125\n",
            "in training loop, epoch 4, step 58, the loss is 175103.90625\n",
            "in training loop, epoch 4, step 59, the loss is 184735.53125\n",
            "in training loop, epoch 4, step 60, the loss is 244447.234375\n",
            "in training loop, epoch 4, step 61, the loss is 205073.0625\n",
            "in training loop, epoch 4, step 62, the loss is 217166.46875\n",
            "in training loop, epoch 4, step 63, the loss is 221380.78125\n",
            "in training loop, epoch 4, step 64, the loss is 258866.0625\n",
            "in training loop, epoch 4, step 65, the loss is 187213.0\n",
            "in training loop, epoch 4, step 66, the loss is 130369.4375\n",
            "in training loop, epoch 4, step 67, the loss is 182238.109375\n",
            "in training loop, epoch 4, step 68, the loss is 193742.96875\n",
            "in training loop, epoch 4, step 69, the loss is 147513.4375\n",
            "in training loop, epoch 4, step 70, the loss is 145564.953125\n",
            "in training loop, epoch 4, step 71, the loss is 186864.765625\n",
            "in training loop, epoch 4, step 72, the loss is 223936.59375\n",
            "in training loop, epoch 4, step 73, the loss is 175915.203125\n",
            "in training loop, epoch 4, step 74, the loss is 135585.515625\n",
            "in training loop, epoch 4, step 75, the loss is 139104.140625\n",
            "in training loop, epoch 4, step 76, the loss is 154426.65625\n",
            "in training loop, epoch 4, step 77, the loss is 121031.9453125\n",
            "in training loop, epoch 4, step 78, the loss is 330883.8125\n",
            "in training loop, epoch 4, step 79, the loss is 438771.90625\n",
            "in training loop, epoch 4, step 80, the loss is 296104.09375\n",
            "in training loop, epoch 4, step 81, the loss is 323136.96875\n",
            "in training loop, epoch 4, step 82, the loss is 198165.8125\n",
            "in training loop, epoch 4, step 83, the loss is 216480.140625\n",
            "in training loop, epoch 4, step 84, the loss is 136122.03125\n",
            "in training loop, epoch 4, step 85, the loss is 139903.390625\n",
            "in training loop, epoch 4, step 86, the loss is 189717.375\n",
            "in training loop, epoch 4, step 87, the loss is 163936.671875\n",
            "in training loop, epoch 4, step 88, the loss is 164920.390625\n",
            "in training loop, epoch 4, step 89, the loss is 241201.921875\n",
            "in training loop, epoch 4, step 90, the loss is 158216.125\n",
            "in training loop, epoch 4, step 91, the loss is 188492.78125\n",
            "in training loop, epoch 4, step 92, the loss is 174386.125\n",
            "in training loop, epoch 4, step 93, the loss is 171767.3125\n",
            "in training loop, epoch 4, step 94, the loss is 208401.96875\n",
            "in training loop, epoch 4, step 95, the loss is 284315.125\n",
            "in training loop, epoch 4, step 96, the loss is 204679.609375\n",
            "in training loop, epoch 4, step 97, the loss is 204817.359375\n",
            "in training loop, epoch 4, step 98, the loss is 126534.5546875\n",
            "in training loop, epoch 4, step 99, the loss is 150609.421875\n",
            "in training loop, epoch 4, step 100, the loss is 188239.34375\n",
            "in training loop, epoch 4, step 101, the loss is 165587.046875\n",
            "in training loop, epoch 4, step 102, the loss is 125859.0546875\n",
            "in training loop, epoch 4, step 103, the loss is 163600.34375\n",
            "in training loop, epoch 4, step 104, the loss is 126844.203125\n",
            "in training loop, epoch 4, step 105, the loss is 155448.171875\n",
            "in training loop, epoch 4, step 106, the loss is 184296.609375\n",
            "in training loop, epoch 4, step 107, the loss is 171355.21875\n",
            "in training loop, epoch 4, step 108, the loss is 198956.96875\n",
            "in training loop, epoch 4, step 109, the loss is 232668.90625\n",
            "in training loop, epoch 4, step 110, the loss is 130134.1640625\n",
            "in training loop, epoch 4, step 111, the loss is 216514.46875\n",
            "in training loop, epoch 4, step 112, the loss is 121544.1484375\n",
            "in training loop, epoch 4, step 113, the loss is 117009.125\n",
            "in training loop, epoch 4, step 114, the loss is 143518.90625\n",
            "in training loop, epoch 4, step 115, the loss is 108215.125\n",
            "in training loop, epoch 4, step 116, the loss is 181545.59375\n",
            "in training loop, epoch 4, step 117, the loss is 121961.875\n",
            "in training loop, epoch 4, step 118, the loss is 174450.34375\n",
            "in training loop, epoch 4, step 119, the loss is 151810.0625\n",
            "in training loop, epoch 4, step 120, the loss is 136704.921875\n",
            "in training loop, epoch 4, step 121, the loss is 159450.03125\n",
            "in training loop, epoch 4, step 122, the loss is 152943.5625\n",
            "in training loop, epoch 4, step 123, the loss is 135518.59375\n",
            "in training loop, epoch 4, step 124, the loss is 154116.640625\n",
            "in training loop, epoch 4, step 125, the loss is 204139.75\n",
            "in training loop, epoch 4, step 126, the loss is 111080.875\n",
            "in training loop, epoch 4, step 127, the loss is 152224.84375\n",
            "in training loop, epoch 4, step 128, the loss is 151124.84375\n",
            "in training loop, epoch 4, step 129, the loss is 170629.0625\n",
            "in training loop, epoch 4, step 130, the loss is 229106.5\n",
            "in training loop, epoch 4, step 131, the loss is 157684.125\n",
            "in training loop, epoch 4, step 132, the loss is 119959.6796875\n",
            "in training loop, epoch 4, step 133, the loss is 215865.78125\n",
            "in training loop, epoch 4, step 134, the loss is 183582.296875\n",
            "in training loop, epoch 4, step 135, the loss is 220632.65625\n",
            "in training loop, epoch 4, step 136, the loss is 174776.515625\n",
            "in training loop, epoch 4, step 137, the loss is 276170.3125\n",
            "in training loop, epoch 4, step 138, the loss is 167065.59375\n",
            "in training loop, epoch 4, step 139, the loss is 112353.96875\n",
            "in training loop, epoch 4, step 140, the loss is 150584.796875\n",
            "in training loop, epoch 4, step 141, the loss is 183865.296875\n",
            "in training loop, epoch 4, step 142, the loss is 177878.484375\n",
            "in training loop, epoch 4, step 143, the loss is 251517.84375\n",
            "in training loop, epoch 4, step 144, the loss is 216260.28125\n",
            "in training loop, epoch 4, step 145, the loss is 133391.40625\n",
            "in training loop, epoch 4, step 146, the loss is 131171.265625\n",
            "in training loop, epoch 4, step 147, the loss is 132397.171875\n",
            "in training loop, epoch 4, step 148, the loss is 151849.40625\n",
            "in training loop, epoch 4, step 149, the loss is 175048.765625\n",
            "in training loop, epoch 4, step 150, the loss is 143650.15625\n",
            "in training loop, epoch 4, step 151, the loss is 127819.0703125\n",
            "in training loop, epoch 4, step 152, the loss is 155100.375\n",
            "in training loop, epoch 4, step 153, the loss is 369355.90625\n",
            "in training loop, epoch 4, step 154, the loss is 100535.0078125\n",
            "in training loop, epoch 4, step 155, the loss is 128069.4609375\n",
            "in training loop, epoch 4, step 156, the loss is 176629.671875\n",
            "in training loop, epoch 4, step 157, the loss is 205049.4375\n",
            "in training loop, epoch 4, step 158, the loss is 201865.953125\n",
            "in training loop, epoch 4, step 159, the loss is 194755.796875\n",
            "in training loop, epoch 4, step 160, the loss is 199066.375\n",
            "in training loop, epoch 4, step 161, the loss is 230155.640625\n",
            "in training loop, epoch 4, step 162, the loss is 111312.09375\n",
            "in training loop, epoch 4, step 163, the loss is 237122.25\n",
            "in training loop, epoch 4, step 164, the loss is 211634.953125\n",
            "in training loop, epoch 4, step 165, the loss is 216473.25\n",
            "in training loop, epoch 4, step 166, the loss is 162467.390625\n",
            "in training loop, epoch 4, step 167, the loss is 160876.359375\n",
            "in training loop, epoch 4, step 168, the loss is 161200.0625\n",
            "in training loop, epoch 4, step 169, the loss is 156806.140625\n",
            "in training loop, epoch 4, step 170, the loss is 154902.109375\n",
            "in training loop, epoch 4, step 171, the loss is 144870.125\n",
            "in training loop, epoch 4, step 172, the loss is 162490.640625\n",
            "in training loop, epoch 4, step 173, the loss is 136716.03125\n",
            "in training loop, epoch 4, step 174, the loss is 164597.0625\n",
            "in training loop, epoch 4, step 175, the loss is 138172.9375\n",
            "in training loop, epoch 4, step 176, the loss is 180432.6875\n",
            "in training loop, epoch 4, step 177, the loss is 134949.484375\n",
            "in training loop, epoch 4, step 178, the loss is 205509.609375\n",
            "in training loop, epoch 4, step 179, the loss is 143680.921875\n",
            "in training loop, epoch 4, step 180, the loss is 161083.40625\n",
            "in training loop, epoch 4, step 181, the loss is 133253.53125\n",
            "in training loop, epoch 4, step 182, the loss is 153015.40625\n",
            "in training loop, epoch 4, step 183, the loss is 195115.953125\n",
            "in training loop, epoch 4, step 184, the loss is 175021.171875\n",
            "in training loop, epoch 4, step 185, the loss is 178237.765625\n",
            "in training loop, epoch 4, step 186, the loss is 147604.890625\n",
            "in training loop, epoch 4, step 187, the loss is 205208.59375\n",
            "in training loop, epoch 4, step 188, the loss is 197498.078125\n",
            "in training loop, epoch 4, step 189, the loss is 133680.4375\n",
            "in training loop, epoch 4, step 190, the loss is 134398.34375\n",
            "in training loop, epoch 4, step 191, the loss is 217631.4375\n",
            "in training loop, epoch 4, step 192, the loss is 156195.1875\n",
            "in training loop, epoch 4, step 193, the loss is 113841.203125\n",
            "in training loop, epoch 4, step 194, the loss is 138552.40625\n",
            "in training loop, epoch 4, step 195, the loss is 172203.4375\n",
            "in training loop, epoch 4, step 196, the loss is 134278.953125\n",
            "in training loop, epoch 4, step 197, the loss is 143009.90625\n",
            "in training loop, epoch 4, step 198, the loss is 168401.859375\n",
            "in training loop, epoch 4, step 199, the loss is 225534.34375\n",
            "in training loop, epoch 4, step 200, the loss is 205532.8125\n",
            "in training loop, epoch 4, step 201, the loss is 287379.6875\n",
            "in training loop, epoch 4, step 202, the loss is 156379.3125\n",
            "in training loop, epoch 4, step 203, the loss is 176944.125\n",
            "in training loop, epoch 4, step 204, the loss is 276998.8125\n",
            "in training loop, epoch 4, step 205, the loss is 254355.1875\n",
            "in training loop, epoch 4, step 206, the loss is 1012243.6875\n",
            "in training loop, epoch 4, step 207, the loss is 140317.203125\n",
            "in training loop, epoch 4, step 208, the loss is 194838.328125\n",
            "in training loop, epoch 4, step 209, the loss is 221812.3125\n",
            "in training loop, epoch 4, step 210, the loss is 568021.0\n",
            "in training loop, epoch 4, step 211, the loss is 250736.65625\n",
            "in training loop, epoch 4, step 212, the loss is 240870.15625\n",
            "in training loop, epoch 4, step 213, the loss is 259594.890625\n",
            "in training loop, epoch 4, step 214, the loss is 274733.5\n",
            "in training loop, epoch 4, step 215, the loss is 230213.5\n",
            "in training loop, epoch 4, step 216, the loss is 265552.71875\n",
            "in training loop, epoch 4, step 217, the loss is 153701.59375\n",
            "in training loop, epoch 4, step 218, the loss is 275572.9375\n",
            "in training loop, epoch 4, step 219, the loss is 215552.46875\n",
            "in training loop, epoch 4, step 220, the loss is 238769.171875\n",
            "in training loop, epoch 4, step 221, the loss is 261028.109375\n",
            "in training loop, epoch 4, step 222, the loss is 224453.546875\n",
            "in training loop, epoch 4, step 223, the loss is 258123.015625\n",
            "in training loop, epoch 4, step 224, the loss is 429781.21875\n",
            "in training loop, epoch 4, step 225, the loss is 274360.21875\n",
            "in training loop, epoch 4, step 226, the loss is 216690.140625\n",
            "in training loop, epoch 4, step 227, the loss is 174360.5625\n",
            "in training loop, epoch 4, step 228, the loss is 248737.359375\n",
            "in training loop, epoch 4, step 229, the loss is 200044.640625\n",
            "in training loop, epoch 4, step 230, the loss is 199623.484375\n",
            "in training loop, epoch 4, step 231, the loss is 234702.046875\n",
            "in training loop, epoch 4, step 232, the loss is 235390.015625\n",
            "in training loop, epoch 4, step 233, the loss is 240818.21875\n",
            "in training loop, epoch 4, step 234, the loss is 371918.71875\n",
            "in training loop, epoch 4, step 235, the loss is 357195.625\n",
            "in training loop, epoch 4, step 236, the loss is 200369.453125\n",
            "in training loop, epoch 4, step 237, the loss is 319848.59375\n",
            "in training loop, epoch 4, step 238, the loss is 432272.75\n",
            "in training loop, epoch 4, step 239, the loss is 236282.90625\n",
            "in training loop, epoch 4, step 240, the loss is 239194.296875\n",
            "in training loop, epoch 4, step 241, the loss is 265042.9375\n",
            "in training loop, epoch 4, step 242, the loss is 240070.84375\n",
            "in training loop, epoch 4, step 243, the loss is 292461.3125\n",
            "in training loop, epoch 4, step 244, the loss is 682758.75\n",
            "in training loop, epoch 4, step 245, the loss is 203935.34375\n",
            "in training loop, epoch 4, step 246, the loss is 225544.875\n",
            "in training loop, epoch 4, step 247, the loss is 220490.40625\n",
            "in training loop, epoch 4, step 248, the loss is 305155.25\n",
            "in training loop, epoch 4, step 249, the loss is 223456.078125\n",
            "in training loop, epoch 4, step 250, the loss is 329168.03125\n",
            "in training loop, epoch 4, step 251, the loss is 268929.75\n",
            "in training loop, epoch 4, step 252, the loss is 380730.125\n",
            "in training loop, epoch 4, step 253, the loss is 277379.0\n",
            "in training loop, epoch 4, step 254, the loss is 452982.09375\n",
            "in training loop, epoch 4, step 255, the loss is 295436.84375\n",
            "in training loop, epoch 4, step 256, the loss is 298394.0\n",
            "in training loop, epoch 4, step 257, the loss is 222748.15625\n",
            "in training loop, epoch 4, step 258, the loss is 220670.375\n",
            "in training loop, epoch 4, step 259, the loss is 192871.625\n",
            "in training loop, epoch 4, step 260, the loss is 269662.625\n",
            "in training loop, epoch 4, step 261, the loss is 336201.21875\n",
            "in training loop, epoch 4, step 262, the loss is 297675.90625\n",
            "in training loop, epoch 4, step 263, the loss is 241851.46875\n",
            "in training loop, epoch 4, step 264, the loss is 251465.15625\n",
            "in training loop, epoch 4, step 265, the loss is 178925.03125\n",
            "in training loop, epoch 4, step 266, the loss is 257909.90625\n",
            "in training loop, epoch 4, step 267, the loss is 238251.671875\n",
            "in training loop, epoch 4, step 268, the loss is 267327.34375\n",
            "in training loop, epoch 4, step 269, the loss is 354909.5625\n",
            "in training loop, epoch 4, step 270, the loss is 216127.328125\n",
            "in training loop, epoch 4, step 271, the loss is 197524.625\n",
            "in training loop, epoch 4, step 272, the loss is 226750.59375\n",
            "in training loop, epoch 4, step 273, the loss is 305149.15625\n",
            "in training loop, epoch 4, step 274, the loss is 258217.3125\n",
            "in training loop, epoch 4, step 275, the loss is 190571.96875\n",
            "in training loop, epoch 4, step 276, the loss is 308011.59375\n",
            "in training loop, epoch 4, step 277, the loss is 368897.90625\n",
            "in training loop, epoch 4, step 278, the loss is 276634.0625\n",
            "in training loop, epoch 4, step 279, the loss is 222852.46875\n",
            "in training loop, epoch 4, step 280, the loss is 255160.453125\n",
            "in training loop, epoch 4, step 281, the loss is 328596.90625\n",
            "in training loop, epoch 4, step 282, the loss is 362385.40625\n",
            "in training loop, epoch 4, step 283, the loss is 253054.15625\n",
            "in training loop, epoch 4, step 284, the loss is 247580.828125\n",
            "in training loop, epoch 4, step 285, the loss is 263207.15625\n",
            "in training loop, epoch 4, step 286, the loss is 223983.0625\n",
            "in training loop, epoch 4, step 287, the loss is 617435.375\n",
            "in training loop, epoch 4, step 288, the loss is 213608.109375\n",
            "in training loop, epoch 4, step 289, the loss is 228862.125\n",
            "in training loop, epoch 4, step 290, the loss is 336890.71875\n",
            "in training loop, epoch 4, step 291, the loss is 254608.84375\n",
            "in training loop, epoch 4, step 292, the loss is 215073.03125\n",
            "in training loop, epoch 4, step 293, the loss is 179759.828125\n",
            "in training loop, epoch 4, step 294, the loss is 234156.71875\n",
            "in training loop, epoch 4, step 295, the loss is 228085.46875\n",
            "in training loop, epoch 4, step 296, the loss is 199535.265625\n",
            "in training loop, epoch 4, step 297, the loss is 215309.8125\n",
            "in training loop, epoch 4, step 298, the loss is 274086.9375\n",
            "in training loop, epoch 4, step 299, the loss is 247469.234375\n",
            "in training loop, epoch 4, step 300, the loss is 204850.1875\n",
            "in training loop, epoch 4, step 301, the loss is 293077.375\n",
            "in training loop, epoch 4, step 302, the loss is 188453.109375\n",
            "in training loop, epoch 4, step 303, the loss is 264088.125\n",
            "in training loop, epoch 4, step 304, the loss is 240359.578125\n",
            "in training loop, epoch 4, step 305, the loss is 240672.453125\n",
            "in training loop, epoch 4, step 306, the loss is 205598.546875\n",
            "in training loop, epoch 4, step 307, the loss is 262910.5625\n",
            "in training loop, epoch 4, step 308, the loss is 174356.359375\n",
            "in training loop, epoch 4, step 309, the loss is 231008.921875\n",
            "in training loop, epoch 4, step 310, the loss is 244785.984375\n",
            "in training loop, epoch 4, step 311, the loss is 218848.3125\n",
            "in training loop, epoch 4, step 312, the loss is 213782.640625\n",
            "in training loop, epoch 4, step 313, the loss is 160535.921875\n",
            "in training loop, epoch 4, step 314, the loss is 238500.734375\n",
            "in training loop, epoch 4, step 315, the loss is 146539.296875\n",
            "in training loop, epoch 4, step 316, the loss is 166430.5625\n",
            "in training loop, epoch 4, step 317, the loss is 228161.984375\n",
            "in training loop, epoch 4, step 318, the loss is 589944.375\n",
            "in training loop, epoch 4, step 319, the loss is 207837.796875\n",
            "in training loop, epoch 4, step 320, the loss is 220475.296875\n",
            "in training loop, epoch 4, step 321, the loss is 302961.09375\n",
            "in training loop, epoch 4, step 322, the loss is 234252.828125\n",
            "in training loop, epoch 4, step 323, the loss is 348194.5\n",
            "in training loop, epoch 4, step 324, the loss is 219981.328125\n",
            "in training loop, epoch 4, step 325, the loss is 298600.65625\n",
            "in training loop, epoch 4, step 326, the loss is 211146.296875\n",
            "in training loop, epoch 4, step 327, the loss is 275665.03125\n",
            "in training loop, epoch 4, step 328, the loss is 203631.171875\n",
            "in training loop, epoch 4, step 329, the loss is 201816.15625\n",
            "in training loop, epoch 4, step 330, the loss is 256112.0\n",
            "in training loop, epoch 4, step 331, the loss is 253021.84375\n",
            "in training loop, epoch 4, step 332, the loss is 301855.6875\n",
            "in training loop, epoch 4, step 333, the loss is 255713.59375\n",
            "in training loop, epoch 4, step 334, the loss is 266092.03125\n",
            "in training loop, epoch 4, step 335, the loss is 180526.1875\n",
            "in training loop, epoch 4, step 336, the loss is 216880.96875\n",
            "in training loop, epoch 4, step 337, the loss is 260497.46875\n",
            "in training loop, epoch 4, step 338, the loss is 387076.0\n",
            "in training loop, epoch 4, step 339, the loss is 225266.171875\n",
            "in training loop, epoch 4, step 340, the loss is 344052.84375\n",
            "in training loop, epoch 4, step 341, the loss is 238229.03125\n",
            "in training loop, epoch 4, step 342, the loss is 296851.71875\n",
            "in training loop, epoch 4, step 343, the loss is 420114.15625\n",
            "in training loop, epoch 4, step 344, the loss is 286665.59375\n",
            "in training loop, epoch 4, step 345, the loss is 165217.84375\n",
            "in training loop, epoch 4, step 346, the loss is 206381.75\n",
            "in training loop, epoch 4, step 347, the loss is 199697.359375\n",
            "in training loop, epoch 4, step 348, the loss is 218194.9375\n",
            "in training loop, epoch 4, step 349, the loss is 252867.953125\n",
            "in training loop, epoch 4, step 350, the loss is 221774.59375\n",
            "in training loop, epoch 4, step 351, the loss is 245975.109375\n",
            "in training loop, epoch 4, step 352, the loss is 201878.21875\n",
            "in training loop, epoch 4, step 353, the loss is 165260.3125\n",
            "in training loop, epoch 4, step 354, the loss is 242625.28125\n",
            "in training loop, epoch 4, step 355, the loss is 201436.546875\n",
            "in training loop, epoch 4, step 356, the loss is 224315.34375\n",
            "in training loop, epoch 4, step 357, the loss is 176822.625\n",
            "in training loop, epoch 4, step 358, the loss is 172194.296875\n",
            "in training loop, epoch 4, step 359, the loss is 172808.796875\n",
            "in training loop, epoch 4, step 360, the loss is 203941.203125\n",
            "in training loop, epoch 4, step 361, the loss is 201140.015625\n",
            "in training loop, epoch 4, step 362, the loss is 278196.3125\n",
            "in training loop, epoch 4, step 363, the loss is 218630.53125\n",
            "in training loop, epoch 4, step 364, the loss is 151364.109375\n",
            "in training loop, epoch 4, step 365, the loss is 227320.609375\n",
            "in training loop, epoch 4, step 366, the loss is 195906.734375\n",
            "in training loop, epoch 4, step 367, the loss is 207657.90625\n",
            "in training loop, epoch 4, step 368, the loss is 184687.875\n",
            "in training loop, epoch 4, step 369, the loss is 218698.9375\n",
            "in training loop, epoch 4, step 370, the loss is 236360.25\n",
            "in training loop, epoch 4, step 371, the loss is 129594.953125\n",
            "in training loop, epoch 4, step 372, the loss is 182627.484375\n",
            "in training loop, epoch 4, step 373, the loss is 199810.015625\n",
            "in training loop, epoch 4, step 374, the loss is 166154.84375\n",
            "in training loop, epoch 4, step 375, the loss is 174686.984375\n",
            "in training loop, epoch 4, step 376, the loss is 202142.390625\n",
            "in training loop, epoch 4, step 377, the loss is 220638.625\n",
            "in training loop, epoch 4, step 378, the loss is 180030.1875\n",
            "in training loop, epoch 4, step 379, the loss is 253713.75\n",
            "in training loop, epoch 4, step 380, the loss is 205552.28125\n",
            "in training loop, epoch 4, step 381, the loss is 231541.203125\n",
            "in training loop, epoch 4, step 382, the loss is 201577.984375\n",
            "in training loop, epoch 4, step 383, the loss is 172606.328125\n",
            "in training loop, epoch 4, step 384, the loss is 168777.828125\n",
            "in training loop, epoch 4, step 385, the loss is 253003.109375\n",
            "in training loop, epoch 4, step 386, the loss is 153906.3125\n",
            "in training loop, epoch 4, step 387, the loss is 224681.328125\n",
            "in training loop, epoch 4, step 388, the loss is 214184.265625\n",
            "in training loop, epoch 4, step 389, the loss is 193917.40625\n",
            "in training loop, epoch 4, step 390, the loss is 144215.28125\n",
            "in training loop, epoch 4, step 391, the loss is 135680.984375\n",
            "in training loop, epoch 4, step 392, the loss is 206208.140625\n",
            "in training loop, epoch 4, step 393, the loss is 172116.140625\n",
            "in training loop, epoch 4, step 394, the loss is 167428.765625\n",
            "in training loop, epoch 4, step 395, the loss is 112060.5625\n",
            "in training loop, epoch 4, step 396, the loss is 193805.71875\n",
            "in training loop, epoch 4, step 397, the loss is 169676.40625\n",
            "in training loop, epoch 4, step 398, the loss is 136184.890625\n",
            "in training loop, epoch 4, step 399, the loss is 129073.7890625\n",
            "in training loop, epoch 4, step 400, the loss is 199985.265625\n",
            "in training loop, epoch 4, step 401, the loss is 244165.359375\n",
            "in training loop, epoch 4, step 402, the loss is 1057496.375\n",
            "in training loop, epoch 4, step 403, the loss is 173527.078125\n",
            "in training loop, epoch 4, step 404, the loss is 233872.359375\n",
            "in training loop, epoch 4, step 405, the loss is 172395.078125\n",
            "in training loop, epoch 4, step 406, the loss is 559906.4375\n",
            "in training loop, epoch 4, step 407, the loss is 240484.234375\n",
            "in training loop, epoch 4, step 408, the loss is 246134.0\n",
            "in training loop, epoch 4, step 409, the loss is 274158.59375\n",
            "in training loop, epoch 4, step 410, the loss is 257350.4375\n",
            "in training loop, epoch 4, step 411, the loss is 270955.875\n",
            "in training loop, epoch 4, step 412, the loss is 279224.09375\n",
            "in training loop, epoch 4, step 413, the loss is 242490.0625\n",
            "in training loop, epoch 4, step 414, the loss is 214634.984375\n",
            "in training loop, epoch 4, step 415, the loss is 200901.796875\n",
            "in training loop, epoch 4, step 416, the loss is 208124.703125\n",
            "in training loop, epoch 4, step 417, the loss is 339060.09375\n",
            "in training loop, epoch 4, step 418, the loss is 320718.25\n",
            "in training loop, epoch 4, step 419, the loss is 195741.40625\n",
            "in training loop, epoch 4, step 420, the loss is 267715.90625\n",
            "in training loop, epoch 4, step 421, the loss is 522290.0\n",
            "in training loop, epoch 4, step 422, the loss is 271548.4375\n",
            "in training loop, epoch 4, step 423, the loss is 262883.1875\n",
            "in training loop, epoch 4, step 424, the loss is 232835.25\n",
            "in training loop, epoch 4, step 425, the loss is 331984.4375\n",
            "in training loop, epoch 4, step 426, the loss is 257966.984375\n",
            "in training loop, epoch 4, step 427, the loss is 295332.625\n",
            "in training loop, epoch 4, step 428, the loss is 224137.34375\n",
            "in training loop, epoch 4, step 429, the loss is 204778.0625\n",
            "in training loop, epoch 4, step 430, the loss is 271198.8125\n",
            "in training loop, epoch 4, step 431, the loss is 238139.40625\n",
            "in training loop, epoch 4, step 432, the loss is 251523.859375\n",
            "in training loop, epoch 4, step 433, the loss is 240046.265625\n",
            "in training loop, epoch 4, step 434, the loss is 224009.6875\n",
            "in training loop, epoch 4, step 435, the loss is 234999.46875\n",
            "in training loop, epoch 4, step 436, the loss is 343790.4375\n",
            "in training loop, epoch 4, step 437, the loss is 220593.25\n",
            "in training loop, epoch 4, step 438, the loss is 257881.515625\n",
            "in training loop, epoch 4, step 439, the loss is 208016.75\n",
            "in training loop, epoch 4, step 440, the loss is 389448.5625\n",
            "in training loop, epoch 4, step 441, the loss is 257389.359375\n",
            "in training loop, epoch 4, step 442, the loss is 211307.96875\n",
            "in training loop, epoch 4, step 443, the loss is 263393.65625\n",
            "in training loop, epoch 4, step 444, the loss is 278840.875\n",
            "in training loop, epoch 4, step 445, the loss is 267181.78125\n",
            "in training loop, epoch 4, step 446, the loss is 244708.671875\n",
            "in training loop, epoch 4, step 447, the loss is 294653.53125\n",
            "in training loop, epoch 4, step 448, the loss is 246432.078125\n",
            "in training loop, epoch 4, step 449, the loss is 195510.703125\n",
            "in training loop, epoch 4, step 450, the loss is 317392.34375\n",
            "in training loop, epoch 4, step 451, the loss is 233972.875\n",
            "in training loop, epoch 4, step 452, the loss is 208636.296875\n",
            "in training loop, epoch 4, step 453, the loss is 229933.8125\n",
            "in training loop, epoch 4, step 454, the loss is 205963.625\n",
            "in training loop, epoch 4, step 455, the loss is 195433.953125\n",
            "in training loop, epoch 4, step 456, the loss is 272150.625\n",
            "in training loop, epoch 4, step 457, the loss is 182846.25\n",
            "in training loop, epoch 4, step 458, the loss is 164323.625\n",
            "in training loop, epoch 4, step 459, the loss is 270638.96875\n",
            "in training loop, epoch 4, step 460, the loss is 182304.96875\n",
            "in training loop, epoch 4, step 461, the loss is 287652.8125\n",
            "in training loop, epoch 4, step 462, the loss is 210026.28125\n",
            "in training loop, epoch 4, step 463, the loss is 252733.203125\n",
            "in training loop, epoch 4, step 464, the loss is 179162.9375\n",
            "in training loop, epoch 4, step 465, the loss is 201521.953125\n",
            "in training loop, epoch 4, step 466, the loss is 218930.375\n",
            "in training loop, epoch 4, step 467, the loss is 245118.234375\n",
            "in training loop, epoch 4, step 468, the loss is 193182.703125\n",
            "in training loop, epoch 4, step 469, the loss is 170611.296875\n",
            "in training loop, epoch 4, step 470, the loss is 186499.21875\n",
            "in training loop, epoch 4, step 471, the loss is 207227.640625\n",
            "in training loop, epoch 4, step 472, the loss is 263477.46875\n",
            "in training loop, epoch 4, step 473, the loss is 247414.0\n",
            "in training loop, epoch 4, step 474, the loss is 326149.125\n",
            "in training loop, epoch 4, step 475, the loss is 170481.9375\n",
            "in training loop, epoch 4, step 476, the loss is 301833.3125\n",
            "in training loop, epoch 4, step 477, the loss is 298865.0\n",
            "in training loop, epoch 4, step 478, the loss is 351308.90625\n",
            "in training loop, epoch 4, step 479, the loss is 202641.0\n",
            "in training loop, epoch 4, step 480, the loss is 213703.078125\n",
            "in training loop, epoch 4, step 481, the loss is 205776.015625\n",
            "in training loop, epoch 4, step 482, the loss is 222034.171875\n",
            "in training loop, epoch 4, step 483, the loss is 208182.859375\n",
            "in training loop, epoch 4, step 484, the loss is 217500.84375\n",
            "in training loop, epoch 4, step 485, the loss is 281652.9375\n",
            "in training loop, epoch 4, step 486, the loss is 255372.296875\n",
            "in training loop, epoch 4, step 487, the loss is 272901.5\n",
            "in training loop, epoch 4, step 488, the loss is 276734.90625\n",
            "in training loop, epoch 4, step 489, the loss is 407597.5625\n",
            "in training loop, epoch 4, step 490, the loss is 169574.28125\n",
            "in training loop, epoch 4, step 491, the loss is 167307.3125\n",
            "in training loop, epoch 4, step 492, the loss is 225321.015625\n",
            "in training loop, epoch 4, step 493, the loss is 254135.15625\n",
            "in training loop, epoch 4, step 494, the loss is 233743.703125\n",
            "in training loop, epoch 4, step 495, the loss is 224272.5\n",
            "in training loop, epoch 4, step 496, the loss is 258448.5\n",
            "in training loop, epoch 4, step 497, the loss is 250073.609375\n",
            "in training loop, epoch 4, step 498, the loss is 254103.46875\n",
            "in training loop, epoch 4, step 499, the loss is 241298.3125\n",
            "in training loop, epoch 4, step 500, the loss is 235821.265625\n",
            "in training loop, epoch 4, step 501, the loss is 163427.328125\n",
            "in training loop, epoch 4, step 502, the loss is 267806.59375\n",
            "in training loop, epoch 4, step 503, the loss is 203130.921875\n",
            "in training loop, epoch 4, step 504, the loss is 226580.375\n",
            "in training loop, epoch 4, step 505, the loss is 211265.953125\n",
            "in training loop, epoch 4, step 506, the loss is 253878.15625\n",
            "in training loop, epoch 4, step 507, the loss is 221279.15625\n",
            "in training loop, epoch 4, step 508, the loss is 214216.65625\n",
            "in training loop, epoch 4, step 509, the loss is 177813.171875\n",
            "in training loop, epoch 4, step 510, the loss is 220780.765625\n",
            "in training loop, epoch 4, step 511, the loss is 251680.65625\n",
            "in training loop, epoch 4, step 512, the loss is 194069.25\n",
            "in training loop, epoch 4, step 513, the loss is 183180.9375\n",
            "in training loop, epoch 4, step 514, the loss is 193964.71875\n",
            "in training loop, epoch 4, step 515, the loss is 185714.625\n",
            "in training loop, epoch 4, step 516, the loss is 193139.1875\n",
            "in training loop, epoch 4, step 517, the loss is 185707.75\n",
            "in training loop, epoch 4, step 518, the loss is 168115.09375\n",
            "in training loop, epoch 4, step 519, the loss is 199111.3125\n",
            "in training loop, epoch 4, step 520, the loss is 143888.03125\n",
            "in training loop, epoch 4, step 521, the loss is 180609.375\n",
            "in training loop, epoch 4, step 522, the loss is 143559.3125\n",
            "in training loop, epoch 4, step 523, the loss is 155468.40625\n",
            "in training loop, epoch 4, step 524, the loss is 204272.15625\n",
            "in training loop, epoch 4, step 525, the loss is 212670.015625\n",
            "in training loop, epoch 4, step 526, the loss is 220980.703125\n",
            "in training loop, epoch 4, step 527, the loss is 261086.296875\n",
            "in training loop, epoch 4, step 528, the loss is 177738.65625\n",
            "in training loop, epoch 4, step 529, the loss is 150131.796875\n",
            "in training loop, epoch 4, step 530, the loss is 202560.765625\n",
            "in training loop, epoch 4, step 531, the loss is 191283.8125\n",
            "in training loop, epoch 4, step 532, the loss is 240233.25\n",
            "in training loop, epoch 4, step 533, the loss is 246283.96875\n",
            "in training loop, epoch 4, step 534, the loss is 220562.46875\n",
            "in training loop, epoch 4, step 535, the loss is 313940.28125\n",
            "in training loop, epoch 4, step 536, the loss is 164103.265625\n",
            "in training loop, epoch 4, step 537, the loss is 200516.609375\n",
            "in training loop, epoch 4, step 538, the loss is 181061.03125\n",
            "in training loop, epoch 4, step 539, the loss is 202910.765625\n",
            "in training loop, epoch 4, step 540, the loss is 213725.828125\n",
            "in training loop, epoch 4, step 541, the loss is 292957.21875\n",
            "in training loop, epoch 4, step 542, the loss is 200896.984375\n",
            "in training loop, epoch 4, step 543, the loss is 248427.59375\n",
            "in training loop, epoch 4, step 544, the loss is 155237.359375\n",
            "in training loop, epoch 4, step 545, the loss is 156131.78125\n",
            "in training loop, epoch 4, step 546, the loss is 209713.828125\n",
            "in training loop, epoch 4, step 547, the loss is 193476.71875\n",
            "in training loop, epoch 4, step 548, the loss is 191991.796875\n",
            "in training loop, epoch 4, step 549, the loss is 242894.921875\n",
            "in training loop, epoch 4, step 550, the loss is 216698.359375\n",
            "in training loop, epoch 4, step 551, the loss is 217497.34375\n",
            "in training loop, epoch 4, step 552, the loss is 208056.703125\n",
            "in training loop, epoch 4, step 553, the loss is 159895.75\n",
            "in training loop, epoch 4, step 554, the loss is 218384.046875\n",
            "in training loop, epoch 4, step 555, the loss is 183369.234375\n",
            "in training loop, epoch 4, step 556, the loss is 304023.03125\n",
            "in training loop, epoch 4, step 557, the loss is 195630.125\n",
            "in training loop, epoch 4, step 558, the loss is 185104.265625\n",
            "in training loop, epoch 4, step 559, the loss is 204995.390625\n",
            "in training loop, epoch 4, step 560, the loss is 179625.046875\n",
            "in training loop, epoch 4, step 561, the loss is 197868.4375\n",
            "in training loop, epoch 4, step 562, the loss is 223215.046875\n",
            "in training loop, epoch 4, step 563, the loss is 206601.171875\n",
            "in training loop, epoch 4, step 564, the loss is 191212.484375\n",
            "in training loop, epoch 4, step 565, the loss is 245185.71875\n",
            "in training loop, epoch 4, step 566, the loss is 166006.65625\n",
            "in training loop, epoch 4, step 567, the loss is 252560.59375\n",
            "in training loop, epoch 4, step 568, the loss is 216101.65625\n",
            "in training loop, epoch 4, step 569, the loss is 237992.65625\n",
            "in training loop, epoch 4, step 570, the loss is 215239.671875\n",
            "in training loop, epoch 4, step 571, the loss is 443337.8125\n",
            "in training loop, epoch 4, step 572, the loss is 221167.84375\n",
            "in training loop, epoch 4, step 573, the loss is 527142.0625\n",
            "in training loop, epoch 4, step 574, the loss is 198517.296875\n",
            "in training loop, epoch 4, step 575, the loss is 194062.125\n",
            "in training loop, epoch 4, step 576, the loss is 443187.1875\n",
            "in training loop, epoch 4, step 577, the loss is 225277.359375\n",
            "in training loop, epoch 4, step 578, the loss is 264138.4375\n",
            "in training loop, epoch 4, step 579, the loss is 250605.03125\n",
            "in training loop, epoch 4, step 580, the loss is 258113.421875\n",
            "in training loop, epoch 4, step 581, the loss is 321236.625\n",
            "in training loop, epoch 4, step 582, the loss is 190171.828125\n",
            "in training loop, epoch 4, step 583, the loss is 382762.375\n",
            "in training loop, epoch 4, step 584, the loss is 204398.0625\n",
            "in training loop, epoch 4, step 585, the loss is 251929.90625\n",
            "in training loop, epoch 4, step 586, the loss is 244041.125\n",
            "in training loop, epoch 4, step 587, the loss is 239205.78125\n",
            "in training loop, epoch 4, step 588, the loss is 281606.90625\n",
            "in training loop, epoch 4, step 589, the loss is 189206.9375\n",
            "in training loop, epoch 4, step 590, the loss is 220758.15625\n",
            "in training loop, epoch 4, step 591, the loss is 207091.234375\n",
            "in training loop, epoch 4, step 592, the loss is 215929.421875\n",
            "in training loop, epoch 4, step 593, the loss is 264193.46875\n",
            "in training loop, epoch 4, step 594, the loss is 178035.140625\n",
            "in training loop, epoch 4, step 595, the loss is 232956.3125\n",
            "in training loop, epoch 4, step 596, the loss is 198276.203125\n",
            "in training loop, epoch 4, step 597, the loss is 205846.1875\n",
            "in training loop, epoch 4, step 598, the loss is 208089.015625\n",
            "in training loop, epoch 4, step 599, the loss is 276129.03125\n",
            "in training loop, epoch 4, step 600, the loss is 217572.25\n",
            "in training loop, epoch 4, step 601, the loss is 214806.453125\n",
            "in training loop, epoch 4, step 602, the loss is 257990.625\n",
            "in training loop, epoch 4, step 603, the loss is 255768.890625\n",
            "in training loop, epoch 4, step 604, the loss is 276795.46875\n",
            "in training loop, epoch 4, step 605, the loss is 202067.453125\n",
            "in training loop, epoch 4, step 606, the loss is 198153.671875\n",
            "in training loop, epoch 4, step 607, the loss is 151611.953125\n",
            "in training loop, epoch 4, step 608, the loss is 220265.6875\n",
            "in training loop, epoch 4, step 609, the loss is 304021.1875\n",
            "in training loop, epoch 4, step 610, the loss is 241990.578125\n",
            "in training loop, epoch 4, step 611, the loss is 186583.9375\n",
            "in training loop, epoch 4, step 612, the loss is 236969.828125\n",
            "in training loop, epoch 4, step 613, the loss is 195632.0625\n",
            "in training loop, epoch 4, step 614, the loss is 199374.9375\n",
            "in training loop, epoch 4, step 615, the loss is 219549.90625\n",
            "in training loop, epoch 4, step 616, the loss is 230325.984375\n",
            "in training loop, epoch 4, step 617, the loss is 215858.6875\n",
            "in training loop, epoch 4, step 618, the loss is 203280.125\n",
            "in training loop, epoch 4, step 619, the loss is 189198.3125\n",
            "in training loop, epoch 4, step 620, the loss is 225042.265625\n",
            "in training loop, epoch 4, step 621, the loss is 219361.15625\n",
            "in training loop, epoch 4, step 622, the loss is 160952.953125\n",
            "in training loop, epoch 4, step 623, the loss is 176028.734375\n",
            "in training loop, epoch 4, step 624, the loss is 247162.984375\n",
            "in training loop, epoch 4, step 625, the loss is 209248.96875\n",
            "in training loop, epoch 4, step 626, the loss is 222798.046875\n",
            "in training loop, epoch 4, step 627, the loss is 208859.1875\n",
            "in training loop, epoch 4, step 628, the loss is 235107.234375\n",
            "in training loop, epoch 4, step 629, the loss is 179111.15625\n",
            "in training loop, epoch 4, step 630, the loss is 189227.875\n",
            "in training loop, epoch 4, step 631, the loss is 168240.0625\n",
            "in training loop, epoch 4, step 632, the loss is 177763.578125\n",
            "in training loop, epoch 4, step 633, the loss is 197511.78125\n",
            "in training loop, epoch 4, step 634, the loss is 221272.59375\n",
            "in training loop, epoch 4, step 635, the loss is 313522.40625\n",
            "in training loop, epoch 4, step 636, the loss is 267186.3125\n",
            "in training loop, epoch 4, step 637, the loss is 198287.125\n",
            "in training loop, epoch 4, step 638, the loss is 180998.984375\n",
            "in training loop, epoch 4, step 639, the loss is 162472.46875\n",
            "in training loop, epoch 4, step 640, the loss is 220167.359375\n",
            "in training loop, epoch 4, step 641, the loss is 214556.625\n",
            "in training loop, epoch 4, step 642, the loss is 230446.53125\n",
            "in training loop, epoch 4, step 643, the loss is 202666.359375\n",
            "in training loop, epoch 4, step 644, the loss is 207845.390625\n",
            "in training loop, epoch 4, step 645, the loss is 319376.0\n",
            "in training loop, epoch 4, step 646, the loss is 200837.078125\n",
            "in training loop, epoch 4, step 647, the loss is 188208.578125\n",
            "in training loop, epoch 4, step 648, the loss is 216334.28125\n",
            "in training loop, epoch 4, step 649, the loss is 221175.125\n",
            "in training loop, epoch 4, step 650, the loss is 182188.1875\n",
            "in training loop, epoch 4, step 651, the loss is 277523.25\n",
            "in training loop, epoch 4, step 652, the loss is 167905.734375\n",
            "in training loop, epoch 4, step 653, the loss is 199544.375\n",
            "in training loop, epoch 4, step 654, the loss is 170550.84375\n",
            "in training loop, epoch 4, step 655, the loss is 219402.796875\n",
            "in training loop, epoch 4, step 656, the loss is 185310.375\n",
            "in training loop, epoch 4, step 657, the loss is 158582.09375\n",
            "in training loop, epoch 4, step 658, the loss is 248865.640625\n",
            "in training loop, epoch 4, step 659, the loss is 249390.625\n",
            "in training loop, epoch 4, step 660, the loss is 211647.71875\n",
            "in training loop, epoch 4, step 661, the loss is 165165.71875\n",
            "in training loop, epoch 4, step 662, the loss is 183076.0625\n",
            "in training loop, epoch 4, step 663, the loss is 178504.046875\n",
            "in training loop, epoch 4, step 664, the loss is 438042.09375\n",
            "in training loop, epoch 4, step 665, the loss is 235839.65625\n",
            "in training loop, epoch 4, step 666, the loss is 515837.9375\n",
            "in training loop, epoch 4, step 667, the loss is 230386.0\n",
            "in training loop, epoch 4, step 668, the loss is 195947.296875\n",
            "in training loop, epoch 4, step 669, the loss is 280928.59375\n",
            "in training loop, epoch 4, step 670, the loss is 293678.03125\n",
            "in training loop, epoch 4, step 671, the loss is 344819.6875\n",
            "in training loop, epoch 4, step 672, the loss is 298836.28125\n",
            "in training loop, epoch 4, step 673, the loss is 189240.09375\n",
            "in training loop, epoch 4, step 674, the loss is 227103.0625\n",
            "in training loop, epoch 4, step 675, the loss is 191099.9375\n",
            "in training loop, epoch 4, step 676, the loss is 247158.5625\n",
            "in training loop, epoch 4, step 677, the loss is 236862.515625\n",
            "in training loop, epoch 4, step 678, the loss is 283351.5\n",
            "in training loop, epoch 4, step 679, the loss is 204921.671875\n",
            "in training loop, epoch 4, step 680, the loss is 241835.78125\n",
            "in training loop, epoch 4, step 681, the loss is 189847.796875\n",
            "in training loop, epoch 4, step 682, the loss is 260315.953125\n",
            "in training loop, epoch 4, step 683, the loss is 221223.1875\n",
            "in training loop, epoch 4, step 684, the loss is 240113.484375\n",
            "in training loop, epoch 4, step 685, the loss is 232573.46875\n",
            "in training loop, epoch 4, step 686, the loss is 179721.359375\n",
            "in training loop, epoch 4, step 687, the loss is 153497.4375\n",
            "in training loop, epoch 4, step 688, the loss is 200430.765625\n",
            "in training loop, epoch 4, step 689, the loss is 209738.625\n",
            "in training loop, epoch 4, step 690, the loss is 271767.09375\n",
            "in training loop, epoch 4, step 691, the loss is 225506.53125\n",
            "in training loop, epoch 4, step 692, the loss is 377086.75\n",
            "in training loop, epoch 4, step 693, the loss is 230619.078125\n",
            "in training loop, epoch 4, step 694, the loss is 178374.90625\n",
            "in training loop, epoch 4, step 695, the loss is 199357.859375\n",
            "in training loop, epoch 4, step 696, the loss is 243427.015625\n",
            "in training loop, epoch 4, step 697, the loss is 196380.859375\n",
            "in training loop, epoch 4, step 698, the loss is 314380.53125\n",
            "in training loop, epoch 4, step 699, the loss is 266152.90625\n",
            "in training loop, epoch 4, step 700, the loss is 136058.609375\n",
            "in training loop, epoch 4, step 701, the loss is 269497.8125\n",
            "in training loop, epoch 4, step 702, the loss is 170480.359375\n",
            "in training loop, epoch 4, step 703, the loss is 217902.65625\n",
            "in training loop, epoch 4, step 704, the loss is 187667.234375\n",
            "in training loop, epoch 4, step 705, the loss is 176232.8125\n",
            "in training loop, epoch 4, step 706, the loss is 210299.796875\n",
            "in training loop, epoch 4, step 707, the loss is 191361.625\n",
            "in training loop, epoch 4, step 708, the loss is 208468.140625\n",
            "in training loop, epoch 4, step 709, the loss is 203100.703125\n",
            "in training loop, epoch 4, step 710, the loss is 178456.9375\n",
            "in training loop, epoch 4, step 711, the loss is 242647.671875\n",
            "in training loop, epoch 4, step 712, the loss is 198010.28125\n",
            "in training loop, epoch 4, step 713, the loss is 221685.328125\n",
            "in training loop, epoch 4, step 714, the loss is 173573.515625\n",
            "in training loop, epoch 4, step 715, the loss is 226983.59375\n",
            "in training loop, epoch 4, step 716, the loss is 229060.90625\n",
            "in training loop, epoch 4, step 717, the loss is 231625.4375\n",
            "in training loop, epoch 4, step 718, the loss is 228950.6875\n",
            "in training loop, epoch 4, step 719, the loss is 178519.4375\n",
            "in training loop, epoch 4, step 720, the loss is 185023.0\n",
            "in training loop, epoch 4, step 721, the loss is 180408.109375\n",
            "in training loop, epoch 4, step 722, the loss is 264695.125\n",
            "in training loop, epoch 4, step 723, the loss is 330005.78125\n",
            "in training loop, epoch 4, step 724, the loss is 252273.65625\n",
            "in training loop, epoch 4, step 725, the loss is 216443.890625\n",
            "in training loop, epoch 4, step 726, the loss is 210379.75\n",
            "in training loop, epoch 4, step 727, the loss is 306859.34375\n",
            "in training loop, epoch 4, step 728, the loss is 284381.375\n",
            "in training loop, epoch 4, step 729, the loss is 284027.25\n",
            "in training loop, epoch 4, step 730, the loss is 341736.84375\n",
            "in training loop, epoch 4, step 731, the loss is 267557.9375\n",
            "in training loop, epoch 4, step 732, the loss is 246869.1875\n",
            "in training loop, epoch 4, step 733, the loss is 284801.40625\n",
            "in training loop, epoch 4, step 734, the loss is 410455.0\n",
            "in training loop, epoch 4, step 735, the loss is 248324.484375\n",
            "in training loop, epoch 4, step 736, the loss is 259085.75\n",
            "in training loop, epoch 4, step 737, the loss is 179436.625\n",
            "in training loop, epoch 4, step 738, the loss is 256589.0\n",
            "in training loop, epoch 4, step 739, the loss is 274039.875\n",
            "in training loop, epoch 4, step 740, the loss is 320723.46875\n",
            "in training loop, epoch 4, step 741, the loss is 213783.515625\n",
            "in training loop, epoch 4, step 742, the loss is 258649.375\n",
            "in training loop, epoch 4, step 743, the loss is 171253.375\n",
            "in training loop, epoch 4, step 744, the loss is 172748.734375\n",
            "in training loop, epoch 4, step 745, the loss is 231514.8125\n",
            "in training loop, epoch 4, step 746, the loss is 218395.6875\n",
            "in training loop, epoch 4, step 747, the loss is 393933.96875\n",
            "in training loop, epoch 4, step 748, the loss is 192809.609375\n",
            "in training loop, epoch 4, step 749, the loss is 251226.5625\n",
            "in training loop, epoch 4, step 750, the loss is 133845.890625\n",
            "in training loop, epoch 4, step 751, the loss is 223279.546875\n",
            "in training loop, epoch 4, step 752, the loss is 234266.71875\n",
            "in training loop, epoch 4, step 753, the loss is 230914.90625\n",
            "in training loop, epoch 4, step 754, the loss is 209893.203125\n",
            "in training loop, epoch 4, step 755, the loss is 199165.109375\n",
            "in training loop, epoch 4, step 756, the loss is 174826.671875\n",
            "in training loop, epoch 4, step 757, the loss is 232535.78125\n",
            "in training loop, epoch 4, step 758, the loss is 201759.3125\n",
            "in training loop, epoch 4, step 759, the loss is 179822.15625\n",
            "in training loop, epoch 4, step 760, the loss is 144631.125\n",
            "in training loop, epoch 4, step 761, the loss is 155556.265625\n",
            "in training loop, epoch 4, step 762, the loss is 156476.34375\n",
            "in training loop, epoch 4, step 763, the loss is 266549.03125\n",
            "in training loop, epoch 4, step 764, the loss is 178029.4375\n",
            "in training loop, epoch 4, step 765, the loss is 248552.125\n",
            "in training loop, epoch 4, step 766, the loss is 199643.734375\n",
            "in training loop, epoch 4, step 767, the loss is 215880.78125\n",
            "in training loop, epoch 4, step 768, the loss is 158099.265625\n",
            "in training loop, epoch 4, step 769, the loss is 256457.109375\n",
            "in training loop, epoch 4, step 770, the loss is 175724.28125\n",
            "in training loop, epoch 4, step 771, the loss is 169767.375\n",
            "in training loop, epoch 4, step 772, the loss is 215471.75\n",
            "in training loop, epoch 4, step 773, the loss is 218559.484375\n",
            "in training loop, epoch 4, step 774, the loss is 187565.625\n",
            "in training loop, epoch 4, step 775, the loss is 174979.015625\n",
            "in training loop, epoch 4, step 776, the loss is 253078.90625\n",
            "in training loop, epoch 4, step 777, the loss is 274002.59375\n",
            "in training loop, epoch 4, step 778, the loss is 159063.4375\n",
            "in training loop, epoch 4, step 779, the loss is 292654.125\n",
            "in training loop, epoch 4, step 780, the loss is 170316.703125\n",
            "in training loop, epoch 4, step 781, the loss is 190616.8125\n",
            "in training loop, epoch 4, step 782, the loss is 200005.625\n",
            "in training loop, epoch 4, step 783, the loss is 219181.1875\n",
            "in training loop, epoch 4, step 784, the loss is 202223.984375\n",
            "in training loop, epoch 4, step 785, the loss is 306064.5\n",
            "in training loop, epoch 4, step 786, the loss is 230301.75\n",
            "in training loop, epoch 4, step 787, the loss is 166682.40625\n",
            "in training loop, epoch 4, step 788, the loss is 206127.5625\n",
            "in training loop, epoch 4, step 789, the loss is 204476.4375\n",
            "in training loop, epoch 4, step 790, the loss is 176606.859375\n",
            "in training loop, epoch 4, step 791, the loss is 221547.203125\n",
            "in training loop, epoch 4, step 792, the loss is 252997.5\n",
            "in training loop, epoch 4, step 793, the loss is 225484.671875\n",
            "in training loop, epoch 4, step 794, the loss is 326137.90625\n",
            "in training loop, epoch 4, step 795, the loss is 179431.171875\n",
            "in training loop, epoch 4, step 796, the loss is 162420.4375\n",
            "in training loop, epoch 4, step 797, the loss is 181396.578125\n",
            "in training loop, epoch 4, step 798, the loss is 185198.40625\n",
            "in training loop, epoch 4, step 799, the loss is 139893.921875\n",
            "in training loop, epoch 4, step 800, the loss is 270059.90625\n",
            "in training loop, epoch 4, step 801, the loss is 221344.125\n",
            "in training loop, epoch 4, step 802, the loss is 239017.171875\n",
            "in training loop, epoch 4, step 803, the loss is 176304.0\n",
            "in training loop, epoch 4, step 804, the loss is 212394.015625\n",
            "in training loop, epoch 4, step 805, the loss is 185366.71875\n",
            "in training loop, epoch 4, step 806, the loss is 205783.859375\n",
            "in training loop, epoch 4, step 807, the loss is 187833.09375\n",
            "in training loop, epoch 4, step 808, the loss is 216889.140625\n",
            "in training loop, epoch 4, step 809, the loss is 230724.671875\n",
            "in training loop, epoch 4, step 810, the loss is 140444.8125\n",
            "in training loop, epoch 4, step 811, the loss is 238532.203125\n",
            "in training loop, epoch 4, step 812, the loss is 197137.375\n",
            "in training loop, epoch 4, step 813, the loss is 145738.140625\n",
            "in training loop, epoch 4, step 814, the loss is 251224.21875\n",
            "in training loop, epoch 4, step 815, the loss is 208011.734375\n",
            "in training loop, epoch 4, step 816, the loss is 196038.5625\n",
            "in training loop, epoch 4, step 817, the loss is 169260.265625\n",
            "in training loop, epoch 4, step 818, the loss is 201067.59375\n",
            "in training loop, epoch 4, step 819, the loss is 239574.15625\n",
            "in training loop, epoch 4, step 820, the loss is 212628.28125\n",
            "in training loop, epoch 4, step 821, the loss is 190780.296875\n",
            "in training loop, epoch 4, step 822, the loss is 159849.25\n",
            "in training loop, epoch 4, step 823, the loss is 225471.828125\n",
            "in training loop, epoch 4, step 824, the loss is 164943.625\n",
            "in training loop, epoch 4, step 825, the loss is 180544.15625\n",
            "in training loop, epoch 4, step 826, the loss is 244723.625\n",
            "in training loop, epoch 4, step 827, the loss is 161286.46875\n",
            "in training loop, epoch 4, step 828, the loss is 187494.203125\n",
            "in training loop, epoch 4, step 829, the loss is 227742.390625\n",
            "in training loop, epoch 4, step 830, the loss is 200225.90625\n",
            "in training loop, epoch 4, step 831, the loss is 186273.6875\n",
            "in training loop, epoch 4, step 832, the loss is 179245.546875\n",
            "in training loop, epoch 4, step 833, the loss is 170362.640625\n",
            "in training loop, epoch 4, step 834, the loss is 194211.609375\n",
            "in training loop, epoch 4, step 835, the loss is 155721.875\n",
            "in training loop, epoch 4, step 836, the loss is 185250.828125\n",
            "in training loop, epoch 4, step 837, the loss is 197157.4375\n",
            "in training loop, epoch 4, step 838, the loss is 202038.75\n",
            "in training loop, epoch 4, step 839, the loss is 145589.671875\n",
            "in training loop, epoch 4, step 840, the loss is 188526.90625\n",
            "in training loop, epoch 4, step 841, the loss is 224646.84375\n",
            "in training loop, epoch 4, step 842, the loss is 166112.6875\n",
            "in training loop, epoch 4, step 843, the loss is 167509.65625\n",
            "in training loop, epoch 4, step 844, the loss is 173410.375\n",
            "in training loop, epoch 4, step 845, the loss is 308590.5\n",
            "in training loop, epoch 4, step 846, the loss is 187536.59375\n",
            "in training loop, epoch 4, step 847, the loss is 156139.921875\n",
            "in training loop, epoch 4, step 848, the loss is 218162.25\n",
            "in training loop, epoch 4, step 849, the loss is 195466.0\n",
            "in training loop, epoch 4, step 850, the loss is 245856.515625\n",
            "in training loop, epoch 4, step 851, the loss is 224959.125\n",
            "in training loop, epoch 4, step 852, the loss is 189187.59375\n",
            "in training loop, epoch 4, step 853, the loss is 219017.828125\n",
            "in training loop, epoch 4, step 854, the loss is 223425.921875\n",
            "in training loop, epoch 4, step 855, the loss is 231652.28125\n",
            "in training loop, epoch 4, step 856, the loss is 281377.5625\n",
            "in training loop, epoch 4, step 857, the loss is 214042.46875\n",
            "in training loop, epoch 4, step 858, the loss is 166467.34375\n",
            "in training loop, epoch 4, step 859, the loss is 227735.625\n",
            "in training loop, epoch 4, step 860, the loss is 285442.0\n",
            "in training loop, epoch 4, step 861, the loss is 243092.765625\n",
            "in training loop, epoch 4, step 862, the loss is 236353.25\n",
            "in training loop, epoch 4, step 863, the loss is 184000.53125\n",
            "in training loop, epoch 4, step 864, the loss is 150624.46875\n",
            "in training loop, epoch 4, step 865, the loss is 259086.015625\n",
            "in training loop, epoch 4, step 866, the loss is 374990.96875\n",
            "in training loop, epoch 4, step 867, the loss is 292783.0\n",
            "in training loop, epoch 4, step 868, the loss is 268463.0625\n",
            "in training loop, epoch 4, step 869, the loss is 265111.0\n",
            "in training loop, epoch 4, step 870, the loss is 196008.375\n",
            "in training loop, epoch 4, step 871, the loss is 285571.25\n",
            "in training loop, epoch 4, step 872, the loss is 381123.9375\n",
            "in training loop, epoch 4, step 873, the loss is 231496.3125\n",
            "in training loop, epoch 4, step 874, the loss is 214649.53125\n",
            "in training loop, epoch 4, step 875, the loss is 196560.328125\n",
            "in training loop, epoch 4, step 876, the loss is 303379.25\n",
            "in training loop, epoch 4, step 877, the loss is 225970.015625\n",
            "in training loop, epoch 4, step 878, the loss is 236104.15625\n",
            "in training loop, epoch 4, step 879, the loss is 190831.703125\n",
            "in training loop, epoch 4, step 880, the loss is 221702.453125\n",
            "in training loop, epoch 4, step 881, the loss is 207775.09375\n",
            "in training loop, epoch 4, step 882, the loss is 188540.359375\n",
            "in training loop, epoch 4, step 883, the loss is 309810.53125\n",
            "in training loop, epoch 4, step 884, the loss is 207093.875\n",
            "in training loop, epoch 4, step 885, the loss is 275235.71875\n",
            "in training loop, epoch 4, step 886, the loss is 181510.65625\n",
            "in training loop, epoch 4, step 887, the loss is 215479.328125\n",
            "in training loop, epoch 4, step 888, the loss is 172643.125\n",
            "in training loop, epoch 4, step 889, the loss is 238095.140625\n",
            "in training loop, epoch 4, step 890, the loss is 216487.53125\n",
            "in training loop, epoch 4, step 891, the loss is 183300.453125\n",
            "in training loop, epoch 4, step 892, the loss is 224381.40625\n",
            "in training loop, epoch 4, step 893, the loss is 177168.453125\n",
            "in training loop, epoch 4, step 894, the loss is 166979.625\n",
            "in training loop, epoch 4, step 895, the loss is 186397.203125\n",
            "in training loop, epoch 4, step 896, the loss is 203899.359375\n",
            "in training loop, epoch 4, step 897, the loss is 255770.671875\n",
            "in training loop, epoch 4, step 898, the loss is 210449.78125\n",
            "in training loop, epoch 4, step 899, the loss is 240807.21875\n",
            "in training loop, epoch 4, step 900, the loss is 133061.546875\n",
            "in training loop, epoch 4, step 901, the loss is 305754.03125\n",
            "in training loop, epoch 4, step 902, the loss is 208964.875\n",
            "in training loop, epoch 4, step 903, the loss is 107342.65625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "k-fold 0:: Epoch 4: train loss 220462.36599488385 val loss 531802.0779702971\n",
            "At fold 0 with train_loss:\n",
            "Before doing mean loss list calc\n",
            "FOLD 1\n",
            "--------------------------------\n",
            "length of of train_loader is 904 & length of traindataset is 2008\n",
            "length of of test_loader is 101\n",
            "length of of val_loader is 1004\n",
            "in training loop, epoch 1, step 0, the loss is 158597.421875\n",
            "in training loop, epoch 1, step 1, the loss is 264847.625\n",
            "in training loop, epoch 1, step 2, the loss is 165464.15625\n",
            "in training loop, epoch 1, step 3, the loss is 405449.8125\n",
            "in training loop, epoch 1, step 4, the loss is 207159.53125\n",
            "in training loop, epoch 1, step 5, the loss is 394962.46875\n",
            "in training loop, epoch 1, step 6, the loss is 190601.390625\n",
            "in training loop, epoch 1, step 7, the loss is 179462.0625\n",
            "in training loop, epoch 1, step 8, the loss is 286324.8125\n",
            "in training loop, epoch 1, step 9, the loss is 199168.3125\n",
            "in training loop, epoch 1, step 10, the loss is 197540.09375\n",
            "in training loop, epoch 1, step 11, the loss is 226115.96875\n",
            "in training loop, epoch 1, step 12, the loss is 258652.84375\n",
            "in training loop, epoch 1, step 13, the loss is 170538.515625\n",
            "in training loop, epoch 1, step 14, the loss is 251365.546875\n",
            "in training loop, epoch 1, step 15, the loss is 237251.046875\n",
            "in training loop, epoch 1, step 16, the loss is 164775.8125\n",
            "in training loop, epoch 1, step 17, the loss is 190038.375\n",
            "in training loop, epoch 1, step 18, the loss is 163213.484375\n",
            "in training loop, epoch 1, step 19, the loss is 260235.8125\n",
            "in training loop, epoch 1, step 20, the loss is 144505.59375\n",
            "in training loop, epoch 1, step 21, the loss is 408086.71875\n",
            "in training loop, epoch 1, step 22, the loss is 160455.125\n",
            "in training loop, epoch 1, step 23, the loss is 190040.78125\n",
            "in training loop, epoch 1, step 24, the loss is 171601.15625\n",
            "in training loop, epoch 1, step 25, the loss is 174109.515625\n",
            "in training loop, epoch 1, step 26, the loss is 247052.09375\n",
            "in training loop, epoch 1, step 27, the loss is 264583.75\n",
            "in training loop, epoch 1, step 28, the loss is 237687.1875\n",
            "in training loop, epoch 1, step 29, the loss is 159270.21875\n",
            "in training loop, epoch 1, step 30, the loss is 174225.59375\n",
            "in training loop, epoch 1, step 31, the loss is 386457.84375\n",
            "in training loop, epoch 1, step 32, the loss is 172378.1875\n",
            "in training loop, epoch 1, step 33, the loss is 179723.828125\n",
            "in training loop, epoch 1, step 34, the loss is 166846.625\n",
            "in training loop, epoch 1, step 35, the loss is 174472.796875\n",
            "in training loop, epoch 1, step 36, the loss is 150973.71875\n",
            "in training loop, epoch 1, step 37, the loss is 194575.0\n",
            "in training loop, epoch 1, step 38, the loss is 113413.5234375\n",
            "in training loop, epoch 1, step 39, the loss is 135569.0625\n",
            "in training loop, epoch 1, step 40, the loss is 358681.0\n",
            "in training loop, epoch 1, step 41, the loss is 119789.71875\n",
            "in training loop, epoch 1, step 42, the loss is 424707.3125\n",
            "in training loop, epoch 1, step 43, the loss is 379349.03125\n",
            "in training loop, epoch 1, step 44, the loss is 172242.75\n",
            "in training loop, epoch 1, step 45, the loss is 113773.5625\n",
            "in training loop, epoch 1, step 46, the loss is 128001.875\n",
            "in training loop, epoch 1, step 47, the loss is 171507.078125\n",
            "in training loop, epoch 1, step 48, the loss is 145938.34375\n",
            "in training loop, epoch 1, step 49, the loss is 178922.15625\n",
            "in training loop, epoch 1, step 50, the loss is 180450.25\n",
            "in training loop, epoch 1, step 51, the loss is 164012.796875\n",
            "in training loop, epoch 1, step 52, the loss is 133078.46875\n",
            "in training loop, epoch 1, step 53, the loss is 196251.390625\n",
            "in training loop, epoch 1, step 54, the loss is 126931.5\n",
            "in training loop, epoch 1, step 55, the loss is 134322.046875\n",
            "in training loop, epoch 1, step 56, the loss is 268730.28125\n",
            "in training loop, epoch 1, step 57, the loss is 118925.7109375\n",
            "in training loop, epoch 1, step 58, the loss is 307774.125\n",
            "in training loop, epoch 1, step 59, the loss is 148558.9375\n",
            "in training loop, epoch 1, step 60, the loss is 501756.78125\n",
            "in training loop, epoch 1, step 61, the loss is 174273.359375\n",
            "in training loop, epoch 1, step 62, the loss is 193726.28125\n",
            "in training loop, epoch 1, step 63, the loss is 430652.375\n",
            "in training loop, epoch 1, step 64, the loss is 131450.078125\n",
            "in training loop, epoch 1, step 65, the loss is 155873.3125\n",
            "in training loop, epoch 1, step 66, the loss is 369111.125\n",
            "in training loop, epoch 1, step 67, the loss is 237195.328125\n",
            "in training loop, epoch 1, step 68, the loss is 567768.0625\n",
            "in training loop, epoch 1, step 69, the loss is 166796.9375\n",
            "in training loop, epoch 1, step 70, the loss is 172005.8125\n",
            "in training loop, epoch 1, step 71, the loss is 203526.640625\n",
            "in training loop, epoch 1, step 72, the loss is 194370.53125\n",
            "in training loop, epoch 1, step 73, the loss is 194111.328125\n",
            "in training loop, epoch 1, step 74, the loss is 197157.1875\n",
            "in training loop, epoch 1, step 75, the loss is 141152.078125\n",
            "in training loop, epoch 1, step 76, the loss is 216301.015625\n",
            "in training loop, epoch 1, step 77, the loss is 486399.8125\n",
            "in training loop, epoch 1, step 78, the loss is 161245.796875\n",
            "in training loop, epoch 1, step 79, the loss is 373869.375\n",
            "in training loop, epoch 1, step 80, the loss is 209541.953125\n",
            "in training loop, epoch 1, step 81, the loss is 143702.390625\n",
            "in training loop, epoch 1, step 82, the loss is 148563.125\n",
            "in training loop, epoch 1, step 83, the loss is 360895.8125\n",
            "in training loop, epoch 1, step 84, the loss is 266770.1875\n",
            "in training loop, epoch 1, step 85, the loss is 191510.484375\n",
            "in training loop, epoch 1, step 86, the loss is 147850.328125\n",
            "in training loop, epoch 1, step 87, the loss is 130985.9609375\n",
            "in training loop, epoch 1, step 88, the loss is 324263.5625\n",
            "in training loop, epoch 1, step 89, the loss is 349498.40625\n",
            "in training loop, epoch 1, step 90, the loss is 152397.984375\n",
            "in training loop, epoch 1, step 91, the loss is 515501.0\n",
            "in training loop, epoch 1, step 92, the loss is 150411.3125\n",
            "in training loop, epoch 1, step 93, the loss is 172796.328125\n",
            "in training loop, epoch 1, step 94, the loss is 141541.0\n",
            "in training loop, epoch 1, step 95, the loss is 181262.0625\n",
            "in training loop, epoch 1, step 96, the loss is 364281.0\n",
            "in training loop, epoch 1, step 97, the loss is 154073.6875\n",
            "in training loop, epoch 1, step 98, the loss is 418595.21875\n",
            "in training loop, epoch 1, step 99, the loss is 157938.84375\n",
            "in training loop, epoch 1, step 100, the loss is 169170.46875\n",
            "in training loop, epoch 1, step 101, the loss is 347787.96875\n",
            "in training loop, epoch 1, step 102, the loss is 182593.5\n",
            "in training loop, epoch 1, step 103, the loss is 205354.46875\n",
            "in training loop, epoch 1, step 104, the loss is 179191.375\n",
            "in training loop, epoch 1, step 105, the loss is 146058.59375\n",
            "in training loop, epoch 1, step 106, the loss is 158171.8125\n",
            "in training loop, epoch 1, step 107, the loss is 463427.09375\n",
            "in training loop, epoch 1, step 108, the loss is 146641.5625\n",
            "in training loop, epoch 1, step 109, the loss is 260782.296875\n",
            "in training loop, epoch 1, step 110, the loss is 134634.765625\n",
            "in training loop, epoch 1, step 111, the loss is 162011.71875\n",
            "in training loop, epoch 1, step 112, the loss is 231767.03125\n",
            "in training loop, epoch 1, step 113, the loss is 144778.1875\n",
            "in training loop, epoch 1, step 114, the loss is 223245.90625\n",
            "in training loop, epoch 1, step 115, the loss is 201987.28125\n",
            "in training loop, epoch 1, step 116, the loss is 127832.0078125\n",
            "in training loop, epoch 1, step 117, the loss is 189994.46875\n",
            "in training loop, epoch 1, step 118, the loss is 136308.328125\n",
            "in training loop, epoch 1, step 119, the loss is 198024.875\n",
            "in training loop, epoch 1, step 120, the loss is 356721.375\n",
            "in training loop, epoch 1, step 121, the loss is 132869.046875\n",
            "in training loop, epoch 1, step 122, the loss is 177746.875\n",
            "in training loop, epoch 1, step 123, the loss is 299883.78125\n",
            "in training loop, epoch 1, step 124, the loss is 174649.0\n",
            "in training loop, epoch 1, step 125, the loss is 130930.21875\n",
            "in training loop, epoch 1, step 126, the loss is 185858.1875\n",
            "in training loop, epoch 1, step 127, the loss is 135632.046875\n",
            "in training loop, epoch 1, step 128, the loss is 120892.46875\n",
            "in training loop, epoch 1, step 129, the loss is 484679.96875\n",
            "in training loop, epoch 1, step 130, the loss is 129652.09375\n",
            "in training loop, epoch 1, step 131, the loss is 126480.4140625\n",
            "in training loop, epoch 1, step 132, the loss is 131579.734375\n",
            "in training loop, epoch 1, step 133, the loss is 546718.4375\n",
            "in training loop, epoch 1, step 134, the loss is 161471.75\n",
            "in training loop, epoch 1, step 135, the loss is 174224.6875\n",
            "in training loop, epoch 1, step 136, the loss is 167824.34375\n",
            "in training loop, epoch 1, step 137, the loss is 142302.8125\n",
            "in training loop, epoch 1, step 138, the loss is 155511.53125\n",
            "in training loop, epoch 1, step 139, the loss is 120545.2578125\n",
            "in training loop, epoch 1, step 140, the loss is 389391.875\n",
            "in training loop, epoch 1, step 141, the loss is 271089.34375\n",
            "in training loop, epoch 1, step 142, the loss is 160375.890625\n",
            "in training loop, epoch 1, step 143, the loss is 193898.546875\n",
            "in training loop, epoch 1, step 144, the loss is 107849.859375\n",
            "in training loop, epoch 1, step 145, the loss is 179340.921875\n",
            "in training loop, epoch 1, step 146, the loss is 171989.015625\n",
            "in training loop, epoch 1, step 147, the loss is 134013.8125\n",
            "in training loop, epoch 1, step 148, the loss is 191638.3125\n",
            "in training loop, epoch 1, step 149, the loss is 167194.515625\n",
            "in training loop, epoch 1, step 150, the loss is 143704.5625\n",
            "in training loop, epoch 1, step 151, the loss is 140207.53125\n",
            "in training loop, epoch 1, step 152, the loss is 158585.296875\n",
            "in training loop, epoch 1, step 153, the loss is 193801.21875\n",
            "in training loop, epoch 1, step 154, the loss is 355011.46875\n",
            "in training loop, epoch 1, step 155, the loss is 160371.34375\n",
            "in training loop, epoch 1, step 156, the loss is 376630.84375\n",
            "in training loop, epoch 1, step 157, the loss is 276868.34375\n",
            "in training loop, epoch 1, step 158, the loss is 129470.734375\n",
            "in training loop, epoch 1, step 159, the loss is 182828.71875\n",
            "in training loop, epoch 1, step 160, the loss is 461262.0\n",
            "in training loop, epoch 1, step 161, the loss is 191694.09375\n",
            "in training loop, epoch 1, step 162, the loss is 156353.921875\n",
            "in training loop, epoch 1, step 163, the loss is 356628.53125\n",
            "in training loop, epoch 1, step 164, the loss is 178960.625\n",
            "in training loop, epoch 1, step 165, the loss is 175448.296875\n",
            "in training loop, epoch 1, step 166, the loss is 188978.75\n",
            "in training loop, epoch 1, step 167, the loss is 95800.28125\n",
            "in training loop, epoch 1, step 168, the loss is 153373.90625\n",
            "in training loop, epoch 1, step 169, the loss is 141467.515625\n",
            "in training loop, epoch 1, step 170, the loss is 158094.3125\n",
            "in training loop, epoch 1, step 171, the loss is 303826.78125\n",
            "in training loop, epoch 1, step 172, the loss is 154512.625\n",
            "in training loop, epoch 1, step 173, the loss is 181238.5625\n",
            "in training loop, epoch 1, step 174, the loss is 154288.84375\n",
            "in training loop, epoch 1, step 175, the loss is 118617.7578125\n",
            "in training loop, epoch 1, step 176, the loss is 150428.8125\n",
            "in training loop, epoch 1, step 177, the loss is 189668.984375\n",
            "in training loop, epoch 1, step 178, the loss is 272616.21875\n",
            "in training loop, epoch 1, step 179, the loss is 163269.84375\n",
            "in training loop, epoch 1, step 180, the loss is 150530.625\n",
            "in training loop, epoch 1, step 181, the loss is 137647.5\n",
            "in training loop, epoch 1, step 182, the loss is 156932.421875\n",
            "in training loop, epoch 1, step 183, the loss is 153896.4375\n",
            "in training loop, epoch 1, step 184, the loss is 132223.859375\n",
            "in training loop, epoch 1, step 185, the loss is 130403.84375\n",
            "in training loop, epoch 1, step 186, the loss is 212443.90625\n",
            "in training loop, epoch 1, step 187, the loss is 159841.0\n",
            "in training loop, epoch 1, step 188, the loss is 355790.125\n",
            "in training loop, epoch 1, step 189, the loss is 179519.75\n",
            "in training loop, epoch 1, step 190, the loss is 179663.28125\n",
            "in training loop, epoch 1, step 191, the loss is 379213.78125\n",
            "in training loop, epoch 1, step 192, the loss is 163686.90625\n",
            "in training loop, epoch 1, step 193, the loss is 186120.75\n",
            "in training loop, epoch 1, step 194, the loss is 208558.0625\n",
            "in training loop, epoch 1, step 195, the loss is 146542.75\n",
            "in training loop, epoch 1, step 196, the loss is 235978.40625\n",
            "in training loop, epoch 1, step 197, the loss is 158739.84375\n",
            "in training loop, epoch 1, step 198, the loss is 338804.59375\n",
            "in training loop, epoch 1, step 199, the loss is 500326.125\n",
            "in training loop, epoch 1, step 200, the loss is 402752.28125\n",
            "in training loop, epoch 1, step 201, the loss is 150097.71875\n",
            "in training loop, epoch 1, step 202, the loss is 230754.328125\n",
            "in training loop, epoch 1, step 203, the loss is 388951.25\n",
            "in training loop, epoch 1, step 204, the loss is 180343.390625\n",
            "in training loop, epoch 1, step 205, the loss is 188696.296875\n",
            "in training loop, epoch 1, step 206, the loss is 158094.21875\n",
            "in training loop, epoch 1, step 207, the loss is 174761.65625\n",
            "in training loop, epoch 1, step 208, the loss is 276705.375\n",
            "in training loop, epoch 1, step 209, the loss is 139487.875\n",
            "in training loop, epoch 1, step 210, the loss is 169796.1875\n",
            "in training loop, epoch 1, step 211, the loss is 171996.375\n",
            "in training loop, epoch 1, step 212, the loss is 150799.453125\n",
            "in training loop, epoch 1, step 213, the loss is 205745.375\n",
            "in training loop, epoch 1, step 214, the loss is 558985.875\n",
            "in training loop, epoch 1, step 215, the loss is 151212.453125\n",
            "in training loop, epoch 1, step 216, the loss is 154777.109375\n",
            "in training loop, epoch 1, step 217, the loss is 244651.796875\n",
            "in training loop, epoch 1, step 218, the loss is 220496.703125\n",
            "in training loop, epoch 1, step 219, the loss is 201447.34375\n",
            "in training loop, epoch 1, step 220, the loss is 452649.8125\n",
            "in training loop, epoch 1, step 221, the loss is 201077.15625\n",
            "in training loop, epoch 1, step 222, the loss is 265731.875\n",
            "in training loop, epoch 1, step 223, the loss is 139649.75\n",
            "in training loop, epoch 1, step 224, the loss is 155028.859375\n",
            "in training loop, epoch 1, step 225, the loss is 156307.640625\n",
            "in training loop, epoch 1, step 226, the loss is 173792.9375\n",
            "in training loop, epoch 1, step 227, the loss is 157507.34375\n",
            "in training loop, epoch 1, step 228, the loss is 280931.71875\n",
            "in training loop, epoch 1, step 229, the loss is 135706.40625\n",
            "in training loop, epoch 1, step 230, the loss is 144636.40625\n",
            "in training loop, epoch 1, step 231, the loss is 358824.53125\n",
            "in training loop, epoch 1, step 232, the loss is 334057.40625\n",
            "in training loop, epoch 1, step 233, the loss is 144002.4375\n",
            "in training loop, epoch 1, step 234, the loss is 153568.609375\n",
            "in training loop, epoch 1, step 235, the loss is 123351.671875\n",
            "in training loop, epoch 1, step 236, the loss is 131413.453125\n",
            "in training loop, epoch 1, step 237, the loss is 120270.5\n",
            "in training loop, epoch 1, step 238, the loss is 235821.15625\n",
            "in training loop, epoch 1, step 239, the loss is 153548.09375\n",
            "in training loop, epoch 1, step 240, the loss is 173058.390625\n",
            "in training loop, epoch 1, step 241, the loss is 159774.59375\n",
            "in training loop, epoch 1, step 242, the loss is 185914.3125\n",
            "in training loop, epoch 1, step 243, the loss is 214380.5625\n",
            "in training loop, epoch 1, step 244, the loss is 130789.15625\n",
            "in training loop, epoch 1, step 245, the loss is 126032.78125\n",
            "in training loop, epoch 1, step 246, the loss is 143049.71875\n",
            "in training loop, epoch 1, step 247, the loss is 130074.671875\n",
            "in training loop, epoch 1, step 248, the loss is 168388.59375\n",
            "in training loop, epoch 1, step 249, the loss is 425330.125\n",
            "in training loop, epoch 1, step 250, the loss is 155106.203125\n",
            "in training loop, epoch 1, step 251, the loss is 171564.78125\n",
            "in training loop, epoch 1, step 252, the loss is 148690.4375\n",
            "in training loop, epoch 1, step 253, the loss is 493477.6875\n",
            "in training loop, epoch 1, step 254, the loss is 303423.5\n",
            "in training loop, epoch 1, step 255, the loss is 184774.78125\n",
            "in training loop, epoch 1, step 256, the loss is 246082.734375\n",
            "in training loop, epoch 1, step 257, the loss is 218229.90625\n",
            "in training loop, epoch 1, step 258, the loss is 395551.34375\n",
            "in training loop, epoch 1, step 259, the loss is 441269.875\n",
            "in training loop, epoch 1, step 260, the loss is 216416.59375\n",
            "in training loop, epoch 1, step 261, the loss is 166517.890625\n",
            "in training loop, epoch 1, step 262, the loss is 240244.6875\n",
            "in training loop, epoch 1, step 263, the loss is 380848.15625\n",
            "in training loop, epoch 1, step 264, the loss is 491433.125\n",
            "in training loop, epoch 1, step 265, the loss is 194877.09375\n",
            "in training loop, epoch 1, step 266, the loss is 298939.875\n",
            "in training loop, epoch 1, step 267, the loss is 196690.15625\n",
            "in training loop, epoch 1, step 268, the loss is 375959.625\n",
            "in training loop, epoch 1, step 269, the loss is 428792.28125\n",
            "in training loop, epoch 1, step 270, the loss is 138971.625\n",
            "in training loop, epoch 1, step 271, the loss is 194366.75\n",
            "in training loop, epoch 1, step 272, the loss is 294069.25\n",
            "in training loop, epoch 1, step 273, the loss is 292411.15625\n",
            "in training loop, epoch 1, step 274, the loss is 241493.796875\n",
            "in training loop, epoch 1, step 275, the loss is 206790.953125\n",
            "in training loop, epoch 1, step 276, the loss is 285030.34375\n",
            "in training loop, epoch 1, step 277, the loss is 419580.875\n",
            "in training loop, epoch 1, step 278, the loss is 265493.875\n",
            "in training loop, epoch 1, step 279, the loss is 260158.421875\n",
            "in training loop, epoch 1, step 280, the loss is 517776.875\n",
            "in training loop, epoch 1, step 281, the loss is 242482.71875\n",
            "in training loop, epoch 1, step 282, the loss is 172585.734375\n",
            "in training loop, epoch 1, step 283, the loss is 235453.0\n",
            "in training loop, epoch 1, step 284, the loss is 217760.28125\n",
            "in training loop, epoch 1, step 285, the loss is 238187.21875\n",
            "in training loop, epoch 1, step 286, the loss is 301741.75\n",
            "in training loop, epoch 1, step 287, the loss is 194866.9375\n",
            "in training loop, epoch 1, step 288, the loss is 266637.25\n",
            "in training loop, epoch 1, step 289, the loss is 202570.84375\n",
            "in training loop, epoch 1, step 290, the loss is 229735.796875\n",
            "in training loop, epoch 1, step 291, the loss is 207160.65625\n",
            "in training loop, epoch 1, step 292, the loss is 353065.75\n",
            "in training loop, epoch 1, step 293, the loss is 360676.8125\n",
            "in training loop, epoch 1, step 294, the loss is 186631.890625\n",
            "in training loop, epoch 1, step 295, the loss is 218704.375\n",
            "in training loop, epoch 1, step 296, the loss is 153890.6875\n",
            "in training loop, epoch 1, step 297, the loss is 341487.125\n",
            "in training loop, epoch 1, step 298, the loss is 646164.3125\n",
            "in training loop, epoch 1, step 299, the loss is 260437.0\n",
            "in training loop, epoch 1, step 300, the loss is 265909.0\n",
            "in training loop, epoch 1, step 301, the loss is 229879.984375\n",
            "in training loop, epoch 1, step 302, the loss is 195242.796875\n",
            "in training loop, epoch 1, step 303, the loss is 309152.03125\n",
            "in training loop, epoch 1, step 304, the loss is 318425.8125\n",
            "in training loop, epoch 1, step 305, the loss is 289986.71875\n",
            "in training loop, epoch 1, step 306, the loss is 294899.09375\n",
            "in training loop, epoch 1, step 307, the loss is 221184.28125\n",
            "in training loop, epoch 1, step 308, the loss is 197864.78125\n",
            "in training loop, epoch 1, step 309, the loss is 250755.234375\n",
            "in training loop, epoch 1, step 310, the loss is 252536.25\n",
            "in training loop, epoch 1, step 311, the loss is 250645.34375\n",
            "in training loop, epoch 1, step 312, the loss is 263986.875\n",
            "in training loop, epoch 1, step 313, the loss is 488035.375\n",
            "in training loop, epoch 1, step 314, the loss is 838172.375\n",
            "in training loop, epoch 1, step 315, the loss is 194471.734375\n",
            "in training loop, epoch 1, step 316, the loss is 441776.15625\n",
            "in training loop, epoch 1, step 317, the loss is 188580.359375\n",
            "in training loop, epoch 1, step 318, the loss is 434261.78125\n",
            "in training loop, epoch 1, step 319, the loss is 215361.625\n",
            "in training loop, epoch 1, step 320, the loss is 273216.46875\n",
            "in training loop, epoch 1, step 321, the loss is 272417.4375\n",
            "in training loop, epoch 1, step 322, the loss is 253767.625\n",
            "in training loop, epoch 1, step 323, the loss is 185161.296875\n",
            "in training loop, epoch 1, step 324, the loss is 195135.53125\n",
            "in training loop, epoch 1, step 325, the loss is 451679.3125\n",
            "in training loop, epoch 1, step 326, the loss is 164840.09375\n",
            "in training loop, epoch 1, step 327, the loss is 253151.75\n",
            "in training loop, epoch 1, step 328, the loss is 376632.28125\n",
            "in training loop, epoch 1, step 329, the loss is 143378.625\n",
            "in training loop, epoch 1, step 330, the loss is 161461.390625\n",
            "in training loop, epoch 1, step 331, the loss is 154299.140625\n",
            "in training loop, epoch 1, step 332, the loss is 250384.125\n",
            "in training loop, epoch 1, step 333, the loss is 359885.34375\n",
            "in training loop, epoch 1, step 334, the loss is 190799.703125\n",
            "in training loop, epoch 1, step 335, the loss is 200906.984375\n",
            "in training loop, epoch 1, step 336, the loss is 178981.3125\n",
            "in training loop, epoch 1, step 337, the loss is 419199.6875\n",
            "in training loop, epoch 1, step 338, the loss is 165614.75\n",
            "in training loop, epoch 1, step 339, the loss is 427269.40625\n",
            "in training loop, epoch 1, step 340, the loss is 329486.46875\n",
            "in training loop, epoch 1, step 341, the loss is 206086.890625\n",
            "in training loop, epoch 1, step 342, the loss is 378769.6875\n",
            "in training loop, epoch 1, step 343, the loss is 157185.5625\n",
            "in training loop, epoch 1, step 344, the loss is 178818.46875\n",
            "in training loop, epoch 1, step 345, the loss is 165902.171875\n",
            "in training loop, epoch 1, step 346, the loss is 170513.71875\n",
            "in training loop, epoch 1, step 347, the loss is 381961.09375\n",
            "in training loop, epoch 1, step 348, the loss is 193293.625\n",
            "in training loop, epoch 1, step 349, the loss is 295987.96875\n",
            "in training loop, epoch 1, step 350, the loss is 186521.859375\n",
            "in training loop, epoch 1, step 351, the loss is 221580.625\n",
            "in training loop, epoch 1, step 352, the loss is 259810.140625\n",
            "in training loop, epoch 1, step 353, the loss is 342474.84375\n",
            "in training loop, epoch 1, step 354, the loss is 187575.578125\n",
            "in training loop, epoch 1, step 355, the loss is 138174.390625\n",
            "in training loop, epoch 1, step 356, the loss is 275794.6875\n",
            "in training loop, epoch 1, step 357, the loss is 210116.203125\n",
            "in training loop, epoch 1, step 358, the loss is 257143.921875\n",
            "in training loop, epoch 1, step 359, the loss is 239318.375\n",
            "in training loop, epoch 1, step 360, the loss is 165352.5625\n",
            "in training loop, epoch 1, step 361, the loss is 160789.109375\n",
            "in training loop, epoch 1, step 362, the loss is 166710.09375\n",
            "in training loop, epoch 1, step 363, the loss is 167129.0625\n",
            "in training loop, epoch 1, step 364, the loss is 375357.1875\n",
            "in training loop, epoch 1, step 365, the loss is 219708.15625\n",
            "in training loop, epoch 1, step 366, the loss is 231773.453125\n",
            "in training loop, epoch 1, step 367, the loss is 207172.921875\n",
            "in training loop, epoch 1, step 368, the loss is 234478.78125\n",
            "in training loop, epoch 1, step 369, the loss is 345040.71875\n",
            "in training loop, epoch 1, step 370, the loss is 200478.859375\n",
            "in training loop, epoch 1, step 371, the loss is 195299.34375\n",
            "in training loop, epoch 1, step 372, the loss is 401919.28125\n",
            "in training loop, epoch 1, step 373, the loss is 193678.875\n",
            "in training loop, epoch 1, step 374, the loss is 225075.078125\n",
            "in training loop, epoch 1, step 375, the loss is 193033.875\n",
            "in training loop, epoch 1, step 376, the loss is 178386.6875\n",
            "in training loop, epoch 1, step 377, the loss is 529036.375\n",
            "in training loop, epoch 1, step 378, the loss is 208462.578125\n",
            "in training loop, epoch 1, step 379, the loss is 897154.875\n",
            "in training loop, epoch 1, step 380, the loss is 149633.546875\n",
            "in training loop, epoch 1, step 381, the loss is 223433.625\n",
            "in training loop, epoch 1, step 382, the loss is 222344.734375\n",
            "in training loop, epoch 1, step 383, the loss is 212070.8125\n",
            "in training loop, epoch 1, step 384, the loss is 180557.375\n",
            "in training loop, epoch 1, step 385, the loss is 332258.9375\n",
            "in training loop, epoch 1, step 386, the loss is 157497.0625\n",
            "in training loop, epoch 1, step 387, the loss is 211252.953125\n",
            "in training loop, epoch 1, step 388, the loss is 158515.953125\n",
            "in training loop, epoch 1, step 389, the loss is 195820.765625\n",
            "in training loop, epoch 1, step 390, the loss is 183453.953125\n",
            "in training loop, epoch 1, step 391, the loss is 136902.71875\n",
            "in training loop, epoch 1, step 392, the loss is 182840.9375\n",
            "in training loop, epoch 1, step 393, the loss is 166411.3125\n",
            "in training loop, epoch 1, step 394, the loss is 182777.1875\n",
            "in training loop, epoch 1, step 395, the loss is 169042.578125\n",
            "in training loop, epoch 1, step 396, the loss is 186939.234375\n",
            "in training loop, epoch 1, step 397, the loss is 220736.40625\n",
            "in training loop, epoch 1, step 398, the loss is 521728.3125\n",
            "in training loop, epoch 1, step 399, the loss is 158129.484375\n",
            "in training loop, epoch 1, step 400, the loss is 194744.546875\n",
            "in training loop, epoch 1, step 401, the loss is 267540.875\n",
            "in training loop, epoch 1, step 402, the loss is 154888.5\n",
            "in training loop, epoch 1, step 403, the loss is 245125.796875\n",
            "in training loop, epoch 1, step 404, the loss is 165505.671875\n",
            "in training loop, epoch 1, step 405, the loss is 174503.53125\n",
            "in training loop, epoch 1, step 406, the loss is 166212.484375\n",
            "in training loop, epoch 1, step 407, the loss is 169262.96875\n",
            "in training loop, epoch 1, step 408, the loss is 158255.390625\n",
            "in training loop, epoch 1, step 409, the loss is 150469.96875\n",
            "in training loop, epoch 1, step 410, the loss is 126948.7734375\n",
            "in training loop, epoch 1, step 411, the loss is 266328.15625\n",
            "in training loop, epoch 1, step 412, the loss is 224213.046875\n",
            "in training loop, epoch 1, step 413, the loss is 170393.0\n",
            "in training loop, epoch 1, step 414, the loss is 429317.875\n",
            "in training loop, epoch 1, step 415, the loss is 152885.359375\n",
            "in training loop, epoch 1, step 416, the loss is 175215.734375\n",
            "in training loop, epoch 1, step 417, the loss is 176715.921875\n",
            "in training loop, epoch 1, step 418, the loss is 182637.5625\n",
            "in training loop, epoch 1, step 419, the loss is 297310.3125\n",
            "in training loop, epoch 1, step 420, the loss is 235004.59375\n",
            "in training loop, epoch 1, step 421, the loss is 125778.328125\n",
            "in training loop, epoch 1, step 422, the loss is 180256.703125\n",
            "in training loop, epoch 1, step 423, the loss is 190742.578125\n",
            "in training loop, epoch 1, step 424, the loss is 163363.328125\n",
            "in training loop, epoch 1, step 425, the loss is 186024.234375\n",
            "in training loop, epoch 1, step 426, the loss is 406311.625\n",
            "in training loop, epoch 1, step 427, the loss is 451978.03125\n",
            "in training loop, epoch 1, step 428, the loss is 194135.09375\n",
            "in training loop, epoch 1, step 429, the loss is 180440.609375\n",
            "in training loop, epoch 1, step 430, the loss is 701433.25\n",
            "in training loop, epoch 1, step 431, the loss is 206752.59375\n",
            "in training loop, epoch 1, step 432, the loss is 493390.9375\n",
            "in training loop, epoch 1, step 433, the loss is 190115.59375\n",
            "in training loop, epoch 1, step 434, the loss is 564285.1875\n",
            "in training loop, epoch 1, step 435, the loss is 310248.65625\n",
            "in training loop, epoch 1, step 436, the loss is 237064.6875\n",
            "in training loop, epoch 1, step 437, the loss is 201185.421875\n",
            "in training loop, epoch 1, step 438, the loss is 264326.03125\n",
            "in training loop, epoch 1, step 439, the loss is 369952.78125\n",
            "in training loop, epoch 1, step 440, the loss is 402296.4375\n",
            "in training loop, epoch 1, step 441, the loss is 295982.8125\n",
            "in training loop, epoch 1, step 442, the loss is 325087.625\n",
            "in training loop, epoch 1, step 443, the loss is 501473.21875\n",
            "in training loop, epoch 1, step 444, the loss is 427660.46875\n",
            "in training loop, epoch 1, step 445, the loss is 286020.0625\n",
            "in training loop, epoch 1, step 446, the loss is 324025.78125\n",
            "in training loop, epoch 1, step 447, the loss is 414235.71875\n",
            "in training loop, epoch 1, step 448, the loss is 316719.25\n",
            "in training loop, epoch 1, step 449, the loss is 471642.5625\n",
            "in training loop, epoch 1, step 450, the loss is 448201.25\n",
            "in training loop, epoch 1, step 451, the loss is 260669.53125\n",
            "in training loop, epoch 1, step 452, the loss is 509138.15625\n",
            "in training loop, epoch 1, step 453, the loss is 181591.71875\n",
            "in training loop, epoch 1, step 454, the loss is 180050.96875\n",
            "in training loop, epoch 1, step 455, the loss is 236535.984375\n",
            "in training loop, epoch 1, step 456, the loss is 1670049.75\n",
            "in training loop, epoch 1, step 457, the loss is 216190.265625\n",
            "in training loop, epoch 1, step 458, the loss is 267217.53125\n",
            "in training loop, epoch 1, step 459, the loss is 541479.5625\n",
            "in training loop, epoch 1, step 460, the loss is 669690.0\n",
            "in training loop, epoch 1, step 461, the loss is 318935.59375\n",
            "in training loop, epoch 1, step 462, the loss is 319636.375\n",
            "in training loop, epoch 1, step 463, the loss is 368568.15625\n",
            "in training loop, epoch 1, step 464, the loss is 371948.03125\n",
            "in training loop, epoch 1, step 465, the loss is 341309.5625\n",
            "in training loop, epoch 1, step 466, the loss is 241160.0625\n",
            "in training loop, epoch 1, step 467, the loss is 243153.484375\n",
            "in training loop, epoch 1, step 468, the loss is 1772469.25\n",
            "in training loop, epoch 1, step 469, the loss is 413392.125\n",
            "in training loop, epoch 1, step 470, the loss is 317322.375\n",
            "in training loop, epoch 1, step 471, the loss is 367216.59375\n",
            "in training loop, epoch 1, step 472, the loss is 285312.5\n",
            "in training loop, epoch 1, step 473, the loss is 801252.0625\n",
            "in training loop, epoch 1, step 474, the loss is 278266.5\n",
            "in training loop, epoch 1, step 475, the loss is 248444.734375\n",
            "in training loop, epoch 1, step 476, the loss is 260184.234375\n",
            "in training loop, epoch 1, step 477, the loss is 271849.40625\n",
            "in training loop, epoch 1, step 478, the loss is 228151.921875\n",
            "in training loop, epoch 1, step 479, the loss is 234635.0625\n",
            "in training loop, epoch 1, step 480, the loss is 780591.625\n",
            "in training loop, epoch 1, step 481, the loss is 459723.71875\n",
            "in training loop, epoch 1, step 482, the loss is 407973.6875\n",
            "in training loop, epoch 1, step 483, the loss is 244171.15625\n",
            "in training loop, epoch 1, step 484, the loss is 243541.015625\n",
            "in training loop, epoch 1, step 485, the loss is 209471.703125\n",
            "in training loop, epoch 1, step 486, the loss is 238438.265625\n",
            "in training loop, epoch 1, step 487, the loss is 246556.796875\n",
            "in training loop, epoch 1, step 488, the loss is 218565.515625\n",
            "in training loop, epoch 1, step 489, the loss is 242611.71875\n",
            "in training loop, epoch 1, step 490, the loss is 227558.421875\n",
            "in training loop, epoch 1, step 491, the loss is 250838.75\n",
            "in training loop, epoch 1, step 492, the loss is 193841.84375\n",
            "in training loop, epoch 1, step 493, the loss is 393244.71875\n",
            "in training loop, epoch 1, step 494, the loss is 250737.09375\n",
            "in training loop, epoch 1, step 495, the loss is 333339.0625\n",
            "in training loop, epoch 1, step 496, the loss is 653639.25\n",
            "in training loop, epoch 1, step 497, the loss is 476143.28125\n",
            "in training loop, epoch 1, step 498, the loss is 217732.640625\n",
            "in training loop, epoch 1, step 499, the loss is 198605.53125\n",
            "in training loop, epoch 1, step 500, the loss is 307252.96875\n",
            "in training loop, epoch 1, step 501, the loss is 217902.625\n",
            "in training loop, epoch 1, step 502, the loss is 646361.6875\n",
            "in training loop, epoch 1, step 503, the loss is 442065.65625\n",
            "in training loop, epoch 1, step 504, the loss is 360358.96875\n",
            "in training loop, epoch 1, step 505, the loss is 193307.921875\n",
            "in training loop, epoch 1, step 506, the loss is 255974.3125\n",
            "in training loop, epoch 1, step 507, the loss is 254352.3125\n",
            "in training loop, epoch 1, step 508, the loss is 191460.234375\n",
            "in training loop, epoch 1, step 509, the loss is 216587.59375\n",
            "in training loop, epoch 1, step 510, the loss is 158586.359375\n",
            "in training loop, epoch 1, step 511, the loss is 173794.484375\n",
            "in training loop, epoch 1, step 512, the loss is 212559.09375\n",
            "in training loop, epoch 1, step 513, the loss is 171713.515625\n",
            "in training loop, epoch 1, step 514, the loss is 261815.96875\n",
            "in training loop, epoch 1, step 515, the loss is 206930.125\n",
            "in training loop, epoch 1, step 516, the loss is 198987.671875\n",
            "in training loop, epoch 1, step 517, the loss is 167579.921875\n",
            "in training loop, epoch 1, step 518, the loss is 152210.9375\n",
            "in training loop, epoch 1, step 519, the loss is 317633.40625\n",
            "in training loop, epoch 1, step 520, the loss is 318826.65625\n",
            "in training loop, epoch 1, step 521, the loss is 209703.859375\n",
            "in training loop, epoch 1, step 522, the loss is 150920.78125\n",
            "in training loop, epoch 1, step 523, the loss is 181343.9375\n",
            "in training loop, epoch 1, step 524, the loss is 247461.65625\n",
            "in training loop, epoch 1, step 525, the loss is 180478.90625\n",
            "in training loop, epoch 1, step 526, the loss is 233830.5\n",
            "in training loop, epoch 1, step 527, the loss is 183313.921875\n",
            "in training loop, epoch 1, step 528, the loss is 231699.578125\n",
            "in training loop, epoch 1, step 529, the loss is 186246.84375\n",
            "in training loop, epoch 1, step 530, the loss is 265780.9375\n",
            "in training loop, epoch 1, step 531, the loss is 289175.59375\n",
            "in training loop, epoch 1, step 532, the loss is 442360.75\n",
            "in training loop, epoch 1, step 533, the loss is 210534.84375\n",
            "in training loop, epoch 1, step 534, the loss is 204048.09375\n",
            "in training loop, epoch 1, step 535, the loss is 187020.03125\n",
            "in training loop, epoch 1, step 536, the loss is 231774.890625\n",
            "in training loop, epoch 1, step 537, the loss is 224038.34375\n",
            "in training loop, epoch 1, step 538, the loss is 303784.90625\n",
            "in training loop, epoch 1, step 539, the loss is 185151.078125\n",
            "in training loop, epoch 1, step 540, the loss is 208836.984375\n",
            "in training loop, epoch 1, step 541, the loss is 185635.0625\n",
            "in training loop, epoch 1, step 542, the loss is 139294.265625\n",
            "in training loop, epoch 1, step 543, the loss is 186290.359375\n",
            "in training loop, epoch 1, step 544, the loss is 229829.171875\n",
            "in training loop, epoch 1, step 545, the loss is 180174.859375\n",
            "in training loop, epoch 1, step 546, the loss is 160669.5625\n",
            "in training loop, epoch 1, step 547, the loss is 153022.1875\n",
            "in training loop, epoch 1, step 548, the loss is 161728.125\n",
            "in training loop, epoch 1, step 549, the loss is 177412.296875\n",
            "in training loop, epoch 1, step 550, the loss is 345519.5\n",
            "in training loop, epoch 1, step 551, the loss is 158970.34375\n",
            "in training loop, epoch 1, step 552, the loss is 169369.1875\n",
            "in training loop, epoch 1, step 553, the loss is 153547.53125\n",
            "in training loop, epoch 1, step 554, the loss is 203559.421875\n",
            "in training loop, epoch 1, step 555, the loss is 178686.015625\n",
            "in training loop, epoch 1, step 556, the loss is 451979.5\n",
            "in training loop, epoch 1, step 557, the loss is 317346.25\n",
            "in training loop, epoch 1, step 558, the loss is 181941.890625\n",
            "in training loop, epoch 1, step 559, the loss is 398850.65625\n",
            "in training loop, epoch 1, step 560, the loss is 196268.640625\n",
            "in training loop, epoch 1, step 561, the loss is 469174.6875\n",
            "in training loop, epoch 1, step 562, the loss is 193040.640625\n",
            "in training loop, epoch 1, step 563, the loss is 183284.109375\n",
            "in training loop, epoch 1, step 564, the loss is 168408.328125\n",
            "in training loop, epoch 1, step 565, the loss is 302365.65625\n",
            "in training loop, epoch 1, step 566, the loss is 148098.03125\n",
            "in training loop, epoch 1, step 567, the loss is 241342.484375\n",
            "in training loop, epoch 1, step 568, the loss is 123337.90625\n",
            "in training loop, epoch 1, step 569, the loss is 358353.625\n",
            "in training loop, epoch 1, step 570, the loss is 182999.78125\n",
            "in training loop, epoch 1, step 571, the loss is 157662.375\n",
            "in training loop, epoch 1, step 572, the loss is 145302.890625\n",
            "in training loop, epoch 1, step 573, the loss is 638898.625\n",
            "in training loop, epoch 1, step 574, the loss is 152069.3125\n",
            "in training loop, epoch 1, step 575, the loss is 248858.875\n",
            "in training loop, epoch 1, step 576, the loss is 192489.578125\n",
            "in training loop, epoch 1, step 577, the loss is 188703.640625\n",
            "in training loop, epoch 1, step 578, the loss is 178884.828125\n",
            "in training loop, epoch 1, step 579, the loss is 229536.734375\n",
            "in training loop, epoch 1, step 580, the loss is 204502.046875\n",
            "in training loop, epoch 1, step 581, the loss is 489202.03125\n",
            "in training loop, epoch 1, step 582, the loss is 170339.265625\n",
            "in training loop, epoch 1, step 583, the loss is 245490.90625\n",
            "in training loop, epoch 1, step 584, the loss is 198844.15625\n",
            "in training loop, epoch 1, step 585, the loss is 195905.15625\n",
            "in training loop, epoch 1, step 586, the loss is 555706.5\n",
            "in training loop, epoch 1, step 587, the loss is 383607.0\n",
            "in training loop, epoch 1, step 588, the loss is 187529.9375\n",
            "in training loop, epoch 1, step 589, the loss is 211315.4375\n",
            "in training loop, epoch 1, step 590, the loss is 181468.8125\n",
            "in training loop, epoch 1, step 591, the loss is 440825.4375\n",
            "in training loop, epoch 1, step 592, the loss is 248273.28125\n",
            "in training loop, epoch 1, step 593, the loss is 165343.546875\n",
            "in training loop, epoch 1, step 594, the loss is 188314.140625\n",
            "in training loop, epoch 1, step 595, the loss is 184669.734375\n",
            "in training loop, epoch 1, step 596, the loss is 414846.5\n",
            "in training loop, epoch 1, step 597, the loss is 217280.859375\n",
            "in training loop, epoch 1, step 598, the loss is 179630.0\n",
            "in training loop, epoch 1, step 599, the loss is 143400.96875\n",
            "in training loop, epoch 1, step 600, the loss is 181857.09375\n",
            "in training loop, epoch 1, step 601, the loss is 170491.5625\n",
            "in training loop, epoch 1, step 602, the loss is 185548.765625\n",
            "in training loop, epoch 1, step 603, the loss is 244214.46875\n",
            "in training loop, epoch 1, step 604, the loss is 153509.765625\n",
            "in training loop, epoch 1, step 605, the loss is 208340.078125\n",
            "in training loop, epoch 1, step 606, the loss is 194604.75\n",
            "in training loop, epoch 1, step 607, the loss is 172115.84375\n",
            "in training loop, epoch 1, step 608, the loss is 413135.8125\n",
            "in training loop, epoch 1, step 609, the loss is 183589.4375\n",
            "in training loop, epoch 1, step 610, the loss is 177168.421875\n",
            "in training loop, epoch 1, step 611, the loss is 206592.125\n",
            "in training loop, epoch 1, step 612, the loss is 205141.203125\n",
            "in training loop, epoch 1, step 613, the loss is 188416.703125\n",
            "in training loop, epoch 1, step 614, the loss is 244733.25\n",
            "in training loop, epoch 1, step 615, the loss is 417737.8125\n",
            "in training loop, epoch 1, step 616, the loss is 150643.1875\n",
            "in training loop, epoch 1, step 617, the loss is 233621.9375\n",
            "in training loop, epoch 1, step 618, the loss is 138752.125\n",
            "in training loop, epoch 1, step 619, the loss is 171566.5\n",
            "in training loop, epoch 1, step 620, the loss is 367927.0\n",
            "in training loop, epoch 1, step 621, the loss is 146974.921875\n",
            "in training loop, epoch 1, step 622, the loss is 178072.109375\n",
            "in training loop, epoch 1, step 623, the loss is 149660.671875\n",
            "in training loop, epoch 1, step 624, the loss is 192128.609375\n",
            "in training loop, epoch 1, step 625, the loss is 177537.90625\n",
            "in training loop, epoch 1, step 626, the loss is 439672.15625\n",
            "in training loop, epoch 1, step 627, the loss is 457421.875\n",
            "in training loop, epoch 1, step 628, the loss is 170975.90625\n",
            "in training loop, epoch 1, step 629, the loss is 217479.59375\n",
            "in training loop, epoch 1, step 630, the loss is 206356.234375\n",
            "in training loop, epoch 1, step 631, the loss is 339842.96875\n",
            "in training loop, epoch 1, step 632, the loss is 204362.9375\n",
            "in training loop, epoch 1, step 633, the loss is 150207.1875\n",
            "in training loop, epoch 1, step 634, the loss is 153837.875\n",
            "in training loop, epoch 1, step 635, the loss is 135080.8125\n",
            "in training loop, epoch 1, step 636, the loss is 432716.9375\n",
            "in training loop, epoch 1, step 637, the loss is 150146.375\n",
            "in training loop, epoch 1, step 638, the loss is 219642.078125\n",
            "in training loop, epoch 1, step 639, the loss is 252577.40625\n",
            "in training loop, epoch 1, step 640, the loss is 171599.703125\n",
            "in training loop, epoch 1, step 641, the loss is 285084.09375\n",
            "in training loop, epoch 1, step 642, the loss is 179183.421875\n",
            "in training loop, epoch 1, step 643, the loss is 174916.0\n",
            "in training loop, epoch 1, step 644, the loss is 214340.484375\n",
            "in training loop, epoch 1, step 645, the loss is 186624.25\n",
            "in training loop, epoch 1, step 646, the loss is 167728.15625\n",
            "in training loop, epoch 1, step 647, the loss is 206863.875\n",
            "in training loop, epoch 1, step 648, the loss is 242223.359375\n",
            "in training loop, epoch 1, step 649, the loss is 219852.4375\n",
            "in training loop, epoch 1, step 650, the loss is 217016.75\n",
            "in training loop, epoch 1, step 651, the loss is 151821.875\n",
            "in training loop, epoch 1, step 652, the loss is 259617.8125\n",
            "in training loop, epoch 1, step 653, the loss is 227588.828125\n",
            "in training loop, epoch 1, step 654, the loss is 163868.875\n",
            "in training loop, epoch 1, step 655, the loss is 454081.59375\n",
            "in training loop, epoch 1, step 656, the loss is 202542.28125\n",
            "in training loop, epoch 1, step 657, the loss is 432973.59375\n",
            "in training loop, epoch 1, step 658, the loss is 241057.75\n",
            "in training loop, epoch 1, step 659, the loss is 221455.03125\n",
            "in training loop, epoch 1, step 660, the loss is 179622.5625\n",
            "in training loop, epoch 1, step 661, the loss is 243075.8125\n",
            "in training loop, epoch 1, step 662, the loss is 144939.125\n",
            "in training loop, epoch 1, step 663, the loss is 220392.40625\n",
            "in training loop, epoch 1, step 664, the loss is 207482.375\n",
            "in training loop, epoch 1, step 665, the loss is 338052.5\n",
            "in training loop, epoch 1, step 666, the loss is 182974.40625\n",
            "in training loop, epoch 1, step 667, the loss is 210046.0\n",
            "in training loop, epoch 1, step 668, the loss is 159323.125\n",
            "in training loop, epoch 1, step 669, the loss is 291117.21875\n",
            "in training loop, epoch 1, step 670, the loss is 200907.59375\n",
            "in training loop, epoch 1, step 671, the loss is 210424.421875\n",
            "in training loop, epoch 1, step 672, the loss is 182160.515625\n",
            "in training loop, epoch 1, step 673, the loss is 533619.9375\n",
            "in training loop, epoch 1, step 674, the loss is 184964.3125\n",
            "in training loop, epoch 1, step 675, the loss is 156828.359375\n",
            "in training loop, epoch 1, step 676, the loss is 151641.0\n",
            "in training loop, epoch 1, step 677, the loss is 375694.53125\n",
            "in training loop, epoch 1, step 678, the loss is 188165.234375\n",
            "in training loop, epoch 1, step 679, the loss is 300053.125\n",
            "in training loop, epoch 1, step 680, the loss is 182176.9375\n",
            "in training loop, epoch 1, step 681, the loss is 300672.4375\n",
            "in training loop, epoch 1, step 682, the loss is 147303.578125\n",
            "in training loop, epoch 1, step 683, the loss is 464488.125\n",
            "in training loop, epoch 1, step 684, the loss is 169187.09375\n",
            "in training loop, epoch 1, step 685, the loss is 175908.8125\n",
            "in training loop, epoch 1, step 686, the loss is 531909.125\n",
            "in training loop, epoch 1, step 687, the loss is 212511.15625\n",
            "in training loop, epoch 1, step 688, the loss is 489086.21875\n",
            "in training loop, epoch 1, step 689, the loss is 180794.984375\n",
            "in training loop, epoch 1, step 690, the loss is 137788.25\n",
            "in training loop, epoch 1, step 691, the loss is 249534.59375\n",
            "in training loop, epoch 1, step 692, the loss is 184172.1875\n",
            "in training loop, epoch 1, step 693, the loss is 217831.046875\n",
            "in training loop, epoch 1, step 694, the loss is 221664.65625\n",
            "in training loop, epoch 1, step 695, the loss is 423410.9375\n",
            "in training loop, epoch 1, step 696, the loss is 163444.90625\n",
            "in training loop, epoch 1, step 697, the loss is 339305.71875\n",
            "in training loop, epoch 1, step 698, the loss is 155487.40625\n",
            "in training loop, epoch 1, step 699, the loss is 174983.0\n",
            "in training loop, epoch 1, step 700, the loss is 157960.15625\n",
            "in training loop, epoch 1, step 701, the loss is 307711.09375\n",
            "in training loop, epoch 1, step 702, the loss is 179581.875\n",
            "in training loop, epoch 1, step 703, the loss is 211660.953125\n",
            "in training loop, epoch 1, step 704, the loss is 142349.46875\n",
            "in training loop, epoch 1, step 705, the loss is 214627.3125\n",
            "in training loop, epoch 1, step 706, the loss is 169044.03125\n",
            "in training loop, epoch 1, step 707, the loss is 160549.28125\n",
            "in training loop, epoch 1, step 708, the loss is 153364.828125\n",
            "in training loop, epoch 1, step 709, the loss is 426746.59375\n",
            "in training loop, epoch 1, step 710, the loss is 288436.0\n",
            "in training loop, epoch 1, step 711, the loss is 273689.34375\n",
            "in training loop, epoch 1, step 712, the loss is 416444.875\n",
            "in training loop, epoch 1, step 713, the loss is 166592.578125\n",
            "in training loop, epoch 1, step 714, the loss is 156708.046875\n",
            "in training loop, epoch 1, step 715, the loss is 153062.828125\n",
            "in training loop, epoch 1, step 716, the loss is 190237.390625\n",
            "in training loop, epoch 1, step 717, the loss is 216764.8125\n",
            "in training loop, epoch 1, step 718, the loss is 187395.203125\n",
            "in training loop, epoch 1, step 719, the loss is 468092.59375\n",
            "in training loop, epoch 1, step 720, the loss is 211587.09375\n",
            "in training loop, epoch 1, step 721, the loss is 200607.4375\n",
            "in training loop, epoch 1, step 722, the loss is 465950.125\n",
            "in training loop, epoch 1, step 723, the loss is 171012.53125\n",
            "in training loop, epoch 1, step 724, the loss is 193625.578125\n",
            "in training loop, epoch 1, step 725, the loss is 208378.1875\n",
            "in training loop, epoch 1, step 726, the loss is 464328.9375\n",
            "in training loop, epoch 1, step 727, the loss is 218045.90625\n",
            "in training loop, epoch 1, step 728, the loss is 154961.9375\n",
            "in training loop, epoch 1, step 729, the loss is 190594.125\n",
            "in training loop, epoch 1, step 730, the loss is 150146.296875\n",
            "in training loop, epoch 1, step 731, the loss is 269112.75\n",
            "in training loop, epoch 1, step 732, the loss is 131017.09375\n",
            "in training loop, epoch 1, step 733, the loss is 178604.9375\n",
            "in training loop, epoch 1, step 734, the loss is 175918.09375\n",
            "in training loop, epoch 1, step 735, the loss is 171327.796875\n",
            "in training loop, epoch 1, step 736, the loss is 216871.3125\n",
            "in training loop, epoch 1, step 737, the loss is 166688.375\n",
            "in training loop, epoch 1, step 738, the loss is 209206.765625\n",
            "in training loop, epoch 1, step 739, the loss is 488646.78125\n",
            "in training loop, epoch 1, step 740, the loss is 161559.265625\n",
            "in training loop, epoch 1, step 741, the loss is 165035.828125\n",
            "in training loop, epoch 1, step 742, the loss is 222573.671875\n",
            "in training loop, epoch 1, step 743, the loss is 169981.75\n",
            "in training loop, epoch 1, step 744, the loss is 162716.25\n",
            "in training loop, epoch 1, step 745, the loss is 393409.8125\n",
            "in training loop, epoch 1, step 746, the loss is 161598.859375\n",
            "in training loop, epoch 1, step 747, the loss is 158605.0625\n",
            "in training loop, epoch 1, step 748, the loss is 306358.78125\n",
            "in training loop, epoch 1, step 749, the loss is 472718.96875\n",
            "in training loop, epoch 1, step 750, the loss is 155170.15625\n",
            "in training loop, epoch 1, step 751, the loss is 213790.453125\n",
            "in training loop, epoch 1, step 752, the loss is 149360.234375\n",
            "in training loop, epoch 1, step 753, the loss is 403413.3125\n",
            "in training loop, epoch 1, step 754, the loss is 229443.609375\n",
            "in training loop, epoch 1, step 755, the loss is 137745.90625\n",
            "in training loop, epoch 1, step 756, the loss is 242475.484375\n",
            "in training loop, epoch 1, step 757, the loss is 263029.59375\n",
            "in training loop, epoch 1, step 758, the loss is 167793.21875\n",
            "in training loop, epoch 1, step 759, the loss is 238680.046875\n",
            "in training loop, epoch 1, step 760, the loss is 210093.8125\n",
            "in training loop, epoch 1, step 761, the loss is 187307.859375\n",
            "in training loop, epoch 1, step 762, the loss is 248267.984375\n",
            "in training loop, epoch 1, step 763, the loss is 168719.03125\n",
            "in training loop, epoch 1, step 764, the loss is 227789.96875\n",
            "in training loop, epoch 1, step 765, the loss is 210156.328125\n",
            "in training loop, epoch 1, step 766, the loss is 590134.875\n",
            "in training loop, epoch 1, step 767, the loss is 185539.90625\n",
            "in training loop, epoch 1, step 768, the loss is 230020.9375\n",
            "in training loop, epoch 1, step 769, the loss is 167146.90625\n",
            "in training loop, epoch 1, step 770, the loss is 374995.21875\n",
            "in training loop, epoch 1, step 771, the loss is 189780.65625\n",
            "in training loop, epoch 1, step 772, the loss is 143088.71875\n",
            "in training loop, epoch 1, step 773, the loss is 246114.203125\n",
            "in training loop, epoch 1, step 774, the loss is 158077.15625\n",
            "in training loop, epoch 1, step 775, the loss is 203998.125\n",
            "in training loop, epoch 1, step 776, the loss is 173210.359375\n",
            "in training loop, epoch 1, step 777, the loss is 170847.484375\n",
            "in training loop, epoch 1, step 778, the loss is 162614.109375\n",
            "in training loop, epoch 1, step 779, the loss is 156328.03125\n",
            "in training loop, epoch 1, step 780, the loss is 155255.359375\n",
            "in training loop, epoch 1, step 781, the loss is 158518.546875\n",
            "in training loop, epoch 1, step 782, the loss is 172055.125\n",
            "in training loop, epoch 1, step 783, the loss is 179045.46875\n",
            "in training loop, epoch 1, step 784, the loss is 184932.703125\n",
            "in training loop, epoch 1, step 785, the loss is 149315.703125\n",
            "in training loop, epoch 1, step 786, the loss is 178074.40625\n",
            "in training loop, epoch 1, step 787, the loss is 153676.9375\n",
            "in training loop, epoch 1, step 788, the loss is 203238.96875\n",
            "in training loop, epoch 1, step 789, the loss is 164430.390625\n",
            "in training loop, epoch 1, step 790, the loss is 153328.390625\n",
            "in training loop, epoch 1, step 791, the loss is 191398.671875\n",
            "in training loop, epoch 1, step 792, the loss is 163287.5625\n",
            "in training loop, epoch 1, step 793, the loss is 205393.265625\n",
            "in training loop, epoch 1, step 794, the loss is 348821.03125\n",
            "in training loop, epoch 1, step 795, the loss is 222788.046875\n",
            "in training loop, epoch 1, step 796, the loss is 210683.796875\n",
            "in training loop, epoch 1, step 797, the loss is 196273.78125\n",
            "in training loop, epoch 1, step 798, the loss is 134528.578125\n",
            "in training loop, epoch 1, step 799, the loss is 183770.578125\n",
            "in training loop, epoch 1, step 800, the loss is 400281.8125\n",
            "in training loop, epoch 1, step 801, the loss is 164066.59375\n",
            "in training loop, epoch 1, step 802, the loss is 184378.4375\n",
            "in training loop, epoch 1, step 803, the loss is 195392.859375\n",
            "in training loop, epoch 1, step 804, the loss is 256002.09375\n",
            "in training loop, epoch 1, step 805, the loss is 144000.953125\n",
            "in training loop, epoch 1, step 806, the loss is 143644.96875\n",
            "in training loop, epoch 1, step 807, the loss is 206033.5625\n",
            "in training loop, epoch 1, step 808, the loss is 249462.8125\n",
            "in training loop, epoch 1, step 809, the loss is 129847.15625\n",
            "in training loop, epoch 1, step 810, the loss is 142125.421875\n",
            "in training loop, epoch 1, step 811, the loss is 186391.265625\n",
            "in training loop, epoch 1, step 812, the loss is 348591.25\n",
            "in training loop, epoch 1, step 813, the loss is 422369.84375\n",
            "in training loop, epoch 1, step 814, the loss is 269043.625\n",
            "in training loop, epoch 1, step 815, the loss is 121226.8984375\n",
            "in training loop, epoch 1, step 816, the loss is 160008.171875\n",
            "in training loop, epoch 1, step 817, the loss is 460820.4375\n",
            "in training loop, epoch 1, step 818, the loss is 202511.5625\n",
            "in training loop, epoch 1, step 819, the loss is 195335.15625\n",
            "in training loop, epoch 1, step 820, the loss is 187396.921875\n",
            "in training loop, epoch 1, step 821, the loss is 210132.1875\n",
            "in training loop, epoch 1, step 822, the loss is 179979.875\n",
            "in training loop, epoch 1, step 823, the loss is 170235.5\n",
            "in training loop, epoch 1, step 824, the loss is 167064.546875\n",
            "in training loop, epoch 1, step 825, the loss is 193608.546875\n",
            "in training loop, epoch 1, step 826, the loss is 216506.390625\n",
            "in training loop, epoch 1, step 827, the loss is 168561.171875\n",
            "in training loop, epoch 1, step 828, the loss is 191523.578125\n",
            "in training loop, epoch 1, step 829, the loss is 166290.0625\n",
            "in training loop, epoch 1, step 830, the loss is 181022.84375\n",
            "in training loop, epoch 1, step 831, the loss is 353246.84375\n",
            "in training loop, epoch 1, step 832, the loss is 152735.1875\n",
            "in training loop, epoch 1, step 833, the loss is 440121.6875\n",
            "in training loop, epoch 1, step 834, the loss is 126006.84375\n",
            "in training loop, epoch 1, step 835, the loss is 346106.9375\n",
            "in training loop, epoch 1, step 836, the loss is 394020.0\n",
            "in training loop, epoch 1, step 837, the loss is 224784.921875\n",
            "in training loop, epoch 1, step 838, the loss is 162245.234375\n",
            "in training loop, epoch 1, step 839, the loss is 174922.65625\n",
            "in training loop, epoch 1, step 840, the loss is 185909.078125\n",
            "in training loop, epoch 1, step 841, the loss is 211971.125\n",
            "in training loop, epoch 1, step 842, the loss is 167103.921875\n",
            "in training loop, epoch 1, step 843, the loss is 151817.765625\n",
            "in training loop, epoch 1, step 844, the loss is 132144.75\n",
            "in training loop, epoch 1, step 845, the loss is 237053.40625\n",
            "in training loop, epoch 1, step 846, the loss is 159554.90625\n",
            "in training loop, epoch 1, step 847, the loss is 118490.4375\n",
            "in training loop, epoch 1, step 848, the loss is 232084.6875\n",
            "in training loop, epoch 1, step 849, the loss is 495098.1875\n",
            "in training loop, epoch 1, step 850, the loss is 157888.390625\n",
            "in training loop, epoch 1, step 851, the loss is 213604.3125\n",
            "in training loop, epoch 1, step 852, the loss is 190143.53125\n",
            "in training loop, epoch 1, step 853, the loss is 168001.921875\n",
            "in training loop, epoch 1, step 854, the loss is 129868.4375\n",
            "in training loop, epoch 1, step 855, the loss is 187680.3125\n",
            "in training loop, epoch 1, step 856, the loss is 390956.15625\n",
            "in training loop, epoch 1, step 857, the loss is 178502.453125\n",
            "in training loop, epoch 1, step 858, the loss is 163223.53125\n",
            "in training loop, epoch 1, step 859, the loss is 160011.578125\n",
            "in training loop, epoch 1, step 860, the loss is 188467.25\n",
            "in training loop, epoch 1, step 861, the loss is 576595.375\n",
            "in training loop, epoch 1, step 862, the loss is 159912.21875\n",
            "in training loop, epoch 1, step 863, the loss is 237913.5\n",
            "in training loop, epoch 1, step 864, the loss is 343785.96875\n",
            "in training loop, epoch 1, step 865, the loss is 214762.046875\n",
            "in training loop, epoch 1, step 866, the loss is 752259.0625\n",
            "in training loop, epoch 1, step 867, the loss is 290530.5625\n",
            "in training loop, epoch 1, step 868, the loss is 254097.421875\n",
            "in training loop, epoch 1, step 869, the loss is 362324.28125\n",
            "in training loop, epoch 1, step 870, the loss is 192924.8125\n",
            "in training loop, epoch 1, step 871, the loss is 155140.71875\n",
            "in training loop, epoch 1, step 872, the loss is 422163.8125\n",
            "in training loop, epoch 1, step 873, the loss is 381497.4375\n",
            "in training loop, epoch 1, step 874, the loss is 198312.0625\n",
            "in training loop, epoch 1, step 875, the loss is 255017.21875\n",
            "in training loop, epoch 1, step 876, the loss is 208637.640625\n",
            "in training loop, epoch 1, step 877, the loss is 181408.46875\n",
            "in training loop, epoch 1, step 878, the loss is 228990.296875\n",
            "in training loop, epoch 1, step 879, the loss is 168555.453125\n",
            "in training loop, epoch 1, step 880, the loss is 196728.375\n",
            "in training loop, epoch 1, step 881, the loss is 258369.140625\n",
            "in training loop, epoch 1, step 882, the loss is 399080.59375\n",
            "in training loop, epoch 1, step 883, the loss is 431680.25\n",
            "in training loop, epoch 1, step 884, the loss is 169138.09375\n",
            "in training loop, epoch 1, step 885, the loss is 200991.21875\n",
            "in training loop, epoch 1, step 886, the loss is 198612.96875\n",
            "in training loop, epoch 1, step 887, the loss is 414551.53125\n",
            "in training loop, epoch 1, step 888, the loss is 242022.828125\n",
            "in training loop, epoch 1, step 889, the loss is 354452.75\n",
            "in training loop, epoch 1, step 890, the loss is 212059.046875\n",
            "in training loop, epoch 1, step 891, the loss is 161226.40625\n",
            "in training loop, epoch 1, step 892, the loss is 302441.28125\n",
            "in training loop, epoch 1, step 893, the loss is 244544.578125\n",
            "in training loop, epoch 1, step 894, the loss is 375534.8125\n",
            "in training loop, epoch 1, step 895, the loss is 212919.203125\n",
            "in training loop, epoch 1, step 896, the loss is 202720.359375\n",
            "in training loop, epoch 1, step 897, the loss is 142332.90625\n",
            "in training loop, epoch 1, step 898, the loss is 196591.671875\n",
            "in training loop, epoch 1, step 899, the loss is 262868.125\n",
            "in training loop, epoch 1, step 900, the loss is 180207.1875\n",
            "in training loop, epoch 1, step 901, the loss is 189662.21875\n",
            "in training loop, epoch 1, step 902, the loss is 249046.125\n",
            "in training loop, epoch 1, step 903, the loss is 105804.203125\n",
            "k-fold 1:: Epoch 1: train loss 243241.46728947732 val loss 248156.74559096535\n",
            "in training loop, epoch 2, step 0, the loss is 222503.546875\n",
            "in training loop, epoch 2, step 1, the loss is 137553.09375\n",
            "in training loop, epoch 2, step 2, the loss is 244687.296875\n",
            "in training loop, epoch 2, step 3, the loss is 120309.953125\n",
            "in training loop, epoch 2, step 4, the loss is 254442.59375\n",
            "in training loop, epoch 2, step 5, the loss is 123413.890625\n",
            "in training loop, epoch 2, step 6, the loss is 229688.765625\n",
            "in training loop, epoch 2, step 7, the loss is 157485.828125\n",
            "in training loop, epoch 2, step 8, the loss is 122972.2734375\n",
            "in training loop, epoch 2, step 9, the loss is 167902.140625\n",
            "in training loop, epoch 2, step 10, the loss is 142197.9375\n",
            "in training loop, epoch 2, step 11, the loss is 114980.1015625\n",
            "in training loop, epoch 2, step 12, the loss is 127854.5234375\n",
            "in training loop, epoch 2, step 13, the loss is 248630.015625\n",
            "in training loop, epoch 2, step 14, the loss is 114436.3515625\n",
            "in training loop, epoch 2, step 15, the loss is 126936.953125\n",
            "in training loop, epoch 2, step 16, the loss is 223126.21875\n",
            "in training loop, epoch 2, step 17, the loss is 143528.796875\n",
            "in training loop, epoch 2, step 18, the loss is 133958.59375\n",
            "in training loop, epoch 2, step 19, the loss is 103148.1640625\n",
            "in training loop, epoch 2, step 20, the loss is 144124.140625\n",
            "in training loop, epoch 2, step 21, the loss is 133765.09375\n",
            "in training loop, epoch 2, step 22, the loss is 173622.25\n",
            "in training loop, epoch 2, step 23, the loss is 116133.046875\n",
            "in training loop, epoch 2, step 24, the loss is 151736.375\n",
            "in training loop, epoch 2, step 25, the loss is 149911.0625\n",
            "in training loop, epoch 2, step 26, the loss is 189485.9375\n",
            "in training loop, epoch 2, step 27, the loss is 98698.2890625\n",
            "in training loop, epoch 2, step 28, the loss is 164649.421875\n",
            "in training loop, epoch 2, step 29, the loss is 156348.453125\n",
            "in training loop, epoch 2, step 30, the loss is 110194.5234375\n",
            "in training loop, epoch 2, step 31, the loss is 170810.25\n",
            "in training loop, epoch 2, step 32, the loss is 103909.859375\n",
            "in training loop, epoch 2, step 33, the loss is 198519.46875\n",
            "in training loop, epoch 2, step 34, the loss is 127411.078125\n",
            "in training loop, epoch 2, step 35, the loss is 103469.7265625\n",
            "in training loop, epoch 2, step 36, the loss is 226818.765625\n",
            "in training loop, epoch 2, step 37, the loss is 135152.109375\n",
            "in training loop, epoch 2, step 38, the loss is 249940.953125\n",
            "in training loop, epoch 2, step 39, the loss is 150580.21875\n",
            "in training loop, epoch 2, step 40, the loss is 187539.1875\n",
            "in training loop, epoch 2, step 41, the loss is 156134.125\n",
            "in training loop, epoch 2, step 42, the loss is 199137.5625\n",
            "in training loop, epoch 2, step 43, the loss is 237285.15625\n",
            "in training loop, epoch 2, step 44, the loss is 90759.2578125\n",
            "in training loop, epoch 2, step 45, the loss is 138723.546875\n",
            "in training loop, epoch 2, step 46, the loss is 109631.671875\n",
            "in training loop, epoch 2, step 47, the loss is 125462.625\n",
            "in training loop, epoch 2, step 48, the loss is 114328.4921875\n",
            "in training loop, epoch 2, step 49, the loss is 146767.8125\n",
            "in training loop, epoch 2, step 50, the loss is 107190.0234375\n",
            "in training loop, epoch 2, step 51, the loss is 95480.5234375\n",
            "in training loop, epoch 2, step 52, the loss is 107342.6171875\n",
            "in training loop, epoch 2, step 53, the loss is 217734.4375\n",
            "in training loop, epoch 2, step 54, the loss is 156199.203125\n",
            "in training loop, epoch 2, step 55, the loss is 144826.890625\n",
            "in training loop, epoch 2, step 56, the loss is 198655.140625\n",
            "in training loop, epoch 2, step 57, the loss is 129851.15625\n",
            "in training loop, epoch 2, step 58, the loss is 97359.25\n",
            "in training loop, epoch 2, step 59, the loss is 117157.984375\n",
            "in training loop, epoch 2, step 60, the loss is 143596.703125\n",
            "in training loop, epoch 2, step 61, the loss is 151946.078125\n",
            "in training loop, epoch 2, step 62, the loss is 174137.890625\n",
            "in training loop, epoch 2, step 63, the loss is 163613.3125\n",
            "in training loop, epoch 2, step 64, the loss is 215900.953125\n",
            "in training loop, epoch 2, step 65, the loss is 131395.65625\n",
            "in training loop, epoch 2, step 66, the loss is 197108.21875\n",
            "in training loop, epoch 2, step 67, the loss is 165679.421875\n",
            "in training loop, epoch 2, step 68, the loss is 129305.265625\n",
            "in training loop, epoch 2, step 69, the loss is 142888.09375\n",
            "in training loop, epoch 2, step 70, the loss is 83499.921875\n",
            "in training loop, epoch 2, step 71, the loss is 148580.640625\n",
            "in training loop, epoch 2, step 72, the loss is 229514.96875\n",
            "in training loop, epoch 2, step 73, the loss is 156062.75\n",
            "in training loop, epoch 2, step 74, the loss is 205848.4375\n",
            "in training loop, epoch 2, step 75, the loss is 373928.34375\n",
            "in training loop, epoch 2, step 76, the loss is 145652.59375\n",
            "in training loop, epoch 2, step 77, the loss is 124063.9921875\n",
            "in training loop, epoch 2, step 78, the loss is 243178.625\n",
            "in training loop, epoch 2, step 79, the loss is 116110.15625\n",
            "in training loop, epoch 2, step 80, the loss is 145017.484375\n",
            "in training loop, epoch 2, step 81, the loss is 168679.625\n",
            "in training loop, epoch 2, step 82, the loss is 187156.09375\n",
            "in training loop, epoch 2, step 83, the loss is 107407.5546875\n",
            "in training loop, epoch 2, step 84, the loss is 183650.984375\n",
            "in training loop, epoch 2, step 85, the loss is 164232.9375\n",
            "in training loop, epoch 2, step 86, the loss is 129541.421875\n",
            "in training loop, epoch 2, step 87, the loss is 115449.890625\n",
            "in training loop, epoch 2, step 88, the loss is 439817.6875\n",
            "in training loop, epoch 2, step 89, the loss is 98784.171875\n",
            "in training loop, epoch 2, step 90, the loss is 175303.140625\n",
            "in training loop, epoch 2, step 91, the loss is 203800.609375\n",
            "in training loop, epoch 2, step 92, the loss is 357907.9375\n",
            "in training loop, epoch 2, step 93, the loss is 215878.46875\n",
            "in training loop, epoch 2, step 94, the loss is 140294.75\n",
            "in training loop, epoch 2, step 95, the loss is 128325.109375\n",
            "in training loop, epoch 2, step 96, the loss is 142132.21875\n",
            "in training loop, epoch 2, step 97, the loss is 278918.03125\n",
            "in training loop, epoch 2, step 98, the loss is 233302.03125\n",
            "in training loop, epoch 2, step 99, the loss is 168282.0\n",
            "in training loop, epoch 2, step 100, the loss is 222846.484375\n",
            "in training loop, epoch 2, step 101, the loss is 144047.84375\n",
            "in training loop, epoch 2, step 102, the loss is 156859.453125\n",
            "in training loop, epoch 2, step 103, the loss is 142784.65625\n",
            "in training loop, epoch 2, step 104, the loss is 260357.046875\n",
            "in training loop, epoch 2, step 105, the loss is 110777.4296875\n",
            "in training loop, epoch 2, step 106, the loss is 129363.90625\n",
            "in training loop, epoch 2, step 107, the loss is 171275.375\n",
            "in training loop, epoch 2, step 108, the loss is 171051.9375\n",
            "in training loop, epoch 2, step 109, the loss is 121925.515625\n",
            "in training loop, epoch 2, step 110, the loss is 163452.796875\n",
            "in training loop, epoch 2, step 111, the loss is 204164.640625\n",
            "in training loop, epoch 2, step 112, the loss is 163832.0625\n",
            "in training loop, epoch 2, step 113, the loss is 170008.15625\n",
            "in training loop, epoch 2, step 114, the loss is 110264.875\n",
            "in training loop, epoch 2, step 115, the loss is 101192.625\n",
            "in training loop, epoch 2, step 116, the loss is 146643.265625\n",
            "in training loop, epoch 2, step 117, the loss is 266355.90625\n",
            "in training loop, epoch 2, step 118, the loss is 125621.265625\n",
            "in training loop, epoch 2, step 119, the loss is 151119.484375\n",
            "in training loop, epoch 2, step 120, the loss is 117954.640625\n",
            "in training loop, epoch 2, step 121, the loss is 103150.046875\n",
            "in training loop, epoch 2, step 122, the loss is 114298.859375\n",
            "in training loop, epoch 2, step 123, the loss is 243677.96875\n",
            "in training loop, epoch 2, step 124, the loss is 206210.609375\n",
            "in training loop, epoch 2, step 125, the loss is 118319.53125\n",
            "in training loop, epoch 2, step 126, the loss is 180982.53125\n",
            "in training loop, epoch 2, step 127, the loss is 205476.078125\n",
            "in training loop, epoch 2, step 128, the loss is 93525.53125\n",
            "in training loop, epoch 2, step 129, the loss is 115872.6015625\n",
            "in training loop, epoch 2, step 130, the loss is 254945.109375\n",
            "in training loop, epoch 2, step 131, the loss is 221514.71875\n",
            "in training loop, epoch 2, step 132, the loss is 233115.96875\n",
            "in training loop, epoch 2, step 133, the loss is 129234.3984375\n",
            "in training loop, epoch 2, step 134, the loss is 207000.0\n",
            "in training loop, epoch 2, step 135, the loss is 134647.4375\n",
            "in training loop, epoch 2, step 136, the loss is 190413.703125\n",
            "in training loop, epoch 2, step 137, the loss is 148748.5625\n",
            "in training loop, epoch 2, step 138, the loss is 95555.0234375\n",
            "in training loop, epoch 2, step 139, the loss is 139714.609375\n",
            "in training loop, epoch 2, step 140, the loss is 87061.46875\n",
            "in training loop, epoch 2, step 141, the loss is 123350.9140625\n",
            "in training loop, epoch 2, step 142, the loss is 108308.890625\n",
            "in training loop, epoch 2, step 143, the loss is 185991.40625\n",
            "in training loop, epoch 2, step 144, the loss is 117078.71875\n",
            "in training loop, epoch 2, step 145, the loss is 165963.671875\n",
            "in training loop, epoch 2, step 146, the loss is 147730.34375\n",
            "in training loop, epoch 2, step 147, the loss is 135365.265625\n",
            "in training loop, epoch 2, step 148, the loss is 116724.6875\n",
            "in training loop, epoch 2, step 149, the loss is 184772.125\n",
            "in training loop, epoch 2, step 150, the loss is 113597.640625\n",
            "in training loop, epoch 2, step 151, the loss is 108141.640625\n",
            "in training loop, epoch 2, step 152, the loss is 175261.484375\n",
            "in training loop, epoch 2, step 153, the loss is 120290.0234375\n",
            "in training loop, epoch 2, step 154, the loss is 170311.75\n",
            "in training loop, epoch 2, step 155, the loss is 138736.40625\n",
            "in training loop, epoch 2, step 156, the loss is 316034.1875\n",
            "in training loop, epoch 2, step 157, the loss is 167272.84375\n",
            "in training loop, epoch 2, step 158, the loss is 219396.28125\n",
            "in training loop, epoch 2, step 159, the loss is 159547.0625\n",
            "in training loop, epoch 2, step 160, the loss is 196037.875\n",
            "in training loop, epoch 2, step 161, the loss is 146154.671875\n",
            "in training loop, epoch 2, step 162, the loss is 159164.28125\n",
            "in training loop, epoch 2, step 163, the loss is 117766.2734375\n",
            "in training loop, epoch 2, step 164, the loss is 123221.5390625\n",
            "in training loop, epoch 2, step 165, the loss is 210625.6875\n",
            "in training loop, epoch 2, step 166, the loss is 179591.34375\n",
            "in training loop, epoch 2, step 167, the loss is 113778.109375\n",
            "in training loop, epoch 2, step 168, the loss is 137301.953125\n",
            "in training loop, epoch 2, step 169, the loss is 146451.984375\n",
            "in training loop, epoch 2, step 170, the loss is 254412.90625\n",
            "in training loop, epoch 2, step 171, the loss is 138851.078125\n",
            "in training loop, epoch 2, step 172, the loss is 142168.5625\n",
            "in training loop, epoch 2, step 173, the loss is 149327.609375\n",
            "in training loop, epoch 2, step 174, the loss is 212080.125\n",
            "in training loop, epoch 2, step 175, the loss is 171493.078125\n",
            "in training loop, epoch 2, step 176, the loss is 116613.5\n",
            "in training loop, epoch 2, step 177, the loss is 136970.9375\n",
            "in training loop, epoch 2, step 178, the loss is 125142.390625\n",
            "in training loop, epoch 2, step 179, the loss is 151222.5625\n",
            "in training loop, epoch 2, step 180, the loss is 115684.0234375\n",
            "in training loop, epoch 2, step 181, the loss is 154697.109375\n",
            "in training loop, epoch 2, step 182, the loss is 193345.328125\n",
            "in training loop, epoch 2, step 183, the loss is 95670.125\n",
            "in training loop, epoch 2, step 184, the loss is 120944.40625\n",
            "in training loop, epoch 2, step 185, the loss is 160549.28125\n",
            "in training loop, epoch 2, step 186, the loss is 163553.0625\n",
            "in training loop, epoch 2, step 187, the loss is 147539.75\n",
            "in training loop, epoch 2, step 188, the loss is 127246.3125\n",
            "in training loop, epoch 2, step 189, the loss is 96271.328125\n",
            "in training loop, epoch 2, step 190, the loss is 211544.421875\n",
            "in training loop, epoch 2, step 191, the loss is 189546.359375\n",
            "in training loop, epoch 2, step 192, the loss is 102029.5546875\n",
            "in training loop, epoch 2, step 193, the loss is 182855.609375\n",
            "in training loop, epoch 2, step 194, the loss is 121418.7578125\n",
            "in training loop, epoch 2, step 195, the loss is 123463.734375\n",
            "in training loop, epoch 2, step 196, the loss is 120755.0625\n",
            "in training loop, epoch 2, step 197, the loss is 163255.421875\n",
            "in training loop, epoch 2, step 198, the loss is 88654.015625\n",
            "in training loop, epoch 2, step 199, the loss is 128523.875\n",
            "in training loop, epoch 2, step 200, the loss is 116417.171875\n",
            "in training loop, epoch 2, step 201, the loss is 101695.515625\n",
            "in training loop, epoch 2, step 202, the loss is 144645.265625\n",
            "in training loop, epoch 2, step 203, the loss is 205805.953125\n",
            "in training loop, epoch 2, step 204, the loss is 112762.84375\n",
            "in training loop, epoch 2, step 205, the loss is 205876.296875\n",
            "in training loop, epoch 2, step 206, the loss is 162485.203125\n",
            "in training loop, epoch 2, step 207, the loss is 124302.3984375\n",
            "in training loop, epoch 2, step 208, the loss is 104979.390625\n",
            "in training loop, epoch 2, step 209, the loss is 125312.890625\n",
            "in training loop, epoch 2, step 210, the loss is 104187.265625\n",
            "in training loop, epoch 2, step 211, the loss is 140350.53125\n",
            "in training loop, epoch 2, step 212, the loss is 180303.875\n",
            "in training loop, epoch 2, step 213, the loss is 103357.5078125\n",
            "in training loop, epoch 2, step 214, the loss is 146546.390625\n",
            "in training loop, epoch 2, step 215, the loss is 225786.171875\n",
            "in training loop, epoch 2, step 216, the loss is 251118.4375\n",
            "in training loop, epoch 2, step 217, the loss is 155030.671875\n",
            "in training loop, epoch 2, step 218, the loss is 332506.53125\n",
            "in training loop, epoch 2, step 219, the loss is 155319.9375\n",
            "in training loop, epoch 2, step 220, the loss is 145967.5625\n",
            "in training loop, epoch 2, step 221, the loss is 203560.625\n",
            "in training loop, epoch 2, step 222, the loss is 154623.0625\n",
            "in training loop, epoch 2, step 223, the loss is 364698.25\n",
            "in training loop, epoch 2, step 224, the loss is 412628.71875\n",
            "in training loop, epoch 2, step 225, the loss is 118514.1015625\n",
            "in training loop, epoch 2, step 226, the loss is 154720.125\n",
            "in training loop, epoch 2, step 227, the loss is 158704.171875\n",
            "in training loop, epoch 2, step 228, the loss is 192117.625\n",
            "in training loop, epoch 2, step 229, the loss is 224001.671875\n",
            "in training loop, epoch 2, step 230, the loss is 312386.09375\n",
            "in training loop, epoch 2, step 231, the loss is 125345.3828125\n",
            "in training loop, epoch 2, step 232, the loss is 186259.609375\n",
            "in training loop, epoch 2, step 233, the loss is 159445.125\n",
            "in training loop, epoch 2, step 234, the loss is 169125.046875\n",
            "in training loop, epoch 2, step 235, the loss is 146086.96875\n",
            "in training loop, epoch 2, step 236, the loss is 161925.875\n",
            "in training loop, epoch 2, step 237, the loss is 169135.046875\n",
            "in training loop, epoch 2, step 238, the loss is 118837.828125\n",
            "in training loop, epoch 2, step 239, the loss is 130487.296875\n",
            "in training loop, epoch 2, step 240, the loss is 242527.625\n",
            "in training loop, epoch 2, step 241, the loss is 198039.5\n",
            "in training loop, epoch 2, step 242, the loss is 162229.03125\n",
            "in training loop, epoch 2, step 243, the loss is 148325.796875\n",
            "in training loop, epoch 2, step 244, the loss is 117498.9296875\n",
            "in training loop, epoch 2, step 245, the loss is 158121.515625\n",
            "in training loop, epoch 2, step 246, the loss is 181446.90625\n",
            "in training loop, epoch 2, step 247, the loss is 192602.25\n",
            "in training loop, epoch 2, step 248, the loss is 177834.328125\n",
            "in training loop, epoch 2, step 249, the loss is 125035.234375\n",
            "in training loop, epoch 2, step 250, the loss is 137500.21875\n",
            "in training loop, epoch 2, step 251, the loss is 266556.4375\n",
            "in training loop, epoch 2, step 252, the loss is 152083.296875\n",
            "in training loop, epoch 2, step 253, the loss is 126409.6640625\n",
            "in training loop, epoch 2, step 254, the loss is 261622.765625\n",
            "in training loop, epoch 2, step 255, the loss is 154536.96875\n",
            "in training loop, epoch 2, step 256, the loss is 217155.828125\n",
            "in training loop, epoch 2, step 257, the loss is 254569.109375\n",
            "in training loop, epoch 2, step 258, the loss is 155316.3125\n",
            "in training loop, epoch 2, step 259, the loss is 145818.0625\n",
            "in training loop, epoch 2, step 260, the loss is 203766.71875\n",
            "in training loop, epoch 2, step 261, the loss is 161453.25\n",
            "in training loop, epoch 2, step 262, the loss is 161602.6875\n",
            "in training loop, epoch 2, step 263, the loss is 385823.6875\n",
            "in training loop, epoch 2, step 264, the loss is 207055.421875\n",
            "in training loop, epoch 2, step 265, the loss is 109856.3984375\n",
            "in training loop, epoch 2, step 266, the loss is 237793.75\n",
            "in training loop, epoch 2, step 267, the loss is 185559.796875\n",
            "in training loop, epoch 2, step 268, the loss is 222879.5\n",
            "in training loop, epoch 2, step 269, the loss is 218735.75\n",
            "in training loop, epoch 2, step 270, the loss is 212531.859375\n",
            "in training loop, epoch 2, step 271, the loss is 184331.9375\n",
            "in training loop, epoch 2, step 272, the loss is 147452.28125\n",
            "in training loop, epoch 2, step 273, the loss is 204606.296875\n",
            "in training loop, epoch 2, step 274, the loss is 214995.953125\n",
            "in training loop, epoch 2, step 275, the loss is 306038.46875\n",
            "in training loop, epoch 2, step 276, the loss is 450848.625\n",
            "in training loop, epoch 2, step 277, the loss is 196958.203125\n",
            "in training loop, epoch 2, step 278, the loss is 195688.71875\n",
            "in training loop, epoch 2, step 279, the loss is 134378.90625\n",
            "in training loop, epoch 2, step 280, the loss is 221279.265625\n",
            "in training loop, epoch 2, step 281, the loss is 266289.5\n",
            "in training loop, epoch 2, step 282, the loss is 287303.65625\n",
            "in training loop, epoch 2, step 283, the loss is 274319.1875\n",
            "in training loop, epoch 2, step 284, the loss is 142234.484375\n",
            "in training loop, epoch 2, step 285, the loss is 153408.78125\n",
            "in training loop, epoch 2, step 286, the loss is 205772.078125\n",
            "in training loop, epoch 2, step 287, the loss is 153838.21875\n",
            "in training loop, epoch 2, step 288, the loss is 260223.203125\n",
            "in training loop, epoch 2, step 289, the loss is 159409.765625\n",
            "in training loop, epoch 2, step 290, the loss is 196775.984375\n",
            "in training loop, epoch 2, step 291, the loss is 300533.1875\n",
            "in training loop, epoch 2, step 292, the loss is 162721.875\n",
            "in training loop, epoch 2, step 293, the loss is 253779.453125\n",
            "in training loop, epoch 2, step 294, the loss is 243921.03125\n",
            "in training loop, epoch 2, step 295, the loss is 306679.5\n",
            "in training loop, epoch 2, step 296, the loss is 197005.671875\n",
            "in training loop, epoch 2, step 297, the loss is 229854.1875\n",
            "in training loop, epoch 2, step 298, the loss is 171827.90625\n",
            "in training loop, epoch 2, step 299, the loss is 229807.625\n",
            "in training loop, epoch 2, step 300, the loss is 149724.828125\n",
            "in training loop, epoch 2, step 301, the loss is 233855.515625\n",
            "in training loop, epoch 2, step 302, the loss is 213278.25\n",
            "in training loop, epoch 2, step 303, the loss is 147827.6875\n",
            "in training loop, epoch 2, step 304, the loss is 327055.84375\n",
            "in training loop, epoch 2, step 305, the loss is 205929.25\n",
            "in training loop, epoch 2, step 306, the loss is 206690.15625\n",
            "in training loop, epoch 2, step 307, the loss is 308707.0625\n",
            "in training loop, epoch 2, step 308, the loss is 211518.28125\n",
            "in training loop, epoch 2, step 309, the loss is 171531.8125\n",
            "in training loop, epoch 2, step 310, the loss is 171046.9375\n",
            "in training loop, epoch 2, step 311, the loss is 252018.71875\n",
            "in training loop, epoch 2, step 312, the loss is 194511.640625\n",
            "in training loop, epoch 2, step 313, the loss is 140357.4375\n",
            "in training loop, epoch 2, step 314, the loss is 149958.171875\n",
            "in training loop, epoch 2, step 315, the loss is 193990.125\n",
            "in training loop, epoch 2, step 316, the loss is 157753.78125\n",
            "in training loop, epoch 2, step 317, the loss is 114311.7421875\n",
            "in training loop, epoch 2, step 318, the loss is 136334.53125\n",
            "in training loop, epoch 2, step 319, the loss is 120127.65625\n",
            "in training loop, epoch 2, step 320, the loss is 143210.96875\n",
            "in training loop, epoch 2, step 321, the loss is 317455.6875\n",
            "in training loop, epoch 2, step 322, the loss is 119262.1328125\n",
            "in training loop, epoch 2, step 323, the loss is 305057.1875\n",
            "in training loop, epoch 2, step 324, the loss is 132088.796875\n",
            "in training loop, epoch 2, step 325, the loss is 217246.578125\n",
            "in training loop, epoch 2, step 326, the loss is 177042.90625\n",
            "in training loop, epoch 2, step 327, the loss is 167543.40625\n",
            "in training loop, epoch 2, step 328, the loss is 168957.109375\n",
            "in training loop, epoch 2, step 329, the loss is 187571.25\n",
            "in training loop, epoch 2, step 330, the loss is 133062.484375\n",
            "in training loop, epoch 2, step 331, the loss is 284598.5\n",
            "in training loop, epoch 2, step 332, the loss is 206243.765625\n",
            "in training loop, epoch 2, step 333, the loss is 134105.578125\n",
            "in training loop, epoch 2, step 334, the loss is 159827.40625\n",
            "in training loop, epoch 2, step 335, the loss is 151527.859375\n",
            "in training loop, epoch 2, step 336, the loss is 147656.5\n",
            "in training loop, epoch 2, step 337, the loss is 155325.421875\n",
            "in training loop, epoch 2, step 338, the loss is 394240.90625\n",
            "in training loop, epoch 2, step 339, the loss is 181151.921875\n",
            "in training loop, epoch 2, step 340, the loss is 129784.2265625\n",
            "in training loop, epoch 2, step 341, the loss is 128401.296875\n",
            "in training loop, epoch 2, step 342, the loss is 152779.59375\n",
            "in training loop, epoch 2, step 343, the loss is 165857.234375\n",
            "in training loop, epoch 2, step 344, the loss is 170651.515625\n",
            "in training loop, epoch 2, step 345, the loss is 140321.1875\n",
            "in training loop, epoch 2, step 346, the loss is 187441.9375\n",
            "in training loop, epoch 2, step 347, the loss is 155992.9375\n",
            "in training loop, epoch 2, step 348, the loss is 135397.65625\n",
            "in training loop, epoch 2, step 349, the loss is 149968.578125\n",
            "in training loop, epoch 2, step 350, the loss is 133133.40625\n",
            "in training loop, epoch 2, step 351, the loss is 110065.5234375\n",
            "in training loop, epoch 2, step 352, the loss is 170265.765625\n",
            "in training loop, epoch 2, step 353, the loss is 236384.28125\n",
            "in training loop, epoch 2, step 354, the loss is 163723.703125\n",
            "in training loop, epoch 2, step 355, the loss is 171052.9375\n",
            "in training loop, epoch 2, step 356, the loss is 232964.03125\n",
            "in training loop, epoch 2, step 357, the loss is 160111.34375\n",
            "in training loop, epoch 2, step 358, the loss is 201184.28125\n",
            "in training loop, epoch 2, step 359, the loss is 330887.84375\n",
            "in training loop, epoch 2, step 360, the loss is 172893.5\n",
            "in training loop, epoch 2, step 361, the loss is 232530.515625\n",
            "in training loop, epoch 2, step 362, the loss is 250786.21875\n",
            "in training loop, epoch 2, step 363, the loss is 265510.1875\n",
            "in training loop, epoch 2, step 364, the loss is 581948.9375\n",
            "in training loop, epoch 2, step 365, the loss is 204278.46875\n",
            "in training loop, epoch 2, step 366, the loss is 331979.0\n",
            "in training loop, epoch 2, step 367, the loss is 297610.5625\n",
            "in training loop, epoch 2, step 368, the loss is 226560.515625\n",
            "in training loop, epoch 2, step 369, the loss is 259260.21875\n",
            "in training loop, epoch 2, step 370, the loss is 624770.875\n",
            "in training loop, epoch 2, step 371, the loss is 241605.28125\n",
            "in training loop, epoch 2, step 372, the loss is 244740.765625\n",
            "in training loop, epoch 2, step 373, the loss is 523128.6875\n",
            "in training loop, epoch 2, step 374, the loss is 207877.84375\n",
            "in training loop, epoch 2, step 375, the loss is 161171.03125\n",
            "in training loop, epoch 2, step 376, the loss is 185586.265625\n",
            "in training loop, epoch 2, step 377, the loss is 165462.609375\n",
            "in training loop, epoch 2, step 378, the loss is 520213.53125\n",
            "in training loop, epoch 2, step 379, the loss is 251978.375\n",
            "in training loop, epoch 2, step 380, the loss is 293638.71875\n",
            "in training loop, epoch 2, step 381, the loss is 219133.40625\n",
            "in training loop, epoch 2, step 382, the loss is 292986.65625\n",
            "in training loop, epoch 2, step 383, the loss is 172521.125\n",
            "in training loop, epoch 2, step 384, the loss is 195724.96875\n",
            "in training loop, epoch 2, step 385, the loss is 173598.875\n",
            "in training loop, epoch 2, step 386, the loss is 191839.75\n",
            "in training loop, epoch 2, step 387, the loss is 193120.828125\n",
            "in training loop, epoch 2, step 388, the loss is 230699.875\n",
            "in training loop, epoch 2, step 389, the loss is 153568.140625\n",
            "in training loop, epoch 2, step 390, the loss is 226033.234375\n",
            "in training loop, epoch 2, step 391, the loss is 201698.65625\n",
            "in training loop, epoch 2, step 392, the loss is 176530.359375\n",
            "in training loop, epoch 2, step 393, the loss is 170404.375\n",
            "in training loop, epoch 2, step 394, the loss is 186999.0625\n",
            "in training loop, epoch 2, step 395, the loss is 160784.875\n",
            "in training loop, epoch 2, step 396, the loss is 150680.71875\n",
            "in training loop, epoch 2, step 397, the loss is 303001.5\n",
            "in training loop, epoch 2, step 398, the loss is 155149.046875\n",
            "in training loop, epoch 2, step 399, the loss is 141818.0625\n",
            "in training loop, epoch 2, step 400, the loss is 117937.359375\n",
            "in training loop, epoch 2, step 401, the loss is 198923.984375\n",
            "in training loop, epoch 2, step 402, the loss is 203510.390625\n",
            "in training loop, epoch 2, step 403, the loss is 120963.40625\n",
            "in training loop, epoch 2, step 404, the loss is 150102.421875\n",
            "in training loop, epoch 2, step 405, the loss is 110294.671875\n",
            "in training loop, epoch 2, step 406, the loss is 265816.03125\n",
            "in training loop, epoch 2, step 407, the loss is 228004.875\n",
            "in training loop, epoch 2, step 408, the loss is 106661.4140625\n",
            "in training loop, epoch 2, step 409, the loss is 113791.703125\n",
            "in training loop, epoch 2, step 410, the loss is 168821.484375\n",
            "in training loop, epoch 2, step 411, the loss is 106594.625\n",
            "in training loop, epoch 2, step 412, the loss is 133629.859375\n",
            "in training loop, epoch 2, step 413, the loss is 137902.09375\n",
            "in training loop, epoch 2, step 414, the loss is 237000.96875\n",
            "in training loop, epoch 2, step 415, the loss is 130065.7109375\n",
            "in training loop, epoch 2, step 416, the loss is 145674.484375\n",
            "in training loop, epoch 2, step 417, the loss is 157772.53125\n",
            "in training loop, epoch 2, step 418, the loss is 121434.75\n",
            "in training loop, epoch 2, step 419, the loss is 118717.296875\n",
            "in training loop, epoch 2, step 420, the loss is 122476.296875\n",
            "in training loop, epoch 2, step 421, the loss is 147437.453125\n",
            "in training loop, epoch 2, step 422, the loss is 139511.953125\n",
            "in training loop, epoch 2, step 423, the loss is 147223.171875\n",
            "in training loop, epoch 2, step 424, the loss is 177895.0625\n",
            "in training loop, epoch 2, step 425, the loss is 112113.046875\n",
            "in training loop, epoch 2, step 426, the loss is 137076.9375\n",
            "in training loop, epoch 2, step 427, the loss is 165935.28125\n",
            "in training loop, epoch 2, step 428, the loss is 122248.8203125\n",
            "in training loop, epoch 2, step 429, the loss is 115255.6796875\n",
            "in training loop, epoch 2, step 430, the loss is 132269.0\n",
            "in training loop, epoch 2, step 431, the loss is 287273.1875\n",
            "in training loop, epoch 2, step 432, the loss is 135057.375\n",
            "in training loop, epoch 2, step 433, the loss is 113582.890625\n",
            "in training loop, epoch 2, step 434, the loss is 271660.78125\n",
            "in training loop, epoch 2, step 435, the loss is 118523.2578125\n",
            "in training loop, epoch 2, step 436, the loss is 131632.4375\n",
            "in training loop, epoch 2, step 437, the loss is 162438.03125\n",
            "in training loop, epoch 2, step 438, the loss is 138907.140625\n",
            "in training loop, epoch 2, step 439, the loss is 119479.1015625\n",
            "in training loop, epoch 2, step 440, the loss is 241748.0\n",
            "in training loop, epoch 2, step 441, the loss is 118312.5703125\n",
            "in training loop, epoch 2, step 442, the loss is 122654.3515625\n",
            "in training loop, epoch 2, step 443, the loss is 128795.78125\n",
            "in training loop, epoch 2, step 444, the loss is 136293.40625\n",
            "in training loop, epoch 2, step 445, the loss is 121836.65625\n",
            "in training loop, epoch 2, step 446, the loss is 172807.296875\n",
            "in training loop, epoch 2, step 447, the loss is 205891.09375\n",
            "in training loop, epoch 2, step 448, the loss is 134246.796875\n",
            "in training loop, epoch 2, step 449, the loss is 336000.65625\n",
            "in training loop, epoch 2, step 450, the loss is 148887.890625\n",
            "in training loop, epoch 2, step 451, the loss is 98566.953125\n",
            "in training loop, epoch 2, step 452, the loss is 170998.5625\n",
            "in training loop, epoch 2, step 453, the loss is 127804.90625\n",
            "in training loop, epoch 2, step 454, the loss is 335852.0625\n",
            "in training loop, epoch 2, step 455, the loss is 127626.390625\n",
            "in training loop, epoch 2, step 456, the loss is 194078.984375\n",
            "in training loop, epoch 2, step 457, the loss is 155923.53125\n",
            "in training loop, epoch 2, step 458, the loss is 154450.984375\n",
            "in training loop, epoch 2, step 459, the loss is 165806.90625\n",
            "in training loop, epoch 2, step 460, the loss is 150842.40625\n",
            "in training loop, epoch 2, step 461, the loss is 263715.6875\n",
            "in training loop, epoch 2, step 462, the loss is 137054.6875\n",
            "in training loop, epoch 2, step 463, the loss is 133577.96875\n",
            "in training loop, epoch 2, step 464, the loss is 205314.34375\n",
            "in training loop, epoch 2, step 465, the loss is 1520465.0\n",
            "in training loop, epoch 2, step 466, the loss is 309074.8125\n",
            "in training loop, epoch 2, step 467, the loss is 294911.53125\n",
            "in training loop, epoch 2, step 468, the loss is 607557.0625\n",
            "in training loop, epoch 2, step 469, the loss is 237505.40625\n",
            "in training loop, epoch 2, step 470, the loss is 188832.046875\n",
            "in training loop, epoch 2, step 471, the loss is 270520.125\n",
            "in training loop, epoch 2, step 472, the loss is 274727.71875\n",
            "in training loop, epoch 2, step 473, the loss is 353320.5625\n",
            "in training loop, epoch 2, step 474, the loss is 224939.46875\n",
            "in training loop, epoch 2, step 475, the loss is 259359.5\n",
            "in training loop, epoch 2, step 476, the loss is 214582.40625\n",
            "in training loop, epoch 2, step 477, the loss is 396466.625\n",
            "in training loop, epoch 2, step 478, the loss is 215915.59375\n",
            "in training loop, epoch 2, step 479, the loss is 332210.71875\n",
            "in training loop, epoch 2, step 480, the loss is 224462.890625\n",
            "in training loop, epoch 2, step 481, the loss is 209206.484375\n",
            "in training loop, epoch 2, step 482, the loss is 344180.0625\n",
            "in training loop, epoch 2, step 483, the loss is 171057.046875\n",
            "in training loop, epoch 2, step 484, the loss is 185399.625\n",
            "in training loop, epoch 2, step 485, the loss is 456915.53125\n",
            "in training loop, epoch 2, step 486, the loss is 375966.90625\n",
            "in training loop, epoch 2, step 487, the loss is 148149.65625\n",
            "in training loop, epoch 2, step 488, the loss is 224420.90625\n",
            "in training loop, epoch 2, step 489, the loss is 251877.15625\n",
            "in training loop, epoch 2, step 490, the loss is 277465.375\n",
            "in training loop, epoch 2, step 491, the loss is 226074.46875\n",
            "in training loop, epoch 2, step 492, the loss is 283322.9375\n",
            "in training loop, epoch 2, step 493, the loss is 508022.1875\n",
            "in training loop, epoch 2, step 494, the loss is 221694.5625\n",
            "in training loop, epoch 2, step 495, the loss is 222985.6875\n",
            "in training loop, epoch 2, step 496, the loss is 205050.0\n",
            "in training loop, epoch 2, step 497, the loss is 366508.9375\n",
            "in training loop, epoch 2, step 498, the loss is 250916.390625\n",
            "in training loop, epoch 2, step 499, the loss is 220418.125\n",
            "in training loop, epoch 2, step 500, the loss is 173657.375\n",
            "in training loop, epoch 2, step 501, the loss is 338508.375\n",
            "in training loop, epoch 2, step 502, the loss is 385896.78125\n",
            "in training loop, epoch 2, step 503, the loss is 179787.65625\n",
            "in training loop, epoch 2, step 504, the loss is 157875.109375\n",
            "in training loop, epoch 2, step 505, the loss is 180218.34375\n",
            "in training loop, epoch 2, step 506, the loss is 189608.5625\n",
            "in training loop, epoch 2, step 507, the loss is 147041.234375\n",
            "in training loop, epoch 2, step 508, the loss is 188474.3125\n",
            "in training loop, epoch 2, step 509, the loss is 183774.71875\n",
            "in training loop, epoch 2, step 510, the loss is 142561.265625\n",
            "in training loop, epoch 2, step 511, the loss is 187886.5625\n",
            "in training loop, epoch 2, step 512, the loss is 137943.109375\n",
            "in training loop, epoch 2, step 513, the loss is 155630.484375\n",
            "in training loop, epoch 2, step 514, the loss is 243632.4375\n",
            "in training loop, epoch 2, step 515, the loss is 158050.984375\n",
            "in training loop, epoch 2, step 516, the loss is 126071.7265625\n",
            "in training loop, epoch 2, step 517, the loss is 123712.484375\n",
            "in training loop, epoch 2, step 518, the loss is 203483.640625\n",
            "in training loop, epoch 2, step 519, the loss is 137483.21875\n",
            "in training loop, epoch 2, step 520, the loss is 157322.40625\n",
            "in training loop, epoch 2, step 521, the loss is 164549.796875\n",
            "in training loop, epoch 2, step 522, the loss is 169538.828125\n",
            "in training loop, epoch 2, step 523, the loss is 173273.546875\n",
            "in training loop, epoch 2, step 524, the loss is 158872.34375\n",
            "in training loop, epoch 2, step 525, the loss is 150317.609375\n",
            "in training loop, epoch 2, step 526, the loss is 113000.0625\n",
            "in training loop, epoch 2, step 527, the loss is 238660.921875\n",
            "in training loop, epoch 2, step 528, the loss is 155463.3125\n",
            "in training loop, epoch 2, step 529, the loss is 146223.6875\n",
            "in training loop, epoch 2, step 530, the loss is 181143.40625\n",
            "in training loop, epoch 2, step 531, the loss is 342189.1875\n",
            "in training loop, epoch 2, step 532, the loss is 149405.15625\n",
            "in training loop, epoch 2, step 533, the loss is 186292.375\n",
            "in training loop, epoch 2, step 534, the loss is 160144.75\n",
            "in training loop, epoch 2, step 535, the loss is 194039.09375\n",
            "in training loop, epoch 2, step 536, the loss is 123361.875\n",
            "in training loop, epoch 2, step 537, the loss is 224321.125\n",
            "in training loop, epoch 2, step 538, the loss is 197145.75\n",
            "in training loop, epoch 2, step 539, the loss is 191416.6875\n",
            "in training loop, epoch 2, step 540, the loss is 142835.640625\n",
            "in training loop, epoch 2, step 541, the loss is 158141.671875\n",
            "in training loop, epoch 2, step 542, the loss is 228228.46875\n",
            "in training loop, epoch 2, step 543, the loss is 130387.15625\n",
            "in training loop, epoch 2, step 544, the loss is 244375.84375\n",
            "in training loop, epoch 2, step 545, the loss is 148509.59375\n",
            "in training loop, epoch 2, step 546, the loss is 119333.1015625\n",
            "in training loop, epoch 2, step 547, the loss is 177128.625\n",
            "in training loop, epoch 2, step 548, the loss is 258838.046875\n",
            "in training loop, epoch 2, step 549, the loss is 300865.9375\n",
            "in training loop, epoch 2, step 550, the loss is 136289.84375\n",
            "in training loop, epoch 2, step 551, the loss is 228178.8125\n",
            "in training loop, epoch 2, step 552, the loss is 168307.96875\n",
            "in training loop, epoch 2, step 553, the loss is 197774.96875\n",
            "in training loop, epoch 2, step 554, the loss is 144561.1875\n",
            "in training loop, epoch 2, step 555, the loss is 227886.390625\n",
            "in training loop, epoch 2, step 556, the loss is 159541.0\n",
            "in training loop, epoch 2, step 557, the loss is 203062.0\n",
            "in training loop, epoch 2, step 558, the loss is 194156.875\n",
            "in training loop, epoch 2, step 559, the loss is 188320.921875\n",
            "in training loop, epoch 2, step 560, the loss is 166145.28125\n",
            "in training loop, epoch 2, step 561, the loss is 155868.0\n",
            "in training loop, epoch 2, step 562, the loss is 124526.9609375\n",
            "in training loop, epoch 2, step 563, the loss is 181933.40625\n",
            "in training loop, epoch 2, step 564, the loss is 138491.65625\n",
            "in training loop, epoch 2, step 565, the loss is 286158.625\n",
            "in training loop, epoch 2, step 566, the loss is 120860.5859375\n",
            "in training loop, epoch 2, step 567, the loss is 196124.015625\n",
            "in training loop, epoch 2, step 568, the loss is 151266.34375\n",
            "in training loop, epoch 2, step 569, the loss is 242309.59375\n",
            "in training loop, epoch 2, step 570, the loss is 110533.625\n",
            "in training loop, epoch 2, step 571, the loss is 166198.1875\n",
            "in training loop, epoch 2, step 572, the loss is 196419.078125\n",
            "in training loop, epoch 2, step 573, the loss is 135591.1875\n",
            "in training loop, epoch 2, step 574, the loss is 192622.09375\n",
            "in training loop, epoch 2, step 575, the loss is 153583.84375\n",
            "in training loop, epoch 2, step 576, the loss is 178737.046875\n",
            "in training loop, epoch 2, step 577, the loss is 186084.234375\n",
            "in training loop, epoch 2, step 578, the loss is 201380.890625\n",
            "in training loop, epoch 2, step 579, the loss is 163905.78125\n",
            "in training loop, epoch 2, step 580, the loss is 149208.59375\n",
            "in training loop, epoch 2, step 581, the loss is 152726.59375\n",
            "in training loop, epoch 2, step 582, the loss is 159025.703125\n",
            "in training loop, epoch 2, step 583, the loss is 185876.65625\n",
            "in training loop, epoch 2, step 584, the loss is 128389.0859375\n",
            "in training loop, epoch 2, step 585, the loss is 159575.171875\n",
            "in training loop, epoch 2, step 586, the loss is 112122.9609375\n",
            "in training loop, epoch 2, step 587, the loss is 241742.6875\n",
            "in training loop, epoch 2, step 588, the loss is 244258.3125\n",
            "in training loop, epoch 2, step 589, the loss is 193997.515625\n",
            "in training loop, epoch 2, step 590, the loss is 125056.46875\n",
            "in training loop, epoch 2, step 591, the loss is 155727.640625\n",
            "in training loop, epoch 2, step 592, the loss is 220740.0\n",
            "in training loop, epoch 2, step 593, the loss is 180954.375\n",
            "in training loop, epoch 2, step 594, the loss is 246694.03125\n",
            "in training loop, epoch 2, step 595, the loss is 196089.921875\n",
            "in training loop, epoch 2, step 596, the loss is 159563.109375\n",
            "in training loop, epoch 2, step 597, the loss is 155855.8125\n",
            "in training loop, epoch 2, step 598, the loss is 116840.1796875\n",
            "in training loop, epoch 2, step 599, the loss is 152163.90625\n",
            "in training loop, epoch 2, step 600, the loss is 194727.0\n",
            "in training loop, epoch 2, step 601, the loss is 152366.390625\n",
            "in training loop, epoch 2, step 602, the loss is 339270.8125\n",
            "in training loop, epoch 2, step 603, the loss is 127314.234375\n",
            "in training loop, epoch 2, step 604, the loss is 129642.984375\n",
            "in training loop, epoch 2, step 605, the loss is 135587.1875\n",
            "in training loop, epoch 2, step 606, the loss is 116901.5703125\n",
            "in training loop, epoch 2, step 607, the loss is 154292.265625\n",
            "in training loop, epoch 2, step 608, the loss is 158483.15625\n",
            "in training loop, epoch 2, step 609, the loss is 110619.453125\n",
            "in training loop, epoch 2, step 610, the loss is 158562.375\n",
            "in training loop, epoch 2, step 611, the loss is 217947.5625\n",
            "in training loop, epoch 2, step 612, the loss is 220480.375\n",
            "in training loop, epoch 2, step 613, the loss is 201244.609375\n",
            "in training loop, epoch 2, step 614, the loss is 144778.21875\n",
            "in training loop, epoch 2, step 615, the loss is 119244.9375\n",
            "in training loop, epoch 2, step 616, the loss is 239913.484375\n",
            "in training loop, epoch 2, step 617, the loss is 160848.546875\n",
            "in training loop, epoch 2, step 618, the loss is 167679.765625\n",
            "in training loop, epoch 2, step 619, the loss is 122923.6015625\n",
            "in training loop, epoch 2, step 620, the loss is 271808.0\n",
            "in training loop, epoch 2, step 621, the loss is 107065.890625\n",
            "in training loop, epoch 2, step 622, the loss is 276915.875\n",
            "in training loop, epoch 2, step 623, the loss is 144975.8125\n",
            "in training loop, epoch 2, step 624, the loss is 182777.265625\n",
            "in training loop, epoch 2, step 625, the loss is 134727.21875\n",
            "in training loop, epoch 2, step 626, the loss is 140670.75\n",
            "in training loop, epoch 2, step 627, the loss is 290116.71875\n",
            "in training loop, epoch 2, step 628, the loss is 149860.65625\n",
            "in training loop, epoch 2, step 629, the loss is 151121.671875\n",
            "in training loop, epoch 2, step 630, the loss is 185019.3125\n",
            "in training loop, epoch 2, step 631, the loss is 129921.34375\n",
            "in training loop, epoch 2, step 632, the loss is 115880.0625\n",
            "in training loop, epoch 2, step 633, the loss is 176733.8125\n",
            "in training loop, epoch 2, step 634, the loss is 258782.15625\n",
            "in training loop, epoch 2, step 635, the loss is 532419.5625\n",
            "in training loop, epoch 2, step 636, the loss is 271479.34375\n",
            "in training loop, epoch 2, step 637, the loss is 183466.65625\n",
            "in training loop, epoch 2, step 638, the loss is 129106.65625\n",
            "in training loop, epoch 2, step 639, the loss is 215923.984375\n",
            "in training loop, epoch 2, step 640, the loss is 185059.6875\n",
            "in training loop, epoch 2, step 641, the loss is 166428.0\n",
            "in training loop, epoch 2, step 642, the loss is 170374.59375\n",
            "in training loop, epoch 2, step 643, the loss is 256417.5625\n",
            "in training loop, epoch 2, step 644, the loss is 114499.34375\n",
            "in training loop, epoch 2, step 645, the loss is 123192.078125\n",
            "in training loop, epoch 2, step 646, the loss is 258519.6875\n",
            "in training loop, epoch 2, step 647, the loss is 215129.71875\n",
            "in training loop, epoch 2, step 648, the loss is 110933.546875\n",
            "in training loop, epoch 2, step 649, the loss is 161426.15625\n",
            "in training loop, epoch 2, step 650, the loss is 148206.46875\n",
            "in training loop, epoch 2, step 651, the loss is 136491.59375\n",
            "in training loop, epoch 2, step 652, the loss is 158773.109375\n",
            "in training loop, epoch 2, step 653, the loss is 252134.65625\n",
            "in training loop, epoch 2, step 654, the loss is 287196.5625\n",
            "in training loop, epoch 2, step 655, the loss is 118314.078125\n",
            "in training loop, epoch 2, step 656, the loss is 154950.34375\n",
            "in training loop, epoch 2, step 657, the loss is 220356.625\n",
            "in training loop, epoch 2, step 658, the loss is 124059.578125\n",
            "in training loop, epoch 2, step 659, the loss is 131287.3125\n",
            "in training loop, epoch 2, step 660, the loss is 141779.578125\n",
            "in training loop, epoch 2, step 661, the loss is 213870.921875\n",
            "in training loop, epoch 2, step 662, the loss is 162770.8125\n",
            "in training loop, epoch 2, step 663, the loss is 114653.5234375\n",
            "in training loop, epoch 2, step 664, the loss is 221626.53125\n",
            "in training loop, epoch 2, step 665, the loss is 220716.59375\n",
            "in training loop, epoch 2, step 666, the loss is 130854.4453125\n",
            "in training loop, epoch 2, step 667, the loss is 201788.140625\n",
            "in training loop, epoch 2, step 668, the loss is 252404.078125\n",
            "in training loop, epoch 2, step 669, the loss is 168568.5\n",
            "in training loop, epoch 2, step 670, the loss is 284749.78125\n",
            "in training loop, epoch 2, step 671, the loss is 137986.46875\n",
            "in training loop, epoch 2, step 672, the loss is 194045.0625\n",
            "in training loop, epoch 2, step 673, the loss is 125249.6953125\n",
            "in training loop, epoch 2, step 674, the loss is 147185.5\n",
            "in training loop, epoch 2, step 675, the loss is 243960.9375\n",
            "in training loop, epoch 2, step 676, the loss is 303215.25\n",
            "in training loop, epoch 2, step 677, the loss is 178416.78125\n",
            "in training loop, epoch 2, step 678, the loss is 222483.84375\n",
            "in training loop, epoch 2, step 679, the loss is 300843.1875\n",
            "in training loop, epoch 2, step 680, the loss is 200325.859375\n",
            "in training loop, epoch 2, step 681, the loss is 147147.640625\n",
            "in training loop, epoch 2, step 682, the loss is 157079.75\n",
            "in training loop, epoch 2, step 683, the loss is 117341.484375\n",
            "in training loop, epoch 2, step 684, the loss is 292992.6875\n",
            "in training loop, epoch 2, step 685, the loss is 152991.59375\n",
            "in training loop, epoch 2, step 686, the loss is 206073.09375\n",
            "in training loop, epoch 2, step 687, the loss is 207259.171875\n",
            "in training loop, epoch 2, step 688, the loss is 136675.796875\n",
            "in training loop, epoch 2, step 689, the loss is 193403.578125\n",
            "in training loop, epoch 2, step 690, the loss is 247583.65625\n",
            "in training loop, epoch 2, step 691, the loss is 321628.1875\n",
            "in training loop, epoch 2, step 692, the loss is 227304.6875\n",
            "in training loop, epoch 2, step 693, the loss is 160016.140625\n",
            "in training loop, epoch 2, step 694, the loss is 120109.40625\n",
            "in training loop, epoch 2, step 695, the loss is 200547.328125\n",
            "in training loop, epoch 2, step 696, the loss is 213644.15625\n",
            "in training loop, epoch 2, step 697, the loss is 165560.90625\n",
            "in training loop, epoch 2, step 698, the loss is 172769.546875\n",
            "in training loop, epoch 2, step 699, the loss is 236661.34375\n",
            "in training loop, epoch 2, step 700, the loss is 169345.1875\n",
            "in training loop, epoch 2, step 701, the loss is 261802.171875\n",
            "in training loop, epoch 2, step 702, the loss is 168667.140625\n",
            "in training loop, epoch 2, step 703, the loss is 126953.6796875\n",
            "in training loop, epoch 2, step 704, the loss is 177877.5625\n",
            "in training loop, epoch 2, step 705, the loss is 144935.234375\n",
            "in training loop, epoch 2, step 706, the loss is 151634.265625\n",
            "in training loop, epoch 2, step 707, the loss is 238497.78125\n",
            "in training loop, epoch 2, step 708, the loss is 179370.28125\n",
            "in training loop, epoch 2, step 709, the loss is 126955.0625\n",
            "in training loop, epoch 2, step 710, the loss is 255377.0625\n",
            "in training loop, epoch 2, step 711, the loss is 135497.9375\n",
            "in training loop, epoch 2, step 712, the loss is 129023.46875\n",
            "in training loop, epoch 2, step 713, the loss is 139110.671875\n",
            "in training loop, epoch 2, step 714, the loss is 178651.359375\n",
            "in training loop, epoch 2, step 715, the loss is 143757.859375\n",
            "in training loop, epoch 2, step 716, the loss is 161464.78125\n",
            "in training loop, epoch 2, step 717, the loss is 129768.765625\n",
            "in training loop, epoch 2, step 718, the loss is 190556.984375\n",
            "in training loop, epoch 2, step 719, the loss is 135795.65625\n",
            "in training loop, epoch 2, step 720, the loss is 273074.84375\n",
            "in training loop, epoch 2, step 721, the loss is 133882.109375\n",
            "in training loop, epoch 2, step 722, the loss is 104547.6484375\n",
            "in training loop, epoch 2, step 723, the loss is 146332.6875\n",
            "in training loop, epoch 2, step 724, the loss is 151362.984375\n",
            "in training loop, epoch 2, step 725, the loss is 142681.875\n",
            "in training loop, epoch 2, step 726, the loss is 118159.9296875\n",
            "in training loop, epoch 2, step 727, the loss is 145819.671875\n",
            "in training loop, epoch 2, step 728, the loss is 198181.390625\n",
            "in training loop, epoch 2, step 729, the loss is 161309.015625\n",
            "in training loop, epoch 2, step 730, the loss is 131407.5625\n",
            "in training loop, epoch 2, step 731, the loss is 182821.40625\n",
            "in training loop, epoch 2, step 732, the loss is 152891.875\n",
            "in training loop, epoch 2, step 733, the loss is 115108.1171875\n",
            "in training loop, epoch 2, step 734, the loss is 146277.78125\n",
            "in training loop, epoch 2, step 735, the loss is 173661.453125\n",
            "in training loop, epoch 2, step 736, the loss is 162562.4375\n",
            "in training loop, epoch 2, step 737, the loss is 261795.921875\n",
            "in training loop, epoch 2, step 738, the loss is 148899.25\n",
            "in training loop, epoch 2, step 739, the loss is 150190.78125\n",
            "in training loop, epoch 2, step 740, the loss is 129202.0\n",
            "in training loop, epoch 2, step 741, the loss is 175512.875\n",
            "in training loop, epoch 2, step 742, the loss is 128963.796875\n",
            "in training loop, epoch 2, step 743, the loss is 277690.71875\n",
            "in training loop, epoch 2, step 744, the loss is 121285.4765625\n",
            "in training loop, epoch 2, step 745, the loss is 277402.3125\n",
            "in training loop, epoch 2, step 746, the loss is 137826.609375\n",
            "in training loop, epoch 2, step 747, the loss is 131002.21875\n",
            "in training loop, epoch 2, step 748, the loss is 135371.375\n",
            "in training loop, epoch 2, step 749, the loss is 154516.125\n",
            "in training loop, epoch 2, step 750, the loss is 313445.03125\n",
            "in training loop, epoch 2, step 751, the loss is 196851.90625\n",
            "in training loop, epoch 2, step 752, the loss is 144438.46875\n",
            "in training loop, epoch 2, step 753, the loss is 139328.828125\n",
            "in training loop, epoch 2, step 754, the loss is 251077.8125\n",
            "in training loop, epoch 2, step 755, the loss is 164150.34375\n",
            "in training loop, epoch 2, step 756, the loss is 140687.09375\n",
            "in training loop, epoch 2, step 757, the loss is 140250.109375\n",
            "in training loop, epoch 2, step 758, the loss is 153711.625\n",
            "in training loop, epoch 2, step 759, the loss is 172486.5625\n",
            "in training loop, epoch 2, step 760, the loss is 180581.265625\n",
            "in training loop, epoch 2, step 761, the loss is 199380.390625\n",
            "in training loop, epoch 2, step 762, the loss is 175208.25\n",
            "in training loop, epoch 2, step 763, the loss is 143934.78125\n",
            "in training loop, epoch 2, step 764, the loss is 188462.59375\n",
            "in training loop, epoch 2, step 765, the loss is 177249.59375\n",
            "in training loop, epoch 2, step 766, the loss is 173996.421875\n",
            "in training loop, epoch 2, step 767, the loss is 241068.140625\n",
            "in training loop, epoch 2, step 768, the loss is 215084.71875\n",
            "in training loop, epoch 2, step 769, the loss is 147911.40625\n",
            "in training loop, epoch 2, step 770, the loss is 129236.2734375\n",
            "in training loop, epoch 2, step 771, the loss is 166706.625\n",
            "in training loop, epoch 2, step 772, the loss is 171793.28125\n",
            "in training loop, epoch 2, step 773, the loss is 115906.421875\n",
            "in training loop, epoch 2, step 774, the loss is 145391.34375\n",
            "in training loop, epoch 2, step 775, the loss is 95093.328125\n",
            "in training loop, epoch 2, step 776, the loss is 163663.6875\n",
            "in training loop, epoch 2, step 777, the loss is 139359.015625\n",
            "in training loop, epoch 2, step 778, the loss is 103077.265625\n",
            "in training loop, epoch 2, step 779, the loss is 273970.03125\n",
            "in training loop, epoch 2, step 780, the loss is 159339.578125\n",
            "in training loop, epoch 2, step 781, the loss is 140487.59375\n",
            "in training loop, epoch 2, step 782, the loss is 187706.203125\n",
            "in training loop, epoch 2, step 783, the loss is 162303.984375\n",
            "in training loop, epoch 2, step 784, the loss is 127338.421875\n",
            "in training loop, epoch 2, step 785, the loss is 143542.625\n",
            "in training loop, epoch 2, step 786, the loss is 224441.8125\n",
            "in training loop, epoch 2, step 787, the loss is 150218.1875\n",
            "in training loop, epoch 2, step 788, the loss is 158593.46875\n",
            "in training loop, epoch 2, step 789, the loss is 125559.9765625\n",
            "in training loop, epoch 2, step 790, the loss is 196533.515625\n",
            "in training loop, epoch 2, step 791, the loss is 157503.34375\n",
            "in training loop, epoch 2, step 792, the loss is 170474.21875\n",
            "in training loop, epoch 2, step 793, the loss is 204927.84375\n",
            "in training loop, epoch 2, step 794, the loss is 116660.234375\n",
            "in training loop, epoch 2, step 795, the loss is 131656.125\n",
            "in training loop, epoch 2, step 796, the loss is 146400.875\n",
            "in training loop, epoch 2, step 797, the loss is 142801.828125\n",
            "in training loop, epoch 2, step 798, the loss is 199481.53125\n",
            "in training loop, epoch 2, step 799, the loss is 170264.71875\n",
            "in training loop, epoch 2, step 800, the loss is 249163.1875\n",
            "in training loop, epoch 2, step 801, the loss is 258431.890625\n",
            "in training loop, epoch 2, step 802, the loss is 202639.234375\n",
            "in training loop, epoch 2, step 803, the loss is 334938.1875\n",
            "in training loop, epoch 2, step 804, the loss is 170708.328125\n",
            "in training loop, epoch 2, step 805, the loss is 206840.671875\n",
            "in training loop, epoch 2, step 806, the loss is 186682.90625\n",
            "in training loop, epoch 2, step 807, the loss is 229049.84375\n",
            "in training loop, epoch 2, step 808, the loss is 186210.890625\n",
            "in training loop, epoch 2, step 809, the loss is 297696.28125\n",
            "in training loop, epoch 2, step 810, the loss is 219935.234375\n",
            "in training loop, epoch 2, step 811, the loss is 194856.484375\n",
            "in training loop, epoch 2, step 812, the loss is 181911.625\n",
            "in training loop, epoch 2, step 813, the loss is 151568.53125\n",
            "in training loop, epoch 2, step 814, the loss is 177816.953125\n",
            "in training loop, epoch 2, step 815, the loss is 163740.4375\n",
            "in training loop, epoch 2, step 816, the loss is 148270.703125\n",
            "in training loop, epoch 2, step 817, the loss is 245350.328125\n",
            "in training loop, epoch 2, step 818, the loss is 149433.96875\n",
            "in training loop, epoch 2, step 819, the loss is 296633.65625\n",
            "in training loop, epoch 2, step 820, the loss is 139457.71875\n",
            "in training loop, epoch 2, step 821, the loss is 97643.4375\n",
            "in training loop, epoch 2, step 822, the loss is 129741.03125\n",
            "in training loop, epoch 2, step 823, the loss is 154683.3125\n",
            "in training loop, epoch 2, step 824, the loss is 151085.984375\n",
            "in training loop, epoch 2, step 825, the loss is 473613.75\n",
            "in training loop, epoch 2, step 826, the loss is 524370.0625\n",
            "in training loop, epoch 2, step 827, the loss is 330341.625\n",
            "in training loop, epoch 2, step 828, the loss is 109792.015625\n",
            "in training loop, epoch 2, step 829, the loss is 154568.546875\n",
            "in training loop, epoch 2, step 830, the loss is 384107.78125\n",
            "in training loop, epoch 2, step 831, the loss is 271433.4375\n",
            "in training loop, epoch 2, step 832, the loss is 110031.78125\n",
            "in training loop, epoch 2, step 833, the loss is 161137.0625\n",
            "in training loop, epoch 2, step 834, the loss is 190974.078125\n",
            "in training loop, epoch 2, step 835, the loss is 254991.484375\n",
            "in training loop, epoch 2, step 836, the loss is 302835.21875\n",
            "in training loop, epoch 2, step 837, the loss is 184439.203125\n",
            "in training loop, epoch 2, step 838, the loss is 142583.984375\n",
            "in training loop, epoch 2, step 839, the loss is 158849.6875\n",
            "in training loop, epoch 2, step 840, the loss is 188661.8125\n",
            "in training loop, epoch 2, step 841, the loss is 198705.5\n",
            "in training loop, epoch 2, step 842, the loss is 182455.9375\n",
            "in training loop, epoch 2, step 843, the loss is 143289.828125\n",
            "in training loop, epoch 2, step 844, the loss is 143981.1875\n",
            "in training loop, epoch 2, step 845, the loss is 344869.65625\n",
            "in training loop, epoch 2, step 846, the loss is 169221.015625\n",
            "in training loop, epoch 2, step 847, the loss is 181586.40625\n",
            "in training loop, epoch 2, step 848, the loss is 164332.671875\n",
            "in training loop, epoch 2, step 849, the loss is 154379.671875\n",
            "in training loop, epoch 2, step 850, the loss is 191909.375\n",
            "in training loop, epoch 2, step 851, the loss is 235087.65625\n",
            "in training loop, epoch 2, step 852, the loss is 132823.390625\n",
            "in training loop, epoch 2, step 853, the loss is 219001.25\n",
            "in training loop, epoch 2, step 854, the loss is 195838.640625\n",
            "in training loop, epoch 2, step 855, the loss is 157587.765625\n",
            "in training loop, epoch 2, step 856, the loss is 174368.46875\n",
            "in training loop, epoch 2, step 857, the loss is 188649.375\n",
            "in training loop, epoch 2, step 858, the loss is 115308.0390625\n",
            "in training loop, epoch 2, step 859, the loss is 125890.875\n",
            "in training loop, epoch 2, step 860, the loss is 175466.328125\n",
            "in training loop, epoch 2, step 861, the loss is 324262.78125\n",
            "in training loop, epoch 2, step 862, the loss is 279563.28125\n",
            "in training loop, epoch 2, step 863, the loss is 160139.28125\n",
            "in training loop, epoch 2, step 864, the loss is 245862.46875\n",
            "in training loop, epoch 2, step 865, the loss is 262939.78125\n",
            "in training loop, epoch 2, step 866, the loss is 188575.171875\n",
            "in training loop, epoch 2, step 867, the loss is 196849.28125\n",
            "in training loop, epoch 2, step 868, the loss is 168209.875\n",
            "in training loop, epoch 2, step 869, the loss is 174824.390625\n",
            "in training loop, epoch 2, step 870, the loss is 315790.21875\n",
            "in training loop, epoch 2, step 871, the loss is 130061.9140625\n",
            "in training loop, epoch 2, step 872, the loss is 161821.84375\n",
            "in training loop, epoch 2, step 873, the loss is 187744.0\n",
            "in training loop, epoch 2, step 874, the loss is 224564.15625\n",
            "in training loop, epoch 2, step 875, the loss is 240978.96875\n",
            "in training loop, epoch 2, step 876, the loss is 134903.875\n",
            "in training loop, epoch 2, step 877, the loss is 172426.375\n",
            "in training loop, epoch 2, step 878, the loss is 314046.875\n",
            "in training loop, epoch 2, step 879, the loss is 164881.046875\n",
            "in training loop, epoch 2, step 880, the loss is 148609.0\n",
            "in training loop, epoch 2, step 881, the loss is 122447.015625\n",
            "in training loop, epoch 2, step 882, the loss is 313350.25\n",
            "in training loop, epoch 2, step 883, the loss is 354879.4375\n",
            "in training loop, epoch 2, step 884, the loss is 165794.046875\n",
            "in training loop, epoch 2, step 885, the loss is 134468.625\n",
            "in training loop, epoch 2, step 886, the loss is 134920.90625\n",
            "in training loop, epoch 2, step 887, the loss is 235627.21875\n",
            "in training loop, epoch 2, step 888, the loss is 129798.046875\n",
            "in training loop, epoch 2, step 889, the loss is 128688.09375\n",
            "in training loop, epoch 2, step 890, the loss is 121209.421875\n",
            "in training loop, epoch 2, step 891, the loss is 237009.375\n",
            "in training loop, epoch 2, step 892, the loss is 227077.734375\n",
            "in training loop, epoch 2, step 893, the loss is 159255.96875\n",
            "in training loop, epoch 2, step 894, the loss is 327134.40625\n",
            "in training loop, epoch 2, step 895, the loss is 206787.296875\n",
            "in training loop, epoch 2, step 896, the loss is 225377.734375\n",
            "in training loop, epoch 2, step 897, the loss is 146463.734375\n",
            "in training loop, epoch 2, step 898, the loss is 125288.1015625\n",
            "in training loop, epoch 2, step 899, the loss is 122854.828125\n",
            "in training loop, epoch 2, step 900, the loss is 176528.484375\n",
            "in training loop, epoch 2, step 901, the loss is 117790.1953125\n",
            "in training loop, epoch 2, step 902, the loss is 117121.625\n",
            "in training loop, epoch 2, step 903, the loss is 108061.203125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAAI4CAYAAAB3OR9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzde5zOdf7/8efb0DgfcqqMbygUc8KgtHShlRAVwpIZ7beDDjZtbdpfG5027Wq1toPUFkkhu9lE+Updi5QyViGHBhOT0qJrkPPM+/fHfObTZ8Y1l3G45nM1Pe6329zM9f4c3q9rPv54Xu/r/Xl/jLVWAAAAAApU8LsAAAAAIJYQkAEAAAAPAjIAAADgQUAGAAAAPAjIAAAAgEdFvwuIFfXq1bNNmjQp0z5/+OEHVatWrUz7RPRwPcsfrmn5wvUsX7ie5Y8f1zQzM3OXtbZ+8XYCsqNJkyZauXJlmfYZDAYVCATKtE9ED9ez/OGali9cz/KF61nOLF+uVatWqe0dd5Rpt8aYr8K1M8UCAAAA/vr979XsxRf9rsJFQAYAAAA8CMgAAACABwEZAAAA8CAgAwAAAB6sYgEAAAB/PfWUslauVJrfdTgIyAAAAPBXaqr2h0J+V+FiigUAAAD89d57qpOZ6XcVLgIyAAAA/PXoozp/+nS/q3ARkAEAAAAPAjIAAADgQUAGAAAAPAjIAAAAgAfLvAEAAMBfzz+vjStWqKPfdTgIyAAAAPBXy5Y6+M03flfhYooFAAAA/DVvnuouX+53FS4CMgAAAPz15JNqPHu231W4CMgAAACABwEZAAAA8CAgAwAAAB4EZAAAAMCDZd588sLnL2jNnjValblKFU1FVapQSRUrVHR/4kycKlYoub3INlNRcRU87ebHY9x2Tx/GGL/fPgAAwI+mT9f6jz7SpX7X4SAg++S9be9p8/7N+vCLD3XMHlO+zS+zvuNMXJGg7Q3c4dormhO8DtMWZ+KOC/cltnsCvjf0ewO+N/S77YR+AADKh8aNdXjzZr+rcBGQfTKrzywFg0EFAgFJUr7N17H8YwU/9tiPv3t+juYfVZ7NO35b8f3DHV+afU5w7KFjh3Q0/2iRffLy89x9juYf/bHd5pV56C8+wu4N3McFfydce0fYT/jBoKQ2p/3L/V/q4NaDxwX84qHfbTeRP0AQ+gEAPxuzZqn+unWSk4v8RkCOERVMBZ0Vd5bOijvL71LOGG/oLwzPxQN+YeB2g7YnlB/NP1okgBf5oJCfV7oPAhE+IBzJP6K8Y3nHhX43+BcL/cfyj8nKRn7TS87c388b6kuaWlPSNJqT+rBQ0rcHp/DNwYk+RBD6AQBhPfecGoVC0sMP+12JJAIyoujnEvoLg/+HH32odu3bFQncRYK/J3AXD/glfYA42W8OjuQf0YFjB07qw8IJQ/8ZFC6ERxpljzQ/v/gc/bAfCIrNzy88v/vtQbEwX/yDws6jO7V93/aI048I/QBQ/hCQgZMQKfTXr1RfzWo386Gq01MY1vPy84qOmHtC/smMsheG+ZLai2wr/u1BmA8R3tBf4oeFYsH/jIb+f0befLIj75Hm5xcP/cfdgOuE/uLfHET6huBk2wn9AEBABn724irEKU5xUpzflZw5pZ6CE2afPPvjB4U169aoecvmx7V7jy8M+N7Qf6L5+cVDf0nfHkQt9J9ASSPvpZlGc8K24nPxPaE/4oeFUnyIiDT9iNAP4GQQkAGUO4WhPz4u/rTOE58dr8CFgTNS0+k6ndB/svPzi3xzUGyfkr45OJJ3RAfyi4b+sMHfc2xZqmgqysgo/rX446bXRJqfX6oPCKWci1/q1X1KcZMvoR+ILgIyAPwEnKnQHyustQVz+k9zLv5RezTsTb7hXm/9aqvObXRuyR8YirUdPnZYP+T/cPy3BxG+JShLJY28n2h+fsUKBUtnnvS3BGfom4NI048I/T9jc+Zo3Ycf6jK/63AQkAEAZc4YU7AmexmG/uDeoAIdAlE7v7VWeTYv/Hz7MMG/xNV97NGiAdwTyktaxae08/MLQ3+kbx6Kf0tQlkoK4eFG2X/Y/4Omvju1SOgvPsp+ynPxI3yIKOkmX0L/aapXT0dr1fK7ChcBGQCAM8AYUxCQVLFcjfQXBu/TuQG3MPQXH3mP+EEh0jcK9qiO/nBURqZUob94W1k61fn5xUN/ab8lKOkpu97zlfYmX/cDQbFlR6MS+qdO1TkbNrAOMgAAiG1u6K8Qe3HB+7Ctk+EN/ScK05Hm4ntDf/EPC6Va3SfCh4hwof9E3xKUpXDBuTRTayI9JGvEpGk6O6+KNH58mb6XksTe/3gAAIAoieXQf6pOJvSf1GtPwD9uFZ9w7cXm53tD/8FjByM+A6DfoT1qWPEcv/+UrvLzvwMAAOBnqFyE/mcDCoVCflfhquB3AQAAAEAsISADAAAAHj/hsXgAAACUCwsW6PMlS9TF7zocjCADAADAX1WrKr9yZb+rcBGQAQAA4K9nn9V5c+f6XYWLKRYAAADw1+zZasAqFgAAAEBsIiADAAAAHgRkAAAAwIOADAAAAHhwkx4AAAD8FQxqdTCogN91OBhBBgAAADwIyAAAAPDXhAlqPGuW31W4mGIBAAAAf739tuqyDjIAAAAQmwjIAAAAgAcBGQAAAPAgIAMAAMBfVaooLz7e7ypc3KQHAAAAf73zjtawDjIAAAAQm6IakI0xo40x64wxa40xrxtjKhtjmhpjVhhjsowxs4wxZzn7xjuvs5ztTTznud9p32iMudLT3tNpyzLGjPG0h+0DAAAAMeiRR3T+K6/4XYUragHZGNNI0ihJadbaRElxkgZLekLSRGvthZK+l/Rr55BfS/reaZ/o7CdjTCvnuNaSekp61hgTZ4yJk/SMpKsktZI0xNlXEfoAAABArFm8WHVWrfK7Cle0p1hUlFTFGFNRUlVJ30jqJmmOs32apGuc3/s5r+Vs726MMU77TGvtYWvtVklZkjo4P1nW2i3W2iOSZkrq5xxTUh8AAABARFG7Sc9a+7UxZoKkbZIOSvo/SZmSQtbaY85uOZIaOb83krTdOfaYMSZXUl2n/WPPqb3HbC/W3tE5pqQ+ijDG3CzpZklq2LChgsHgKb3XU7V///4y7xPRw/Usf7im5QvXs3zhepYvqaGQ8vLyYuaaRi0gG2PqqGD0t6mkkKQ3VDBFImZYa6dImiJJaWlpNhAIlGn/wWBQZd0noofrWf5wTcsXrmf5wvUsZ2rXVigUiplrGs1l3q6QtNVa+19JMsb8U9JlkmobYyo6I7wJkr529v9aUmNJOc6UjFqSdnvaC3mPCde+O0IfAAAAiDV16+pofr7fVbiiOQd5m6RLjDFVnXnB3SV9IekDSQOcfdIl/cv5/S3ntZzt71trrdM+2Fnloqmk5pI+kfSppObOihVnqeBGvrecY0rqAwAAALHmH//Quocf9rsKV9QCsrV2hQpulFslaY3T1xRJ90m62xiTpYL5wn93Dvm7pLpO+92SxjjnWSdptgrC9buSbrfW5jmjw3dIWihpvaTZzr6K0AcAAAAQUVSfpGetHStpbLHmLSpYgaL4vockDSzhPI9JeixM+wJJC8K0h+0DAAAAMej++9V02zbpZzAHGQAAADixjz5SrVDI7ypcPGoaAAAA8CAgAwAAAB4EZAAAAMCDOcgAAADwV0KCDleq5HcVLgIyAAAA/PXqq1ofDKqh33U4mGIBAAAAeDCCDAAAAH/ddZcuzMlhHWQAAABAkrR6taqzDjIAAAAQmwjIAAAAgAcBGQAAAPAgIAMAAMBfLVroQEKC31W4uEkPAAAA/poyRZuCQZ3ndx0ORpABAAAAD0aQAQAA4K+bb1aLHTtYBxkAAACQJG3apKqsgwwAAADEJgIyAAAA4EFABgAAADwIyAAAAPBXaqr2X3ih31W4uEkPAAAA/nrqKWUFg4qVR4UwggwAAAB4MIIMAAAAfw0bpot37mQdZAAAAECSlJOjeNZBBgAAAGITARkAAADwICADAAAAHsxBBgAAgL8uvVS527aptt91OAjIAAAA8Nfjj2trMKjz/a7DwRQLAAAAwIMRZAAAAPirf3+1/u9/pSVL/K5EEiPIAAAA8Nvu3aq0d6/fVbgIyAAAAIAHARkAAADwICADAAAAHtykBwAAAH91767vt25lHWQAAABAkvSHP+irYFBN/a7DwRQLAAAAwIMRZAAAAPjrqquUtGePtGKF35VIYgQZAAAAfjt4UHGHD/tdhYuADAAAAHgQkAEAAAAPAjIAAADgwU16AAAA8FefPtq9eTPrIAMAAACSpHvu0fZgUBf4XYeDKRYAAACAByPIAAAA8FcgoNRQSFq92u9KJDGCDAAAABRBQAYAAAA8CMgAAACABwEZAAAA8OAmPQAAAPjr+uv13aZNMbMOMiPIAAAA8Ndtt2nHNdf4XYWLgAwAAAB/HTigCocO+V2FiykWAAAA8FevXkoOhaSePf2uRBIjyAAAAEARBGQAAADAg4AMAAAAeBCQAQAAAA9u0gMAAIC/MjL07YYNrIMMAAAASCoIyDGygoUUxYBsjGlpjFnt+dlrjLnLGHO2MWaRMeZL5986zv7GGDPJGJNljPncGNPWc650Z/8vjTHpnvZ2xpg1zjGTjDHGaQ/bBwAAAGLQrl2qlJvrdxWuqAVka+1Ga22qtTZVUjtJByS9KWmMpMXW2uaSFjuvJekqSc2dn5slPScVhF1JYyV1lNRB0lhP4H1O0k2e4wo/epTUBwAAAGLNgAFqPXas31W4ymqKRXdJm621X0nqJ2ma0z5NUuFzBftJesUW+FhSbWPMuZKulLTIWrvHWvu9pEWSejrbalprP7bWWkmvFDtXuD4AAACAiMoqIA+W9Lrze0Nr7TfO799Kauj83kjSds8xOU5bpPacMO2R+gAAAAAiivoqFsaYsyT1lXR/8W3WWmuMsdHsP1IfxpibVTCdQw0bNlQwGIxmKcfZv39/mfeJ6OF6lj9c0/KF61m+cD3Ll9RQSHl5eTFzTctimberJK2y1u50Xu80xpxrrf3GmSbxndP+taTGnuMSnLavJQWKtQed9oQw+0fqowhr7RRJUyQpLS3NBgKBcLtFTTAYVFn3iejhepY/XNPyhetZvnA9y5natRUKhWLmmpbFFIsh+nF6hSS9JalwJYp0Sf/ytA93VrO4RFKuM01ioaQexpg6zs15PSQtdLbtNcZc4qxeMbzYucL1AQAAgFgzcqS+7tvX7ypcUR1BNsZUk/RLSbd4msdLmm2M+bWkryRd77QvkNRLUpYKVrwYIUnW2j3GmEckfers97C1do/z+22SpkqqIukd5ydSHwAAAIg1gwbpvzEyvUKKckC21v4gqW6xtt0qWNWi+L5W0u0lnOclSS+FaV8pKTFMe9g+AAAAEIO2b1f8d2FnxPqCR00DAADAXzfcoItDIen62PjSn0dNAwAAAB4EZAAAAMCDgAwAAAB4EJABAAAAD27SAwAAgL9++1ttX7NGtf2uw0FABgAAgL+uvlq7a9TwuwoXUywAAADgr40bVWXbNr+rcDGCDAAAAH/dcotahkLS8OF+VyKJEWQAAACgCAIyAAAA4EFABgAAADwIyAAAAIAHN+kBAADAXw88oK8++4x1kAEAAABJ0hVX6PuKsRNLmWIBAAAAf61erepZWX5X4SIgAwAAwF933aULn37a7ypcBGQAAADAg4AMAAAAeBCQAQAAAA8CMgAAAOARO+tpAAAA4Ofpj3/UllWr1NbvOhwEZAAAAPirUyftPXLE7ypcTLEAAACAv5YvV821a/2uwkVABgAAgL9+/3s1e/FFv6twEZABAAAADwIyAAAA4EFABgAAADwIyAAAAIAHy7wBAADAX089payVK5Xmdx0OAjIAAAD8lZqq/aGQ31W4mGIBAAAAf733nupkZvpdhYuADAAAAH89+qjOnz7d7ypcBGQAAADAg4AMAAAAeBCQAQAAAA8CMgAAAODBMm8AAADw1/PPa+OKFerodx0OAjIAAAD81bKlDn7zjd9VuJhiAQAAAH/Nm6e6y5f7XYWLgAwAAAB/PfmkGs+e7XcVLgIyAAAA4EFABgAAADwIyAAAAIAHARkAAADwYJk3AAAA+Gv6dK3/6CNd6ncdDkaQAQAA4K/GjXW4QQO/q3ARkAEAAOCvWbNU//33/a7CRUAGAACAv557To3eesvvKlwEZAAAAMCDgAwAAAB4EJABAAAADwIyAAAA4ME6yAAAAPDXnDla9+GHuszvOhyMIAMAAMBf9erpaK1aflfhIiADAADAX1On6px33/W7ChcBGQAAAP4iIAMAAACxi4AMAAAAeBCQAQAAAA8CMgAAAODBOsgAAADw14IF+nzJEnXxuw4HI8gAAADwV9Wqyq9c2e8qXFENyMaY2saYOcaYDcaY9caYS40xZxtjFhljvnT+rePsa4wxk4wxWcaYz40xbT3nSXf2/9IYk+5pb2eMWeMcM8kYY5z2sH0AAAAgBj37rM6bO9fvKlzRHkH+q6R3rbUXSUqRtF7SGEmLrbXNJS12XkvSVZKaOz83S3pOKgi7ksZK6iipg6SxnsD7nKSbPMf1dNpL6gMAAACxZvZsNQgG/a7CFbWAbIypJamLpL9LkrX2iLU2JKmfpGnObtMkXeP83k/SK7bAx5JqG2POlXSlpEXW2j3W2u8lLZLU09lW01r7sbXWSnql2LnC9QEAAABEFM2b9JpK+q+kl40xKZIyJf1GUkNr7TfOPt9Kauj83kjSds/xOU5bpPacMO2K0EcRxpibVTBarYYNGypYxp9c9u/fX+Z9Inq4nuUP17R84XqWL1zP8iU1FFJeXl7MXNNoBuSKktpKutNau8IY81cVm+pgrbXGGBvFGiL2Ya2dImmKJKWlpdlAIBDNUo4TDAZV1n0ierie5Q/XtHzhepYvXM9ypnZthUKhmLmm0ZyDnCMpx1q7wnk9RwWBeaczPULOv98527+W1NhzfILTFqk9IUy7IvQBAAAARBS1gGyt/VbSdmNMS6epu6QvJL0lqXAlinRJ/3J+f0vScGc1i0sk5TrTJBZK6mGMqePcnNdD0kJn215jzCXO6hXDi50rXB8AAACINcGgVj/1lN9VuKL9oJA7Jc0wxpwlaYukESoI5bONMb+W9JWk6519F0jqJSlL0gFnX1lr9xhjHpH0qbPfw9baPc7vt0maKqmKpHecH0kaX0IfAAAAQERRDcjW2tWS0sJs6h5mXyvp9hLO85Kkl8K0r5SUGKZ9d7g+AAAAEIMmTFDjzZulGJmDzKOmAQAA4K+331bdUMjvKlw8ahoAAADwICADAAAAHgRkAAAAwIOADAAAAH9VqaK8+Hi/q3Bxkx4AAAD89c47WhMMKuB3HQ5GkAEAAAAPAjIAAAD89cgjOv+VV/yuwsUUCwAAAPhr8WLVYR1kAAAAIDYRkAEAAAAPAjIAAADgwRxkAAAA+KtuXR3Nz/e7ChcBGQAAAP76xz+0jnWQAQAAgNjECDIAAAD8df/9arptmxQI+F2JJAIyAAAA/PbRR6rFOsgAAABAbCIgAwAAAB4EZAAAAMCDOcgAAADwV0KCDleq5HcVLgIyAAAA/PXqq1ofDKqh33U4CMgRHD16VDk5OTp06FBUzl+rVi2tX78+KudG2SvL61m5cmUlJCSoUgx92gYAoLwgIEeQk5OjGjVqqEmTJjLGnPHz79u3TzVq1Djj54U/yup6Wmu1e/du5eTkqGnTplHvDwCAqLvrLl2YkxMz6yBzk14Ehw4dUt26daMSjoFTZYxR3bp1o/bNBgAAZW71alXPyvK7ChcB+QQIx4hF/L8EACB6CMgxbPfu3UpNTVVqaqrOOeccNWrUyH195MiRiMeuXLlSo0aNOmEfnTp1OiO1BoNB9enT54ycCwAAwE/MQY5hdevW1erVqyVJ48aNU/Xq1XXPPfe4248dO6aKFcNfwrS0NKWlpZ2wj+XLl5+ZYgEAAMoJRpB/YjIyMnTrrbeqY8eO+t3vfqdPPvlEl156qdq0aaNOnTpp48aNkoqO6I4bN0433nijAoGAmjVrpkmTJrnnq169urt/IBDQgAEDdNFFF2no0KGy1kqSFixYoIsuukjt2rXTqFGjTmqk+PXXX1dSUpISExN13333SZLy8vKUkZGhxMREJSUlaeLEiZKkSZMmqVWrVkpOTtbgwYNP/48FAAB+Glq00IGEBL+rcDGCXEoPzVunL3bsPaPnbF6vih7tn3rSx+Xk5Gj58uWKi4vT3r17tXTpUlWsWFHvvfeefv/73+sf//jHccds2LBBH3zwgfbt26eWLVtq5MiRxy0R9p///Efr1q3Teeedp8suu0wffvih0tLSdMstt2jJkiVq2rSphgwZUuo6d+zYofvuu0+ZmZmqU6eOevTooblz56px48b6+uuvtXbtWklSKBSSJI0fP15bt25VfHy82wYAAH4GpkzRpmBQ5/ldh4MR5J+ggQMHKi4uTpKUm5urgQMHKjExUaNHj9a6devCHtO7d2/Fx8erXr16atCggXbu3HncPh06dFBCQoIqVKig1NRUZWdna8OGDWrWrJm7nNjJBORPP/1UgUBA9evXV8WKFTV06FAtWbJEzZo105YtW3TnnXfq3XffVc2aNSVJycnJGjp0qF599dUSp44AAABEGymklMZe3fqMn3Pfvn2ndFy1atXc3//whz+oa9euevPNN5Wdna1ACesHxsfHu7/HxcXp2LFjp7TPmVCnTh199tlnWrhwoSZPnqzZs2frpZde0vz587VkyRLNmzdPjz32mNasWUNQBgDg5+Dmm9Vixw7WQcaZkZubq0aNGkmSpk6desbP37JlS23ZskXZ2dmSpFmzZpX62A4dOujf//63du3apby8PL3++uu6/PLLtWvXLuXn56t///569NFHtWrVKuXn52v79u3q2rWrnnjiCeXm5mr//v1n/P0AAIAYtGmTqubk+F2Fi+G5n7jf/e53Sk9P16OPPqrevXuf8fNXqVJFzz77rHr27Klq1aqpffv2Je67ePFiJXgm2L/xxhsaP368unbtKmutevfurX79+umzzz7TiBEjlJ+fL0l6/PHHlZeXp2HDhik3N1fWWo0aNUq1a9c+4+8HAADgREzhSgU/d2lpaXblypVF2tavX6+LL744an3+VB41vX//flWvXl3WWt1+++1q3ry5Ro8e7XdZMaesr2e0/3/ix9VdUD5wPcsXrmc5EwgoFAqptrO8bVkxxmRaa49bF5cpFjihF154QampqWrdurVyc3N1yy23+F0SAABA1DDFAic0evRoRowBAED0pKZqf06OYmVyJQEZAAAA/nrqKWUFg4qVR4UwxQIAAADwYAQZAAAA/ho2TBfv3Bkz6yATkAEAAOCvnBzFh0J+V+FiikUM69q1qxYuXFik7amnntLIkSNLPCYQCKhwubpevXopFOY/27hx4zRhwoSIfc+dO1dffPGF+/rBBx/Ue++9dzLlhxUMBtWnT5/TPg8AAEC0EJBj2JAhQzRz5swibTNnztSQIUNKdfyCBQtO+WEbxQPyww8/rCuuuOKUzgUAAPBTQkCOYQMGDND8+fN15MgRSVJ2drZ27Nihzp07a+TIkUpLS1Pr1q01duzYsMc3adJEu3btkiQ99thjatGihX7xi19o48aN7j4vvPCC2rdvr5SUFPXv318HDhzQ8uXL9dZbb+nee+9VamqqNm/erIyMDM2ZM0dSwRPz2rRpo6SkJN144406fPiw29/YsWPVtm1bJSUlacOGDaV+r6+//rqSkpKUmJio++67T5KUl5enjIwMJSYmKikpSRMnTpQkTZo0Sa1atVJycrIGDx58kn9VAACAyJiDXFrvjJG+XXNGTxlft6XU9y8lbj/77LPVoUMHvfPOO+rXr59mzpyp66+/XsYYPfbYYzr77LOVl5en7t276/PPP1dycnLY82RmZmrmzJlavXq1jh07prZt26pdu3aSpOuuu0433XSTJOmBBx7Q3//+d915553q27ev+vTpowEDBhQ516FDh5SRkaHFixerRYsWGj58uJ577jndddddkqR69epp1apVevbZZzVhwgS9+OKLJ/w77NixQ/fdd58yMzNVp04d9ejRQ3PnzlXjxo319ddfa+3atZLkThcZP368tm7dqvj4+LBTSAAAwE/MpZcqd9u2mFkHmRHkGOedZuGdXjF79my1bdtWbdq00bp164pMhyhu6dKluvbaa1W1alXVrFlTffv2dbetXbtWnTt3VlJSkmbMmKF169ZFrGfjxo1q2rSpWrRoIUlKT0/XkiVL3O3XXXedJKldu3bKzs4u1Xv89NNPFQgEVL9+fVWsWFFDhw7VkiVL1KxZM23ZskV33nmn3n33XdWsWVOSlJycrKFDh+rVV19VxYp8xgMA4Cfv8ce11RmwiwWki9K6avwZP+Xhfft01gn26devn0aPHq1Vq1bpwIEDateunbZu3aoJEybo008/VZ06dZSRkaFDhw6dUg0ZGRmaO3euUlJSNHXqVAWDwVM6T6H4+HhJUlxcnI4dO3Za56pTp44+++wzLVy4UJMnT9bs2bP10ksvaf78+VqyZInmzZunxx57TGvWrCEoAwCAM6ZUI8jGmN8YY2qaAn83xqwyxvSIdnGQqlevrq5du+rGG290R4/37t2ratWqqVatWtq5c6feeeediOfo0qWL5s6dq4MHD2rfvn2aN2+eu23fvn0699xzdfToUc2YMcNtr1Gjhvbt23fcuVq2bKns7GxlZWVJkqZPn67LL7/8tN5jhw4d9O9//1u7du1SXl6eXn/9dV1++eXatWuX8vPz1b9/fz366KNatWqV8vPztX37dnXt2lVPPPGEcnNztX///tPqHwAA+Kx/f7V+8EG/q3CVdtjtRmvtX40xV0qqI+kGSdMl/V/UKoNryJAhuvbaa92pFikpKWrTpo0uuugiNW7cWJdddlnE49u2batBgwYpJSVFDRo0UPv27d1tjzzyiDp27Kj69eurY8eObigePHiwbrrpJk2aNMm9OU+SKleurJdfflkDBw7UsWPH1L59e916660n9X4WL16shIQfHyb5xhtvaPz48eratausterdu7f69c0YwZYAACAASURBVOunzz77TCNGjFB+fr4k6fHHH1deXp6GDRum3NxcWWs1atSoU16pAwAAxIjdu1Vp716/q3AZa+2JdzLmc2ttsjHmr5KC1to3jTH/sda2iX6JZSMtLc0Wrh9caP369br44ouj1ue+fftUo0aNqJ0fZausr2e0/3+iYN3uQIw81Qmnj+tZvnA9y5lAQKFQSLVXry7Tbo0xmdbatOLtpb1JL9MY83+SeklaaIypISn/TBYIAAAAxILSTrH4taRUSVustQeMMWdLGhG9sgAAAAB/lHYE+VJJG621IWPMMEkPSMqNXlkAAAD42ejeXd+3bet3Fa7SBuTnJB0wxqRI+q2kzZJeiVpVAAAA+Pn4wx/01fDhflfhKm1APmYL7ubrJ+lpa+0zkri7DAAAAOVOaecg7zPG3K+C5d06G2MqSKoUvbIAAADws3HVVUras0dascLvSiSVfgR5kKTDKlgP+VtJCZL+HLWq4Pr22281ePBgXXDBBWrXrp169eqlTZs2RbXPadOmuQ8lKbRr1y7Vr19fhw8fDnvM1KlTdccdd0iSJk+erFdeOX4GTnZ2thITEyP2nZ2drddee819vXLlSo0aNepk30JYTZo00a5du87IuQAAwBl08KDiSsgYfihVQHZC8QxJtYwxfSQdstYyBznKrLW69tprFQgEtHnzZmVmZurxxx/Xzp07i+x3uo90Lu7aa6/VokWLdODAAbdtzpw5uvrqq91HSUdy6623avgpziMqHpDT0tI0adKkUzoXAADAqSjto6avl/SJpIGSrpe0whgzIJqFQfrggw9UqVKlIk+qS0lJUefOnRUMBtW5c2f17dtXrVq10qFDhzRixAglJSWpTZs2+uCDDyRJ69atU4cOHZSamqrk5GR9+eWX+uGHH9S7d2+lpKQoMTFRs2bNKtJvzZo1dfnllxd5JPXMmTM1ZMgQzZs3Tx07dlSbNm10xRVXHBfWJWncuHGaMGGCJCkzM1MpKSlKSUnRM8884+6TnZ2tzp07q23btmrbtq2WL18uSRozZoyWLl2q1NRUTZw4UcFgUH369JEk7dmzR9dcc42Sk5N1ySWX6PPPP3f7u/HGGxUIBNSsWbOTCtTZ2dnq1q2bkpOT1b17d23btk1SwdP9EhMTlZKSoi5dupT4twQAAOVPaecg/z9J7a2130mSMaa+pPckzYl4VDnyxCdPaMOeDWf0nM2qN9MffvGHErevXbtW7dq1K3H7qlWrtHbtWjVt2lRPPvmkjDFas2aNNmzYoB49emjTpk2aPHmyfvOb32jo0KE6cuSI8vLytGDBAp133nmaP3++JCk39/gV+4YMGaIZM2Zo0KBB2rFjhzZt2qRu3bpp7969+vjjj2WM0Ysvvqg//elPevLJJ0usccSIEXr66afVpUsX3XvvvW57gwYNtGjRIlWuXFlffvmlhgwZopUrV2r8+PGaMGGC3n77bUkFT0oqNHbsWLVp00Zz587V+++/r+HDh2u188SdDRs26IMPPtC+ffvUsmVLjRw5UpUqnXia/J133qn09HSlp6frpZde0qhRozR37lw9/PDDWrhwoRo1aqRQKCRJYf+WAACg/CntHOQKheHYsfskjkWUdOjQQU2bNpUkLVu2TMOGDZMkXXTRRTr//PO1adMmXXrppfrjH/+oJ554Ql999ZWqVKmipKQkLVq0SPfdd5+WLl2qWrVqHXfu3r1768MPP9TevXs1e/Zs9e/fX3FxccrJydGVV16ppKQk/fnPf9a6detKrC8UCikUCrkjsDfccIO77ejRo7rpppuUlJSkgQMH6osvvjjh+122bJl7jm7dumn37t3a6zy3vXfv3oqPj1e9evXUoEGDsCPb4Xz00Uf61a9+5da3bNkySdJll12mjIwMvfDCC24QDve3BAAAZ0CfPtp96aV+V+Eq7Qjyu8aYhZJed14PkrQgOiXFpvs63HfGz7lv376I21u3bq05c0oepK9WrdoJ+/jVr36ljh07av78+erVq5eef/55devWTatWrdKCBQv0wAMPqHv37nrwwQeLHFelShX17NlTb775pmbOnKm//OUvkgpGXO+++2717dtXwWBQ48aNO/EbDWPixIlq2LChPvvsM+Xn56ty5cqndJ5C3rnRcXFxpz0ve/LkyVqxYoXmz5+vdu3aKTMzs8S/JQAAOE333KPtwaAu8LsOR2lv0rtX0hRJyc7PFGvtmU+MKKJbt246fPiwpkyZ4rZ9/vnnWrp06XH7du7cWTNmzJAkbdq0Sdu2bVPLli21ZcsWNWvWTKNGjVK/fv30+eefa8eOHapataqGDRume++9V6tWrQrb/5AhQ/SXv/xFO3fu1KXOp7rc3Fw1atRIUsFqF5HUrl1btWvXdkdlC+srPM+5556rChUqaPr06e4obY0aNUr84OB9j8FgUPXq1VPNmjUj1nAinTp10syZM936OnfuLEnavHmzOnbsqIcfflj169fX9u3bw/4tAQBA+VPqaRLW2n9Ya+92ft4szTHGmGxjzBpjzGpjzEqn7WxjzCJjzJfOv3WcdmOMmWSMyTLGfG6Maes5T7qz/5fGmHRPezvn/FnOsSZSHz81xhi9+eabeu+993TBBReodevWuv/++3XOOecct+9tt92m/Px8JSUladCgQZo6dari4+M1e/ZsJSYmKjU1VWvXrtXw4cO1Zs0a92azhx56SA888EDY/n/5y19qx44dGjRokJw/rcaNG6eBAweqXbt2qlev3gnfw8svv6zbb79dqampKnjWzI/1Tps2TSkpKdqwYYM7Gp6cnKy4uDilpKRo4sSJRc41btw4ZWZmKjk5WWPGjDlhQA8nOTlZCQkJSkhI0N13362//e1vevnll5WcnKzp06frr3/9qyTp3nvvVVJSkhITE9WpUyelpKSE/VsCAIAzIBBQ6l13+V2Fy3hDy3EbjdknKdwORpK11kYcvjPGZEtKs9bu8rT9SdIea+14Y8wYSXWstfcZY3pJulNSL0kdJf3VWtvRGHO2pJWS0pxaMiW1s9Z+b4z5RNIoSStUMOVjkrX2nZL6iFRrWlqaXblyZZG29evX6+KLL4502GnZt2+fatTggYTlRVlfz2j//0TBNxWBQMDvMnCGcD3LF65nORMIKBQKqbZz831ZMcZkWmvTirdHHEG21taw1tYM81PjROE4gn6SCof+pkm6xtP+ii3wsaTaxphzJV0paZG1do+19ntJiyT1dLbVtNZ+7DwG+5Vi5wrXBwAAABBRaW/SO1VW0v8ZY6yk5621UyQ1tNZ+42z/VlJD5/dGkrZ7js1x2iK154RpV4Q+ijDG3CzpZklq2LBhkSXFJKlWrVonvJHudOTl5UX1/ChbZX09Dx06dNz/WZxZ+/fv529cjnA9yxeuZ/mSGgopLy8vZq5ptAPyL6y1XxtjGkhaZIwpspCwtdY64TlqIvXhBPYpUsEUi+Jf1axfvz6qX5kzxaJ8KevrWblyZbVp06bM+vs54ivc8oXrWb5wPcuZ2rUVCoVi5ppGdS1ja+3Xzr/fSXpTUgdJO53pEXL+LVxf+WtJjT2HJzhtkdoTwrQrQh8AAACINddfr+9iJBxLUQzIxphqxpgahb9L6iFpraS3JBWuRJEu6V/O729JGu6sZnGJpFxnmsRCST2MMXWc1Sh6SFrobNtrjLnEWb1ieLFzhesDAAAAsea227Tjmti5ZSyaUywaSnrTWR6soqTXrLXvGmM+lTTbGPNrSV9Jut7Zf4EKVrDIknRA0ghJstbuMcY8IulTZ7+HrbV7nN9vkzRVUhVJ7zg/kjS+hD4AAAAQaw4cUIVDh/yuwhW1gGyt3SIpJUz7bkndw7RbSbeXcK6XJL0Upn2lpMTS9vFTFBcXp6SkJPf14MGDNWbMmFIfP27cOFWvXl333HNPqfb/+OOP9Zvf/EaHDx/W4cOHNWjQII0bN07BYFBnnXWWOnXqdNLv4UQ6deqk5cuXn5FzffLJJ7rnnnu0c+dOVa1aVe3atdOkSZP0pz/96aT+DiU52b9nSd566y198cUXEa9ldna2li9f7j4KGwCAcqtXLyWHQlLPnn5XIin6N+nhNFWpUkWrT3FNwFN53HJ6erpmz56tlJQU5eXlaePGjZIKboaoXr16VALymQrHO3fu1MCBAzVz5kz3yX9z5syJyZVC+vbtq759+0bcJzs7W6+99hoBGQCAMhbVm/QQPQ8//LDat2+vxMRE3Xzzze5T6gKBgO666y6lpaW5T4WTCh6d3Lat+3BCffnll0VeF/ruu+907rnnSioYvW7VqpWys7M1efJkTZw4UampqVq6dKmys7PVrVs3JScnq3v37tq2bZskKSMjQ7feeqvS0tLUokULvf3225KkqVOnql+/fgoEAmrevLkeeught8/q1atL+vGO5AEDBuiiiy7S0KFD3fe1YMECXXTRRWrXrp1GjRqlPn36HFf7M888o/T0dDccS9KAAQPUsGHBKn9ffPGFAoGAmjVrpkmTJrn7vPrqq+6TBW+55Rb3sdfvvvuu2rZtq5SUFHXvfvwXEi+88IKuuuoqHTx4UIFAQL/73e+UmpqqxMREffLJJ5KkPXv26JprrlFycrIuueQS9/HUU6dO1R133OH+zUaNGqVOnTqpWbNmmjNnjiRpzJgxWrp0qVJTU497qiAAAIgeRpBPRri7K6+/XrrtNunAAalXr+O3Z2QU/OzaJQ0YUHTbvHkn7PLgwYNKTU11X99///0aNGiQ7rjjDj344IOSpBtuuEFvv/22rr76aknSkSNHVPhUwHHjxkmSLrjgAtWqVUurV69WamqqXn75ZY0YMeK4/kaPHq2WLVsqEAioZ8+eSk9PV5MmTXTrrbcWmVpw9dVXKz09Xenp6XrppZc0atQozZ07V1LByOcnn3yizZs3q2vXrsrKypJUMP1h7dq1qlq1qtq3b6/evXsrLa3ow2v+85//aN26dTrvvPN02WWX6cMPP1RaWppuueUWLVmyRE2bNtWQIUPC/q3Wrl2r9PT0sNskacOGDfrggw+0b98+tWzZUiNHjlRWVpZmzZqlDz/8UJUqVdJtt92mGTNm6KqrrtJNN93k9rlnz54i53r66ae1aNEizZ07V/Hx8e61Wr16tZYsWaIbb7xRa9eu1dixY9WmTRvNnTtX77//voYPHx72G4FvvvlGy5Yt04YNG9S3b18NGDBA48eP14QJE9wPGQAAoGwwghzjCqdYFP4MGjRIkvTBBx+oY8eOSkpK0vvvv69169a5xxTuU9z//u//6uWXX1ZeXp5mzZoV9qv7Bx98UCtXrlSPHj302muvqWcJc4E++ugj9/gbbrhBy5Ytc7ddf/31qlChgpo3b65mzZppw4aC5a9/+ctfqm7duqpSpYquu+66IscU6tChgxISElShQgWlpqYqOztbGzZsULNmzdS0aVNJKjEgn0jv3r0VHx+vevXqqUGDBtq5c6cWL16szMxMtW/fXqmpqVq8eLG2bNmijz/+WF26dHH7PPvss93zvPLKK3rnnXc0Z84cNxxLBaPVktSlSxft3btXoVBIy5Yt0w033CBJ6tatm3bv3q29e/ceV9s111yjChUqqFWrVtq5c+cpvT8AAHBmMIJ8MiI93aVq1cjb69U7fvspzo09dOiQbrvtNq1cuVKNGzfWuHHjdMhz52e1atXCHte/f3899NBD6tatm9q1a6e6deuG3e+CCy7QyJEjddNNN6l+/fravXv3SdXnrFxy3OuS2r28gTMuLu6k5lG3bt1amZmZ6tevX9jt4c5trVV6eroef/zxIvvOizC6n5SUpNWrVysnJ8cN0OHeT7j3VxJvbYXTSgAA+NnIyNC3Gzaott91OBhB/gkqDMP16tXT/v373TmrJ1K5cmVdeeWVGjlyZNjpFZI0f/58N6B9+eWXiouLU+3atVWjRo0iN7t16tRJM2fOlCTNmDFDnTt3dre98cYbys/P1+bNm7Vlyxa1bNlSkrRo0SLt2bNHBw8e1Ny5c3XZZZeVqu6WLVtqy5Ytys7OliTNmjUr7H533HGHpk2bphUrVrht//znPyOOyHbv3l1z5szRd98VPEtmz549+uqrr3TJJZdoyZIl2rp1q9teqE2bNnr++efVt29f7dixo0hfkrRs2TLVqlVLtWrVUufOnTVjxgxJBXOs69Wrp5o1a5bqfRf/mwMAUG5lZOjbGFnBQmIEOeYVn4Pcs2dPjR8/XjfddJMSExN1zjnnqH379qU+39ChQ/Xmm2+qR48eYbdPnz5do0ePVtWqVVWxYkXNmDFDcXFxuvrqqzVgwAD961//0t/+9jf97W9/04gRI/TnP/9Z9evX18svv+ye43/+53/UoUMH7d27V5MnT1blypUlFUyf6N+/v3JycjRs2LDj5h+XpEqVKnr22WfVs2dPVatWrcT327BhQ82cOVP33HOPvvvuO1WoUEFdunQpcZqIJLVq1UqPPvqoevToofz8fFWqVEnPPPOMLrnkEk2ZMkXXXXed8vPz1aBBAy1atMg97he/+IUmTJig3r17u+3x8fFq06aNjh49qpdeKliVcNy4cbrxxhuVnJysqlWratq0aaV6z5KUnJysuLg4paSkKCMjQ6NHjy71sQAA/KTs2qVKubl+V+EyfJ1bIC0tzRbe2FZo/fr1uvjii6PW5759+1SjRo2onT+cCRMmKDc3V4888khUzp+RkaE+ffq483ELTZ06VStXrtTTTz99Sufdv3+/qlevLmutbr/9djVv3jymAmMgENBDDz2kyy+/vMz6jPb/T/y4sgrKB65n+cL1LGcCAYVCIdU+xaVtT5UxJtNae9yIHSPIPyPXXnutNm/erPfff9/vUk7aCy+8oGnTpunIkSNq06aNbrnlFr9LAgAA5RQB+WfkzTffjHofU6dODduekZGhjIyMUz7v6NGjY2rEuLhgMMh8YQAAyglu0gMAAAA8CMgnwBxtxCL+XwIAED0E5AgqV66s3bt3E0YQU6y12r17t7s6CAAAP3kjR+rrvn39rsLFHOQIEhISlJOTo//+979ROf+hQ4cIOeVIWV7PypUrKyEhoUz6AgAg6gYN0n8jPXCtjBGQI6hUqVKRJ6WdacFgUG3atIna+VG2uJ4AAJyi7dsV7zy0KxYQkAEAAOCvG27QxaGQdP31flciiTnIAAAAQBEEZAAAAMCDgAwAAAB4EJABAAAAD27SAwAAgL9++1ttX7NGtf2uw0FABgAAgL+uvlq7a9TwuwoXUywAAADgr40bVWXbNr+rcDGCDAAAAH/dcotahkLS8OF+VyKJEWQAAACgCAIyAAAA4EFABgAAADwIyAAAAIAHN+kBAADAXw88oK8++4x1kAEAAABJ0hVX6PuKsRNLmWIBAAAAf61erepZWX5X4SIgAwAAwF933aULn37a7ypcBGQAAADAg4AMAAAAeBCQAQAAAA8CMgAAAOARO+tpAAAA4Ofpj3/UllWr1NbvOhwEZAAAAPirUyftPXLE7ypcTLEAAACAv5YvV821a/2uwkVABgAAgL9+/3s1e/FFv6twEZABAAAADwIyAAAA4EFABgAAADwIyAAAAIAHy7wBAADAX089payVK5Xmdx0OAjIAAAD8lZqq/aGQ31W4mGIBAAAAf733nupkZvpdhYuADAAAAH89+qjOnz7d7ypcBGQAAADAg4AMAAAAeBCQAQAAAA8CMgAAAODBMm8AAADw1/PPa+OKFerodx0OAjIAAAD81bKlDn7zjd9VuJhiAQAAAH/Nm6e6y5f7XYWLgAwAAAB/PfmkGs+e7XcVLgIyAAAA4EFABgAAADwIyAAAAIAHARkAAADwYJk3AAAA+Gv6dK3/6CNd6ncdDkaQAQAA4K/GjXW4QQO/q3BFPSAbY+KMMf8xxrztvG5qjFlhjMkyxswyxpzltMc7r7Oc7U0857jfad9ojLnS097TacsyxozxtIftAwAAADFo1izVf/99v6twlcUI8m8krfe8fkLSRGvthZK+l/Rrp/3Xkr532ic6+8kY00rSYEmtJfWU9KwTuuMkPSPpKkmtJA1x9o3UBwAAAGLNc8+p0Vtv+V2FK6oB2RiTIKm3pBed10ZSN0lznF2mSbrG+b2f81rO9u7O/v0kzbTWHrbWbpWUJamD85Nlrd1irT0iaaakfifoAwAAAIgo2jfpPSXpd5JqOK/rSgpZa485r3MkNXJ+byRpuyRZa48ZY3Kd/RtJ+thzTu8x24u1dzxBH0UYY26WdLMkNWzYUMFg8OTf4WnYv39/mfeJ6OF6lj9c0/KF61m+cD3Ll9RQSHl5eTFzTaMWkI0xfSR9Z63NNMYEotXP6bDWTpE0RZLS0tJsIBAo0/6DwaDKuk9ED9ez/OGali9cz/KF61nO1K6tUCgUM9c0miPIl0nqa4zpJamypJqS/iqptjGmojPCmyDpa2f/ryU1lpRjjKkoqZak3Z72Qt5jwrXvjtAHAAAAEFHU5iBba++31iZYa5uo4Ca79621QyV9IGmAs1u6pH85v7/lvJaz/X1rrXXaBzurXDSV1FzSJ5I+ldTcWbHiLKePt5xjSuoDAAAAsWbOHK176CG/q3D5sQ7yfZLuNsZkqWC+8N+d9r9Lquu03y1pjCRZa9dJmi3pC0nvSrrdWpvnjA7fIWmhClbJmO3sG6kPAAAAxJp69XS0Vi2/q3CVyZP0rLVBSUHn9y0qWIGi+D6HJA0s4fjHJD0Wpn2BpAVh2sP2AQAAgBg0darO2bBBipE5yDxJDwAAAP6aOlXnvPuu31W4CMgAAACABwEZAAAA8CAgAwAAAB4EZAAAAMCjTFaxAAAAAEq0YIE+X7JEXfyuw8EIMgAAAPxVtaryK1f2uwoXARkAAAD+evZZnTd3rt9VuJhiAQAAAH/Nnq0GoZDfVbgYQQYAAAA8CMgAAACABwEZAAAA8CAgAwAAAB7cpAcAAAB/BYNaHQwq4HcdDkaQAQAAAA8CMgAAAPw1YYIaz5rldxUuplgAAADAX2+/rbqsgwwAAADEJgIyAAAA4EFABgAAADwIyAAAAPBXlSrKi4/3uwoXN+kBAADAX++8ozWsgwwAAADEJgIyAAAA/PXIIzr/lVf8rsLFFAsAAAD4a/Fi1WEdZAAAACA2EZABAAAADwIyAAAA4MEcZAAAAPirbl0dzc/3uwoXARkAAAD++sc/tI51kAEAAIDYxAgyAAAA/HX//Wq6bZsUCPhdiSQCMgAAAPz20UeqxTrIAAAAQGwiIAMAAAAeBGQAAADAgznIAAAA8FdCgg5XquR3FS4CMgAAAPz16qtaHwyqod91OJhiAQAAAHgwggwAAAB/3XWXLszJYR1kAAAAQJK0erWqsw4yAAAAEJsIyAAAAIAHARkAAADwICADAADAXy1a6EBCgt9VuLhJDwAAAP6aMkWbgkGd53cdDkaQAQAAAA9GkAEAAOCvm29Wix07WAcZAAAAkCRt2qSqrIMMAAAAxCYCMgAAAOBBQAYAAAA8CMgAAADwV2qq9l94od9VuLhJDwAAAP566illBYOKlUeFMIIMAAAAeDCCDAAAAH8NG6aLd+5kHWQAAABAkpSTo3jWQQYAAABiEwEZAAAA8CAgAwAAAB7MQQYAAIC/Lr1Uudu2qbbfdTgIyAAAAPDX449razCo8/2uw8EUCwAAAMAjagHZGFPZGPOJMeYzY8w6Y8xDTntTY8wKY0yWMWaWMeYspz3eeZ3lbG/iOdf9TvtGY8yVnvaeTluWMWaMpz1sHwAAAIhB/fur9YMP+l2FK5ojyIcldbPWpkhKldTTGHOJpCckTbTWXijpe0m/dvb/taTvnfaJzn4yxrSSNFjS/2/v3oMkO8v7jn+fvs19Z0ZarSQkIRkkEwTGK7zhGtubiLgkCiOSCAEJulBgXAZSli2nuATHFMYUuYBNXIClFBTCJgFZCbB2xMVcBgyFJIRYJKQVymqR9qLVrrQ7Pbfu2b49+eOc7jl9ne7Rdp+Z1u9T1bWn3z7d5509O7vPvvO8z/MC4Argk2aWNLMk8AngSuBS4E3huXS4hoiIiIhsNidOkF5cjHsWNX0LkD2wHD5Nhw8H/gVwezh+K/C68Piq8Dnh65ebmYXjX3D3U+7+C2A/8JLwsd/dD7h7AfgCcFX4nnbXEBERERHpqK+b9MJV3h8DFxOs9j4CZN29FJ5yGDgvPD4POATg7iUzWwDODMfvjHxs9D2HGsZfGr6n3TUa5/d24O0AZ599NnNzcxv6OjdqeXl54NeU/tH9HD66p8NF93O46H4Ol53ZLOVyedPc074GyO5eBnaa2QzwJeCf9PN6vXL3W4BbAHbt2uW7B9z/e25ujkFfU/pH93P46J4OF93P4aL7OWRmZshms5vmng6kioW7Z4HvAC8HZsysGpifDxwJj48AFwCEr08DJ6LjDe9pN36iwzVEREREZLO5/HLmX/ziuGdR088qFmeFK8eY2RjwL4F9BIHy1eFp1wNfCY/3hM8JX/+2u3s4/sawysUvAZcAdwM/Ai4JK1ZkCDby7Qnf0+4aIiIiIrLZ/PEf89h118U9i5p+plicC9wa5iEngNvc/e/N7EHgC2b2IeAnwKfD8z8N/LWZ7QdOEgS8uPsDZnYb8CBQAt4Zpm5gZu8Cvg4kgc+4+wPhZ727zTVERERERDrqW4Ds7vcBl7UYP0BQgaJxfBV4fZvP+jPgz1qM3wHc0e01RERERGQTuvJKfuXkSbjrrrhnAqiTnoiIiIjELZ8neepU3LOoUYAsIiIiIhKhAFlEREREJEIBsoiIiIhIRF8bhYiIiIiIrOs1r+HEI48wE/c8QgqQRURERCRef/RHHJqb47lxzyOkFAsRERERkQitIIuIiIhIvHbvZmc2C3v3xj0TQCvIIiIiIiJ1FCCLiIiIiEQoQBYRERERiVCALCIiIiISoU16IiIiIhKva67h+MMPb5o6yFpBFhEREZF4veMdAUsrXgAAG9BJREFUPP6618U9ixoFyCIiIiISr1yOxOpq3LOoUYqFiIiIiMTr1a/mRdksXHFF3DMBtIIsIiIiIlJHAbKIiIiISIQCZBERERGRCAXIIiIiIiIR2qQnIiIiIvG64QaeeOgh1UEWEREREQGCAHmTVLAABcgiIiIiErenniK9sBD3LGqUYiEiIiIi8br6al6QzcJVV8U9E0AryCIiIiIidRQgi4iIiIhEKEAWEREREYlQgCwiIiIiEqFNeiIiIiISr9/7PY488MCmqYOsAFlERERE4vWGN/Dk3Fzcs6hRgByXu27h2Y/dB3fug/R4+BiDTOQ4PVE/lsyAWdwzFxERETm9Dh1i5PjxuGdRowA5LnffzHNO7Idf9PAeS6wFzekxyFSPowF25PV05PXMeMO5bcaS+iMhIiIiA3bttTw/m4Vrrol7JoAC5Pi86x6++51v8psv3wWFHBTzUMxFHvlwvN1YHoor4a95WD7W8Bnhca8S6dar2BsOyBvGUmOQ0N5QERER2bwUIMek4uCJNIzNBo++XKQCpdWGYDoXCchbjXUI0vMnm4P08qne55UabZNWMt4QYDeucK8XpIdjqRGlooiIiMiGKUCOyZUf/0cOnlhh+93fZmYsw8x4mumxNLPja8cz4xlmxtLMjIfH4Xg62eUKbCIRBJqZceDM/nwh5RKUwlXsQmRFuxp8F1Zar2y3CshXF2DpiYYAfQW83OOkrIe0kg4BeacgPZnuy2+niIiIxE8Bckxev+t87vrZ/2PyjDPI5gpk80WOzOeZzxVYyBepePv3To6kwgA6EjxXn49lmB5Ph88zzI6nmQ4D65FU8vR/IckUJKdgZOr0f3ZVqVAfXNcC7MYgu4uAfOXJ1kE6HX7DW0mkmoLmy/JFeOzsdXK/GwPydhs0xyHRh/slIiIi61KAHJO3/fpzuLh8kN27dza9Vqk4S6dKLOSKZPMFsrliLXDO5sJHvhC+XmTf0cXacblDZD2eSTIzlmY6DKhnJ9JMh6vX1QB7eiwIqqMr1qPpmAO1VCZ4jPWpOqJ7JBUl11vudyQgLxcOQ7kI+SPNK+ml1d7nlRw5fbnfjSvp1fcqFUVERDaDm27i0P33qw6ytJdIGNNjQXD6bMa7fp+7s3yqRDZXrAXT8+Hq9EKuEAbW1SC7wMPHlsNzCxTL7QPr0XSilgZSXaWeCVemZ+oC7PpzRtMJbCsEYGZrASNnbPhj7pubY/fu3a1frJSbg+aucr+j6SrV1fOVcCW84dxKsfdJr7eK3VXud4eAXKUJRUSkG7/925yY6uNPo3ukAHmImBlTo2mmRtNc0MP73J1coRwGz4W6Veq1YHtt/MBTy7XjQrnS9nMzqUSwUj0eTfsIAulqishsuJo9HUkVGc8kt0Zg3YtEEkYmgwdn9eca5WKLVJMuAvKmsTzk52HhSEOaygo9p6JY8vTmfrcKyFWaUERk6/v5zxk7eDDuWdToXxbBzJgYSTExkuK8mbGu3+furBYrZPMF5lfq0z6iaSDzYXB98GSO+w4H46vF9oF1OmmRVI8WaSBhbnV0c+PMeJrJkdTwBda9SKYhOQ2j0/35fHcoF1rnea8bkLcYWz7WnCdeyvc+r2Smi7SSNqvg6wTkifJqUA1GpQlFRPrrd3+X52WzcN11cc8EUIAsT4OZMZZJMpYZ49zp7gNrgNVimYX8WvBcTfVoTAPJ5oocyeZ58PEFsvkiuUL7ihaphDVVAImmgcyOr+VfRzc0To2kSCSewYF1t8yCEnqpkf5do1JZq4rS62bMVkF67kRzkF4udD2d3wD4R4L63W1XsaPj69QNb9qgGY6pNKGIyKaiAFliMZpOMppOcva20Z7ed6pUrl+lruVYByvT87m14ycWV3noiSUW8kWWT5XafmYyzPleC6jX0kBmI3nVjaX3to2mFVifbolEEFhmJvp3jXKpddDdIiB/5KH7ee4F57YP0mulCaN54ivg7X9C0pIluk8r2VAjn3GVJhQR6YECZNlSRlJJdmxLsqPHwLpQqrCQj6xS14LsQlO+9VPLBfY/GeRZL622D6zNiATWGSr5Vb70xE9ablhsDLyTCqzjk0xBchuMblv31EO5OZ7bbuNlO+5hPnjraifdlSSMPF95snXueK8S6c6bMdtVO2kVkLcL0lWaUESGhAJkeUbIpBKcNTXCWVO9pQcUyxUW82sr1gu1snthZZD8WrB9aNHZeyjL/EqBxQ6BNcC20VRdKb2ZWl51QxpIpPTe9FiaVLdNYiQ+ZpHShH3qklktTdiy/GC33TIj4/n55rENd8nsMq1k3YC8xUp4alSpKCIyEAqQRTpIJxOcOTnCmZPrB9ZzkTJv5YpHAuu1NJBqzvVCZDybK3I40iTGOxSKmBpJhRU/1lammzcsRnKux4IV60xKgfVQqStN2KcumbXShG1az/dSMeXUEiwfbz630vk/ks2sTZDdXe73jmOPwkO5zhs0k2kF4SJxeP/7eeynP1UdZJFhlkwYsxMZZicyQPf5tJWKs7RaqqV8tCq9txAZfzybrx136r44kUk2lderK70XSQOZnVjb4NiX7ouyNdSVJuyTamnCVpsx192g2VAjPHeiIUjPN5UmvBRg3zpzsmSbfO4N5n63WklXKopIs1e9ivnU5glLN89MRCRoEhM2Ybmwh4XBSsVZLoTdF+s2LNZXBlkIxx96YrHWTKbUIbIeSyfrSunN1tJC6kvv1XKuw/HYuy/K1jCI0oSlU7UA+64fzPHSnS/sLve7Va746tGGlfT80yhNeBpzvxsD8tSoShPK1rN3L5P790Ov+z76RAGyyBBIJIxto0FljQt6aAbo7qwUyus3iAmP9x9frh136r44kkrUBcxtOzA2HI+lh7BJjMTHDNKjwYMzyI+fD8/aeXqvES1NWAum19mg2dTSPtKMZ+Wp5jSWHkoT1qQa87k3kvvdISBXl0w53W68kYuzWXjb2+KeCaAAWeQZzcyYHEkxOZLi/B72k7k7+WJ5Laiu27BYqK1kV1udP/pUjmw+y3yuSKHUoftiMlGXV92qA+PseH2DmJnxDBPD2H1RtoZoacKJ7f25RrnYIR+8XUDeJkjPz8Pi482bOTdUmrDbut/rNfJpE5CrS6bESH/6RKRnZsZ4JsV4JsWzeui+CEGTmGzdhsUWaSBhZ8ZDJ3P8LBzPF9s3ial2X6zruBh9PlHfIKa6kj31TO++KFtDMh08uihNuCHVLpnt2tG3C8jbBenLx1pv3OxVIt1xFfv580uw8LeRFJQ27ehbBeSZ8WCVXako0oYCZBEZqNF0knOmk5wz3Vst62r3xVYNYmql98Ljx7Or7Du6RDZXYKVD98VkwpoaxFSfz45nOH64yMLeI3UNYmbGMkyNqvuiDJFol8x+lSasVILShOuWH+xUMSVynD/J1NIJeOTA2gr5hksTtlvFXmfjZceAvJoPri6ZW5UCZBHZEjbafbFQqjRU/wgC7LpW52GwfXxplYePLbGQK7IUdl/8m317mz4zUW0SE6kMUtcgpvpaQ+C9TU1i5JkqkQgCx8w4p6s04d2R0ppA0CWzKR+8VUDeKh+8IUg/tQRLx5o3bHr7/3C3Zj3kfncoP9gpSFeXzL5QgCwiQy2TSrBjapQdU70F1sVyha9+87tcetk/7dCBMTg+uVLgwJMrZHOdm8SYwbbRtYC62hRmdrx1g5hq5ZBtoyk1iRFZTzIFySkYmerfNUqF9auddNUtMx+UJlw41Byk06FmZyuJVHebMZvGWgXkrSqmDKg04Yc/zIF77+XF/b9SVxQgi4i0kE4m2DZiXLyjtzrApXKFxdVSUxpIkFfd3IHx4IkV5nNFFlfXaRIzmqrPoW7asJhpKrs3PZYmrcBa5PSpdcnsUzuLapfMluUH18n9bhWQrx5t7qxZWu19XsmR9mklveZ+t1pJT4/BK17BYmEDFVv6RAGyiMhplEomOGMiwxkTmZ7eV644S6v1q9QL+SLzK4XIBsa1/OvD8/naOZ2axEyOpCIVP9JN+dTRNJDZcPPi9JiaxIjEoq5LZg81O3tRqbRYBe8y97sxIC+swMqTzedWir3P62iKZ110reogi4jImmTCwpXf3gLrSsVZOlVq2LBYiGxorO/AuO/oYu243CGyHs8k69NAJpobxEyPBUF1dMVaTWJENrlEYnBdMpvyvDts0LzxZs754Tfgvf2bVi8UIIuIbGGJhDE9FgSnz2a86/e5O8unSpGV6bW61Y0dGLO5Ag8fW66V4evUJGY0nVi/QUx0Q2M4PppOqOSeyLDYSJfM2W9QyWb7N6ceKUAWEXkGMjOmRtNMjaa5oIf3uTu5Qrl+s2K7Doy5IgeeWq4dF8odmsSkEuGGxdYNYmqtzqtl+cLjcTWJEZE+UIAsIiJdMzMmRlJMjKQ4r4cmMe7OarES2bDYUHovfF4tvXfwZI77Dgfjq8X2gXW1ScxsQ/WPWhpImFs9M5bh0YUyh07mmBlPM6kmMSLSgQJkERHpOzNjLJNkLDPGudO9d1+sq1vdogNjddX6SDbPg48vkM0XybVoEvOBH34HgFTCmiqARNNAmkrvhRsap0bUJEbkmUABsoiIbGobbRJzqlSuW6X+/t33cv5zn7dWei+3VobvicVVHnpiiYV8keVT7WtZJ8Oc78YOjNGye61K720bTSuwFunkL/6C/ffcw6645xFSgCwiIkNpJJVkx7YkO8LAOvdYit271s+4LpQqLOSLHRrErOVbP7VcYP+TQZ710jpNYtYC6+jKdPOGxcbAW90X5Rlh506WtUlPRERkc8qkEpw1NcJZUyM9va9YrrCYL0bqVlfL7jU3iMnmCjx6YoX5lc7dFwG2jabqSunN1PKqW3dgnA3PU/dF2VK++U1mf/rT4a+DbGYXAJ8Dzibom3iLu3/czM4AvghcBDwKXOPu8xbslvg48GogB9zg7veGn3U98P7woz/k7reG478GfBYYA+4Aft/dvd01+vW1ioiIpJMJzpwc4czJ3gLrcsUjgfVaB8ZqznW0QUw2FzSJqda67th9cSQVVvxYW5mejeRZT0dWr2fD4Hp6LE0mpcBaYvChD3FhNgs33RT3TID+riCXgJvc/V4zmwJ+bGb/ANwAfMvdP2Jm7wHeA7wbuBK4JHy8FPgU8NIw2P0TYBdBoP1jM9sTBryfAn4HuIsgQL4C+Gr4ma2uISIisqkkE8bsRIbZiQww0fX7KhVnabVUS/loVXpvITL+eDZfO+7UfXEik2wqr1dXei+SBjI7sbbBUd0XZZj0LUB296PA0fB4ycz2AecBVwG7w9NuBeYIgtergM+5uwN3mtmMmZ0bnvsP7n4SIAyyrzCzOWCbu98Zjn8OeB1BgNzuGiIiIkMhkbCgNfh4mgvP7P59lYqzXAi7L9ZtWKyvDLIQjj/0xGKtmUypQ2Q9lk5GVqbXNi02lt6r5VyH4+q+KJvRQHKQzewi4DKCld6zw+AZ4AmCFAwIgudDkbcdDsc6jR9uMU6HazTO6+3A2wHOPvts5ubmevvCnqbl5eWBX1P6R/dz+OieDhfdz/a2hY8LksBk+KiTxj3FahlWis5K0VkuBMfL4fPgUWK5WOTkvHPwuLNShOWC06H5IukETKSNyXT4a8aYSFcfMBkeT4bPq8eF/Iru5xDZmc1SLpc3zT3te4BsZpPA/wZudPfFaGH2MF+4w7fN09fpGu5+C3ALwK5du3z3gBPD5+bmGPQ1pX90P4eP7ulw0f2Mh7uTL5bXUj/qNiwWaivZ85H868OLwep1odS+SUzKjNnJUi2vulUHxtm6zY3B+IS6L25OMzNks9lN8z3a1wDZzNIEwfHn3f3/hMPHzOxcdz8aplAcD8ePQF3H0/PDsSOspUtUx+fC8fNbnN/pGiIiIjJAZsZ4JsV4JsWzeui+CEGTmGzdhsW1NJD7HnqE6bN21DozHjqZ42dh4J0vNjeJqap2X6zruBh9PlHfIGYmTGOZUvfF/rr5Zn5+1128NO55hPpZxcKATwP73P1jkZf2ANcDHwl//Upk/F1m9gWCTXoLYYD7deDDZjYbnvdbwHvd/aSZLZrZywhSN64D/nKda4iIiMgWMZpOcs50knOmm5vEzPkhdu9+Ucv3VbsvRlesq01haqX3wuPHs6vsO7pENldgpUX3xapkwpoaxFSfzzaU4YsG2FOj6r7Ylec9j/zRo+ufNyD9XEF+JXAtcL+Z7Q3H3kcQtN5mZm8FHgOuCV+7g6DE236CMm9vAQgD4T8FfhSe98Hqhj3gHayVeftq+KDDNURERGTIbbT7YqFUaaj+EQTYda3Ow2D7+NIqDx9bYiFXZKlD98VEtUlMpDJIXYOY6msNgfe2Z1qTmL/7O868//7hr4Ps7t8H2t3Zy1uc78A723zWZ4DPtBi/B3hhi/ETra4hIiIi0k4mlWDH1Cg7pnoLrIvlSm3Fun0HxuD45EqBA0+ukM11bhJjBttG1wLqalOY2fHWDWKqlUO2jaa2ZpOYj36UC7JZeN/74p4JoE56IiIiIk9LOplg++QI23tsElMqV1hcLTWlgQR51c0dGA+eWGE+V2RxdZ0mMaOp+hzqpg2Lmaaye9NjadJbMbDuEwXIIiIiIjFIJROcMZHhjIlMT+8rV5yl1fpV6oV8kfmVQqSO9Vr+9eH5fO2cTk1iJkdSkYofzfnU0TSQ2XDz4vTYcDaJUYAsIiIisoUkExau/PYWWFcqztKpUsOGxUJkQ2N9B8Z9Rxdrx+UOkfV4JlmfBjLR3CBmeiwIqqMr1pu5SYwCZBEREZFngETCmB4LgtNnM971+9yd5VOlyMp0tG51fQfGbK7Aw8eWa/nYxQ5dYkbTiVoayMeOLjKbdmZOxxd6GihAFhEREZG2zIyp0TRTo+m6hhXrcXdyhXL9ZsXI6vVCZPwvr/9P/PMd+U1TdkwBsoiIiIicdmbGxEiKiZEU563bJGbXpmkzDaDtiiIiIiISry9+kbO+/e24Z1GjAFlERERE4vWpT3Henj1xz6JGAbKIiIiISIQCZBERERGRCAXIIiIiIiIRCpBFRERERCJU5k1ERERE4nX77Tzwgx/wyrjnEdIKsoiIiIjEa/t2itPTcc+iRgGyiIiIiMTrs5/lnK99Le5Z1ChAFhEREZF4KUAWEREREdm8FCCLiIiIiEQoQBYRERERiVCALCIiIiISoTrIIiIiIhKvO+7gvu99j9+Iex4hrSCLiIiISLzGx6mMjsY9ixoFyCIiIiISr09+kmd9+ctxz6JGKRYiIiIiEq/bbmNHNhv3LGq0giwiIiIiEqEAWUREREQkQgGyiIiIiEiEAmQRERERkQhz97jnsCmY2ZPAYwO+7HbgqQFfU/pH93P46J4OF93P4aL7OXziuKcXuvtZjYMKkGNkZve4+6645yGnh+7n8NE9HS66n8NF93P4bKZ7qhQLEREREZEIBcgiIiIiIhEKkON1S9wTkNNK93P46J4OF93P4aL7OXw2zT1VDrKIiIiISIRWkEVEREREIhQgi4iIiIhEKEAeADO7wsx+bmb7zew9LV4fMbMvhq/fZWYXDX6W0q0u7ucfmtmDZnafmX3LzC6MY57SnfXuZ+S8f2NmbmabogSRtNfNPTWza8Lv0wfM7H8Oeo7SvS7+zn22mX3HzH4S/r376jjmKd0xs8+Y2XEz+1mb183M/nt4v+8zsxcPeo6gALnvzCwJfAK4ErgUeJOZXdpw2luBeXe/GPhz4D8PdpbSrS7v50+AXe7+IuB24L8MdpbSrS7vJ2Y2Bfw+cNdgZyi96uaemtklwHuBV7r7C4AbBz5R6UqX36PvB25z98uANwKfHOwspUefBa7o8PqVwCXh4+3ApwYwpyYKkPvvJcB+dz/g7gXgC8BVDedcBdwaHt8OXG5mNsA5SvfWvZ/u/h13z4VP7wTOH/AcpXvdfH8C/CnBf1xXBzk52ZBu7unvAJ9w93kAdz8+4DlK97q5nw5sC4+ngccHOD/pkbt/DzjZ4ZSrgM954E5gxszOHczs1ihA7r/zgEOR54fDsZbnuHsJWADOHMjspFfd3M+otwJf7euM5OlY936GP967wN3/7yAnJhvWzffoLwO/bGY/MLM7zazTapbEq5v7+QHgzWZ2GLgD+PeDmZr0Sa//zvZFatAXFHmmMLM3A7uA34x7LrIxZpYAPgbcEPNU5PRKEfz4djfBT3i+Z2a/4u7ZWGclG/Um4LPu/lEzeznw12b2QnevxD0x2bq0gtx/R4ALIs/PD8danmNmKYIfEZ0YyOykV93cT8zsVcB/BF7r7qcGNDfp3Xr3cwp4ITBnZo8CLwP2aKPeptbN9+hhYI+7F939F8DDBAGzbD7d3M+3ArcBuPsPgVFg+0BmJ/3Q1b+z/aYAuf9+BFxiZr9kZhmCDQR7Gs7ZA1wfHl8NfNvVwWWzWvd+mtllwM0EwbFyGze3jvfT3Rfcfbu7X+TuFxHklL/W3e+JZ7rShW7+zv0yweoxZradIOXiwCAnKV3r5n4eBC4HMLPnEwTITw50lnI67QGuC6tZvAxYcPejg56EUiz6zN1LZvYu4OtAEviMuz9gZh8E7nH3PcCnCX4ktJ8gcf2N8c1YOunyfv5XYBL423Cv5UF3f21sk5a2uryfsoV0eU+/DvyWmT0IlIH/4O76qd0m1OX9vAn4H2b2BwQb9m7QItPmZWb/i+A/qNvDvPE/AdIA7v5XBHnkrwb2AzngLbHMU3+GRERERETWKMVCRERERCRCAbKIiIiISIQCZBERERGRCAXIIiIiIiIRCpBFRERERCIUIIuISBMz221mfx/3PERE4qAAWUREREQkQgGyiMgWZmZvNrO7zWyvmd1sZkkzWzazPzezB8zsW2Z2VnjuTjO708zuM7MvmdlsOH6xmX3TzH5qZvea2XPDj580s9vN7CEz+7yFnW/M7CNm9mD4Of8tpi9dRKRvFCCLiGxRYVvdNwCvdPedBF3h/h0wQdBl7AXAdwk6VQF8Dni3u78IuD8y/nngE+7+q8ArgGpb18uAG4FLgecArzSzM4F/Bbwg/JwP9ferFBEZPAXIIiJb1+XArwE/MrO94fPnABXgi+E5fwP8MzObBmbc/bvh+K3Ab5jZFHCeu38JwN1X3T0XnnO3ux929wqwF7gIWABWgU+b2b8maAUrIjJUFCCLiGxdBtzq7jvDx/Pc/QMtzvMNfv6pyHEZSLl7CXgJcDvwGuBrG/xsEZFNSwGyiMjW9S3gajPbAWBmZ5jZhQR/t18dnvNvge+7+wIwb2a/Ho5fC3zX3ZeAw2b2uvAzRsxsvN0FzWwSmHb3O4A/AH61H1+YiEicUnFPQERENsbdHzSz9wPfMLMEUATeCawALwlfO06QpwxwPfBXYQB8AHhLOH4tcLOZfTD8jNd3uOwU8BUzGyVYwf7D0/xliYjEztw3+pM3ERHZjMxs2d0n456HiMhWpRQLEREREZEIrSCLiIiIiERoBVlEREREJEIBsoiIiIhIhAJkEREREZEIBcgiIiIiIhEKkEVEREREIv4/RD9JkI/7MLgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "k-fold 1:: Epoch 2: train loss 185672.41613661504 val loss 215289.37956373763\n",
            "in training loop, epoch 3, step 0, the loss is 102103.796875\n",
            "in training loop, epoch 3, step 1, the loss is 166689.78125\n",
            "in training loop, epoch 3, step 2, the loss is 98398.0859375\n",
            "in training loop, epoch 3, step 3, the loss is 153732.53125\n",
            "in training loop, epoch 3, step 4, the loss is 87956.390625\n",
            "in training loop, epoch 3, step 5, the loss is 104862.828125\n",
            "in training loop, epoch 3, step 6, the loss is 123138.296875\n",
            "in training loop, epoch 3, step 7, the loss is 147388.0625\n",
            "in training loop, epoch 3, step 8, the loss is 183471.578125\n",
            "in training loop, epoch 3, step 9, the loss is 107108.8125\n",
            "in training loop, epoch 3, step 10, the loss is 120652.1015625\n",
            "in training loop, epoch 3, step 11, the loss is 122477.6171875\n",
            "in training loop, epoch 3, step 12, the loss is 142598.328125\n",
            "in training loop, epoch 3, step 13, the loss is 151861.578125\n",
            "in training loop, epoch 3, step 14, the loss is 133520.171875\n",
            "in training loop, epoch 3, step 15, the loss is 117570.8046875\n",
            "in training loop, epoch 3, step 16, the loss is 149152.9375\n",
            "in training loop, epoch 3, step 17, the loss is 73604.59375\n",
            "in training loop, epoch 3, step 18, the loss is 87094.390625\n",
            "in training loop, epoch 3, step 19, the loss is 80184.5625\n",
            "in training loop, epoch 3, step 20, the loss is 123138.96875\n",
            "in training loop, epoch 3, step 21, the loss is 114071.8125\n",
            "in training loop, epoch 3, step 22, the loss is 207818.703125\n",
            "in training loop, epoch 3, step 23, the loss is 145647.109375\n",
            "in training loop, epoch 3, step 24, the loss is 110627.8828125\n",
            "in training loop, epoch 3, step 25, the loss is 84837.90625\n",
            "in training loop, epoch 3, step 26, the loss is 98248.1875\n",
            "in training loop, epoch 3, step 27, the loss is 83155.4375\n",
            "in training loop, epoch 3, step 28, the loss is 82631.2890625\n",
            "in training loop, epoch 3, step 29, the loss is 138316.921875\n",
            "in training loop, epoch 3, step 30, the loss is 108731.125\n",
            "in training loop, epoch 3, step 31, the loss is 68652.6953125\n",
            "in training loop, epoch 3, step 32, the loss is 77188.9375\n",
            "in training loop, epoch 3, step 33, the loss is 96982.921875\n",
            "in training loop, epoch 3, step 34, the loss is 180009.640625\n",
            "in training loop, epoch 3, step 35, the loss is 67734.1015625\n",
            "in training loop, epoch 3, step 36, the loss is 81155.546875\n",
            "in training loop, epoch 3, step 37, the loss is 109012.484375\n",
            "in training loop, epoch 3, step 38, the loss is 134765.828125\n",
            "in training loop, epoch 3, step 39, the loss is 95932.65625\n",
            "in training loop, epoch 3, step 40, the loss is 210277.421875\n",
            "in training loop, epoch 3, step 41, the loss is 78002.03125\n",
            "in training loop, epoch 3, step 42, the loss is 168805.8125\n",
            "in training loop, epoch 3, step 43, the loss is 118458.265625\n",
            "in training loop, epoch 3, step 44, the loss is 141051.03125\n",
            "in training loop, epoch 3, step 45, the loss is 180409.796875\n",
            "in training loop, epoch 3, step 46, the loss is 102608.140625\n",
            "in training loop, epoch 3, step 47, the loss is 90515.8046875\n",
            "in training loop, epoch 3, step 48, the loss is 107719.703125\n",
            "in training loop, epoch 3, step 49, the loss is 174369.796875\n",
            "in training loop, epoch 3, step 50, the loss is 103931.2890625\n",
            "in training loop, epoch 3, step 51, the loss is 75961.0078125\n",
            "in training loop, epoch 3, step 52, the loss is 124086.1328125\n",
            "in training loop, epoch 3, step 53, the loss is 82911.9375\n",
            "in training loop, epoch 3, step 54, the loss is 137159.296875\n",
            "in training loop, epoch 3, step 55, the loss is 112394.765625\n",
            "in training loop, epoch 3, step 56, the loss is 109748.859375\n",
            "in training loop, epoch 3, step 57, the loss is 113074.6875\n",
            "in training loop, epoch 3, step 58, the loss is 99336.8203125\n",
            "in training loop, epoch 3, step 59, the loss is 106412.125\n",
            "in training loop, epoch 3, step 60, the loss is 139703.5625\n",
            "in training loop, epoch 3, step 61, the loss is 102332.0546875\n",
            "in training loop, epoch 3, step 62, the loss is 158629.828125\n",
            "in training loop, epoch 3, step 63, the loss is 251989.046875\n",
            "in training loop, epoch 3, step 64, the loss is 114452.25\n",
            "in training loop, epoch 3, step 65, the loss is 139976.875\n",
            "in training loop, epoch 3, step 66, the loss is 121289.4375\n",
            "in training loop, epoch 3, step 67, the loss is 98606.8671875\n",
            "in training loop, epoch 3, step 68, the loss is 89730.078125\n",
            "in training loop, epoch 3, step 69, the loss is 138244.078125\n",
            "in training loop, epoch 3, step 70, the loss is 107541.796875\n",
            "in training loop, epoch 3, step 71, the loss is 113820.03125\n",
            "in training loop, epoch 3, step 72, the loss is 110041.9765625\n",
            "in training loop, epoch 3, step 73, the loss is 121490.796875\n",
            "in training loop, epoch 3, step 74, the loss is 77612.296875\n",
            "in training loop, epoch 3, step 75, the loss is 77836.0390625\n",
            "in training loop, epoch 3, step 76, the loss is 87266.90625\n",
            "in training loop, epoch 3, step 77, the loss is 129906.8359375\n",
            "in training loop, epoch 3, step 78, the loss is 140223.703125\n",
            "in training loop, epoch 3, step 79, the loss is 130138.359375\n",
            "in training loop, epoch 3, step 80, the loss is 109961.0390625\n",
            "in training loop, epoch 3, step 81, the loss is 94858.8671875\n",
            "in training loop, epoch 3, step 82, the loss is 128047.265625\n",
            "in training loop, epoch 3, step 83, the loss is 114865.2421875\n",
            "in training loop, epoch 3, step 84, the loss is 141657.359375\n",
            "in training loop, epoch 3, step 85, the loss is 190119.796875\n",
            "in training loop, epoch 3, step 86, the loss is 104543.421875\n",
            "in training loop, epoch 3, step 87, the loss is 144071.28125\n",
            "in training loop, epoch 3, step 88, the loss is 121249.2421875\n",
            "in training loop, epoch 3, step 89, the loss is 119785.359375\n",
            "in training loop, epoch 3, step 90, the loss is 113353.796875\n",
            "in training loop, epoch 3, step 91, the loss is 142481.5\n",
            "in training loop, epoch 3, step 92, the loss is 98516.0234375\n",
            "in training loop, epoch 3, step 93, the loss is 215104.34375\n",
            "in training loop, epoch 3, step 94, the loss is 236799.75\n",
            "in training loop, epoch 3, step 95, the loss is 102290.6171875\n",
            "in training loop, epoch 3, step 96, the loss is 180352.71875\n",
            "in training loop, epoch 3, step 97, the loss is 176192.03125\n",
            "in training loop, epoch 3, step 98, the loss is 126738.140625\n",
            "in training loop, epoch 3, step 99, the loss is 101697.5859375\n",
            "in training loop, epoch 3, step 100, the loss is 319481.25\n",
            "in training loop, epoch 3, step 101, the loss is 118865.765625\n",
            "in training loop, epoch 3, step 102, the loss is 118721.5625\n",
            "in training loop, epoch 3, step 103, the loss is 107155.9140625\n",
            "in training loop, epoch 3, step 104, the loss is 264146.78125\n",
            "in training loop, epoch 3, step 105, the loss is 137013.5\n",
            "in training loop, epoch 3, step 106, the loss is 126800.0625\n",
            "in training loop, epoch 3, step 107, the loss is 91066.421875\n",
            "in training loop, epoch 3, step 108, the loss is 129803.1875\n",
            "in training loop, epoch 3, step 109, the loss is 159015.140625\n",
            "in training loop, epoch 3, step 110, the loss is 98916.7265625\n",
            "in training loop, epoch 3, step 111, the loss is 214368.25\n",
            "in training loop, epoch 3, step 112, the loss is 392025.875\n",
            "in training loop, epoch 3, step 113, the loss is 139814.90625\n",
            "in training loop, epoch 3, step 114, the loss is 179508.265625\n",
            "in training loop, epoch 3, step 115, the loss is 180241.25\n",
            "in training loop, epoch 3, step 116, the loss is 103585.640625\n",
            "in training loop, epoch 3, step 117, the loss is 215106.390625\n",
            "in training loop, epoch 3, step 118, the loss is 167459.796875\n",
            "in training loop, epoch 3, step 119, the loss is 113615.046875\n",
            "in training loop, epoch 3, step 120, the loss is 159362.28125\n",
            "in training loop, epoch 3, step 121, the loss is 136359.6875\n",
            "in training loop, epoch 3, step 122, the loss is 161098.734375\n",
            "in training loop, epoch 3, step 123, the loss is 214089.203125\n",
            "in training loop, epoch 3, step 124, the loss is 84641.0\n",
            "in training loop, epoch 3, step 125, the loss is 108681.8203125\n",
            "in training loop, epoch 3, step 126, the loss is 169520.625\n",
            "in training loop, epoch 3, step 127, the loss is 107274.984375\n",
            "in training loop, epoch 3, step 128, the loss is 112867.046875\n",
            "in training loop, epoch 3, step 129, the loss is 90834.1484375\n",
            "in training loop, epoch 3, step 130, the loss is 210422.3125\n",
            "in training loop, epoch 3, step 131, the loss is 170764.84375\n",
            "in training loop, epoch 3, step 132, the loss is 176058.921875\n",
            "in training loop, epoch 3, step 133, the loss is 140217.4375\n",
            "in training loop, epoch 3, step 134, the loss is 139209.65625\n",
            "in training loop, epoch 3, step 135, the loss is 123895.5234375\n",
            "in training loop, epoch 3, step 136, the loss is 155181.9375\n",
            "in training loop, epoch 3, step 137, the loss is 132281.03125\n",
            "in training loop, epoch 3, step 138, the loss is 105549.65625\n",
            "in training loop, epoch 3, step 139, the loss is 157680.953125\n",
            "in training loop, epoch 3, step 140, the loss is 118332.1640625\n",
            "in training loop, epoch 3, step 141, the loss is 126453.765625\n",
            "in training loop, epoch 3, step 142, the loss is 118095.3671875\n",
            "in training loop, epoch 3, step 143, the loss is 121015.203125\n",
            "in training loop, epoch 3, step 144, the loss is 92962.84375\n",
            "in training loop, epoch 3, step 145, the loss is 126858.125\n",
            "in training loop, epoch 3, step 146, the loss is 100645.1953125\n",
            "in training loop, epoch 3, step 147, the loss is 134856.296875\n",
            "in training loop, epoch 3, step 148, the loss is 188780.609375\n",
            "in training loop, epoch 3, step 149, the loss is 134162.015625\n",
            "in training loop, epoch 3, step 150, the loss is 151200.859375\n",
            "in training loop, epoch 3, step 151, the loss is 90703.2890625\n",
            "in training loop, epoch 3, step 152, the loss is 82270.7421875\n",
            "in training loop, epoch 3, step 153, the loss is 90615.421875\n",
            "in training loop, epoch 3, step 154, the loss is 114492.6640625\n",
            "in training loop, epoch 3, step 155, the loss is 701137.4375\n",
            "in training loop, epoch 3, step 156, the loss is 179905.5\n",
            "in training loop, epoch 3, step 157, the loss is 158515.0\n",
            "in training loop, epoch 3, step 158, the loss is 190775.84375\n",
            "in training loop, epoch 3, step 159, the loss is 134433.140625\n",
            "in training loop, epoch 3, step 160, the loss is 136042.46875\n",
            "in training loop, epoch 3, step 161, the loss is 175300.15625\n",
            "in training loop, epoch 3, step 162, the loss is 162546.6875\n",
            "in training loop, epoch 3, step 163, the loss is 239330.625\n",
            "in training loop, epoch 3, step 164, the loss is 192571.34375\n",
            "in training loop, epoch 3, step 165, the loss is 146227.328125\n",
            "in training loop, epoch 3, step 166, the loss is 134462.703125\n",
            "in training loop, epoch 3, step 167, the loss is 202152.03125\n",
            "in training loop, epoch 3, step 168, the loss is 246702.171875\n",
            "in training loop, epoch 3, step 169, the loss is 151883.984375\n",
            "in training loop, epoch 3, step 170, the loss is 126245.640625\n",
            "in training loop, epoch 3, step 171, the loss is 143320.125\n",
            "in training loop, epoch 3, step 172, the loss is 115966.046875\n",
            "in training loop, epoch 3, step 173, the loss is 123423.984375\n",
            "in training loop, epoch 3, step 174, the loss is 177100.359375\n",
            "in training loop, epoch 3, step 175, the loss is 234266.390625\n",
            "in training loop, epoch 3, step 176, the loss is 287550.0625\n",
            "in training loop, epoch 3, step 177, the loss is 263370.3125\n",
            "in training loop, epoch 3, step 178, the loss is 148767.109375\n",
            "in training loop, epoch 3, step 179, the loss is 147135.859375\n",
            "in training loop, epoch 3, step 180, the loss is 509975.0625\n",
            "in training loop, epoch 3, step 181, the loss is 222501.375\n",
            "in training loop, epoch 3, step 182, the loss is 190588.078125\n",
            "in training loop, epoch 3, step 183, the loss is 269418.78125\n",
            "in training loop, epoch 3, step 184, the loss is 151335.171875\n",
            "in training loop, epoch 3, step 185, the loss is 207446.5\n",
            "in training loop, epoch 3, step 186, the loss is 287295.46875\n",
            "in training loop, epoch 3, step 187, the loss is 392951.5625\n",
            "in training loop, epoch 3, step 188, the loss is 500924.4375\n",
            "in training loop, epoch 3, step 189, the loss is 184877.1875\n",
            "in training loop, epoch 3, step 190, the loss is 292848.53125\n",
            "in training loop, epoch 3, step 191, the loss is 322413.0\n",
            "in training loop, epoch 3, step 192, the loss is 254851.515625\n",
            "in training loop, epoch 3, step 193, the loss is 254228.25\n",
            "in training loop, epoch 3, step 194, the loss is 209085.46875\n",
            "in training loop, epoch 3, step 195, the loss is 224439.0\n",
            "in training loop, epoch 3, step 196, the loss is 155734.3125\n",
            "in training loop, epoch 3, step 197, the loss is 154128.921875\n",
            "in training loop, epoch 3, step 198, the loss is 400232.5625\n",
            "in training loop, epoch 3, step 199, the loss is 163852.875\n",
            "in training loop, epoch 3, step 200, the loss is 219390.8125\n",
            "in training loop, epoch 3, step 201, the loss is 173389.4375\n",
            "in training loop, epoch 3, step 202, the loss is 217704.03125\n",
            "in training loop, epoch 3, step 203, the loss is 220337.984375\n",
            "in training loop, epoch 3, step 204, the loss is 126581.65625\n",
            "in training loop, epoch 3, step 205, the loss is 361312.8125\n",
            "in training loop, epoch 3, step 206, the loss is 139915.140625\n",
            "in training loop, epoch 3, step 207, the loss is 148846.90625\n",
            "in training loop, epoch 3, step 208, the loss is 158741.890625\n",
            "in training loop, epoch 3, step 209, the loss is 200382.9375\n",
            "in training loop, epoch 3, step 210, the loss is 241468.0\n",
            "in training loop, epoch 3, step 211, the loss is 293129.6875\n",
            "in training loop, epoch 3, step 212, the loss is 174483.0\n",
            "in training loop, epoch 3, step 213, the loss is 229420.046875\n",
            "in training loop, epoch 3, step 214, the loss is 156310.71875\n",
            "in training loop, epoch 3, step 215, the loss is 136831.78125\n",
            "in training loop, epoch 3, step 216, the loss is 332908.40625\n",
            "in training loop, epoch 3, step 217, the loss is 219267.34375\n",
            "in training loop, epoch 3, step 218, the loss is 151992.1875\n",
            "in training loop, epoch 3, step 219, the loss is 138166.421875\n",
            "in training loop, epoch 3, step 220, the loss is 166729.65625\n",
            "in training loop, epoch 3, step 221, the loss is 181060.859375\n",
            "in training loop, epoch 3, step 222, the loss is 173268.9375\n",
            "in training loop, epoch 3, step 223, the loss is 203833.1875\n",
            "in training loop, epoch 3, step 224, the loss is 155792.96875\n",
            "in training loop, epoch 3, step 225, the loss is 147855.296875\n",
            "in training loop, epoch 3, step 226, the loss is 152249.40625\n",
            "in training loop, epoch 3, step 227, the loss is 209088.25\n",
            "in training loop, epoch 3, step 228, the loss is 160299.453125\n",
            "in training loop, epoch 3, step 229, the loss is 129497.15625\n",
            "in training loop, epoch 3, step 230, the loss is 157835.359375\n",
            "in training loop, epoch 3, step 231, the loss is 134060.875\n",
            "in training loop, epoch 3, step 232, the loss is 120828.84375\n",
            "in training loop, epoch 3, step 233, the loss is 118363.0703125\n",
            "in training loop, epoch 3, step 234, the loss is 208128.90625\n",
            "in training loop, epoch 3, step 235, the loss is 113460.125\n",
            "in training loop, epoch 3, step 236, the loss is 109672.296875\n",
            "in training loop, epoch 3, step 237, the loss is 116720.390625\n",
            "in training loop, epoch 3, step 238, the loss is 122038.375\n",
            "in training loop, epoch 3, step 239, the loss is 144388.90625\n",
            "in training loop, epoch 3, step 240, the loss is 144501.140625\n",
            "in training loop, epoch 3, step 241, the loss is 98702.953125\n",
            "in training loop, epoch 3, step 242, the loss is 188789.703125\n",
            "in training loop, epoch 3, step 243, the loss is 230764.359375\n",
            "in training loop, epoch 3, step 244, the loss is 129268.4296875\n",
            "in training loop, epoch 3, step 245, the loss is 201398.078125\n",
            "in training loop, epoch 3, step 246, the loss is 152165.890625\n",
            "in training loop, epoch 3, step 247, the loss is 323879.03125\n",
            "in training loop, epoch 3, step 248, the loss is 142020.28125\n",
            "in training loop, epoch 3, step 249, the loss is 129174.046875\n",
            "in training loop, epoch 3, step 250, the loss is 182753.96875\n",
            "in training loop, epoch 3, step 251, the loss is 218187.6875\n",
            "in training loop, epoch 3, step 252, the loss is 135955.21875\n",
            "in training loop, epoch 3, step 253, the loss is 154537.96875\n",
            "in training loop, epoch 3, step 254, the loss is 227784.15625\n",
            "in training loop, epoch 3, step 255, the loss is 240004.4375\n",
            "in training loop, epoch 3, step 256, the loss is 209677.515625\n",
            "in training loop, epoch 3, step 257, the loss is 210827.09375\n",
            "in training loop, epoch 3, step 258, the loss is 147790.265625\n",
            "in training loop, epoch 3, step 259, the loss is 173666.59375\n",
            "in training loop, epoch 3, step 260, the loss is 160687.375\n",
            "in training loop, epoch 3, step 261, the loss is 120260.84375\n",
            "in training loop, epoch 3, step 262, the loss is 133982.734375\n",
            "in training loop, epoch 3, step 263, the loss is 117111.9609375\n",
            "in training loop, epoch 3, step 264, the loss is 131789.703125\n",
            "in training loop, epoch 3, step 265, the loss is 239674.125\n",
            "in training loop, epoch 3, step 266, the loss is 143547.65625\n",
            "in training loop, epoch 3, step 267, the loss is 135632.453125\n",
            "in training loop, epoch 3, step 268, the loss is 114884.359375\n",
            "in training loop, epoch 3, step 269, the loss is 111756.9921875\n",
            "in training loop, epoch 3, step 270, the loss is 88349.7109375\n",
            "in training loop, epoch 3, step 271, the loss is 149331.09375\n",
            "in training loop, epoch 3, step 272, the loss is 263378.09375\n",
            "in training loop, epoch 3, step 273, the loss is 152381.15625\n",
            "in training loop, epoch 3, step 274, the loss is 130062.21875\n",
            "in training loop, epoch 3, step 275, the loss is 162520.890625\n",
            "in training loop, epoch 3, step 276, the loss is 132736.546875\n",
            "in training loop, epoch 3, step 277, the loss is 163229.96875\n",
            "in training loop, epoch 3, step 278, the loss is 125361.3046875\n",
            "in training loop, epoch 3, step 279, the loss is 129235.515625\n",
            "in training loop, epoch 3, step 280, the loss is 125513.7890625\n",
            "in training loop, epoch 3, step 281, the loss is 151536.234375\n",
            "in training loop, epoch 3, step 282, the loss is 119950.3046875\n",
            "in training loop, epoch 3, step 283, the loss is 107617.203125\n",
            "in training loop, epoch 3, step 284, the loss is 140825.609375\n",
            "in training loop, epoch 3, step 285, the loss is 120470.625\n",
            "in training loop, epoch 3, step 286, the loss is 107649.71875\n",
            "in training loop, epoch 3, step 287, the loss is 122651.984375\n",
            "in training loop, epoch 3, step 288, the loss is 225751.0\n",
            "in training loop, epoch 3, step 289, the loss is 236081.0625\n",
            "in training loop, epoch 3, step 290, the loss is 150871.078125\n",
            "in training loop, epoch 3, step 291, the loss is 109796.2734375\n",
            "in training loop, epoch 3, step 292, the loss is 122589.3046875\n",
            "in training loop, epoch 3, step 293, the loss is 169813.640625\n",
            "in training loop, epoch 3, step 294, the loss is 88064.265625\n",
            "in training loop, epoch 3, step 295, the loss is 261038.0625\n",
            "in training loop, epoch 3, step 296, the loss is 121741.3359375\n",
            "in training loop, epoch 3, step 297, the loss is 244813.59375\n",
            "in training loop, epoch 3, step 298, the loss is 98430.671875\n",
            "in training loop, epoch 3, step 299, the loss is 109992.296875\n",
            "in training loop, epoch 3, step 300, the loss is 101432.7734375\n",
            "in training loop, epoch 3, step 301, the loss is 138607.390625\n",
            "in training loop, epoch 3, step 302, the loss is 161377.890625\n",
            "in training loop, epoch 3, step 303, the loss is 130798.453125\n",
            "in training loop, epoch 3, step 304, the loss is 115386.5703125\n",
            "in training loop, epoch 3, step 305, the loss is 199770.0625\n",
            "in training loop, epoch 3, step 306, the loss is 117405.9921875\n",
            "in training loop, epoch 3, step 307, the loss is 112526.515625\n",
            "in training loop, epoch 3, step 308, the loss is 114368.46875\n",
            "in training loop, epoch 3, step 309, the loss is 121365.984375\n",
            "in training loop, epoch 3, step 310, the loss is 250704.46875\n",
            "in training loop, epoch 3, step 311, the loss is 261391.875\n",
            "in training loop, epoch 3, step 312, the loss is 120038.90625\n",
            "in training loop, epoch 3, step 313, the loss is 227864.4375\n",
            "in training loop, epoch 3, step 314, the loss is 164765.15625\n",
            "in training loop, epoch 3, step 315, the loss is 149586.0625\n",
            "in training loop, epoch 3, step 316, the loss is 273337.375\n",
            "in training loop, epoch 3, step 317, the loss is 168588.546875\n",
            "in training loop, epoch 3, step 318, the loss is 119008.0078125\n",
            "in training loop, epoch 3, step 319, the loss is 262825.59375\n",
            "in training loop, epoch 3, step 320, the loss is 269444.96875\n",
            "in training loop, epoch 3, step 321, the loss is 250107.21875\n",
            "in training loop, epoch 3, step 322, the loss is 271412.71875\n",
            "in training loop, epoch 3, step 323, the loss is 198179.484375\n",
            "in training loop, epoch 3, step 324, the loss is 202720.234375\n",
            "in training loop, epoch 3, step 325, the loss is 313144.0\n",
            "in training loop, epoch 3, step 326, the loss is 186173.59375\n",
            "in training loop, epoch 3, step 327, the loss is 134919.328125\n",
            "in training loop, epoch 3, step 328, the loss is 127157.4609375\n",
            "in training loop, epoch 3, step 329, the loss is 192476.59375\n",
            "in training loop, epoch 3, step 330, the loss is 122952.359375\n",
            "in training loop, epoch 3, step 331, the loss is 163189.765625\n",
            "in training loop, epoch 3, step 332, the loss is 154862.09375\n",
            "in training loop, epoch 3, step 333, the loss is 187869.6875\n",
            "in training loop, epoch 3, step 334, the loss is 209334.1875\n",
            "in training loop, epoch 3, step 335, the loss is 171585.78125\n",
            "in training loop, epoch 3, step 336, the loss is 163550.34375\n",
            "in training loop, epoch 3, step 337, the loss is 158529.34375\n",
            "in training loop, epoch 3, step 338, the loss is 275403.0\n",
            "in training loop, epoch 3, step 339, the loss is 348605.0625\n",
            "in training loop, epoch 3, step 340, the loss is 251895.90625\n",
            "in training loop, epoch 3, step 341, the loss is 210467.953125\n",
            "in training loop, epoch 3, step 342, the loss is 248282.40625\n",
            "in training loop, epoch 3, step 343, the loss is 278812.53125\n",
            "in training loop, epoch 3, step 344, the loss is 196831.703125\n",
            "in training loop, epoch 3, step 345, the loss is 291892.0\n",
            "in training loop, epoch 3, step 346, the loss is 159154.453125\n",
            "in training loop, epoch 3, step 347, the loss is 121346.1953125\n",
            "in training loop, epoch 3, step 348, the loss is 139049.703125\n",
            "in training loop, epoch 3, step 349, the loss is 139602.171875\n",
            "in training loop, epoch 3, step 350, the loss is 174601.296875\n",
            "in training loop, epoch 3, step 351, the loss is 162419.640625\n",
            "in training loop, epoch 3, step 352, the loss is 144564.765625\n",
            "in training loop, epoch 3, step 353, the loss is 220804.25\n",
            "in training loop, epoch 3, step 354, the loss is 182552.125\n",
            "in training loop, epoch 3, step 355, the loss is 110505.5390625\n",
            "in training loop, epoch 3, step 356, the loss is 181165.0625\n",
            "in training loop, epoch 3, step 357, the loss is 173217.21875\n",
            "in training loop, epoch 3, step 358, the loss is 177840.40625\n",
            "in training loop, epoch 3, step 359, the loss is 158036.421875\n",
            "in training loop, epoch 3, step 360, the loss is 281572.0\n",
            "in training loop, epoch 3, step 361, the loss is 113118.203125\n",
            "in training loop, epoch 3, step 362, the loss is 209820.5\n",
            "in training loop, epoch 3, step 363, the loss is 126097.5703125\n",
            "in training loop, epoch 3, step 364, the loss is 138160.625\n",
            "in training loop, epoch 3, step 365, the loss is 186023.65625\n",
            "in training loop, epoch 3, step 366, the loss is 106943.46875\n",
            "in training loop, epoch 3, step 367, the loss is 172137.21875\n",
            "in training loop, epoch 3, step 368, the loss is 187996.4375\n",
            "in training loop, epoch 3, step 369, the loss is 144043.6875\n",
            "in training loop, epoch 3, step 370, the loss is 140982.90625\n",
            "in training loop, epoch 3, step 371, the loss is 190143.703125\n",
            "in training loop, epoch 3, step 372, the loss is 241211.46875\n",
            "in training loop, epoch 3, step 373, the loss is 156478.0\n",
            "in training loop, epoch 3, step 374, the loss is 157519.59375\n",
            "in training loop, epoch 3, step 375, the loss is 142768.40625\n",
            "in training loop, epoch 3, step 376, the loss is 238841.453125\n",
            "in training loop, epoch 3, step 377, the loss is 168870.21875\n",
            "in training loop, epoch 3, step 378, the loss is 192899.015625\n",
            "in training loop, epoch 3, step 379, the loss is 253077.8125\n",
            "in training loop, epoch 3, step 380, the loss is 193780.34375\n",
            "in training loop, epoch 3, step 381, the loss is 125017.59375\n",
            "in training loop, epoch 3, step 382, the loss is 162477.96875\n",
            "in training loop, epoch 3, step 383, the loss is 148800.65625\n",
            "in training loop, epoch 3, step 384, the loss is 406855.84375\n",
            "in training loop, epoch 3, step 385, the loss is 148223.6875\n",
            "in training loop, epoch 3, step 386, the loss is 123107.0703125\n",
            "in training loop, epoch 3, step 387, the loss is 154624.921875\n",
            "in training loop, epoch 3, step 388, the loss is 191599.03125\n",
            "in training loop, epoch 3, step 389, the loss is 307431.65625\n",
            "in training loop, epoch 3, step 390, the loss is 167541.328125\n",
            "in training loop, epoch 3, step 391, the loss is 142598.421875\n",
            "in training loop, epoch 3, step 392, the loss is 200169.4375\n",
            "in training loop, epoch 3, step 393, the loss is 478534.8125\n",
            "in training loop, epoch 3, step 394, the loss is 259412.53125\n",
            "in training loop, epoch 3, step 395, the loss is 503148.96875\n",
            "in training loop, epoch 3, step 396, the loss is 254407.46875\n",
            "in training loop, epoch 3, step 397, the loss is 153129.890625\n",
            "in training loop, epoch 3, step 398, the loss is 166257.8125\n",
            "in training loop, epoch 3, step 399, the loss is 224908.59375\n",
            "in training loop, epoch 3, step 400, the loss is 288430.375\n",
            "in training loop, epoch 3, step 401, the loss is 408805.21875\n",
            "in training loop, epoch 3, step 402, the loss is 178071.515625\n",
            "in training loop, epoch 3, step 403, the loss is 312040.34375\n",
            "in training loop, epoch 3, step 404, the loss is 303030.625\n",
            "in training loop, epoch 3, step 405, the loss is 213247.625\n",
            "in training loop, epoch 3, step 406, the loss is 322067.09375\n",
            "in training loop, epoch 3, step 407, the loss is 290545.15625\n",
            "in training loop, epoch 3, step 408, the loss is 233698.921875\n",
            "in training loop, epoch 3, step 409, the loss is 303522.15625\n",
            "in training loop, epoch 3, step 410, the loss is 170364.125\n",
            "in training loop, epoch 3, step 411, the loss is 222734.53125\n",
            "in training loop, epoch 3, step 412, the loss is 176687.390625\n",
            "in training loop, epoch 3, step 413, the loss is 302985.15625\n",
            "in training loop, epoch 3, step 414, the loss is 254001.421875\n",
            "in training loop, epoch 3, step 415, the loss is 344818.59375\n",
            "in training loop, epoch 3, step 416, the loss is 190457.390625\n",
            "in training loop, epoch 3, step 417, the loss is 156245.609375\n",
            "in training loop, epoch 3, step 418, the loss is 240045.53125\n",
            "in training loop, epoch 3, step 419, the loss is 365626.46875\n",
            "in training loop, epoch 3, step 420, the loss is 150973.46875\n",
            "in training loop, epoch 3, step 421, the loss is 1102646.75\n",
            "in training loop, epoch 3, step 422, the loss is 145779.828125\n",
            "in training loop, epoch 3, step 423, the loss is 224110.703125\n",
            "in training loop, epoch 3, step 424, the loss is 312255.25\n",
            "in training loop, epoch 3, step 425, the loss is 356255.875\n",
            "in training loop, epoch 3, step 426, the loss is 272309.3125\n",
            "in training loop, epoch 3, step 427, the loss is 212893.09375\n",
            "in training loop, epoch 3, step 428, the loss is 316267.90625\n",
            "in training loop, epoch 3, step 429, the loss is 307579.4375\n",
            "in training loop, epoch 3, step 430, the loss is 313095.875\n",
            "in training loop, epoch 3, step 431, the loss is 194805.15625\n",
            "in training loop, epoch 3, step 432, the loss is 243161.296875\n",
            "in training loop, epoch 3, step 433, the loss is 213892.734375\n",
            "in training loop, epoch 3, step 434, the loss is 233868.5625\n",
            "in training loop, epoch 3, step 435, the loss is 313367.125\n",
            "in training loop, epoch 3, step 436, the loss is 258690.703125\n",
            "in training loop, epoch 3, step 437, the loss is 354230.71875\n",
            "in training loop, epoch 3, step 438, the loss is 200440.640625\n",
            "in training loop, epoch 3, step 439, the loss is 272000.4375\n",
            "in training loop, epoch 3, step 440, the loss is 182765.078125\n",
            "in training loop, epoch 3, step 441, the loss is 209278.21875\n",
            "in training loop, epoch 3, step 442, the loss is 204621.390625\n",
            "in training loop, epoch 3, step 443, the loss is 328690.125\n",
            "in training loop, epoch 3, step 444, the loss is 217477.0\n",
            "in training loop, epoch 3, step 445, the loss is 195330.71875\n",
            "in training loop, epoch 3, step 446, the loss is 203663.609375\n",
            "in training loop, epoch 3, step 447, the loss is 194393.96875\n",
            "in training loop, epoch 3, step 448, the loss is 240765.671875\n",
            "in training loop, epoch 3, step 449, the loss is 161261.4375\n",
            "in training loop, epoch 3, step 450, the loss is 282839.78125\n",
            "in training loop, epoch 3, step 451, the loss is 377422.875\n",
            "in training loop, epoch 3, step 452, the loss is 191423.359375\n",
            "in training loop, epoch 3, step 453, the loss is 171768.703125\n",
            "in training loop, epoch 3, step 454, the loss is 179128.21875\n",
            "in training loop, epoch 3, step 455, the loss is 216156.265625\n",
            "in training loop, epoch 3, step 456, the loss is 288315.8125\n",
            "in training loop, epoch 3, step 457, the loss is 472526.6875\n",
            "in training loop, epoch 3, step 458, the loss is 204405.375\n",
            "in training loop, epoch 3, step 459, the loss is 334027.25\n",
            "in training loop, epoch 3, step 460, the loss is 433830.1875\n",
            "in training loop, epoch 3, step 461, the loss is 156957.03125\n",
            "in training loop, epoch 3, step 462, the loss is 453481.75\n",
            "in training loop, epoch 3, step 463, the loss is 197681.703125\n",
            "in training loop, epoch 3, step 464, the loss is 307316.53125\n",
            "in training loop, epoch 3, step 465, the loss is 256354.21875\n",
            "in training loop, epoch 3, step 466, the loss is 159649.8125\n",
            "in training loop, epoch 3, step 467, the loss is 263354.5\n",
            "in training loop, epoch 3, step 468, the loss is 243353.34375\n",
            "in training loop, epoch 3, step 469, the loss is 174357.453125\n",
            "in training loop, epoch 3, step 470, the loss is 303129.28125\n",
            "in training loop, epoch 3, step 471, the loss is 148672.859375\n",
            "in training loop, epoch 3, step 472, the loss is 225949.65625\n",
            "in training loop, epoch 3, step 473, the loss is 219871.046875\n",
            "in training loop, epoch 3, step 474, the loss is 152679.234375\n",
            "in training loop, epoch 3, step 475, the loss is 355352.3125\n",
            "in training loop, epoch 3, step 476, the loss is 189002.4375\n",
            "in training loop, epoch 3, step 477, the loss is 298376.375\n",
            "in training loop, epoch 3, step 478, the loss is 386917.9375\n",
            "in training loop, epoch 3, step 479, the loss is 146602.671875\n",
            "in training loop, epoch 3, step 480, the loss is 382165.65625\n",
            "in training loop, epoch 3, step 481, the loss is 365126.96875\n",
            "in training loop, epoch 3, step 482, the loss is 459618.28125\n",
            "in training loop, epoch 3, step 483, the loss is 271738.25\n",
            "in training loop, epoch 3, step 484, the loss is 166430.15625\n",
            "in training loop, epoch 3, step 485, the loss is 208829.890625\n",
            "in training loop, epoch 3, step 486, the loss is 329476.8125\n",
            "in training loop, epoch 3, step 487, the loss is 238196.171875\n",
            "in training loop, epoch 3, step 488, the loss is 177092.6875\n",
            "in training loop, epoch 3, step 489, the loss is 152188.484375\n",
            "in training loop, epoch 3, step 490, the loss is 166952.75\n",
            "in training loop, epoch 3, step 491, the loss is 215352.421875\n",
            "in training loop, epoch 3, step 492, the loss is 290542.1875\n",
            "in training loop, epoch 3, step 493, the loss is 177842.578125\n",
            "in training loop, epoch 3, step 494, the loss is 142785.59375\n",
            "in training loop, epoch 3, step 495, the loss is 177274.5\n",
            "in training loop, epoch 3, step 496, the loss is 205516.578125\n",
            "in training loop, epoch 3, step 497, the loss is 285614.1875\n",
            "in training loop, epoch 3, step 498, the loss is 143665.03125\n",
            "in training loop, epoch 3, step 499, the loss is 138493.8125\n",
            "in training loop, epoch 3, step 500, the loss is 146614.96875\n",
            "in training loop, epoch 3, step 501, the loss is 166905.703125\n",
            "in training loop, epoch 3, step 502, the loss is 171825.6875\n",
            "in training loop, epoch 3, step 503, the loss is 118443.875\n",
            "in training loop, epoch 3, step 504, the loss is 189814.40625\n",
            "in training loop, epoch 3, step 505, the loss is 144860.34375\n",
            "in training loop, epoch 3, step 506, the loss is 199938.40625\n",
            "in training loop, epoch 3, step 507, the loss is 161990.09375\n",
            "in training loop, epoch 3, step 508, the loss is 187788.015625\n",
            "in training loop, epoch 3, step 509, the loss is 174636.0625\n",
            "in training loop, epoch 3, step 510, the loss is 157027.984375\n",
            "in training loop, epoch 3, step 511, the loss is 140851.859375\n",
            "in training loop, epoch 3, step 512, the loss is 111252.78125\n",
            "in training loop, epoch 3, step 513, the loss is 132528.5\n",
            "in training loop, epoch 3, step 514, the loss is 235416.8125\n",
            "in training loop, epoch 3, step 515, the loss is 193149.046875\n",
            "in training loop, epoch 3, step 516, the loss is 195256.890625\n",
            "in training loop, epoch 3, step 517, the loss is 174217.890625\n",
            "in training loop, epoch 3, step 518, the loss is 169375.828125\n",
            "in training loop, epoch 3, step 519, the loss is 145920.59375\n",
            "in training loop, epoch 3, step 520, the loss is 140925.0\n",
            "in training loop, epoch 3, step 521, the loss is 163251.203125\n",
            "in training loop, epoch 3, step 522, the loss is 129600.46875\n",
            "in training loop, epoch 3, step 523, the loss is 143751.5\n",
            "in training loop, epoch 3, step 524, the loss is 118860.0\n",
            "in training loop, epoch 3, step 525, the loss is 110465.234375\n",
            "in training loop, epoch 3, step 526, the loss is 157255.265625\n",
            "in training loop, epoch 3, step 527, the loss is 146834.375\n",
            "in training loop, epoch 3, step 528, the loss is 205916.625\n",
            "in training loop, epoch 3, step 529, the loss is 128777.7578125\n",
            "in training loop, epoch 3, step 530, the loss is 97451.53125\n",
            "in training loop, epoch 3, step 531, the loss is 185183.421875\n",
            "in training loop, epoch 3, step 532, the loss is 292326.25\n",
            "in training loop, epoch 3, step 533, the loss is 197938.53125\n",
            "in training loop, epoch 3, step 534, the loss is 110134.890625\n",
            "in training loop, epoch 3, step 535, the loss is 284308.125\n",
            "in training loop, epoch 3, step 536, the loss is 119206.453125\n",
            "in training loop, epoch 3, step 537, the loss is 152904.109375\n",
            "in training loop, epoch 3, step 538, the loss is 117985.828125\n",
            "in training loop, epoch 3, step 539, the loss is 133399.75\n",
            "in training loop, epoch 3, step 540, the loss is 124390.21875\n",
            "in training loop, epoch 3, step 541, the loss is 170055.21875\n",
            "in training loop, epoch 3, step 542, the loss is 178967.75\n",
            "in training loop, epoch 3, step 543, the loss is 116510.3515625\n",
            "in training loop, epoch 3, step 544, the loss is 98539.484375\n",
            "in training loop, epoch 3, step 545, the loss is 126928.40625\n",
            "in training loop, epoch 3, step 546, the loss is 168852.5625\n",
            "in training loop, epoch 3, step 547, the loss is 102374.65625\n",
            "in training loop, epoch 3, step 548, the loss is 190289.90625\n",
            "in training loop, epoch 3, step 549, the loss is 97708.59375\n",
            "in training loop, epoch 3, step 550, the loss is 92497.0234375\n",
            "in training loop, epoch 3, step 551, the loss is 138904.25\n",
            "in training loop, epoch 3, step 552, the loss is 117685.828125\n",
            "in training loop, epoch 3, step 553, the loss is 252940.59375\n",
            "in training loop, epoch 3, step 554, the loss is 137454.953125\n",
            "in training loop, epoch 3, step 555, the loss is 152845.703125\n",
            "in training loop, epoch 3, step 556, the loss is 138860.609375\n",
            "in training loop, epoch 3, step 557, the loss is 155172.75\n",
            "in training loop, epoch 3, step 558, the loss is 152635.796875\n",
            "in training loop, epoch 3, step 559, the loss is 162694.953125\n",
            "in training loop, epoch 3, step 560, the loss is 138962.140625\n",
            "in training loop, epoch 3, step 561, the loss is 174387.1875\n",
            "in training loop, epoch 3, step 562, the loss is 120004.765625\n",
            "in training loop, epoch 3, step 563, the loss is 160507.375\n",
            "in training loop, epoch 3, step 564, the loss is 107513.3515625\n",
            "in training loop, epoch 3, step 565, the loss is 108738.421875\n",
            "in training loop, epoch 3, step 566, the loss is 144163.28125\n",
            "in training loop, epoch 3, step 567, the loss is 110455.96875\n",
            "in training loop, epoch 3, step 568, the loss is 243546.71875\n",
            "in training loop, epoch 3, step 569, the loss is 141939.15625\n",
            "in training loop, epoch 3, step 570, the loss is 165316.53125\n",
            "in training loop, epoch 3, step 571, the loss is 146047.5625\n",
            "in training loop, epoch 3, step 572, the loss is 193842.09375\n",
            "in training loop, epoch 3, step 573, the loss is 143332.84375\n",
            "in training loop, epoch 3, step 574, the loss is 198372.734375\n",
            "in training loop, epoch 3, step 575, the loss is 130713.6015625\n",
            "in training loop, epoch 3, step 576, the loss is 136181.984375\n",
            "in training loop, epoch 3, step 577, the loss is 168479.53125\n",
            "in training loop, epoch 3, step 578, the loss is 245022.46875\n",
            "in training loop, epoch 3, step 579, the loss is 117310.3515625\n",
            "in training loop, epoch 3, step 580, the loss is 147303.796875\n",
            "in training loop, epoch 3, step 581, the loss is 108834.4375\n",
            "in training loop, epoch 3, step 582, the loss is 90667.265625\n",
            "in training loop, epoch 3, step 583, the loss is 154746.21875\n",
            "in training loop, epoch 3, step 584, the loss is 171801.609375\n",
            "in training loop, epoch 3, step 585, the loss is 155463.5625\n",
            "in training loop, epoch 3, step 586, the loss is 129485.140625\n",
            "in training loop, epoch 3, step 587, the loss is 103014.6171875\n",
            "in training loop, epoch 3, step 588, the loss is 107585.1796875\n",
            "in training loop, epoch 3, step 589, the loss is 166561.8125\n",
            "in training loop, epoch 3, step 590, the loss is 227480.15625\n",
            "in training loop, epoch 3, step 591, the loss is 315535.34375\n",
            "in training loop, epoch 3, step 592, the loss is 226606.65625\n",
            "in training loop, epoch 3, step 593, the loss is 117121.1484375\n",
            "in training loop, epoch 3, step 594, the loss is 123253.7265625\n",
            "in training loop, epoch 3, step 595, the loss is 193620.28125\n",
            "in training loop, epoch 3, step 596, the loss is 178195.40625\n",
            "in training loop, epoch 3, step 597, the loss is 145915.125\n",
            "in training loop, epoch 3, step 598, the loss is 237105.53125\n",
            "in training loop, epoch 3, step 599, the loss is 85988.5390625\n",
            "in training loop, epoch 3, step 600, the loss is 177598.25\n",
            "in training loop, epoch 3, step 601, the loss is 148662.28125\n",
            "in training loop, epoch 3, step 602, the loss is 147070.5\n",
            "in training loop, epoch 3, step 603, the loss is 210881.75\n",
            "in training loop, epoch 3, step 604, the loss is 137722.640625\n",
            "in training loop, epoch 3, step 605, the loss is 127768.109375\n",
            "in training loop, epoch 3, step 606, the loss is 147083.25\n",
            "in training loop, epoch 3, step 607, the loss is 178652.828125\n",
            "in training loop, epoch 3, step 608, the loss is 166399.09375\n",
            "in training loop, epoch 3, step 609, the loss is 254888.015625\n",
            "in training loop, epoch 3, step 610, the loss is 105882.484375\n",
            "in training loop, epoch 3, step 611, the loss is 168989.875\n",
            "in training loop, epoch 3, step 612, the loss is 149505.828125\n",
            "in training loop, epoch 3, step 613, the loss is 147799.59375\n",
            "in training loop, epoch 3, step 614, the loss is 217109.1875\n",
            "in training loop, epoch 3, step 615, the loss is 144495.8125\n",
            "in training loop, epoch 3, step 616, the loss is 133191.90625\n",
            "in training loop, epoch 3, step 617, the loss is 155520.328125\n",
            "in training loop, epoch 3, step 618, the loss is 204018.515625\n",
            "in training loop, epoch 3, step 619, the loss is 171431.59375\n",
            "in training loop, epoch 3, step 620, the loss is 183676.03125\n",
            "in training loop, epoch 3, step 621, the loss is 148924.921875\n",
            "in training loop, epoch 3, step 622, the loss is 101283.8671875\n",
            "in training loop, epoch 3, step 623, the loss is 189966.953125\n",
            "in training loop, epoch 3, step 624, the loss is 168408.328125\n",
            "in training loop, epoch 3, step 625, the loss is 182700.34375\n",
            "in training loop, epoch 3, step 626, the loss is 133903.4375\n",
            "in training loop, epoch 3, step 627, the loss is 173633.09375\n",
            "in training loop, epoch 3, step 628, the loss is 161092.875\n",
            "in training loop, epoch 3, step 629, the loss is 228496.390625\n",
            "in training loop, epoch 3, step 630, the loss is 207207.125\n",
            "in training loop, epoch 3, step 631, the loss is 139140.734375\n",
            "in training loop, epoch 3, step 632, the loss is 175169.1875\n",
            "in training loop, epoch 3, step 633, the loss is 160792.46875\n",
            "in training loop, epoch 3, step 634, the loss is 126854.75\n",
            "in training loop, epoch 3, step 635, the loss is 116556.125\n",
            "in training loop, epoch 3, step 636, the loss is 127127.5390625\n",
            "in training loop, epoch 3, step 637, the loss is 123166.625\n",
            "in training loop, epoch 3, step 638, the loss is 148270.625\n",
            "in training loop, epoch 3, step 639, the loss is 184064.59375\n",
            "in training loop, epoch 3, step 640, the loss is 153189.25\n",
            "in training loop, epoch 3, step 641, the loss is 122848.703125\n",
            "in training loop, epoch 3, step 642, the loss is 145025.15625\n",
            "in training loop, epoch 3, step 643, the loss is 115350.4140625\n",
            "in training loop, epoch 3, step 644, the loss is 180973.46875\n",
            "in training loop, epoch 3, step 645, the loss is 159675.6875\n",
            "in training loop, epoch 3, step 646, the loss is 156283.203125\n",
            "in training loop, epoch 3, step 647, the loss is 138859.484375\n",
            "in training loop, epoch 3, step 648, the loss is 163232.6875\n",
            "in training loop, epoch 3, step 649, the loss is 132429.78125\n",
            "in training loop, epoch 3, step 650, the loss is 175588.125\n",
            "in training loop, epoch 3, step 651, the loss is 125334.0859375\n",
            "in training loop, epoch 3, step 652, the loss is 125248.1328125\n",
            "in training loop, epoch 3, step 653, the loss is 101680.4609375\n",
            "in training loop, epoch 3, step 654, the loss is 108600.140625\n",
            "in training loop, epoch 3, step 655, the loss is 106599.3984375\n",
            "in training loop, epoch 3, step 656, the loss is 86504.3828125\n",
            "in training loop, epoch 3, step 657, the loss is 111532.6328125\n",
            "in training loop, epoch 3, step 658, the loss is 178431.609375\n",
            "in training loop, epoch 3, step 659, the loss is 124383.8359375\n",
            "in training loop, epoch 3, step 660, the loss is 197315.34375\n",
            "in training loop, epoch 3, step 661, the loss is 148301.6875\n",
            "in training loop, epoch 3, step 662, the loss is 131978.640625\n",
            "in training loop, epoch 3, step 663, the loss is 160529.640625\n",
            "in training loop, epoch 3, step 664, the loss is 231431.765625\n",
            "in training loop, epoch 3, step 665, the loss is 104547.4375\n",
            "in training loop, epoch 3, step 666, the loss is 122359.484375\n",
            "in training loop, epoch 3, step 667, the loss is 119751.328125\n",
            "in training loop, epoch 3, step 668, the loss is 308121.78125\n",
            "in training loop, epoch 3, step 669, the loss is 206727.109375\n",
            "in training loop, epoch 3, step 670, the loss is 104411.46875\n",
            "in training loop, epoch 3, step 671, the loss is 130782.8125\n",
            "in training loop, epoch 3, step 672, the loss is 200720.03125\n",
            "in training loop, epoch 3, step 673, the loss is 224090.9375\n",
            "in training loop, epoch 3, step 674, the loss is 174053.9375\n",
            "in training loop, epoch 3, step 675, the loss is 143377.90625\n",
            "in training loop, epoch 3, step 676, the loss is 168544.71875\n",
            "in training loop, epoch 3, step 677, the loss is 132609.78125\n",
            "in training loop, epoch 3, step 678, the loss is 171988.953125\n",
            "in training loop, epoch 3, step 679, the loss is 113030.75\n",
            "in training loop, epoch 3, step 680, the loss is 130479.25\n",
            "in training loop, epoch 3, step 681, the loss is 138923.484375\n",
            "in training loop, epoch 3, step 682, the loss is 121444.3046875\n",
            "in training loop, epoch 3, step 683, the loss is 162115.4375\n",
            "in training loop, epoch 3, step 684, the loss is 113398.203125\n",
            "in training loop, epoch 3, step 685, the loss is 180860.96875\n",
            "in training loop, epoch 3, step 686, the loss is 209299.109375\n",
            "in training loop, epoch 3, step 687, the loss is 100908.75\n",
            "in training loop, epoch 3, step 688, the loss is 95019.4609375\n",
            "in training loop, epoch 3, step 689, the loss is 181352.1875\n",
            "in training loop, epoch 3, step 690, the loss is 153046.5625\n",
            "in training loop, epoch 3, step 691, the loss is 133341.9375\n",
            "in training loop, epoch 3, step 692, the loss is 150052.609375\n",
            "in training loop, epoch 3, step 693, the loss is 138815.6875\n",
            "in training loop, epoch 3, step 694, the loss is 157922.59375\n",
            "in training loop, epoch 3, step 695, the loss is 133004.140625\n",
            "in training loop, epoch 3, step 696, the loss is 130635.4921875\n",
            "in training loop, epoch 3, step 697, the loss is 101156.65625\n",
            "in training loop, epoch 3, step 698, the loss is 111826.515625\n",
            "in training loop, epoch 3, step 699, the loss is 177193.6875\n",
            "in training loop, epoch 3, step 700, the loss is 155592.46875\n",
            "in training loop, epoch 3, step 701, the loss is 217625.984375\n",
            "in training loop, epoch 3, step 702, the loss is 137020.078125\n",
            "in training loop, epoch 3, step 703, the loss is 143262.109375\n",
            "in training loop, epoch 3, step 704, the loss is 130084.9921875\n",
            "in training loop, epoch 3, step 705, the loss is 180909.859375\n",
            "in training loop, epoch 3, step 706, the loss is 157811.609375\n",
            "in training loop, epoch 3, step 707, the loss is 292193.6875\n",
            "in training loop, epoch 3, step 708, the loss is 108693.9453125\n",
            "in training loop, epoch 3, step 709, the loss is 102440.0234375\n",
            "in training loop, epoch 3, step 710, the loss is 223201.75\n",
            "in training loop, epoch 3, step 711, the loss is 106643.7421875\n",
            "in training loop, epoch 3, step 712, the loss is 160696.859375\n",
            "in training loop, epoch 3, step 713, the loss is 168113.453125\n",
            "in training loop, epoch 3, step 714, the loss is 136954.984375\n",
            "in training loop, epoch 3, step 715, the loss is 151956.96875\n",
            "in training loop, epoch 3, step 716, the loss is 133076.671875\n",
            "in training loop, epoch 3, step 717, the loss is 135603.234375\n",
            "in training loop, epoch 3, step 718, the loss is 173486.6875\n",
            "in training loop, epoch 3, step 719, the loss is 142882.578125\n",
            "in training loop, epoch 3, step 720, the loss is 281396.78125\n",
            "in training loop, epoch 3, step 721, the loss is 112726.1171875\n",
            "in training loop, epoch 3, step 722, the loss is 109649.875\n",
            "in training loop, epoch 3, step 723, the loss is 112660.71875\n",
            "in training loop, epoch 3, step 724, the loss is 162734.140625\n",
            "in training loop, epoch 3, step 725, the loss is 267475.6875\n",
            "in training loop, epoch 3, step 726, the loss is 139102.25\n",
            "in training loop, epoch 3, step 727, the loss is 108593.859375\n",
            "in training loop, epoch 3, step 728, the loss is 175685.5625\n",
            "in training loop, epoch 3, step 729, the loss is 119443.0\n",
            "in training loop, epoch 3, step 730, the loss is 156939.859375\n",
            "in training loop, epoch 3, step 731, the loss is 203117.3125\n",
            "in training loop, epoch 3, step 732, the loss is 120403.734375\n",
            "in training loop, epoch 3, step 733, the loss is 112849.875\n",
            "in training loop, epoch 3, step 734, the loss is 117387.109375\n",
            "in training loop, epoch 3, step 735, the loss is 123663.9296875\n",
            "in training loop, epoch 3, step 736, the loss is 127904.71875\n",
            "in training loop, epoch 3, step 737, the loss is 126987.2890625\n",
            "in training loop, epoch 3, step 738, the loss is 221466.734375\n",
            "in training loop, epoch 3, step 739, the loss is 123856.3203125\n",
            "in training loop, epoch 3, step 740, the loss is 148861.0625\n",
            "in training loop, epoch 3, step 741, the loss is 129546.359375\n",
            "in training loop, epoch 3, step 742, the loss is 151520.6875\n",
            "in training loop, epoch 3, step 743, the loss is 184048.609375\n",
            "in training loop, epoch 3, step 744, the loss is 114340.96875\n",
            "in training loop, epoch 3, step 745, the loss is 169903.65625\n",
            "in training loop, epoch 3, step 746, the loss is 201999.921875\n",
            "in training loop, epoch 3, step 747, the loss is 123824.875\n",
            "in training loop, epoch 3, step 748, the loss is 169423.375\n",
            "in training loop, epoch 3, step 749, the loss is 180114.09375\n",
            "in training loop, epoch 3, step 750, the loss is 142886.21875\n",
            "in training loop, epoch 3, step 751, the loss is 111232.15625\n",
            "in training loop, epoch 3, step 752, the loss is 164127.578125\n",
            "in training loop, epoch 3, step 753, the loss is 150657.1875\n",
            "in training loop, epoch 3, step 754, the loss is 146630.5625\n",
            "in training loop, epoch 3, step 755, the loss is 152026.15625\n",
            "in training loop, epoch 3, step 756, the loss is 181280.34375\n",
            "in training loop, epoch 3, step 757, the loss is 140055.921875\n",
            "in training loop, epoch 3, step 758, the loss is 156595.109375\n",
            "in training loop, epoch 3, step 759, the loss is 120635.859375\n",
            "in training loop, epoch 3, step 760, the loss is 169969.9375\n",
            "in training loop, epoch 3, step 761, the loss is 125153.328125\n",
            "in training loop, epoch 3, step 762, the loss is 108296.8046875\n",
            "in training loop, epoch 3, step 763, the loss is 113987.578125\n",
            "in training loop, epoch 3, step 764, the loss is 88081.875\n",
            "in training loop, epoch 3, step 765, the loss is 119326.5625\n",
            "in training loop, epoch 3, step 766, the loss is 147179.21875\n",
            "in training loop, epoch 3, step 767, the loss is 133436.96875\n",
            "in training loop, epoch 3, step 768, the loss is 198688.15625\n",
            "in training loop, epoch 3, step 769, the loss is 306389.0\n",
            "in training loop, epoch 3, step 770, the loss is 136554.0625\n",
            "in training loop, epoch 3, step 771, the loss is 129345.890625\n",
            "in training loop, epoch 3, step 772, the loss is 149542.828125\n",
            "in training loop, epoch 3, step 773, the loss is 199114.59375\n",
            "in training loop, epoch 3, step 774, the loss is 124965.4296875\n",
            "in training loop, epoch 3, step 775, the loss is 129481.0703125\n",
            "in training loop, epoch 3, step 776, the loss is 128575.859375\n",
            "in training loop, epoch 3, step 777, the loss is 117679.734375\n",
            "in training loop, epoch 3, step 778, the loss is 132360.953125\n",
            "in training loop, epoch 3, step 779, the loss is 173380.9375\n",
            "in training loop, epoch 3, step 780, the loss is 153730.3125\n",
            "in training loop, epoch 3, step 781, the loss is 114767.625\n",
            "in training loop, epoch 3, step 782, the loss is 126815.859375\n",
            "in training loop, epoch 3, step 783, the loss is 137093.421875\n",
            "in training loop, epoch 3, step 784, the loss is 181391.921875\n",
            "in training loop, epoch 3, step 785, the loss is 103367.3515625\n",
            "in training loop, epoch 3, step 786, the loss is 137480.9375\n",
            "in training loop, epoch 3, step 787, the loss is 195325.171875\n",
            "in training loop, epoch 3, step 788, the loss is 156489.359375\n",
            "in training loop, epoch 3, step 789, the loss is 119948.046875\n",
            "in training loop, epoch 3, step 790, the loss is 128513.03125\n",
            "in training loop, epoch 3, step 791, the loss is 114588.203125\n",
            "in training loop, epoch 3, step 792, the loss is 137400.9375\n",
            "in training loop, epoch 3, step 793, the loss is 160801.09375\n",
            "in training loop, epoch 3, step 794, the loss is 127704.3828125\n",
            "in training loop, epoch 3, step 795, the loss is 149095.125\n",
            "in training loop, epoch 3, step 796, the loss is 135882.921875\n",
            "in training loop, epoch 3, step 797, the loss is 121344.203125\n",
            "in training loop, epoch 3, step 798, the loss is 101707.0625\n",
            "in training loop, epoch 3, step 799, the loss is 117270.3359375\n",
            "in training loop, epoch 3, step 800, the loss is 152544.234375\n",
            "in training loop, epoch 3, step 801, the loss is 112999.9921875\n",
            "in training loop, epoch 3, step 802, the loss is 129392.640625\n",
            "in training loop, epoch 3, step 803, the loss is 149528.0\n",
            "in training loop, epoch 3, step 804, the loss is 103522.8125\n",
            "in training loop, epoch 3, step 805, the loss is 108330.34375\n",
            "in training loop, epoch 3, step 806, the loss is 101986.7890625\n",
            "in training loop, epoch 3, step 807, the loss is 149739.625\n",
            "in training loop, epoch 3, step 808, the loss is 119011.3984375\n",
            "in training loop, epoch 3, step 809, the loss is 143681.109375\n",
            "in training loop, epoch 3, step 810, the loss is 153764.375\n",
            "in training loop, epoch 3, step 811, the loss is 153203.328125\n",
            "in training loop, epoch 3, step 812, the loss is 99747.1640625\n",
            "in training loop, epoch 3, step 813, the loss is 148636.34375\n",
            "in training loop, epoch 3, step 814, the loss is 97066.3359375\n",
            "in training loop, epoch 3, step 815, the loss is 105246.8984375\n",
            "in training loop, epoch 3, step 816, the loss is 120784.828125\n",
            "in training loop, epoch 3, step 817, the loss is 91773.578125\n",
            "in training loop, epoch 3, step 818, the loss is 119992.859375\n",
            "in training loop, epoch 3, step 819, the loss is 217319.59375\n",
            "in training loop, epoch 3, step 820, the loss is 128286.1875\n",
            "in training loop, epoch 3, step 821, the loss is 205145.59375\n",
            "in training loop, epoch 3, step 822, the loss is 126938.8125\n",
            "in training loop, epoch 3, step 823, the loss is 106867.8125\n",
            "in training loop, epoch 3, step 824, the loss is 103041.5\n",
            "in training loop, epoch 3, step 825, the loss is 191723.640625\n",
            "in training loop, epoch 3, step 826, the loss is 123187.0078125\n",
            "in training loop, epoch 3, step 827, the loss is 102372.328125\n",
            "in training loop, epoch 3, step 828, the loss is 136556.1875\n",
            "in training loop, epoch 3, step 829, the loss is 95038.8203125\n",
            "in training loop, epoch 3, step 830, the loss is 163734.5\n",
            "in training loop, epoch 3, step 831, the loss is 223447.78125\n",
            "in training loop, epoch 3, step 832, the loss is 122631.46875\n",
            "in training loop, epoch 3, step 833, the loss is 145545.875\n",
            "in training loop, epoch 3, step 834, the loss is 128799.5703125\n",
            "in training loop, epoch 3, step 835, the loss is 154812.46875\n",
            "in training loop, epoch 3, step 836, the loss is 103327.8671875\n",
            "in training loop, epoch 3, step 837, the loss is 126656.109375\n",
            "in training loop, epoch 3, step 838, the loss is 177643.453125\n",
            "in training loop, epoch 3, step 839, the loss is 110362.5390625\n",
            "in training loop, epoch 3, step 840, the loss is 153091.96875\n",
            "in training loop, epoch 3, step 841, the loss is 133642.34375\n",
            "in training loop, epoch 3, step 842, the loss is 101548.734375\n",
            "in training loop, epoch 3, step 843, the loss is 141195.78125\n",
            "in training loop, epoch 3, step 844, the loss is 191460.828125\n",
            "in training loop, epoch 3, step 845, the loss is 226957.640625\n",
            "in training loop, epoch 3, step 846, the loss is 120810.3359375\n",
            "in training loop, epoch 3, step 847, the loss is 124034.75\n",
            "in training loop, epoch 3, step 848, the loss is 246340.296875\n",
            "in training loop, epoch 3, step 849, the loss is 128633.140625\n",
            "in training loop, epoch 3, step 850, the loss is 220539.8125\n",
            "in training loop, epoch 3, step 851, the loss is 131191.28125\n",
            "in training loop, epoch 3, step 852, the loss is 331974.875\n",
            "in training loop, epoch 3, step 853, the loss is 105030.5078125\n",
            "in training loop, epoch 3, step 854, the loss is 223363.828125\n",
            "in training loop, epoch 3, step 855, the loss is 157394.78125\n",
            "in training loop, epoch 3, step 856, the loss is 209819.640625\n",
            "in training loop, epoch 3, step 857, the loss is 167071.234375\n",
            "in training loop, epoch 3, step 858, the loss is 246456.0625\n",
            "in training loop, epoch 3, step 859, the loss is 124582.484375\n",
            "in training loop, epoch 3, step 860, the loss is 168978.609375\n",
            "in training loop, epoch 3, step 861, the loss is 153424.609375\n",
            "in training loop, epoch 3, step 862, the loss is 152529.0\n",
            "in training loop, epoch 3, step 863, the loss is 204016.390625\n",
            "in training loop, epoch 3, step 864, the loss is 256313.03125\n",
            "in training loop, epoch 3, step 865, the loss is 103020.6328125\n",
            "in training loop, epoch 3, step 866, the loss is 141579.421875\n",
            "in training loop, epoch 3, step 867, the loss is 121221.421875\n",
            "in training loop, epoch 3, step 868, the loss is 150780.25\n",
            "in training loop, epoch 3, step 869, the loss is 150456.78125\n",
            "in training loop, epoch 3, step 870, the loss is 207570.9375\n",
            "in training loop, epoch 3, step 871, the loss is 117209.359375\n",
            "in training loop, epoch 3, step 872, the loss is 134627.90625\n",
            "in training loop, epoch 3, step 873, the loss is 219004.375\n",
            "in training loop, epoch 3, step 874, the loss is 140005.125\n",
            "in training loop, epoch 3, step 875, the loss is 87873.9765625\n",
            "in training loop, epoch 3, step 876, the loss is 230508.921875\n",
            "in training loop, epoch 3, step 877, the loss is 156026.40625\n",
            "in training loop, epoch 3, step 878, the loss is 245514.5\n",
            "in training loop, epoch 3, step 879, the loss is 149171.875\n",
            "in training loop, epoch 3, step 880, the loss is 125120.703125\n",
            "in training loop, epoch 3, step 881, the loss is 135146.15625\n",
            "in training loop, epoch 3, step 882, the loss is 181701.09375\n",
            "in training loop, epoch 3, step 883, the loss is 134357.0\n",
            "in training loop, epoch 3, step 884, the loss is 171957.953125\n",
            "in training loop, epoch 3, step 885, the loss is 132847.0625\n",
            "in training loop, epoch 3, step 886, the loss is 170711.84375\n",
            "in training loop, epoch 3, step 887, the loss is 99435.515625\n",
            "in training loop, epoch 3, step 888, the loss is 221391.109375\n",
            "in training loop, epoch 3, step 889, the loss is 148472.09375\n",
            "in training loop, epoch 3, step 890, the loss is 182726.5\n",
            "in training loop, epoch 3, step 891, the loss is 135188.0625\n",
            "in training loop, epoch 3, step 892, the loss is 132655.21875\n",
            "in training loop, epoch 3, step 893, the loss is 123046.0859375\n",
            "in training loop, epoch 3, step 894, the loss is 175340.90625\n",
            "in training loop, epoch 3, step 895, the loss is 157772.375\n",
            "in training loop, epoch 3, step 896, the loss is 133266.828125\n",
            "in training loop, epoch 3, step 897, the loss is 103455.609375\n",
            "in training loop, epoch 3, step 898, the loss is 191627.5\n",
            "in training loop, epoch 3, step 899, the loss is 153594.421875\n",
            "in training loop, epoch 3, step 900, the loss is 99987.5234375\n",
            "in training loop, epoch 3, step 901, the loss is 168430.703125\n",
            "in training loop, epoch 3, step 902, the loss is 110388.0234375\n",
            "in training loop, epoch 3, step 903, the loss is 43847.91796875\n",
            "k-fold 1:: Epoch 3: train loss 169912.49776600525 val loss 229784.8323793317\n",
            "in training loop, epoch 4, step 0, the loss is 99536.9609375\n",
            "in training loop, epoch 4, step 1, the loss is 50138.10546875\n",
            "in training loop, epoch 4, step 2, the loss is 145876.75\n",
            "in training loop, epoch 4, step 3, the loss is 98848.40625\n",
            "in training loop, epoch 4, step 4, the loss is 75019.71875\n",
            "in training loop, epoch 4, step 5, the loss is 274934.1875\n",
            "in training loop, epoch 4, step 6, the loss is 121563.0078125\n",
            "in training loop, epoch 4, step 7, the loss is 112336.484375\n",
            "in training loop, epoch 4, step 8, the loss is 95734.2421875\n",
            "in training loop, epoch 4, step 9, the loss is 102913.796875\n",
            "in training loop, epoch 4, step 10, the loss is 122506.96875\n",
            "in training loop, epoch 4, step 11, the loss is 87873.71875\n",
            "in training loop, epoch 4, step 12, the loss is 98928.6640625\n",
            "in training loop, epoch 4, step 13, the loss is 166170.0\n",
            "in training loop, epoch 4, step 14, the loss is 231095.171875\n",
            "in training loop, epoch 4, step 15, the loss is 83926.34375\n",
            "in training loop, epoch 4, step 16, the loss is 80764.953125\n",
            "in training loop, epoch 4, step 17, the loss is 142059.140625\n",
            "in training loop, epoch 4, step 18, the loss is 88387.7578125\n",
            "in training loop, epoch 4, step 19, the loss is 104176.140625\n",
            "in training loop, epoch 4, step 20, the loss is 151979.4375\n",
            "in training loop, epoch 4, step 21, the loss is 139499.984375\n",
            "in training loop, epoch 4, step 22, the loss is 91801.2734375\n",
            "in training loop, epoch 4, step 23, the loss is 126972.140625\n",
            "in training loop, epoch 4, step 24, the loss is 103641.7890625\n",
            "in training loop, epoch 4, step 25, the loss is 90129.9453125\n",
            "in training loop, epoch 4, step 26, the loss is 107281.0390625\n",
            "in training loop, epoch 4, step 27, the loss is 82262.984375\n",
            "in training loop, epoch 4, step 28, the loss is 113848.46875\n",
            "in training loop, epoch 4, step 29, the loss is 152173.03125\n",
            "in training loop, epoch 4, step 30, the loss is 123928.78125\n",
            "in training loop, epoch 4, step 31, the loss is 129640.7109375\n",
            "in training loop, epoch 4, step 32, the loss is 179635.265625\n",
            "in training loop, epoch 4, step 33, the loss is 136026.25\n",
            "in training loop, epoch 4, step 34, the loss is 169547.234375\n",
            "in training loop, epoch 4, step 35, the loss is 115330.234375\n",
            "in training loop, epoch 4, step 36, the loss is 100539.9609375\n",
            "in training loop, epoch 4, step 37, the loss is 108233.71875\n",
            "in training loop, epoch 4, step 38, the loss is 102202.8125\n",
            "in training loop, epoch 4, step 39, the loss is 148947.125\n",
            "in training loop, epoch 4, step 40, the loss is 121883.6328125\n",
            "in training loop, epoch 4, step 41, the loss is 130522.3671875\n",
            "in training loop, epoch 4, step 42, the loss is 104424.921875\n",
            "in training loop, epoch 4, step 43, the loss is 177622.25\n",
            "in training loop, epoch 4, step 44, the loss is 128981.8828125\n",
            "in training loop, epoch 4, step 45, the loss is 120223.625\n",
            "in training loop, epoch 4, step 46, the loss is 195089.578125\n",
            "in training loop, epoch 4, step 47, the loss is 132586.71875\n",
            "in training loop, epoch 4, step 48, the loss is 120656.453125\n",
            "in training loop, epoch 4, step 49, the loss is 72033.0078125\n",
            "in training loop, epoch 4, step 50, the loss is 158266.84375\n",
            "in training loop, epoch 4, step 51, the loss is 73791.765625\n",
            "in training loop, epoch 4, step 52, the loss is 84732.9609375\n",
            "in training loop, epoch 4, step 53, the loss is 72784.484375\n",
            "in training loop, epoch 4, step 54, the loss is 107444.328125\n",
            "in training loop, epoch 4, step 55, the loss is 148239.28125\n",
            "in training loop, epoch 4, step 56, the loss is 81966.3203125\n",
            "in training loop, epoch 4, step 57, the loss is 156332.453125\n",
            "in training loop, epoch 4, step 58, the loss is 95995.2265625\n",
            "in training loop, epoch 4, step 59, the loss is 105312.265625\n",
            "in training loop, epoch 4, step 60, the loss is 151248.6875\n",
            "in training loop, epoch 4, step 61, the loss is 79336.4765625\n",
            "in training loop, epoch 4, step 62, the loss is 73712.140625\n",
            "in training loop, epoch 4, step 63, the loss is 100581.703125\n",
            "in training loop, epoch 4, step 64, the loss is 101491.8828125\n",
            "in training loop, epoch 4, step 65, the loss is 203034.0\n",
            "in training loop, epoch 4, step 66, the loss is 90835.59375\n",
            "in training loop, epoch 4, step 67, the loss is 81677.9609375\n",
            "in training loop, epoch 4, step 68, the loss is 68327.8046875\n",
            "in training loop, epoch 4, step 69, the loss is 112873.8515625\n",
            "in training loop, epoch 4, step 70, the loss is 134839.828125\n",
            "in training loop, epoch 4, step 71, the loss is 76258.5546875\n",
            "in training loop, epoch 4, step 72, the loss is 97753.4140625\n",
            "in training loop, epoch 4, step 73, the loss is 101809.8046875\n",
            "in training loop, epoch 4, step 74, the loss is 101390.8125\n",
            "in training loop, epoch 4, step 75, the loss is 90771.2265625\n",
            "in training loop, epoch 4, step 76, the loss is 111740.3515625\n",
            "in training loop, epoch 4, step 77, the loss is 104036.796875\n",
            "in training loop, epoch 4, step 78, the loss is 113425.828125\n",
            "in training loop, epoch 4, step 79, the loss is 90634.1875\n",
            "in training loop, epoch 4, step 80, the loss is 190372.0\n",
            "in training loop, epoch 4, step 81, the loss is 126451.59375\n",
            "in training loop, epoch 4, step 82, the loss is 129570.8671875\n",
            "in training loop, epoch 4, step 83, the loss is 86725.09375\n",
            "in training loop, epoch 4, step 84, the loss is 89685.1796875\n",
            "in training loop, epoch 4, step 85, the loss is 186932.453125\n",
            "in training loop, epoch 4, step 86, the loss is 116867.515625\n",
            "in training loop, epoch 4, step 87, the loss is 97127.5859375\n",
            "in training loop, epoch 4, step 88, the loss is 84603.6796875\n",
            "in training loop, epoch 4, step 89, the loss is 146034.953125\n",
            "in training loop, epoch 4, step 90, the loss is 87336.1796875\n",
            "in training loop, epoch 4, step 91, the loss is 154533.09375\n",
            "in training loop, epoch 4, step 92, the loss is 82883.6953125\n",
            "in training loop, epoch 4, step 93, the loss is 117912.125\n",
            "in training loop, epoch 4, step 94, the loss is 99777.203125\n",
            "in training loop, epoch 4, step 95, the loss is 91192.7890625\n",
            "in training loop, epoch 4, step 96, the loss is 80220.5\n",
            "in training loop, epoch 4, step 97, the loss is 115212.453125\n",
            "in training loop, epoch 4, step 98, the loss is 163384.140625\n",
            "in training loop, epoch 4, step 99, the loss is 83244.9921875\n",
            "in training loop, epoch 4, step 100, the loss is 99033.2578125\n",
            "in training loop, epoch 4, step 101, the loss is 149299.453125\n",
            "in training loop, epoch 4, step 102, the loss is 105130.59375\n",
            "in training loop, epoch 4, step 103, the loss is 93693.6171875\n",
            "in training loop, epoch 4, step 104, the loss is 92244.375\n",
            "in training loop, epoch 4, step 105, the loss is 173185.296875\n",
            "in training loop, epoch 4, step 106, the loss is 100837.3046875\n",
            "in training loop, epoch 4, step 107, the loss is 89292.078125\n",
            "in training loop, epoch 4, step 108, the loss is 98451.1015625\n",
            "in training loop, epoch 4, step 109, the loss is 82199.3828125\n",
            "in training loop, epoch 4, step 110, the loss is 93751.578125\n",
            "in training loop, epoch 4, step 111, the loss is 132623.28125\n",
            "in training loop, epoch 4, step 112, the loss is 117690.03125\n",
            "in training loop, epoch 4, step 113, the loss is 81993.234375\n",
            "in training loop, epoch 4, step 114, the loss is 80144.3046875\n",
            "in training loop, epoch 4, step 115, the loss is 139172.0\n",
            "in training loop, epoch 4, step 116, the loss is 111936.7890625\n",
            "in training loop, epoch 4, step 117, the loss is 289333.5\n",
            "in training loop, epoch 4, step 118, the loss is 129742.0\n",
            "in training loop, epoch 4, step 119, the loss is 99868.5078125\n",
            "in training loop, epoch 4, step 120, the loss is 174843.203125\n",
            "in training loop, epoch 4, step 121, the loss is 138205.609375\n",
            "in training loop, epoch 4, step 122, the loss is 148400.9375\n",
            "in training loop, epoch 4, step 123, the loss is 109840.3203125\n",
            "in training loop, epoch 4, step 124, the loss is 155051.4375\n",
            "in training loop, epoch 4, step 125, the loss is 113759.8671875\n",
            "in training loop, epoch 4, step 126, the loss is 121563.8828125\n",
            "in training loop, epoch 4, step 127, the loss is 151066.46875\n",
            "in training loop, epoch 4, step 128, the loss is 116509.125\n",
            "in training loop, epoch 4, step 129, the loss is 134339.328125\n",
            "in training loop, epoch 4, step 130, the loss is 162543.78125\n",
            "in training loop, epoch 4, step 131, the loss is 134745.0\n",
            "in training loop, epoch 4, step 132, the loss is 109483.6640625\n",
            "in training loop, epoch 4, step 133, the loss is 197942.90625\n",
            "in training loop, epoch 4, step 134, the loss is 92928.484375\n",
            "in training loop, epoch 4, step 135, the loss is 140526.890625\n",
            "in training loop, epoch 4, step 136, the loss is 106917.65625\n",
            "in training loop, epoch 4, step 137, the loss is 119997.1171875\n",
            "in training loop, epoch 4, step 138, the loss is 176547.328125\n",
            "in training loop, epoch 4, step 139, the loss is 114472.90625\n",
            "in training loop, epoch 4, step 140, the loss is 100214.3828125\n",
            "in training loop, epoch 4, step 141, the loss is 125773.4140625\n",
            "in training loop, epoch 4, step 142, the loss is 108658.953125\n",
            "in training loop, epoch 4, step 143, the loss is 112758.5859375\n",
            "in training loop, epoch 4, step 144, the loss is 79276.1875\n",
            "in training loop, epoch 4, step 145, the loss is 101714.0078125\n",
            "in training loop, epoch 4, step 146, the loss is 128931.6875\n",
            "in training loop, epoch 4, step 147, the loss is 145959.5\n",
            "in training loop, epoch 4, step 148, the loss is 122974.90625\n",
            "in training loop, epoch 4, step 149, the loss is 101577.984375\n",
            "in training loop, epoch 4, step 150, the loss is 102072.828125\n",
            "in training loop, epoch 4, step 151, the loss is 113892.5703125\n",
            "in training loop, epoch 4, step 152, the loss is 86571.5234375\n",
            "in training loop, epoch 4, step 153, the loss is 116235.90625\n",
            "in training loop, epoch 4, step 154, the loss is 84737.5234375\n",
            "in training loop, epoch 4, step 155, the loss is 101124.71875\n",
            "in training loop, epoch 4, step 156, the loss is 109780.609375\n",
            "in training loop, epoch 4, step 157, the loss is 109489.59375\n",
            "in training loop, epoch 4, step 158, the loss is 153508.21875\n",
            "in training loop, epoch 4, step 159, the loss is 79082.0078125\n",
            "in training loop, epoch 4, step 160, the loss is 77623.046875\n",
            "in training loop, epoch 4, step 161, the loss is 149568.9375\n",
            "in training loop, epoch 4, step 162, the loss is 118168.3125\n",
            "in training loop, epoch 4, step 163, the loss is 91665.6640625\n",
            "in training loop, epoch 4, step 164, the loss is 91265.125\n",
            "in training loop, epoch 4, step 165, the loss is 115401.15625\n",
            "in training loop, epoch 4, step 166, the loss is 171658.75\n",
            "in training loop, epoch 4, step 167, the loss is 82517.921875\n",
            "in training loop, epoch 4, step 168, the loss is 106345.578125\n",
            "in training loop, epoch 4, step 169, the loss is 104992.609375\n",
            "in training loop, epoch 4, step 170, the loss is 150174.328125\n",
            "in training loop, epoch 4, step 171, the loss is 142478.0625\n",
            "in training loop, epoch 4, step 172, the loss is 99088.703125\n",
            "in training loop, epoch 4, step 173, the loss is 150350.515625\n",
            "in training loop, epoch 4, step 174, the loss is 214894.9375\n",
            "in training loop, epoch 4, step 175, the loss is 147192.0625\n",
            "in training loop, epoch 4, step 176, the loss is 143429.75\n",
            "in training loop, epoch 4, step 177, the loss is 341995.125\n",
            "in training loop, epoch 4, step 178, the loss is 191643.484375\n",
            "in training loop, epoch 4, step 179, the loss is 253911.703125\n",
            "in training loop, epoch 4, step 180, the loss is 93870.53125\n",
            "in training loop, epoch 4, step 181, the loss is 137348.390625\n",
            "in training loop, epoch 4, step 182, the loss is 139826.0625\n",
            "in training loop, epoch 4, step 183, the loss is 129101.703125\n",
            "in training loop, epoch 4, step 184, the loss is 198861.96875\n",
            "in training loop, epoch 4, step 185, the loss is 124392.9375\n",
            "in training loop, epoch 4, step 186, the loss is 121806.4296875\n",
            "in training loop, epoch 4, step 187, the loss is 106589.390625\n",
            "in training loop, epoch 4, step 188, the loss is 104229.71875\n",
            "in training loop, epoch 4, step 189, the loss is 136946.171875\n",
            "in training loop, epoch 4, step 190, the loss is 114832.875\n",
            "in training loop, epoch 4, step 191, the loss is 121519.4921875\n",
            "in training loop, epoch 4, step 192, the loss is 103321.4296875\n",
            "in training loop, epoch 4, step 193, the loss is 100970.953125\n",
            "in training loop, epoch 4, step 194, the loss is 121048.8203125\n",
            "in training loop, epoch 4, step 195, the loss is 143571.25\n",
            "in training loop, epoch 4, step 196, the loss is 104105.375\n",
            "in training loop, epoch 4, step 197, the loss is 95481.6875\n",
            "in training loop, epoch 4, step 198, the loss is 140024.6875\n",
            "in training loop, epoch 4, step 199, the loss is 85366.8671875\n",
            "in training loop, epoch 4, step 200, the loss is 91266.265625\n",
            "in training loop, epoch 4, step 201, the loss is 91755.703125\n",
            "in training loop, epoch 4, step 202, the loss is 121065.796875\n",
            "in training loop, epoch 4, step 203, the loss is 73252.984375\n",
            "in training loop, epoch 4, step 204, the loss is 138009.546875\n",
            "in training loop, epoch 4, step 205, the loss is 117136.6875\n",
            "in training loop, epoch 4, step 206, the loss is 114914.5859375\n",
            "in training loop, epoch 4, step 207, the loss is 83886.4375\n",
            "in training loop, epoch 4, step 208, the loss is 92325.828125\n",
            "in training loop, epoch 4, step 209, the loss is 89383.046875\n",
            "in training loop, epoch 4, step 210, the loss is 142457.234375\n",
            "in training loop, epoch 4, step 211, the loss is 85561.421875\n",
            "in training loop, epoch 4, step 212, the loss is 105293.96875\n",
            "in training loop, epoch 4, step 213, the loss is 83962.890625\n",
            "in training loop, epoch 4, step 214, the loss is 104932.921875\n",
            "in training loop, epoch 4, step 215, the loss is 102997.0\n",
            "in training loop, epoch 4, step 216, the loss is 172516.671875\n",
            "in training loop, epoch 4, step 217, the loss is 67028.3984375\n",
            "in training loop, epoch 4, step 218, the loss is 102135.671875\n",
            "in training loop, epoch 4, step 219, the loss is 97703.515625\n",
            "in training loop, epoch 4, step 220, the loss is 159017.59375\n",
            "in training loop, epoch 4, step 221, the loss is 126694.28125\n",
            "in training loop, epoch 4, step 222, the loss is 128346.5703125\n",
            "in training loop, epoch 4, step 223, the loss is 120532.1953125\n",
            "in training loop, epoch 4, step 224, the loss is 131440.15625\n",
            "in training loop, epoch 4, step 225, the loss is 145716.90625\n",
            "in training loop, epoch 4, step 226, the loss is 108984.953125\n",
            "in training loop, epoch 4, step 227, the loss is 70619.359375\n",
            "in training loop, epoch 4, step 228, the loss is 119592.5\n",
            "in training loop, epoch 4, step 229, the loss is 162406.65625\n",
            "in training loop, epoch 4, step 230, the loss is 118953.046875\n",
            "in training loop, epoch 4, step 231, the loss is 123341.3125\n",
            "in training loop, epoch 4, step 232, the loss is 115712.46875\n",
            "in training loop, epoch 4, step 233, the loss is 113307.0\n",
            "in training loop, epoch 4, step 234, the loss is 110165.125\n",
            "in training loop, epoch 4, step 235, the loss is 98470.171875\n",
            "in training loop, epoch 4, step 236, the loss is 141444.28125\n",
            "in training loop, epoch 4, step 237, the loss is 146642.5\n",
            "in training loop, epoch 4, step 238, the loss is 134535.0625\n",
            "in training loop, epoch 4, step 239, the loss is 93198.828125\n",
            "in training loop, epoch 4, step 240, the loss is 112189.4375\n",
            "in training loop, epoch 4, step 241, the loss is 90584.875\n",
            "in training loop, epoch 4, step 242, the loss is 108025.1171875\n",
            "in training loop, epoch 4, step 243, the loss is 234608.4375\n",
            "in training loop, epoch 4, step 244, the loss is 84930.8046875\n",
            "in training loop, epoch 4, step 245, the loss is 102175.890625\n",
            "in training loop, epoch 4, step 246, the loss is 99593.734375\n",
            "in training loop, epoch 4, step 247, the loss is 105380.40625\n",
            "in training loop, epoch 4, step 248, the loss is 152308.140625\n",
            "in training loop, epoch 4, step 249, the loss is 127344.78125\n",
            "in training loop, epoch 4, step 250, the loss is 116460.4765625\n",
            "in training loop, epoch 4, step 251, the loss is 77861.546875\n",
            "in training loop, epoch 4, step 252, the loss is 103873.875\n",
            "in training loop, epoch 4, step 253, the loss is 146721.5\n",
            "in training loop, epoch 4, step 254, the loss is 91214.3125\n",
            "in training loop, epoch 4, step 255, the loss is 130185.8984375\n",
            "in training loop, epoch 4, step 256, the loss is 129849.9453125\n",
            "in training loop, epoch 4, step 257, the loss is 120738.984375\n",
            "in training loop, epoch 4, step 258, the loss is 111613.640625\n",
            "in training loop, epoch 4, step 259, the loss is 120087.9375\n",
            "in training loop, epoch 4, step 260, the loss is 94636.140625\n",
            "in training loop, epoch 4, step 261, the loss is 97119.1015625\n",
            "in training loop, epoch 4, step 262, the loss is 176964.28125\n",
            "in training loop, epoch 4, step 263, the loss is 113602.96875\n",
            "in training loop, epoch 4, step 264, the loss is 117703.6171875\n",
            "in training loop, epoch 4, step 265, the loss is 104054.484375\n",
            "in training loop, epoch 4, step 266, the loss is 115940.71875\n",
            "in training loop, epoch 4, step 267, the loss is 114143.5234375\n",
            "in training loop, epoch 4, step 268, the loss is 113766.453125\n",
            "in training loop, epoch 4, step 269, the loss is 114248.0625\n",
            "in training loop, epoch 4, step 270, the loss is 130075.3671875\n",
            "in training loop, epoch 4, step 271, the loss is 111217.90625\n",
            "in training loop, epoch 4, step 272, the loss is 168581.5625\n",
            "in training loop, epoch 4, step 273, the loss is 74495.0078125\n",
            "in training loop, epoch 4, step 274, the loss is 110253.390625\n",
            "in training loop, epoch 4, step 275, the loss is 105046.625\n",
            "in training loop, epoch 4, step 276, the loss is 148576.109375\n",
            "in training loop, epoch 4, step 277, the loss is 128137.78125\n",
            "in training loop, epoch 4, step 278, the loss is 102162.34375\n",
            "in training loop, epoch 4, step 279, the loss is 172693.9375\n",
            "in training loop, epoch 4, step 280, the loss is 83354.2265625\n",
            "in training loop, epoch 4, step 281, the loss is 128925.984375\n",
            "in training loop, epoch 4, step 282, the loss is 98853.421875\n",
            "in training loop, epoch 4, step 283, the loss is 155510.203125\n",
            "in training loop, epoch 4, step 284, the loss is 96732.2578125\n",
            "in training loop, epoch 4, step 285, the loss is 156228.6875\n",
            "in training loop, epoch 4, step 286, the loss is 114890.734375\n",
            "in training loop, epoch 4, step 287, the loss is 101559.4921875\n",
            "in training loop, epoch 4, step 288, the loss is 114620.671875\n",
            "in training loop, epoch 4, step 289, the loss is 91605.9296875\n",
            "in training loop, epoch 4, step 290, the loss is 139346.34375\n",
            "in training loop, epoch 4, step 291, the loss is 91492.140625\n",
            "in training loop, epoch 4, step 292, the loss is 137089.953125\n",
            "in training loop, epoch 4, step 293, the loss is 91248.359375\n",
            "in training loop, epoch 4, step 294, the loss is 192168.53125\n",
            "in training loop, epoch 4, step 295, the loss is 101512.421875\n",
            "in training loop, epoch 4, step 296, the loss is 121856.1796875\n",
            "in training loop, epoch 4, step 297, the loss is 111252.3515625\n",
            "in training loop, epoch 4, step 298, the loss is 107514.390625\n",
            "in training loop, epoch 4, step 299, the loss is 127855.765625\n",
            "in training loop, epoch 4, step 300, the loss is 93806.7578125\n",
            "in training loop, epoch 4, step 301, the loss is 100857.8828125\n",
            "in training loop, epoch 4, step 302, the loss is 76732.640625\n",
            "in training loop, epoch 4, step 303, the loss is 138507.78125\n",
            "in training loop, epoch 4, step 304, the loss is 71009.3671875\n",
            "in training loop, epoch 4, step 305, the loss is 228107.5\n",
            "in training loop, epoch 4, step 306, the loss is 117682.828125\n",
            "in training loop, epoch 4, step 307, the loss is 75584.28125\n",
            "in training loop, epoch 4, step 308, the loss is 169166.90625\n",
            "in training loop, epoch 4, step 309, the loss is 104651.5625\n",
            "in training loop, epoch 4, step 310, the loss is 89321.71875\n",
            "in training loop, epoch 4, step 311, the loss is 120626.9453125\n",
            "in training loop, epoch 4, step 312, the loss is 132122.125\n",
            "in training loop, epoch 4, step 313, the loss is 152180.859375\n",
            "in training loop, epoch 4, step 314, the loss is 179472.59375\n",
            "in training loop, epoch 4, step 315, the loss is 117939.640625\n",
            "in training loop, epoch 4, step 316, the loss is 119764.7578125\n",
            "in training loop, epoch 4, step 317, the loss is 92699.0703125\n",
            "in training loop, epoch 4, step 318, the loss is 148212.484375\n",
            "in training loop, epoch 4, step 319, the loss is 107379.140625\n",
            "in training loop, epoch 4, step 320, the loss is 97907.203125\n",
            "in training loop, epoch 4, step 321, the loss is 169668.328125\n",
            "in training loop, epoch 4, step 322, the loss is 79108.5390625\n",
            "in training loop, epoch 4, step 323, the loss is 101021.640625\n",
            "in training loop, epoch 4, step 324, the loss is 92813.359375\n",
            "in training loop, epoch 4, step 325, the loss is 83673.984375\n",
            "in training loop, epoch 4, step 326, the loss is 112237.1640625\n",
            "in training loop, epoch 4, step 327, the loss is 106890.390625\n",
            "in training loop, epoch 4, step 328, the loss is 117860.4375\n",
            "in training loop, epoch 4, step 329, the loss is 129519.0234375\n",
            "in training loop, epoch 4, step 330, the loss is 98846.8515625\n",
            "in training loop, epoch 4, step 331, the loss is 103106.28125\n",
            "in training loop, epoch 4, step 332, the loss is 126266.28125\n",
            "in training loop, epoch 4, step 333, the loss is 150034.453125\n",
            "in training loop, epoch 4, step 334, the loss is 109882.6796875\n",
            "in training loop, epoch 4, step 335, the loss is 140767.0\n",
            "in training loop, epoch 4, step 336, the loss is 108858.265625\n",
            "in training loop, epoch 4, step 337, the loss is 107840.109375\n",
            "in training loop, epoch 4, step 338, the loss is 89110.53125\n",
            "in training loop, epoch 4, step 339, the loss is 139797.21875\n",
            "in training loop, epoch 4, step 340, the loss is 110250.7890625\n",
            "in training loop, epoch 4, step 341, the loss is 109984.96875\n",
            "in training loop, epoch 4, step 342, the loss is 110694.6953125\n",
            "in training loop, epoch 4, step 343, the loss is 127726.3046875\n",
            "in training loop, epoch 4, step 344, the loss is 98940.9296875\n",
            "in training loop, epoch 4, step 345, the loss is 72102.8984375\n",
            "in training loop, epoch 4, step 346, the loss is 131750.21875\n",
            "in training loop, epoch 4, step 347, the loss is 123600.015625\n",
            "in training loop, epoch 4, step 348, the loss is 139937.59375\n",
            "in training loop, epoch 4, step 349, the loss is 130218.8046875\n",
            "in training loop, epoch 4, step 350, the loss is 140995.25\n",
            "in training loop, epoch 4, step 351, the loss is 99962.875\n",
            "in training loop, epoch 4, step 352, the loss is 90131.3125\n",
            "in training loop, epoch 4, step 353, the loss is 81599.09375\n",
            "in training loop, epoch 4, step 354, the loss is 99403.59375\n",
            "in training loop, epoch 4, step 355, the loss is 89872.65625\n",
            "in training loop, epoch 4, step 356, the loss is 104265.625\n",
            "in training loop, epoch 4, step 357, the loss is 86274.2109375\n",
            "in training loop, epoch 4, step 358, the loss is 124605.7265625\n",
            "in training loop, epoch 4, step 359, the loss is 108629.8515625\n",
            "in training loop, epoch 4, step 360, the loss is 296455.90625\n",
            "in training loop, epoch 4, step 361, the loss is 120550.0546875\n",
            "in training loop, epoch 4, step 362, the loss is 143123.390625\n",
            "in training loop, epoch 4, step 363, the loss is 115411.9375\n",
            "in training loop, epoch 4, step 364, the loss is 91849.1015625\n",
            "in training loop, epoch 4, step 365, the loss is 162758.1875\n",
            "in training loop, epoch 4, step 366, the loss is 107394.7578125\n",
            "in training loop, epoch 4, step 367, the loss is 143656.296875\n",
            "in training loop, epoch 4, step 368, the loss is 143877.46875\n",
            "in training loop, epoch 4, step 369, the loss is 88964.8671875\n",
            "in training loop, epoch 4, step 370, the loss is 172162.40625\n",
            "in training loop, epoch 4, step 371, the loss is 104055.109375\n",
            "in training loop, epoch 4, step 372, the loss is 104755.359375\n",
            "in training loop, epoch 4, step 373, the loss is 250633.765625\n",
            "in training loop, epoch 4, step 374, the loss is 124624.40625\n",
            "in training loop, epoch 4, step 375, the loss is 141236.484375\n",
            "in training loop, epoch 4, step 376, the loss is 145910.984375\n",
            "in training loop, epoch 4, step 377, the loss is 247221.453125\n",
            "in training loop, epoch 4, step 378, the loss is 146501.046875\n",
            "in training loop, epoch 4, step 379, the loss is 112120.234375\n",
            "in training loop, epoch 4, step 380, the loss is 182636.0\n",
            "in training loop, epoch 4, step 381, the loss is 98874.6328125\n",
            "in training loop, epoch 4, step 382, the loss is 195884.296875\n",
            "in training loop, epoch 4, step 383, the loss is 126139.734375\n",
            "in training loop, epoch 4, step 384, the loss is 167040.8125\n",
            "in training loop, epoch 4, step 385, the loss is 146110.484375\n",
            "in training loop, epoch 4, step 386, the loss is 184439.1875\n",
            "in training loop, epoch 4, step 387, the loss is 108144.5390625\n",
            "in training loop, epoch 4, step 388, the loss is 134933.578125\n",
            "in training loop, epoch 4, step 389, the loss is 125473.671875\n",
            "in training loop, epoch 4, step 390, the loss is 266873.1875\n",
            "in training loop, epoch 4, step 391, the loss is 115906.3984375\n",
            "in training loop, epoch 4, step 392, the loss is 110167.96875\n",
            "in training loop, epoch 4, step 393, the loss is 166444.359375\n",
            "in training loop, epoch 4, step 394, the loss is 133668.71875\n",
            "in training loop, epoch 4, step 395, the loss is 102451.2265625\n",
            "in training loop, epoch 4, step 396, the loss is 98263.390625\n",
            "in training loop, epoch 4, step 397, the loss is 145204.25\n",
            "in training loop, epoch 4, step 398, the loss is 103597.3671875\n",
            "in training loop, epoch 4, step 399, the loss is 127837.890625\n",
            "in training loop, epoch 4, step 400, the loss is 157581.25\n",
            "in training loop, epoch 4, step 401, the loss is 119284.90625\n",
            "in training loop, epoch 4, step 402, the loss is 118004.734375\n",
            "in training loop, epoch 4, step 403, the loss is 111588.0\n",
            "in training loop, epoch 4, step 404, the loss is 115787.375\n",
            "in training loop, epoch 4, step 405, the loss is 98274.609375\n",
            "in training loop, epoch 4, step 406, the loss is 118806.9296875\n",
            "in training loop, epoch 4, step 407, the loss is 115237.234375\n",
            "in training loop, epoch 4, step 408, the loss is 177954.75\n",
            "in training loop, epoch 4, step 409, the loss is 116374.0\n",
            "in training loop, epoch 4, step 410, the loss is 120195.0078125\n",
            "in training loop, epoch 4, step 411, the loss is 140938.28125\n",
            "in training loop, epoch 4, step 412, the loss is 223874.8125\n",
            "in training loop, epoch 4, step 413, the loss is 143019.125\n",
            "in training loop, epoch 4, step 414, the loss is 182358.296875\n",
            "in training loop, epoch 4, step 415, the loss is 111808.5390625\n",
            "in training loop, epoch 4, step 416, the loss is 183695.96875\n",
            "in training loop, epoch 4, step 417, the loss is 95745.703125\n",
            "in training loop, epoch 4, step 418, the loss is 172918.421875\n",
            "in training loop, epoch 4, step 419, the loss is 113060.546875\n",
            "in training loop, epoch 4, step 420, the loss is 118760.6796875\n",
            "in training loop, epoch 4, step 421, the loss is 163687.890625\n",
            "in training loop, epoch 4, step 422, the loss is 114079.734375\n",
            "in training loop, epoch 4, step 423, the loss is 143403.0625\n",
            "in training loop, epoch 4, step 424, the loss is 113571.640625\n",
            "in training loop, epoch 4, step 425, the loss is 119303.90625\n",
            "in training loop, epoch 4, step 426, the loss is 148547.453125\n",
            "in training loop, epoch 4, step 427, the loss is 85734.296875\n",
            "in training loop, epoch 4, step 428, the loss is 198961.921875\n",
            "in training loop, epoch 4, step 429, the loss is 95539.6875\n",
            "in training loop, epoch 4, step 430, the loss is 120969.1484375\n",
            "in training loop, epoch 4, step 431, the loss is 102186.2265625\n",
            "in training loop, epoch 4, step 432, the loss is 165279.546875\n",
            "in training loop, epoch 4, step 433, the loss is 98749.703125\n",
            "in training loop, epoch 4, step 434, the loss is 113568.53125\n",
            "in training loop, epoch 4, step 435, the loss is 97759.6015625\n",
            "in training loop, epoch 4, step 436, the loss is 111418.015625\n",
            "in training loop, epoch 4, step 437, the loss is 166078.90625\n",
            "in training loop, epoch 4, step 438, the loss is 138358.71875\n",
            "in training loop, epoch 4, step 439, the loss is 161039.015625\n",
            "in training loop, epoch 4, step 440, the loss is 109065.375\n",
            "in training loop, epoch 4, step 441, the loss is 108177.375\n",
            "in training loop, epoch 4, step 442, the loss is 107024.0546875\n",
            "in training loop, epoch 4, step 443, the loss is 129827.21875\n",
            "in training loop, epoch 4, step 444, the loss is 179896.8125\n",
            "in training loop, epoch 4, step 445, the loss is 98276.3203125\n",
            "in training loop, epoch 4, step 446, the loss is 147116.09375\n",
            "in training loop, epoch 4, step 447, the loss is 109231.734375\n",
            "in training loop, epoch 4, step 448, the loss is 108896.9609375\n",
            "in training loop, epoch 4, step 449, the loss is 105775.5390625\n",
            "in training loop, epoch 4, step 450, the loss is 114236.984375\n",
            "in training loop, epoch 4, step 451, the loss is 110740.078125\n",
            "in training loop, epoch 4, step 452, the loss is 148853.53125\n",
            "in training loop, epoch 4, step 453, the loss is 116327.7890625\n",
            "in training loop, epoch 4, step 454, the loss is 112174.671875\n",
            "in training loop, epoch 4, step 455, the loss is 136141.25\n",
            "in training loop, epoch 4, step 456, the loss is 138974.703125\n",
            "in training loop, epoch 4, step 457, the loss is 221215.84375\n",
            "in training loop, epoch 4, step 458, the loss is 168351.890625\n",
            "in training loop, epoch 4, step 459, the loss is 162337.296875\n",
            "in training loop, epoch 4, step 460, the loss is 146538.359375\n",
            "in training loop, epoch 4, step 461, the loss is 151674.21875\n",
            "in training loop, epoch 4, step 462, the loss is 256740.84375\n",
            "in training loop, epoch 4, step 463, the loss is 104062.734375\n",
            "in training loop, epoch 4, step 464, the loss is 104427.9921875\n",
            "in training loop, epoch 4, step 465, the loss is 119508.453125\n",
            "in training loop, epoch 4, step 466, the loss is 253278.59375\n",
            "in training loop, epoch 4, step 467, the loss is 172381.1875\n",
            "in training loop, epoch 4, step 468, the loss is 119566.9609375\n",
            "in training loop, epoch 4, step 469, the loss is 105290.875\n",
            "in training loop, epoch 4, step 470, the loss is 167333.765625\n",
            "in training loop, epoch 4, step 471, the loss is 99031.0390625\n",
            "in training loop, epoch 4, step 472, the loss is 121068.2890625\n",
            "in training loop, epoch 4, step 473, the loss is 184928.796875\n",
            "in training loop, epoch 4, step 474, the loss is 251044.0625\n",
            "in training loop, epoch 4, step 475, the loss is 106346.671875\n",
            "in training loop, epoch 4, step 476, the loss is 146129.25\n",
            "in training loop, epoch 4, step 477, the loss is 104878.484375\n",
            "in training loop, epoch 4, step 478, the loss is 114462.4453125\n",
            "in training loop, epoch 4, step 479, the loss is 233702.5\n",
            "in training loop, epoch 4, step 480, the loss is 113896.1953125\n",
            "in training loop, epoch 4, step 481, the loss is 86713.2578125\n",
            "in training loop, epoch 4, step 482, the loss is 136527.203125\n",
            "in training loop, epoch 4, step 483, the loss is 104052.2734375\n",
            "in training loop, epoch 4, step 484, the loss is 131128.265625\n",
            "in training loop, epoch 4, step 485, the loss is 165580.421875\n",
            "in training loop, epoch 4, step 486, the loss is 172131.5\n",
            "in training loop, epoch 4, step 487, the loss is 100755.921875\n",
            "in training loop, epoch 4, step 488, the loss is 151075.15625\n",
            "in training loop, epoch 4, step 489, the loss is 126855.515625\n",
            "in training loop, epoch 4, step 490, the loss is 206050.84375\n",
            "in training loop, epoch 4, step 491, the loss is 159978.90625\n",
            "in training loop, epoch 4, step 492, the loss is 114528.359375\n",
            "in training loop, epoch 4, step 493, the loss is 106018.4296875\n",
            "in training loop, epoch 4, step 494, the loss is 99735.625\n",
            "in training loop, epoch 4, step 495, the loss is 127395.109375\n",
            "in training loop, epoch 4, step 496, the loss is 219729.796875\n",
            "in training loop, epoch 4, step 497, the loss is 141605.59375\n",
            "in training loop, epoch 4, step 498, the loss is 110786.0\n",
            "in training loop, epoch 4, step 499, the loss is 124769.4765625\n",
            "in training loop, epoch 4, step 500, the loss is 162782.640625\n",
            "in training loop, epoch 4, step 501, the loss is 149116.71875\n",
            "in training loop, epoch 4, step 502, the loss is 91373.2734375\n",
            "in training loop, epoch 4, step 503, the loss is 108670.984375\n",
            "in training loop, epoch 4, step 504, the loss is 128713.6640625\n",
            "in training loop, epoch 4, step 505, the loss is 188856.53125\n",
            "in training loop, epoch 4, step 506, the loss is 138884.796875\n",
            "in training loop, epoch 4, step 507, the loss is 117186.953125\n",
            "in training loop, epoch 4, step 508, the loss is 119001.1484375\n",
            "in training loop, epoch 4, step 509, the loss is 171718.375\n",
            "in training loop, epoch 4, step 510, the loss is 195004.78125\n",
            "in training loop, epoch 4, step 511, the loss is 128684.8359375\n",
            "in training loop, epoch 4, step 512, the loss is 97628.9765625\n",
            "in training loop, epoch 4, step 513, the loss is 134974.375\n",
            "in training loop, epoch 4, step 514, the loss is 163151.59375\n",
            "in training loop, epoch 4, step 515, the loss is 140426.828125\n",
            "in training loop, epoch 4, step 516, the loss is 114706.171875\n",
            "in training loop, epoch 4, step 517, the loss is 149749.9375\n",
            "in training loop, epoch 4, step 518, the loss is 121191.265625\n",
            "in training loop, epoch 4, step 519, the loss is 97791.859375\n",
            "in training loop, epoch 4, step 520, the loss is 101562.71875\n",
            "in training loop, epoch 4, step 521, the loss is 97876.4765625\n",
            "in training loop, epoch 4, step 522, the loss is 88921.3046875\n",
            "in training loop, epoch 4, step 523, the loss is 96715.171875\n",
            "in training loop, epoch 4, step 524, the loss is 90038.1796875\n",
            "in training loop, epoch 4, step 525, the loss is 96283.828125\n",
            "in training loop, epoch 4, step 526, the loss is 94973.8828125\n",
            "in training loop, epoch 4, step 527, the loss is 78549.734375\n",
            "in training loop, epoch 4, step 528, the loss is 113318.578125\n",
            "in training loop, epoch 4, step 529, the loss is 127881.1953125\n",
            "in training loop, epoch 4, step 530, the loss is 91080.5625\n",
            "in training loop, epoch 4, step 531, the loss is 94193.8515625\n",
            "in training loop, epoch 4, step 532, the loss is 78702.875\n",
            "in training loop, epoch 4, step 533, the loss is 84973.3359375\n",
            "in training loop, epoch 4, step 534, the loss is 102493.15625\n",
            "in training loop, epoch 4, step 535, the loss is 103500.0\n",
            "in training loop, epoch 4, step 536, the loss is 177451.234375\n",
            "in training loop, epoch 4, step 537, the loss is 82023.015625\n",
            "in training loop, epoch 4, step 538, the loss is 111553.1875\n",
            "in training loop, epoch 4, step 539, the loss is 69606.2734375\n",
            "in training loop, epoch 4, step 540, the loss is 86168.4609375\n",
            "in training loop, epoch 4, step 541, the loss is 93720.9765625\n",
            "in training loop, epoch 4, step 542, the loss is 102158.078125\n",
            "in training loop, epoch 4, step 543, the loss is 116342.0\n",
            "in training loop, epoch 4, step 544, the loss is 98147.8359375\n",
            "in training loop, epoch 4, step 545, the loss is 108470.6640625\n",
            "in training loop, epoch 4, step 546, the loss is 107252.421875\n",
            "in training loop, epoch 4, step 547, the loss is 136647.578125\n",
            "in training loop, epoch 4, step 548, the loss is 123420.6796875\n",
            "in training loop, epoch 4, step 549, the loss is 126431.9453125\n",
            "in training loop, epoch 4, step 550, the loss is 108296.6796875\n",
            "in training loop, epoch 4, step 551, the loss is 103352.9765625\n",
            "in training loop, epoch 4, step 552, the loss is 171514.0\n",
            "in training loop, epoch 4, step 553, the loss is 124602.84375\n",
            "in training loop, epoch 4, step 554, the loss is 120241.21875\n",
            "in training loop, epoch 4, step 555, the loss is 130443.40625\n",
            "in training loop, epoch 4, step 556, the loss is 182220.875\n",
            "in training loop, epoch 4, step 557, the loss is 139075.4375\n",
            "in training loop, epoch 4, step 558, the loss is 139622.28125\n",
            "in training loop, epoch 4, step 559, the loss is 92831.4140625\n",
            "in training loop, epoch 4, step 560, the loss is 140330.765625\n",
            "in training loop, epoch 4, step 561, the loss is 97325.7578125\n",
            "in training loop, epoch 4, step 562, the loss is 134200.375\n",
            "in training loop, epoch 4, step 563, the loss is 86913.75\n",
            "in training loop, epoch 4, step 564, the loss is 119140.546875\n",
            "in training loop, epoch 4, step 565, the loss is 145092.40625\n",
            "in training loop, epoch 4, step 566, the loss is 119430.2890625\n",
            "in training loop, epoch 4, step 567, the loss is 102630.109375\n",
            "in training loop, epoch 4, step 568, the loss is 115825.4765625\n",
            "in training loop, epoch 4, step 569, the loss is 109019.5\n",
            "in training loop, epoch 4, step 570, the loss is 131022.5390625\n",
            "in training loop, epoch 4, step 571, the loss is 95833.9140625\n",
            "in training loop, epoch 4, step 572, the loss is 122309.421875\n",
            "in training loop, epoch 4, step 573, the loss is 130647.25\n",
            "in training loop, epoch 4, step 574, the loss is 96914.671875\n",
            "in training loop, epoch 4, step 575, the loss is 84312.4609375\n",
            "in training loop, epoch 4, step 576, the loss is 106206.921875\n",
            "in training loop, epoch 4, step 577, the loss is 88362.140625\n",
            "in training loop, epoch 4, step 578, the loss is 113240.2109375\n",
            "in training loop, epoch 4, step 579, the loss is 123915.7109375\n",
            "in training loop, epoch 4, step 580, the loss is 95979.828125\n",
            "in training loop, epoch 4, step 581, the loss is 177443.34375\n",
            "in training loop, epoch 4, step 582, the loss is 75875.796875\n",
            "in training loop, epoch 4, step 583, the loss is 161451.171875\n",
            "in training loop, epoch 4, step 584, the loss is 121745.484375\n",
            "in training loop, epoch 4, step 585, the loss is 82434.0625\n",
            "in training loop, epoch 4, step 586, the loss is 93947.15625\n",
            "in training loop, epoch 4, step 587, the loss is 95963.5078125\n",
            "in training loop, epoch 4, step 588, the loss is 81167.4140625\n",
            "in training loop, epoch 4, step 589, the loss is 85396.4609375\n",
            "in training loop, epoch 4, step 590, the loss is 133114.765625\n",
            "in training loop, epoch 4, step 591, the loss is 132334.984375\n",
            "in training loop, epoch 4, step 592, the loss is 143862.34375\n",
            "in training loop, epoch 4, step 593, the loss is 181924.4375\n",
            "in training loop, epoch 4, step 594, the loss is 136579.046875\n",
            "in training loop, epoch 4, step 595, the loss is 177390.46875\n",
            "in training loop, epoch 4, step 596, the loss is 82222.9375\n",
            "in training loop, epoch 4, step 597, the loss is 96617.1953125\n",
            "in training loop, epoch 4, step 598, the loss is 119577.0234375\n",
            "in training loop, epoch 4, step 599, the loss is 138445.046875\n",
            "in training loop, epoch 4, step 600, the loss is 103835.6015625\n",
            "in training loop, epoch 4, step 601, the loss is 94322.2890625\n",
            "in training loop, epoch 4, step 602, the loss is 121150.6796875\n",
            "in training loop, epoch 4, step 603, the loss is 164801.203125\n",
            "in training loop, epoch 4, step 604, the loss is 82965.4765625\n",
            "in training loop, epoch 4, step 605, the loss is 93911.015625\n",
            "in training loop, epoch 4, step 606, the loss is 110420.0\n",
            "in training loop, epoch 4, step 607, the loss is 99313.3984375\n",
            "in training loop, epoch 4, step 608, the loss is 80877.5625\n",
            "in training loop, epoch 4, step 609, the loss is 107122.1484375\n",
            "in training loop, epoch 4, step 610, the loss is 127066.390625\n",
            "in training loop, epoch 4, step 611, the loss is 117297.8359375\n",
            "in training loop, epoch 4, step 612, the loss is 141451.8125\n",
            "in training loop, epoch 4, step 613, the loss is 98317.3203125\n",
            "in training loop, epoch 4, step 614, the loss is 162264.0\n",
            "in training loop, epoch 4, step 615, the loss is 110132.2109375\n",
            "in training loop, epoch 4, step 616, the loss is 131321.359375\n",
            "in training loop, epoch 4, step 617, the loss is 114183.4140625\n",
            "in training loop, epoch 4, step 618, the loss is 168430.796875\n",
            "in training loop, epoch 4, step 619, the loss is 106840.5859375\n",
            "in training loop, epoch 4, step 620, the loss is 149802.546875\n",
            "in training loop, epoch 4, step 621, the loss is 99286.59375\n",
            "in training loop, epoch 4, step 622, the loss is 119307.59375\n",
            "in training loop, epoch 4, step 623, the loss is 162468.25\n",
            "in training loop, epoch 4, step 624, the loss is 96006.4921875\n",
            "in training loop, epoch 4, step 625, the loss is 94863.28125\n",
            "in training loop, epoch 4, step 626, the loss is 133568.25\n",
            "in training loop, epoch 4, step 627, the loss is 137130.375\n",
            "in training loop, epoch 4, step 628, the loss is 110484.0078125\n",
            "in training loop, epoch 4, step 629, the loss is 80416.65625\n",
            "in training loop, epoch 4, step 630, the loss is 105750.65625\n",
            "in training loop, epoch 4, step 631, the loss is 108193.34375\n",
            "in training loop, epoch 4, step 632, the loss is 96093.078125\n",
            "in training loop, epoch 4, step 633, the loss is 144638.5\n",
            "in training loop, epoch 4, step 634, the loss is 135262.328125\n",
            "in training loop, epoch 4, step 635, the loss is 87042.3828125\n",
            "in training loop, epoch 4, step 636, the loss is 89392.09375\n",
            "in training loop, epoch 4, step 637, the loss is 117971.125\n",
            "in training loop, epoch 4, step 638, the loss is 112926.578125\n",
            "in training loop, epoch 4, step 639, the loss is 127636.8359375\n",
            "in training loop, epoch 4, step 640, the loss is 173803.53125\n",
            "in training loop, epoch 4, step 641, the loss is 136774.6875\n",
            "in training loop, epoch 4, step 642, the loss is 96298.0859375\n",
            "in training loop, epoch 4, step 643, the loss is 87863.65625\n",
            "in training loop, epoch 4, step 644, the loss is 109143.96875\n",
            "in training loop, epoch 4, step 645, the loss is 81888.46875\n",
            "in training loop, epoch 4, step 646, the loss is 72744.0859375\n",
            "in training loop, epoch 4, step 647, the loss is 139868.15625\n",
            "in training loop, epoch 4, step 648, the loss is 109614.8671875\n",
            "in training loop, epoch 4, step 649, the loss is 136363.84375\n",
            "in training loop, epoch 4, step 650, the loss is 122394.984375\n",
            "in training loop, epoch 4, step 651, the loss is 92450.796875\n",
            "in training loop, epoch 4, step 652, the loss is 132404.359375\n",
            "in training loop, epoch 4, step 653, the loss is 80407.5625\n",
            "in training loop, epoch 4, step 654, the loss is 168717.234375\n",
            "in training loop, epoch 4, step 655, the loss is 176232.765625\n",
            "in training loop, epoch 4, step 656, the loss is 110804.84375\n",
            "in training loop, epoch 4, step 657, the loss is 111850.59375\n",
            "in training loop, epoch 4, step 658, the loss is 100162.234375\n",
            "in training loop, epoch 4, step 659, the loss is 90734.7578125\n",
            "in training loop, epoch 4, step 660, the loss is 101154.1171875\n",
            "in training loop, epoch 4, step 661, the loss is 79802.9140625\n",
            "in training loop, epoch 4, step 662, the loss is 125512.796875\n",
            "in training loop, epoch 4, step 663, the loss is 124415.6875\n",
            "in training loop, epoch 4, step 664, the loss is 121034.203125\n",
            "in training loop, epoch 4, step 665, the loss is 113222.0625\n",
            "in training loop, epoch 4, step 666, the loss is 173313.359375\n",
            "in training loop, epoch 4, step 667, the loss is 141358.5625\n",
            "in training loop, epoch 4, step 668, the loss is 110032.3359375\n",
            "in training loop, epoch 4, step 669, the loss is 135401.203125\n",
            "in training loop, epoch 4, step 670, the loss is 136599.71875\n",
            "in training loop, epoch 4, step 671, the loss is 118740.015625\n",
            "in training loop, epoch 4, step 672, the loss is 132261.8125\n",
            "in training loop, epoch 4, step 673, the loss is 80202.5390625\n",
            "in training loop, epoch 4, step 674, the loss is 163821.25\n",
            "in training loop, epoch 4, step 675, the loss is 138716.65625\n",
            "in training loop, epoch 4, step 676, the loss is 126114.375\n",
            "in training loop, epoch 4, step 677, the loss is 89233.609375\n",
            "in training loop, epoch 4, step 678, the loss is 126035.2890625\n",
            "in training loop, epoch 4, step 679, the loss is 115304.140625\n",
            "in training loop, epoch 4, step 680, the loss is 232729.484375\n",
            "in training loop, epoch 4, step 681, the loss is 108492.8359375\n",
            "in training loop, epoch 4, step 682, the loss is 102941.421875\n",
            "in training loop, epoch 4, step 683, the loss is 179573.71875\n",
            "in training loop, epoch 4, step 684, the loss is 119601.828125\n",
            "in training loop, epoch 4, step 685, the loss is 97973.203125\n",
            "in training loop, epoch 4, step 686, the loss is 157970.015625\n",
            "in training loop, epoch 4, step 687, the loss is 152508.546875\n",
            "in training loop, epoch 4, step 688, the loss is 101543.9921875\n",
            "in training loop, epoch 4, step 689, the loss is 163919.6875\n",
            "in training loop, epoch 4, step 690, the loss is 113077.546875\n",
            "in training loop, epoch 4, step 691, the loss is 97037.5\n",
            "in training loop, epoch 4, step 692, the loss is 110408.828125\n",
            "in training loop, epoch 4, step 693, the loss is 80760.8203125\n",
            "in training loop, epoch 4, step 694, the loss is 173998.421875\n",
            "in training loop, epoch 4, step 695, the loss is 105654.671875\n",
            "in training loop, epoch 4, step 696, the loss is 104792.0546875\n",
            "in training loop, epoch 4, step 697, the loss is 98332.4375\n",
            "in training loop, epoch 4, step 698, the loss is 99174.390625\n",
            "in training loop, epoch 4, step 699, the loss is 111301.9609375\n",
            "in training loop, epoch 4, step 700, the loss is 108694.703125\n",
            "in training loop, epoch 4, step 701, the loss is 128847.078125\n",
            "in training loop, epoch 4, step 702, the loss is 244303.3125\n",
            "in training loop, epoch 4, step 703, the loss is 96795.5546875\n",
            "in training loop, epoch 4, step 704, the loss is 174297.609375\n",
            "in training loop, epoch 4, step 705, the loss is 244356.71875\n",
            "in training loop, epoch 4, step 706, the loss is 140673.625\n",
            "in training loop, epoch 4, step 707, the loss is 135565.34375\n",
            "in training loop, epoch 4, step 708, the loss is 126302.3203125\n",
            "in training loop, epoch 4, step 709, the loss is 177423.453125\n",
            "in training loop, epoch 4, step 710, the loss is 86368.2421875\n",
            "in training loop, epoch 4, step 711, the loss is 198420.421875\n",
            "in training loop, epoch 4, step 712, the loss is 228313.390625\n",
            "in training loop, epoch 4, step 713, the loss is 129906.6640625\n",
            "in training loop, epoch 4, step 714, the loss is 117944.3046875\n",
            "in training loop, epoch 4, step 715, the loss is 126384.84375\n",
            "in training loop, epoch 4, step 716, the loss is 136975.453125\n",
            "in training loop, epoch 4, step 717, the loss is 203038.65625\n",
            "in training loop, epoch 4, step 718, the loss is 122924.65625\n",
            "in training loop, epoch 4, step 719, the loss is 115156.09375\n",
            "in training loop, epoch 4, step 720, the loss is 102078.09375\n",
            "in training loop, epoch 4, step 721, the loss is 211157.546875\n",
            "in training loop, epoch 4, step 722, the loss is 109181.015625\n",
            "in training loop, epoch 4, step 723, the loss is 125218.5234375\n",
            "in training loop, epoch 4, step 724, the loss is 181915.546875\n",
            "in training loop, epoch 4, step 725, the loss is 88893.078125\n",
            "in training loop, epoch 4, step 726, the loss is 165417.1875\n",
            "in training loop, epoch 4, step 727, the loss is 178579.4375\n",
            "in training loop, epoch 4, step 728, the loss is 148728.0625\n",
            "in training loop, epoch 4, step 729, the loss is 142141.078125\n",
            "in training loop, epoch 4, step 730, the loss is 161759.890625\n",
            "in training loop, epoch 4, step 731, the loss is 126818.5859375\n",
            "in training loop, epoch 4, step 732, the loss is 213327.671875\n",
            "in training loop, epoch 4, step 733, the loss is 124627.9375\n",
            "in training loop, epoch 4, step 734, the loss is 124847.1875\n",
            "in training loop, epoch 4, step 735, the loss is 148558.5625\n",
            "in training loop, epoch 4, step 736, the loss is 157482.203125\n",
            "in training loop, epoch 4, step 737, the loss is 171240.484375\n",
            "in training loop, epoch 4, step 738, the loss is 134380.796875\n",
            "in training loop, epoch 4, step 739, the loss is 283066.875\n",
            "in training loop, epoch 4, step 740, the loss is 108949.7734375\n",
            "in training loop, epoch 4, step 741, the loss is 189622.28125\n",
            "in training loop, epoch 4, step 742, the loss is 161869.359375\n",
            "in training loop, epoch 4, step 743, the loss is 152768.3125\n",
            "in training loop, epoch 4, step 744, the loss is 189976.703125\n",
            "in training loop, epoch 4, step 745, the loss is 113948.328125\n",
            "in training loop, epoch 4, step 746, the loss is 155088.625\n",
            "in training loop, epoch 4, step 747, the loss is 113213.0859375\n",
            "in training loop, epoch 4, step 748, the loss is 128974.1875\n",
            "in training loop, epoch 4, step 749, the loss is 110463.140625\n",
            "in training loop, epoch 4, step 750, the loss is 112306.8359375\n",
            "in training loop, epoch 4, step 751, the loss is 114403.2265625\n",
            "in training loop, epoch 4, step 752, the loss is 119230.375\n",
            "in training loop, epoch 4, step 753, the loss is 150977.09375\n",
            "in training loop, epoch 4, step 754, the loss is 164154.859375\n",
            "in training loop, epoch 4, step 755, the loss is 136126.234375\n",
            "in training loop, epoch 4, step 756, the loss is 139793.015625\n",
            "in training loop, epoch 4, step 757, the loss is 104149.890625\n",
            "in training loop, epoch 4, step 758, the loss is 161506.734375\n",
            "in training loop, epoch 4, step 759, the loss is 216244.859375\n",
            "in training loop, epoch 4, step 760, the loss is 137573.1875\n",
            "in training loop, epoch 4, step 761, the loss is 160908.1875\n",
            "in training loop, epoch 4, step 762, the loss is 117598.25\n",
            "in training loop, epoch 4, step 763, the loss is 105101.3125\n",
            "in training loop, epoch 4, step 764, the loss is 325885.4375\n",
            "in training loop, epoch 4, step 765, the loss is 209342.359375\n",
            "in training loop, epoch 4, step 766, the loss is 156210.375\n",
            "in training loop, epoch 4, step 767, the loss is 238874.328125\n",
            "in training loop, epoch 4, step 768, the loss is 145105.0625\n",
            "in training loop, epoch 4, step 769, the loss is 154336.71875\n",
            "in training loop, epoch 4, step 770, the loss is 127112.703125\n",
            "in training loop, epoch 4, step 771, the loss is 324360.125\n",
            "in training loop, epoch 4, step 772, the loss is 145259.578125\n",
            "in training loop, epoch 4, step 773, the loss is 165081.40625\n",
            "in training loop, epoch 4, step 774, the loss is 114524.28125\n",
            "in training loop, epoch 4, step 775, the loss is 302667.25\n",
            "in training loop, epoch 4, step 776, the loss is 121908.2109375\n",
            "in training loop, epoch 4, step 777, the loss is 253456.578125\n",
            "in training loop, epoch 4, step 778, the loss is 157327.609375\n",
            "in training loop, epoch 4, step 779, the loss is 115922.5234375\n",
            "in training loop, epoch 4, step 780, the loss is 167148.15625\n",
            "in training loop, epoch 4, step 781, the loss is 184427.625\n",
            "in training loop, epoch 4, step 782, the loss is 124389.5390625\n",
            "in training loop, epoch 4, step 783, the loss is 128004.5546875\n",
            "in training loop, epoch 4, step 784, the loss is 130243.65625\n",
            "in training loop, epoch 4, step 785, the loss is 145237.765625\n",
            "in training loop, epoch 4, step 786, the loss is 151563.96875\n",
            "in training loop, epoch 4, step 787, the loss is 172931.375\n",
            "in training loop, epoch 4, step 788, the loss is 249826.40625\n",
            "in training loop, epoch 4, step 789, the loss is 165020.796875\n",
            "in training loop, epoch 4, step 790, the loss is 149690.265625\n",
            "in training loop, epoch 4, step 791, the loss is 196262.03125\n",
            "in training loop, epoch 4, step 792, the loss is 134422.265625\n",
            "in training loop, epoch 4, step 793, the loss is 104687.2109375\n",
            "in training loop, epoch 4, step 794, the loss is 131572.5625\n",
            "in training loop, epoch 4, step 795, the loss is 236982.5625\n",
            "in training loop, epoch 4, step 796, the loss is 254357.015625\n",
            "in training loop, epoch 4, step 797, the loss is 175214.359375\n",
            "in training loop, epoch 4, step 798, the loss is 106594.515625\n",
            "in training loop, epoch 4, step 799, the loss is 128675.2890625\n",
            "in training loop, epoch 4, step 800, the loss is 215875.203125\n",
            "in training loop, epoch 4, step 801, the loss is 158200.109375\n",
            "in training loop, epoch 4, step 802, the loss is 186794.171875\n",
            "in training loop, epoch 4, step 803, the loss is 118570.53125\n",
            "in training loop, epoch 4, step 804, the loss is 148443.78125\n",
            "in training loop, epoch 4, step 805, the loss is 203471.453125\n",
            "in training loop, epoch 4, step 806, the loss is 160265.703125\n",
            "in training loop, epoch 4, step 807, the loss is 121541.0390625\n",
            "in training loop, epoch 4, step 808, the loss is 168985.734375\n",
            "in training loop, epoch 4, step 809, the loss is 142824.9375\n",
            "in training loop, epoch 4, step 810, the loss is 173336.765625\n",
            "in training loop, epoch 4, step 811, the loss is 155061.5\n",
            "in training loop, epoch 4, step 812, the loss is 159939.171875\n",
            "in training loop, epoch 4, step 813, the loss is 116894.9375\n",
            "in training loop, epoch 4, step 814, the loss is 120007.15625\n",
            "in training loop, epoch 4, step 815, the loss is 143193.515625\n",
            "in training loop, epoch 4, step 816, the loss is 105785.875\n",
            "in training loop, epoch 4, step 817, the loss is 138764.234375\n",
            "in training loop, epoch 4, step 818, the loss is 155189.03125\n",
            "in training loop, epoch 4, step 819, the loss is 111799.0859375\n",
            "in training loop, epoch 4, step 820, the loss is 207802.90625\n",
            "in training loop, epoch 4, step 821, the loss is 125738.15625\n",
            "in training loop, epoch 4, step 822, the loss is 169755.703125\n",
            "in training loop, epoch 4, step 823, the loss is 117373.8515625\n",
            "in training loop, epoch 4, step 824, the loss is 122328.21875\n",
            "in training loop, epoch 4, step 825, the loss is 130552.2734375\n",
            "in training loop, epoch 4, step 826, the loss is 135403.671875\n",
            "in training loop, epoch 4, step 827, the loss is 127827.65625\n",
            "in training loop, epoch 4, step 828, the loss is 99736.265625\n",
            "in training loop, epoch 4, step 829, the loss is 122496.890625\n",
            "in training loop, epoch 4, step 830, the loss is 123867.8046875\n",
            "in training loop, epoch 4, step 831, the loss is 148949.890625\n",
            "in training loop, epoch 4, step 832, the loss is 101902.3046875\n",
            "in training loop, epoch 4, step 833, the loss is 85535.4140625\n",
            "in training loop, epoch 4, step 834, the loss is 98639.0390625\n",
            "in training loop, epoch 4, step 835, the loss is 108522.15625\n",
            "in training loop, epoch 4, step 836, the loss is 126292.1875\n",
            "in training loop, epoch 4, step 837, the loss is 113327.2578125\n",
            "in training loop, epoch 4, step 838, the loss is 117422.4375\n",
            "in training loop, epoch 4, step 839, the loss is 114819.890625\n",
            "in training loop, epoch 4, step 840, the loss is 156287.0\n",
            "in training loop, epoch 4, step 841, the loss is 112116.6015625\n",
            "in training loop, epoch 4, step 842, the loss is 208000.96875\n",
            "in training loop, epoch 4, step 843, the loss is 95842.953125\n",
            "in training loop, epoch 4, step 844, the loss is 102983.328125\n",
            "in training loop, epoch 4, step 845, the loss is 97440.640625\n",
            "in training loop, epoch 4, step 846, the loss is 224927.953125\n",
            "in training loop, epoch 4, step 847, the loss is 128317.71875\n",
            "in training loop, epoch 4, step 848, the loss is 126847.2109375\n",
            "in training loop, epoch 4, step 849, the loss is 138506.109375\n",
            "in training loop, epoch 4, step 850, the loss is 134347.5625\n",
            "in training loop, epoch 4, step 851, the loss is 99976.3046875\n",
            "in training loop, epoch 4, step 852, the loss is 194384.46875\n",
            "in training loop, epoch 4, step 853, the loss is 133467.203125\n",
            "in training loop, epoch 4, step 854, the loss is 73498.359375\n",
            "in training loop, epoch 4, step 855, the loss is 142883.28125\n",
            "in training loop, epoch 4, step 856, the loss is 141601.359375\n",
            "in training loop, epoch 4, step 857, the loss is 153246.0\n",
            "in training loop, epoch 4, step 858, the loss is 106005.3828125\n",
            "in training loop, epoch 4, step 859, the loss is 149650.25\n",
            "in training loop, epoch 4, step 860, the loss is 96330.671875\n",
            "in training loop, epoch 4, step 861, the loss is 141222.625\n",
            "in training loop, epoch 4, step 862, the loss is 193320.796875\n",
            "in training loop, epoch 4, step 863, the loss is 110080.9375\n",
            "in training loop, epoch 4, step 864, the loss is 116947.2578125\n",
            "in training loop, epoch 4, step 865, the loss is 147087.59375\n",
            "in training loop, epoch 4, step 866, the loss is 128098.421875\n",
            "in training loop, epoch 4, step 867, the loss is 98807.328125\n",
            "in training loop, epoch 4, step 868, the loss is 153043.78125\n",
            "in training loop, epoch 4, step 869, the loss is 95707.7734375\n",
            "in training loop, epoch 4, step 870, the loss is 127016.1484375\n",
            "in training loop, epoch 4, step 871, the loss is 128891.1484375\n",
            "in training loop, epoch 4, step 872, the loss is 127498.109375\n",
            "in training loop, epoch 4, step 873, the loss is 163094.546875\n",
            "in training loop, epoch 4, step 874, the loss is 128806.90625\n",
            "in training loop, epoch 4, step 875, the loss is 128995.8671875\n",
            "in training loop, epoch 4, step 876, the loss is 110542.765625\n",
            "in training loop, epoch 4, step 877, the loss is 90059.28125\n",
            "in training loop, epoch 4, step 878, the loss is 159392.5625\n",
            "in training loop, epoch 4, step 879, the loss is 203627.09375\n",
            "in training loop, epoch 4, step 880, the loss is 168936.078125\n",
            "in training loop, epoch 4, step 881, the loss is 118070.0078125\n",
            "in training loop, epoch 4, step 882, the loss is 138441.953125\n",
            "in training loop, epoch 4, step 883, the loss is 134562.4375\n",
            "in training loop, epoch 4, step 884, the loss is 138953.75\n",
            "in training loop, epoch 4, step 885, the loss is 142682.390625\n",
            "in training loop, epoch 4, step 886, the loss is 189060.09375\n",
            "in training loop, epoch 4, step 887, the loss is 270729.28125\n",
            "in training loop, epoch 4, step 888, the loss is 140083.953125\n",
            "in training loop, epoch 4, step 889, the loss is 172620.234375\n",
            "in training loop, epoch 4, step 890, the loss is 127003.4375\n",
            "in training loop, epoch 4, step 891, the loss is 181220.03125\n",
            "in training loop, epoch 4, step 892, the loss is 306314.15625\n",
            "in training loop, epoch 4, step 893, the loss is 204238.140625\n",
            "in training loop, epoch 4, step 894, the loss is 134605.078125\n",
            "in training loop, epoch 4, step 895, the loss is 103200.75\n",
            "in training loop, epoch 4, step 896, the loss is 216038.796875\n",
            "in training loop, epoch 4, step 897, the loss is 142492.0625\n",
            "in training loop, epoch 4, step 898, the loss is 192815.546875\n",
            "in training loop, epoch 4, step 899, the loss is 137449.6875\n",
            "in training loop, epoch 4, step 900, the loss is 131850.96875\n",
            "in training loop, epoch 4, step 901, the loss is 172962.09375\n",
            "in training loop, epoch 4, step 902, the loss is 145527.90625\n",
            "in training loop, epoch 4, step 903, the loss is 113700.1953125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "k-fold 1:: Epoch 4: train loss 129092.6284093266 val loss 281211.21782178216\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-91b102baf4ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainingWrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mtrain_loss_mean_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0mtest_loss_mean_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0mval_loss_mean_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cross_val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3I63RgNmk73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab6e1185-4025-42a1-81bc-e7620ce9d844"
      },
      "source": [
        "train_loss= []\n",
        "train_losses1=[1,2,3,4]\n",
        "train_losses2=[3,4,9,8]\n",
        "train_loss_list=[]\n",
        "history= dict(train=[])\n",
        "\n",
        "train_loss=np.mean(train_losses1)\n",
        "print(train_loss)\n",
        "history['train'].append(train_loss)\n",
        "print(history)\n",
        "train_loss_list.append(history['train'])\n",
        "print(train_loss_list)\n",
        "train_loss_list = np.mean(train_loss_list,axis=0)\n",
        "print(train_loss_list)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5\n",
            "{'train': [2.5]}\n",
            "[[2.5]]\n",
            "[2.5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZBuytiBCyYc",
        "outputId": "e7ab8d8d-82a2-4da0-f944-1fc6fa61a93c"
      },
      "source": [
        "train_loss= []\n",
        "train_losses1=[3,4,9,8]\n",
        "#train_loss_list=[]\n",
        "#history= dict(train=[])\n",
        "\n",
        "train_loss=np.mean(train_losses1)\n",
        "print(train_loss)\n",
        "history['train'].append(train_loss)\n",
        "print(history)\n",
        "try:\n",
        "  train_loss_list.append(history['train'])\n",
        "except:\n",
        "  np.append(train_loss_list,history['train'])\n",
        "  print(f'train_loss_list exception: {train_loss_list}')\n",
        "\n",
        "print(train_loss_list)\n",
        "train_loss_list = np.mean(train_loss_list,axis=0)\n",
        "print(train_loss_list)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6.0\n",
            "{'train': [2.5, 6.0]}\n",
            "train_loss_list exception: [2.5]\n",
            "[2.5]\n",
            "2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIk79J7SUoYS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}